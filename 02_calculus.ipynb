{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.2: Calculus for Deep Learning\n",
    "\n",
    "Calculus is essential for understanding how neural networks learn. The key insight: **learning = optimization**, and optimization requires derivatives.\n",
    "\n",
    "## Learning Objectives\n",
    "- [ ] Compute partial derivatives of multivariate functions\n",
    "- [ ] Apply the chain rule to composite functions\n",
    "- [ ] Understand gradients as directions of steepest ascent\n",
    "- [ ] Implement gradient descent from scratch\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Derivatives: The Core Concept\n",
    "\n",
    "The **derivative** measures the rate of change of a function:\n",
    "\n",
    "$$f'(x) = \\frac{df}{dx} = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}$$\n",
    "\n",
    "**Intuition**: The derivative tells you how much the output changes when you slightly change the input.\n",
    "\n",
    "### Why This Matters for Deep Learning\n",
    "\n",
    "If `loss = f(weights)`, then the derivative tells us:\n",
    "- **Which direction** to change weights to reduce loss\n",
    "- **How much** each weight affects the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: What Does a Derivative Really Mean?\n\nThe derivative answers a fundamental question: **\"If I wiggle the input a tiny bit, how much does the output wiggle?\"**\n\nThink of it like this:\n- You're turning a dial (input x)\n- A meter responds (output f(x))\n- The derivative tells you: \"For each unit I turn the dial, how many units does the meter move?\"\n\n#### The Formal Definition Unpacked\n\n$$f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}$$\n\n| Component | Meaning |\n|-----------|---------|\n| $f(x + h)$ | Output after nudging input by tiny amount h |\n| $f(x)$ | Original output |\n| $f(x + h) - f(x)$ | Change in output (how much the meter moved) |\n| $h$ | Change in input (how much we turned the dial) |\n| $\\frac{f(x+h) - f(x)}{h}$ | **Rate of change** = output change per unit input change |\n| $\\lim_{h \\to 0}$ | Take h infinitesimally small (instantaneous rate) |\n\n#### Why Do We Care About Rate of Change?\n\n| Context | What the Derivative Tells Us |\n|---------|------------------------------|\n| **Physics** | Velocity = derivative of position. \"How fast am I moving right now?\" |\n| **Economics** | Marginal cost = derivative of total cost. \"Cost of making one more unit?\" |\n| **Machine Learning** | Gradient of loss = derivative of loss w.r.t. weights. \"How does changing this weight affect the error?\" |\n\nIn ML specifically: **Derivatives tell us which direction to adjust weights to reduce error.**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical derivative approximation\n",
    "def numerical_derivative(f, x, h=1e-5):\n",
    "    \"\"\"Approximate derivative using finite differences.\"\"\"\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "# Example: f(x) = x^2\n",
    "f = lambda x: x**2\n",
    "f_prime_analytical = lambda x: 2*x  # We know this\n",
    "\n",
    "x = 3.0\n",
    "print(f\"f(x) = x² at x = {x}\")\n",
    "print(f\"Numerical derivative: {numerical_derivative(f, x):.6f}\")\n",
    "print(f\"Analytical derivative: {f_prime_analytical(x):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Interactive visualization: Tangent lines at multiple points\n# Shows how the derivative (slope) changes across the function\n\ndef plot_multiple_tangents(f, f_prime, x_range, points, title):\n    \"\"\"\n    Plot a function with tangent lines at multiple points.\n    This visualizes how the derivative changes across the function.\n    \"\"\"\n    x = np.linspace(x_range[0], x_range[1], 200)\n    y = f(x)\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Left plot: Function with tangent lines\n    axes[0].plot(x, y, 'b-', linewidth=2, label='f(x)')\n    \n    colors = plt.cm.Reds(np.linspace(0.3, 0.9, len(points)))\n    \n    for x0, color in zip(points, colors):\n        y0 = f(x0)\n        slope = f_prime(x0)\n        \n        # Tangent line: y = f(x0) + f'(x0)(x - x0)\n        x_tangent = np.linspace(x0 - 1.5, x0 + 1.5, 50)\n        y_tangent = y0 + slope * (x_tangent - x0)\n        \n        axes[0].plot(x_tangent, y_tangent, '--', color=color, linewidth=1.5, alpha=0.8)\n        axes[0].scatter([x0], [y0], color=color, s=80, zorder=5)\n        axes[0].annotate(f'slope={slope:.2f}', xy=(x0, y0), xytext=(x0+0.3, y0+0.5),\n                        fontsize=9, color=color)\n    \n    axes[0].set_xlabel('x')\n    axes[0].set_ylabel('f(x)')\n    axes[0].set_title(f'{title}\\nTangent lines show instantaneous slope at each point')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n    axes[0].set_ylim([min(y) - 1, max(y) + 2])\n    \n    # Right plot: The derivative function itself\n    y_prime = f_prime(x)\n    axes[1].plot(x, y_prime, 'r-', linewidth=2, label=\"f'(x) (derivative)\")\n    axes[1].axhline(y=0, color='k', linewidth=0.5)\n    \n    for x0, color in zip(points, colors):\n        slope = f_prime(x0)\n        axes[1].scatter([x0], [slope], color=color, s=80, zorder=5)\n    \n    axes[1].set_xlabel('x')\n    axes[1].set_ylabel(\"f'(x)\")\n    axes[1].set_title(\"The Derivative Function\\nShows the slope at every point\")\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Example 1: f(x) = x^2 (parabola)\nf = lambda x: x**2\nf_prime = lambda x: 2*x\nplot_multiple_tangents(f, f_prime, x_range=(-3, 3), \n                       points=[-2, -1, 0, 1, 2], \n                       title='f(x) = x²')\n\nprint(\"Key observations for f(x) = x²:\")\nprint(\"- At x=0: slope is 0 (bottom of the parabola - minimum!)\")\nprint(\"- Negative x: slope is negative (function decreasing)\")\nprint(\"- Positive x: slope is positive (function increasing)\")\nprint(\"- Slope magnitude increases as we move away from 0\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize derivative as slope of tangent line\n",
    "def plot_tangent(f, f_prime, x0, title):\n",
    "    \"\"\"Plot function with tangent line at x0.\"\"\"\n",
    "    x = np.linspace(x0 - 2, x0 + 2, 100)\n",
    "    y = f(x)\n",
    "    \n",
    "    # Tangent line: y = f(x0) + f'(x0)(x - x0)\n",
    "    slope = f_prime(x0)\n",
    "    tangent = f(x0) + slope * (x - x0)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, y, 'b-', linewidth=2, label='f(x)')\n",
    "    plt.plot(x, tangent, 'r--', linewidth=2, label=f'Tangent (slope = {slope:.2f})')\n",
    "    plt.scatter([x0], [f(x0)], color='red', s=100, zorder=5)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# f(x) = x² at different points\n",
    "f = lambda x: x**2\n",
    "f_prime = lambda x: 2*x\n",
    "\n",
    "plot_tangent(f, f_prime, 1.5, \"f(x) = x² with tangent at x = 1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Example 2: A more complex function - sine wave\nf = lambda x: np.sin(x)\nf_prime = lambda x: np.cos(x)\nplot_multiple_tangents(f, f_prime, x_range=(-2*np.pi, 2*np.pi), \n                       points=[-np.pi, -np.pi/2, 0, np.pi/2, np.pi], \n                       title='f(x) = sin(x)')\n\nprint(\"\\nKey observations for f(x) = sin(x):\")\nprint(\"- At peaks/troughs (x = ±π/2): slope is 0 (maxima/minima)\")\nprint(\"- At zero crossings (x = 0, ±π): slope is ±1 (steepest)\")\nprint(\"- The derivative of sin(x) is cos(x) - shifted by π/2!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Derivatives\n",
    "\n",
    "| Function | Derivative |\n",
    "|----------|------------|\n",
    "| $x^n$ | $nx^{n-1}$ |\n",
    "| $e^x$ | $e^x$ |\n",
    "| $\\ln(x)$ | $1/x$ |\n",
    "| $\\sin(x)$ | $\\cos(x)$ |\n",
    "| $\\cos(x)$ | $-\\sin(x)$ |\n",
    "\n",
    "### Activation Functions and Their Derivatives\n",
    "\n",
    "These are critical for backpropagation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)  # Nice property!\n",
    "\n",
    "# ReLU and its derivative\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# Tanh and its derivative\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "# Plot them all\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "activations = [\n",
    "    (sigmoid, sigmoid_derivative, 'Sigmoid'),\n",
    "    (relu, relu_derivative, 'ReLU'),\n",
    "    (tanh, tanh_derivative, 'Tanh')\n",
    "]\n",
    "\n",
    "for i, (func, deriv, name) in enumerate(activations):\n",
    "    # Function\n",
    "    axes[0, i].plot(x, func(x), 'b-', linewidth=2)\n",
    "    axes[0, i].set_title(f'{name}')\n",
    "    axes[0, i].set_xlabel('x')\n",
    "    axes[0, i].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[0, i].axvline(x=0, color='k', linewidth=0.5)\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Derivative\n",
    "    axes[1, i].plot(x, deriv(x), 'r-', linewidth=2)\n",
    "    axes[1, i].set_title(f'{name} Derivative')\n",
    "    axes[1, i].set_xlabel('x')\n",
    "    axes[1, i].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[1, i].axvline(x=0, color='k', linewidth=0.5)\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observations:\")\n",
    "print(\"- Sigmoid derivative max is 0.25 (causes vanishing gradients)\")\n",
    "print(\"- ReLU derivative is 0 or 1 (no vanishing gradient for positive x)\")\n",
    "print(\"- Tanh derivative max is 1 (better than sigmoid)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Partial Derivatives\n",
    "\n",
    "For functions of multiple variables, **partial derivatives** measure the rate of change with respect to one variable while holding others constant.\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = \\lim_{h \\to 0} \\frac{f(x + h, y) - f(x, y)}{h}$$\n",
    "\n",
    "### Example\n",
    "\n",
    "For $f(x, y) = x^2 + 3xy + y^2$:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = 2x + 3y$$\n",
    "$$\\frac{\\partial f}{\\partial y} = 3x + 2y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: Why Does the Gradient Point \"Uphill\"?\n\nThis is one of the most important insights in optimization. Let's build intuition for WHY the gradient points in the direction of steepest increase.\n\n#### The Gradient as a \"Which Way is Up?\" Detector\n\nImagine you're standing on a hilly surface (the function) and want to find the steepest uphill direction. The gradient is like a compass that always points uphill.\n\n**Mathematical Intuition:**\n\nThe gradient $\\nabla f$ at a point gives you the direction where the function increases **most rapidly**.\n\nThink about it:\n- $\\frac{\\partial f}{\\partial x}$ tells you: \"If I move in the x-direction, how fast does f increase?\"\n- $\\frac{\\partial f}{\\partial y}$ tells you: \"If I move in the y-direction, how fast does f increase?\"\n- The gradient combines these: \"The optimal uphill direction is a blend of these, weighted by how steep each direction is\"\n\n#### Directional Derivatives: Movement in Any Direction\n\nIf you move in direction $\\mathbf{u}$ (a unit vector), the rate of change is:\n\n$$\\frac{\\partial f}{\\partial \\mathbf{u}} = \\nabla f \\cdot \\mathbf{u} = |\\nabla f| \\cos(\\theta)$$\n\nWhere $\\theta$ is the angle between gradient and movement direction.\n\n| Direction relative to gradient | $\\cos(\\theta)$ | Rate of change |\n|-------------------------------|----------------|----------------|\n| Same as gradient ($\\theta = 0$) | 1 | Maximum increase |\n| Perpendicular ($\\theta = 90$) | 0 | No change (contour line) |\n| Opposite ($\\theta = 180$) | -1 | Maximum decrease |\n\n**This is why gradient descent works!** Moving opposite to the gradient gives maximum decrease.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical partial derivatives\n",
    "def partial_derivative(f, point, var_index, h=1e-5):\n",
    "    \"\"\"\n",
    "    Compute partial derivative of f at point with respect to variable var_index.\n",
    "    \n",
    "    Args:\n",
    "        f: Function taking array of variables\n",
    "        point: Array of variable values\n",
    "        var_index: Which variable to differentiate\n",
    "        h: Step size\n",
    "    \"\"\"\n",
    "    point = np.array(point, dtype=float)\n",
    "    point_plus = point.copy()\n",
    "    point_minus = point.copy()\n",
    "    point_plus[var_index] += h\n",
    "    point_minus[var_index] -= h\n",
    "    return (f(point_plus) - f(point_minus)) / (2 * h)\n",
    "\n",
    "# f(x, y) = x² + 3xy + y²\n",
    "def f(p):\n",
    "    x, y = p\n",
    "    return x**2 + 3*x*y + y**2\n",
    "\n",
    "# Analytical partial derivatives\n",
    "def df_dx(x, y):\n",
    "    return 2*x + 3*y\n",
    "\n",
    "def df_dy(x, y):\n",
    "    return 3*x + 2*y\n",
    "\n",
    "# Test at point (2, 3)\n",
    "point = [2, 3]\n",
    "print(f\"At point {point}:\")\n",
    "print(f\"∂f/∂x numerical: {partial_derivative(f, point, 0):.6f}\")\n",
    "print(f\"∂f/∂x analytical: {df_dx(*point):.6f}\")\n",
    "print(f\"∂f/∂y numerical: {partial_derivative(f, point, 1):.6f}\")\n",
    "print(f\"∂f/∂y analytical: {df_dy(*point):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Enhanced gradient field visualization with interactive exploration\n# Shows gradient as arrows pointing uphill, with different movement directions\n\ndef visualize_gradient_directions():\n    \"\"\"\n    Interactive visualization showing:\n    1. Gradient field (arrows pointing uphill)\n    2. How rate of change varies with direction\n    3. Why opposite-to-gradient is the best descent direction\n    \"\"\"\n    \n    fig = plt.figure(figsize=(16, 5))\n    \n    # Define function: f(x,y) = x² + 0.5*y² (elliptical paraboloid)\n    def f(x, y):\n        return x**2 + 0.5*y**2\n    \n    def grad_f(x, y):\n        return np.array([2*x, y])\n    \n    # Create grid\n    x = np.linspace(-3, 3, 100)\n    y = np.linspace(-3, 3, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = f(X, Y)\n    \n    # Plot 1: 3D surface\n    ax1 = fig.add_subplot(131, projection='3d')\n    ax1.plot_surface(X, Y, Z, cmap=cm.viridis, alpha=0.7)\n    ax1.set_xlabel('x')\n    ax1.set_ylabel('y')\n    ax1.set_zlabel('f(x,y)')\n    ax1.set_title('f(x,y) = x² + 0.5y²\\n(Bowl-shaped surface)')\n    \n    # Plot 2: Contour with gradient field\n    ax2 = fig.add_subplot(132)\n    contour = ax2.contour(X, Y, Z, levels=15, cmap=cm.viridis)\n    ax2.clabel(contour, inline=True, fontsize=8)\n    \n    # Sparse grid for gradient arrows\n    x_sparse = np.linspace(-2.5, 2.5, 8)\n    y_sparse = np.linspace(-2.5, 2.5, 8)\n    X_s, Y_s = np.meshgrid(x_sparse, y_sparse)\n    \n    U = 2 * X_s  # ∂f/∂x\n    V = Y_s      # ∂f/∂y\n    \n    # Normalize for visualization\n    mag = np.sqrt(U**2 + V**2) + 1e-10\n    U_norm = U / mag * 0.4\n    V_norm = V / mag * 0.4\n    \n    ax2.quiver(X_s, Y_s, U_norm, V_norm, mag, cmap=cm.Reds, alpha=0.8)\n    ax2.set_xlabel('x')\n    ax2.set_ylabel('y')\n    ax2.set_title('Gradient Field\\nArrows point UPHILL')\n    ax2.set_aspect('equal')\n    ax2.plot([0], [0], 'k*', markersize=15, label='Minimum')\n    ax2.legend()\n    \n    # Plot 3: Directional derivative at a specific point\n    ax3 = fig.add_subplot(133)\n    \n    # Pick a point\n    px, py = 2.0, 1.0\n    grad = grad_f(px, py)\n    grad_mag = np.linalg.norm(grad)\n    \n    # Compute directional derivative for all directions\n    angles = np.linspace(0, 2*np.pi, 100)\n    dir_derivs = []\n    for theta in angles:\n        direction = np.array([np.cos(theta), np.sin(theta)])\n        dir_deriv = np.dot(grad, direction)\n        dir_derivs.append(dir_deriv)\n    \n    # Plot directional derivative vs angle\n    ax3.plot(np.degrees(angles), dir_derivs, 'b-', linewidth=2)\n    ax3.axhline(y=0, color='k', linewidth=0.5)\n    ax3.axhline(y=grad_mag, color='g', linestyle='--', label=f'Max = |grad| = {grad_mag:.2f}')\n    ax3.axhline(y=-grad_mag, color='r', linestyle='--', label=f'Min = -|grad| = {-grad_mag:.2f}')\n    \n    # Mark special directions\n    grad_angle = np.degrees(np.arctan2(grad[1], grad[0]))\n    ax3.axvline(x=grad_angle, color='g', alpha=0.5)\n    ax3.axvline(x=grad_angle + 180, color='r', alpha=0.5)\n    \n    ax3.set_xlabel('Direction (degrees)')\n    ax3.set_ylabel('Rate of change')\n    ax3.set_title(f'Directional Derivative at ({px}, {py})\\nRate of change vs movement direction')\n    ax3.legend(loc='lower right')\n    ax3.set_xticks([0, 90, 180, 270, 360])\n    ax3.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"At point ({px}, {py}):\")\n    print(f\"  Gradient = {grad}\")\n    print(f\"  Gradient magnitude = {grad_mag:.2f}\")\n    print(f\"  Gradient direction = {grad_angle:.1f}°\")\n    print(f\"\\n  To INCREASE f fastest: move at {grad_angle:.1f}° (with the gradient)\")\n    print(f\"  To DECREASE f fastest: move at {grad_angle + 180:.1f}° (against the gradient)\")\n    print(f\"  To stay at same level: move at {grad_angle + 90:.1f}° or {grad_angle - 90:.1f}° (perpendicular)\")\n\nvisualize_gradient_directions()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a 2D function and its partial derivatives\n",
    "def f_2d(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "# Create meshgrid\n",
    "x = np.linspace(-3, 3, 50)\n",
    "y = np.linspace(-3, 3, 50)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f_2d(X, Y)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 3D surface\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.plot_surface(X, Y, Z, cmap=cm.viridis, alpha=0.8)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('f(x,y)')\n",
    "ax1.set_title('f(x,y) = x² + y²')\n",
    "\n",
    "# Contour plot with gradient vectors\n",
    "ax2 = fig.add_subplot(132)\n",
    "contour = ax2.contour(X, Y, Z, levels=15, cmap=cm.viridis)\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Add gradient vectors at some points\n",
    "points = [(-2, -2), (-2, 0), (0, 2), (1, 1), (2, -1)]\n",
    "for px, py in points:\n",
    "    grad_x = 2 * px  # ∂f/∂x = 2x\n",
    "    grad_y = 2 * py  # ∂f/∂y = 2y\n",
    "    ax2.arrow(px, py, grad_x*0.3, grad_y*0.3, head_width=0.15, head_length=0.1, fc='red', ec='red')\n",
    "\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('Contour plot with gradient vectors')\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "# Slice at y=1\n",
    "ax3 = fig.add_subplot(133)\n",
    "y_fixed = 1\n",
    "z_slice = f_2d(x, y_fixed)\n",
    "ax3.plot(x, z_slice, 'b-', linewidth=2)\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('f(x, 1)')\n",
    "ax3.set_title(f'Slice at y = {y_fixed}\\n∂f/∂x = 2x')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. The Gradient\n",
    "\n",
    "The **gradient** is the vector of all partial derivatives:\n",
    "\n",
    "$$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}$$\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "1. **Direction**: Points in the direction of steepest **increase**\n",
    "2. **Magnitude**: Tells how steep that increase is\n",
    "3. **To minimize**: Move in the **opposite** direction (negative gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: Why the Chain Rule is CRITICAL for Deep Learning\n\nThe chain rule is not just another calculus rule - it's the **mathematical heart of backpropagation**. Without it, we couldn't train neural networks.\n\n#### The Core Insight\n\nWhen you have nested functions (f composed with g), changes **propagate** through the chain:\n\n$$\\text{small change in } x \\rightarrow \\text{change in } g(x) \\rightarrow \\text{change in } f(g(x))$$\n\nThe chain rule says: **multiply the rates of change at each step**.\n\n#### Breaking Down the Formula\n\nFor $y = f(g(x))$, let's call $u = g(x)$ (the intermediate value):\n\n$$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$$\n\n| Component | Meaning | In neural network terms |\n|-----------|---------|------------------------|\n| $\\frac{du}{dx}$ | How much does u change when x changes? | \"Local gradient\" of layer |\n| $\\frac{dy}{du}$ | How much does y change when u changes? | \"Upstream gradient\" from later layers |\n| $\\frac{dy}{dx}$ | How much does y change when x changes? | \"Full gradient\" through the network |\n\n#### Visual: How Changes Propagate\n\n```\nInput x                        Output y\n   |                              |\n   v                              v\n   x ----[g]----> u = g(x) ----[f]----> y = f(u)\n   \n   Δx    ->     Δu = (dg/dx)·Δx   ->   Δy = (df/du)·Δu\n                                            = (df/du)·(dg/dx)·Δx\n```\n\nThe change in x gets **amplified (or diminished)** at each step, and the total effect is the product!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Step-by-step example with ACTUAL NUMBERS\n# Let's trace through f(g(x)) = (2x + 1)³ at x = 2\n\nprint(\"=\" * 60)\nprint(\"CHAIN RULE: Step-by-Step with Actual Numbers\")\nprint(\"=\" * 60)\nprint(\"\\nFunction: y = (2x + 1)³\")\nprint(\"This is f(g(x)) where g(x) = 2x + 1 and f(u) = u³\")\nprint(\"\\n\" + \"-\" * 60)\n\nx = 2\nprint(f\"Evaluating at x = {x}\")\n\n# Step 1: Forward pass - compute intermediate and final values\nu = 2*x + 1  # g(x)\ny = u**3      # f(u)\n\nprint(f\"\\n1. FORWARD PASS:\")\nprint(f\"   u = g(x) = 2({x}) + 1 = {u}\")\nprint(f\"   y = f(u) = {u}³ = {y}\")\n\n# Step 2: Compute local derivatives\ndg_dx = 2          # derivative of g(x) = 2x + 1 is 2\ndf_du = 3 * u**2   # derivative of f(u) = u³ is 3u²\n\nprint(f\"\\n2. LOCAL DERIVATIVES (at this point):\")\nprint(f\"   dg/dx = d(2x+1)/dx = 2\")\nprint(f\"   df/du = d(u³)/du = 3u² = 3({u})² = {df_du}\")\n\n# Step 3: Apply chain rule\ndy_dx = df_du * dg_dx\n\nprint(f\"\\n3. CHAIN RULE:\")\nprint(f\"   dy/dx = (df/du) × (dg/dx)\")\nprint(f\"         = {df_du} × {dg_dx}\")\nprint(f\"         = {dy_dx}\")\n\n# Verify with the analytical derivative\n# y = (2x+1)³, so dy/dx = 3(2x+1)² × 2 = 6(2x+1)²\ndy_dx_analytical = 6 * (2*x + 1)**2\nprint(f\"\\n4. VERIFICATION:\")\nprint(f\"   Analytical formula: dy/dx = 6(2x+1)²\")\nprint(f\"   At x = {x}: dy/dx = 6({2*x+1})² = {dy_dx_analytical}\")\nprint(f\"   Match: {dy_dx == dy_dx_analytical} ✓\")\n\n# What does this mean?\nprint(f\"\\n5. INTERPRETATION:\")\nprint(f\"   If we increase x by a tiny amount Δx = 0.001:\")\nprint(f\"   y will increase by approximately {dy_dx} × 0.001 = {dy_dx * 0.001}\")\n\n# Verify numerically\nh = 0.001\ny_original = (2*x + 1)**3\ny_nudged = (2*(x + h) + 1)**3\nactual_change = y_nudged - y_original\nprint(f\"   Actual change: {y_nudged} - {y_original} = {actual_change:.6f}\")\nprint(f\"   Predicted change: {dy_dx * h:.6f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualization: Chain Rule as Signal Propagation\n# Shows how a small change propagates through composed functions\n\ndef visualize_chain_rule_propagation():\n    \"\"\"\n    Visualize how changes propagate through composed functions.\n    f(g(x)) = sin(x²) \n    \"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n    \n    x = np.linspace(-2, 2, 200)\n    \n    # g(x) = x²\n    g = x**2\n    # f(u) = sin(u) where u = g(x)\n    f = np.sin(g)\n    \n    # Plot g(x)\n    axes[0].plot(x, g, 'b-', linewidth=2)\n    axes[0].set_xlabel('x')\n    axes[0].set_ylabel('u = g(x) = x²')\n    axes[0].set_title('Step 1: Inner function\\ng(x) = x²')\n    axes[0].grid(True, alpha=0.3)\n    axes[0].axhline(y=0, color='k', linewidth=0.5)\n    axes[0].axvline(x=0, color='k', linewidth=0.5)\n    \n    # Highlight a point\n    x0 = 1.5\n    u0 = x0**2\n    axes[0].scatter([x0], [u0], color='red', s=100, zorder=5)\n    axes[0].annotate(f'x={x0}\\nu={u0:.2f}', xy=(x0, u0), xytext=(x0+0.3, u0-0.5), fontsize=10)\n    \n    # Plot f(u) = sin(u)\n    u = np.linspace(0, 4, 200)\n    axes[1].plot(u, np.sin(u), 'g-', linewidth=2)\n    axes[1].set_xlabel('u')\n    axes[1].set_ylabel('y = f(u) = sin(u)')\n    axes[1].set_title('Step 2: Outer function\\nf(u) = sin(u)')\n    axes[1].grid(True, alpha=0.3)\n    axes[1].axhline(y=0, color='k', linewidth=0.5)\n    \n    y0 = np.sin(u0)\n    axes[1].scatter([u0], [y0], color='red', s=100, zorder=5)\n    axes[1].annotate(f'u={u0:.2f}\\ny={y0:.2f}', xy=(u0, y0), xytext=(u0+0.3, y0+0.2), fontsize=10)\n    \n    # Plot the composition f(g(x))\n    axes[2].plot(x, f, 'm-', linewidth=2)\n    axes[2].set_xlabel('x')\n    axes[2].set_ylabel('y = f(g(x)) = sin(x²)')\n    axes[2].set_title('Result: Composition\\ny = sin(x²)')\n    axes[2].grid(True, alpha=0.3)\n    axes[2].axhline(y=0, color='k', linewidth=0.5)\n    axes[2].axvline(x=0, color='k', linewidth=0.5)\n    \n    axes[2].scatter([x0], [y0], color='red', s=100, zorder=5)\n    axes[2].annotate(f'x={x0}\\ny={y0:.2f}', xy=(x0, y0), xytext=(x0+0.2, y0+0.3), fontsize=10)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Now show the gradient computation\n    print(\"\\n\" + \"=\" * 60)\n    print(\"CHAIN RULE COMPUTATION for y = sin(x²) at x = 1.5\")\n    print(\"=\" * 60)\n    \n    # At x = 1.5\n    x_val = 1.5\n    u_val = x_val**2\n    y_val = np.sin(u_val)\n    \n    # Local gradients\n    dg_dx = 2 * x_val          # d(x²)/dx = 2x\n    df_du = np.cos(u_val)       # d(sin(u))/du = cos(u)\n    \n    # Chain rule\n    dy_dx = df_du * dg_dx\n    \n    print(f\"\\n1. Forward pass:\")\n    print(f\"   x = {x_val}\")\n    print(f\"   u = x² = {u_val}\")\n    print(f\"   y = sin(u) = {y_val:.4f}\")\n    \n    print(f\"\\n2. Backward pass (computing gradients):\")\n    print(f\"   dy/du = cos(u) = cos({u_val}) = {df_du:.4f}\")\n    print(f\"   du/dx = 2x = 2({x_val}) = {dg_dx}\")\n    \n    print(f\"\\n3. Chain rule:\")\n    print(f\"   dy/dx = (dy/du) × (du/dx)\")\n    print(f\"         = {df_du:.4f} × {dg_dx}\")\n    print(f\"         = {dy_dx:.4f}\")\n    \n    # Verify numerically\n    h = 1e-5\n    numerical_grad = (np.sin((x_val + h)**2) - np.sin((x_val - h)**2)) / (2*h)\n    print(f\"\\n4. Numerical verification: {numerical_grad:.4f}\")\n\nvisualize_chain_rule_propagation()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Computational Graphs: The Key to Backpropagation\n\nA **computational graph** is a visual representation of how a function computes its output. Each node is an operation, and edges show data flow.\n\n**Why does this matter?** Neural networks are just big computational graphs, and backpropagation is just the chain rule applied systematically through the graph!\n\n#### Example: Computing gradients through a simple graph\n\nConsider: $L = (wx + b - y)^2$ (squared error loss)\n\n```\n     w                              \n     |                              \n     v                              \nx -->[ * ]--> z1 -->[ + ]--> z2 -->[ - ]--> z3 -->[ ² ]--> L\n                      ^              ^\n                      |              |\n                      b              y (target)\n```\n\n**Forward pass (left to right):** Compute values at each node\n**Backward pass (right to left):** Compute gradients using chain rule",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Detailed walkthrough of forward and backward pass through computational graph\n# L = (w*x + b - y)²\n\ndef computational_graph_example():\n    \"\"\"\n    Step-by-step forward and backward pass through a computational graph.\n    This is EXACTLY how neural network libraries compute gradients!\n    \"\"\"\n    print(\"=\" * 70)\n    print(\"COMPUTATIONAL GRAPH: L = (w*x + b - y)²\")\n    print(\"=\" * 70)\n    \n    # Input values\n    x = 2.0   # input\n    y = 7.0   # target\n    w = 3.0   # weight\n    b = 1.0   # bias\n    \n    print(f\"\\nInputs: x={x}, y={y}, w={w}, b={b}\")\n    print(\"\\n\" + \"-\" * 70)\n    print(\"FORWARD PASS (compute values left to right)\")\n    print(\"-\" * 70)\n    \n    # Forward pass - compute each node\n    z1 = w * x          # multiplication\n    print(f\"z1 = w * x = {w} * {x} = {z1}\")\n    \n    z2 = z1 + b         # addition\n    print(f\"z2 = z1 + b = {z1} + {b} = {z2}\")\n    \n    z3 = z2 - y         # subtraction (error)\n    print(f\"z3 = z2 - y = {z2} - {y} = {z3}\")\n    \n    L = z3 ** 2         # square (loss)\n    print(f\"L = z3² = {z3}² = {L}\")\n    \n    print(\"\\n\" + \"-\" * 70)\n    print(\"BACKWARD PASS (compute gradients right to left)\")\n    print(\"-\" * 70)\n    print(\"Starting from dL/dL = 1 (gradient of L with respect to itself)\\n\")\n    \n    # Backward pass - apply chain rule at each node\n    \n    # Start with gradient of loss w.r.t. itself\n    dL_dL = 1\n    print(f\"dL/dL = {dL_dL}\")\n    \n    # Node: L = z3² \n    # dL/dz3 = d(z3²)/dz3 = 2*z3\n    dL_dz3 = dL_dL * (2 * z3)\n    print(f\"\\nNode L = z3²:\")\n    print(f\"  Local gradient: d(z3²)/dz3 = 2*z3 = 2*{z3} = {2*z3}\")\n    print(f\"  dL/dz3 = dL/dL × d(z3²)/dz3 = {dL_dL} × {2*z3} = {dL_dz3}\")\n    \n    # Node: z3 = z2 - y\n    # dz3/dz2 = 1, dz3/dy = -1\n    dL_dz2 = dL_dz3 * 1\n    dL_dy = dL_dz3 * (-1)\n    print(f\"\\nNode z3 = z2 - y:\")\n    print(f\"  Local gradients: dz3/dz2 = 1, dz3/dy = -1\")\n    print(f\"  dL/dz2 = dL/dz3 × 1 = {dL_dz3} × 1 = {dL_dz2}\")\n    print(f\"  dL/dy = dL/dz3 × (-1) = {dL_dz3} × (-1) = {dL_dy}\")\n    \n    # Node: z2 = z1 + b\n    # dz2/dz1 = 1, dz2/db = 1\n    dL_dz1 = dL_dz2 * 1\n    dL_db = dL_dz2 * 1\n    print(f\"\\nNode z2 = z1 + b:\")\n    print(f\"  Local gradients: dz2/dz1 = 1, dz2/db = 1\")\n    print(f\"  dL/dz1 = dL/dz2 × 1 = {dL_dz2} × 1 = {dL_dz1}\")\n    print(f\"  dL/db = dL/dz2 × 1 = {dL_dz2} × 1 = {dL_db}\")\n    \n    # Node: z1 = w * x\n    # dz1/dw = x, dz1/dx = w\n    dL_dw = dL_dz1 * x\n    dL_dx = dL_dz1 * w\n    print(f\"\\nNode z1 = w * x:\")\n    print(f\"  Local gradients: dz1/dw = x = {x}, dz1/dx = w = {w}\")\n    print(f\"  dL/dw = dL/dz1 × x = {dL_dz1} × {x} = {dL_dw}\")\n    print(f\"  dL/dx = dL/dz1 × w = {dL_dz1} × {w} = {dL_dx}\")\n    \n    print(\"\\n\" + \"-\" * 70)\n    print(\"SUMMARY OF GRADIENTS\")\n    print(\"-\" * 70)\n    print(f\"dL/dw = {dL_dw}  <- How much changing w affects L\")\n    print(f\"dL/db = {dL_db}  <- How much changing b affects L\")\n    print(f\"dL/dx = {dL_dx}  <- How much changing x affects L\")\n    \n    print(\"\\n\" + \"-\" * 70)\n    print(\"VERIFICATION with numerical gradients\")\n    print(\"-\" * 70)\n    h = 1e-5\n    \n    loss = lambda w, b, x, y: (w*x + b - y)**2\n    \n    dL_dw_num = (loss(w+h, b, x, y) - loss(w-h, b, x, y)) / (2*h)\n    dL_db_num = (loss(w, b+h, x, y) - loss(w, b-h, x, y)) / (2*h)\n    \n    print(f\"dL/dw: analytical = {dL_dw}, numerical = {dL_dw_num:.4f}\")\n    print(f\"dL/db: analytical = {dL_db}, numerical = {dL_db_num:.4f}\")\n    \n    return dL_dw, dL_db\n\ndL_dw, dL_db = computational_graph_example()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: Understanding Gradient Descent\n\nGradient descent is the **optimization engine** of deep learning. Let's build deep intuition.\n\n#### The Core Idea in Plain English\n\nYou're lost on a foggy mountainside and want to reach the lowest valley. What do you do?\n1. **Feel the slope** under your feet (compute gradient)\n2. **Take a step downhill** (move opposite to gradient)\n3. **Repeat** until you reach flat ground (gradient is zero)\n\nThat's gradient descent!\n\n#### The Update Rule Decoded\n\n$$\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\alpha \\nabla L(\\theta)$$\n\n| Component | Meaning | Analogy |\n|-----------|---------|---------|\n| $\\theta$ | Parameters (weights) | Your position on the mountain |\n| $\\nabla L(\\theta)$ | Gradient of loss | Which way is uphill |\n| $-\\nabla L(\\theta)$ | Negative gradient | Which way is downhill |\n| $\\alpha$ | Learning rate | Size of your steps |\n| $\\alpha \\nabla L(\\theta)$ | The actual step | How far you move |\n\n#### The Learning Rate $\\alpha$ is Critical\n\n| Learning rate | What happens | Problem |\n|---------------|--------------|---------|\n| **Too small** | Tiny steps, very slow progress | Takes forever to converge |\n| **Too large** | Big steps, overshoots minimum | Oscillates or diverges |\n| **Just right** | Steady progress, converges | Sweet spot (hard to find!) |\n\nThis is why learning rate scheduling and adaptive optimizers (Adam) are important in practice.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(f, point, h=1e-5):\n",
    "    \"\"\"Compute gradient of f at point using numerical differentiation.\"\"\"\n",
    "    point = np.array(point, dtype=float)\n",
    "    grad = np.zeros_like(point)\n",
    "    for i in range(len(point)):\n",
    "        grad[i] = partial_derivative(f, point, i, h)\n",
    "    return grad\n",
    "\n",
    "# Example: f(x, y) = x² + y²\n",
    "def f(p):\n",
    "    return p[0]**2 + p[1]**2\n",
    "\n",
    "point = np.array([3.0, 4.0])\n",
    "grad = compute_gradient(f, point)\n",
    "\n",
    "print(f\"At point {point}:\")\n",
    "print(f\"f(point) = {f(point)}\")\n",
    "print(f\"Gradient = {grad}\")\n",
    "print(f\"Gradient magnitude = {np.linalg.norm(grad):.4f}\")\n",
    "print(f\"\\nTo decrease f, move in direction: {-grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient field\n",
    "def f_2d(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "# Create grid\n",
    "x = np.linspace(-3, 3, 15)\n",
    "y = np.linspace(-3, 3, 15)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Compute gradient at each point\n",
    "U = 2 * X  # ∂f/∂x\n",
    "V = 2 * Y  # ∂f/∂y\n",
    "\n",
    "# Normalize for better visualization\n",
    "magnitude = np.sqrt(U**2 + V**2)\n",
    "U_norm = U / (magnitude + 1e-10)\n",
    "V_norm = V / (magnitude + 1e-10)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Contour plot\n",
    "x_fine = np.linspace(-3, 3, 100)\n",
    "y_fine = np.linspace(-3, 3, 100)\n",
    "X_fine, Y_fine = np.meshgrid(x_fine, y_fine)\n",
    "Z_fine = f_2d(X_fine, Y_fine)\n",
    "plt.contour(X_fine, Y_fine, Z_fine, levels=15, cmap=cm.viridis, alpha=0.5)\n",
    "\n",
    "# Gradient vectors (pointing uphill)\n",
    "plt.quiver(X, Y, U_norm, V_norm, magnitude, cmap=cm.Reds, alpha=0.8)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Gradient Field of f(x,y) = x² + y²\\nArrows point UPHILL (direction of steepest increase)')\n",
    "plt.colorbar(label='Gradient magnitude')\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Gradients point away from the minimum (origin)\")\n",
    "print(\"To minimize, we follow the NEGATIVE gradient (downhill)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. The Chain Rule\n",
    "\n",
    "The **chain rule** is the foundation of backpropagation. It tells us how to differentiate composite functions.\n",
    "\n",
    "### Single Variable\n",
    "\n",
    "If $y = f(g(x))$, then:\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}$$\n",
    "\n",
    "### Intuition\n",
    "\n",
    "If $x$ changes by a small amount $\\Delta x$:\n",
    "- $g$ changes by $\\frac{dg}{dx} \\cdot \\Delta x$\n",
    "- This causes $f$ to change by $\\frac{df}{dg} \\cdot (\\frac{dg}{dx} \\cdot \\Delta x)$\n",
    "\n",
    "The changes **multiply** through the chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: y = (3x + 2)²\n",
    "# Let g(x) = 3x + 2, f(g) = g²\n",
    "# dy/dx = df/dg * dg/dx = 2g * 3 = 6(3x + 2)\n",
    "\n",
    "def y(x):\n",
    "    return (3*x + 2)**2\n",
    "\n",
    "def dy_dx_analytical(x):\n",
    "    return 6 * (3*x + 2)\n",
    "\n",
    "x = 1.0\n",
    "print(f\"y = (3x + 2)² at x = {x}\")\n",
    "print(f\"y({x}) = {y(x)}\")\n",
    "print(f\"dy/dx numerical: {numerical_derivative(y, x):.6f}\")\n",
    "print(f\"dy/dx analytical (chain rule): {dy_dx_analytical(x):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Comprehensive visualization of gradient descent behavior\n# Shows the path, learning rate effects, and convergence\n\ndef visualize_gradient_descent_comprehensive():\n    \"\"\"\n    Create a comprehensive visualization showing:\n    1. 3D view of the loss surface with descent path\n    2. Top-down view (contour) with path\n    3. Loss over iterations\n    4. Effect of different learning rates\n    \"\"\"\n    \n    # Define a nice loss landscape: f(x,y) = x² + 10*y² (elongated bowl)\n    def f(p):\n        return p[0]**2 + 10*p[1]**2\n    \n    def grad_f(p):\n        return np.array([2*p[0], 20*p[1]])\n    \n    def gradient_descent(start, lr, n_steps):\n        point = np.array(start, dtype=float)\n        history = [point.copy()]\n        for _ in range(n_steps):\n            point = point - lr * grad_f(point)\n            history.append(point.copy())\n        return np.array(history)\n    \n    # Run GD with different learning rates\n    start = [3.0, 1.0]\n    n_steps = 30\n    \n    lr_small = 0.01\n    lr_good = 0.05\n    lr_large = 0.09\n    lr_too_large = 0.11\n    \n    hist_small = gradient_descent(start, lr_small, n_steps)\n    hist_good = gradient_descent(start, lr_good, n_steps)\n    hist_large = gradient_descent(start, lr_large, n_steps)\n    hist_too_large = gradient_descent(start, lr_too_large, n_steps)\n    \n    # Create figure\n    fig = plt.figure(figsize=(16, 10))\n    \n    # Create grid for surface plots\n    x = np.linspace(-4, 4, 100)\n    y = np.linspace(-2, 2, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = X**2 + 10*Y**2\n    \n    # Plot 1: 3D surface with path (good learning rate)\n    ax1 = fig.add_subplot(221, projection='3d')\n    ax1.plot_surface(X, Y, Z, cmap=cm.viridis, alpha=0.6)\n    \n    # Add descent path on surface\n    path_z = [f(p) for p in hist_good]\n    ax1.plot(hist_good[:, 0], hist_good[:, 1], path_z, 'r.-', \n             markersize=8, linewidth=2, label='GD path')\n    ax1.scatter([start[0]], [start[1]], [f(start)], color='green', s=100, marker='o')\n    ax1.scatter([0], [0], [0], color='red', s=100, marker='*')\n    \n    ax1.set_xlabel('x')\n    ax1.set_ylabel('y')\n    ax1.set_zlabel('f(x,y)')\n    ax1.set_title('3D View: Gradient Descent Path\\n(Learning rate = 0.05)')\n    \n    # Plot 2: Contour with all paths\n    ax2 = fig.add_subplot(222)\n    contour = ax2.contour(X, Y, Z, levels=20, cmap=cm.viridis)\n    ax2.clabel(contour, inline=True, fontsize=8)\n    \n    ax2.plot(hist_small[:, 0], hist_small[:, 1], 'b.-', markersize=5, \n             linewidth=1.5, label=f'lr={lr_small} (too small)')\n    ax2.plot(hist_good[:, 0], hist_good[:, 1], 'g.-', markersize=5, \n             linewidth=1.5, label=f'lr={lr_good} (good)')\n    ax2.plot(hist_large[:, 0], hist_large[:, 1], 'orange', marker='.', markersize=5, \n             linewidth=1.5, label=f'lr={lr_large} (large)')\n    ax2.plot(hist_too_large[:, 0], hist_too_large[:, 1], 'r.-', markersize=5, \n             linewidth=1.5, label=f'lr={lr_too_large} (too large)')\n    \n    ax2.scatter([start[0]], [start[1]], color='green', s=150, marker='o', zorder=5, label='Start')\n    ax2.scatter([0], [0], color='red', s=150, marker='*', zorder=5, label='Minimum')\n    \n    ax2.set_xlabel('x')\n    ax2.set_ylabel('y')\n    ax2.set_title('Top View: Different Learning Rates')\n    ax2.legend(loc='upper right', fontsize=9)\n    ax2.set_aspect('equal')\n    \n    # Plot 3: Loss curves\n    ax3 = fig.add_subplot(223)\n    \n    losses_small = [f(p) for p in hist_small]\n    losses_good = [f(p) for p in hist_good]\n    losses_large = [f(p) for p in hist_large]\n    losses_too_large = [f(p) for p in hist_too_large]\n    \n    ax3.plot(losses_small, 'b-', linewidth=2, label=f'lr={lr_small}')\n    ax3.plot(losses_good, 'g-', linewidth=2, label=f'lr={lr_good}')\n    ax3.plot(losses_large, color='orange', linewidth=2, label=f'lr={lr_large}')\n    ax3.plot(losses_too_large, 'r-', linewidth=2, label=f'lr={lr_too_large}')\n    \n    ax3.set_xlabel('Iteration')\n    ax3.set_ylabel('Loss')\n    ax3.set_title('Loss Over Time')\n    ax3.legend()\n    ax3.set_yscale('log')\n    ax3.grid(True, alpha=0.3)\n    \n    # Plot 4: Zoomed in first few steps\n    ax4 = fig.add_subplot(224)\n    \n    # Show step-by-step for good learning rate\n    for i in range(min(8, len(hist_good)-1)):\n        p1 = hist_good[i]\n        p2 = hist_good[i+1]\n        grad = grad_f(p1)\n        \n        # Point\n        ax4.scatter([p1[0]], [p1[1]], color='blue', s=60, zorder=5)\n        ax4.annotate(f'{i}', xy=(p1[0], p1[1]), xytext=(p1[0]+0.1, p1[1]+0.1), fontsize=9)\n        \n        # Gradient (scaled for visualization)\n        ax4.arrow(p1[0], p1[1], -grad[0]*0.02, -grad[1]*0.02, \n                 head_width=0.05, head_length=0.02, fc='red', ec='red', alpha=0.5)\n        \n        # Actual step\n        ax4.arrow(p1[0], p1[1], (p2[0]-p1[0])*0.95, (p2[1]-p1[1])*0.95,\n                 head_width=0.05, head_length=0.02, fc='green', ec='green')\n    \n    contour2 = ax4.contour(X, Y, Z, levels=20, cmap=cm.viridis, alpha=0.5)\n    ax4.set_xlabel('x')\n    ax4.set_ylabel('y')\n    ax4.set_title('Step-by-Step View (lr=0.05)\\nGreen: actual steps, Red: gradient direction')\n    ax4.set_xlim([-1, 4])\n    ax4.set_ylim([-0.5, 1.5])\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"Key observations:\")\n    print(f\"- lr={lr_small}: Very slow convergence, many steps needed\")\n    print(f\"- lr={lr_good}: Good convergence, reaches minimum efficiently\")\n    print(f\"- lr={lr_large}: Oscillates but eventually converges\")\n    print(f\"- lr={lr_too_large}: Oscillates wildly, may diverge\")\n\nvisualize_gradient_descent_comprehensive()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### The Local Minima Problem\n\nReal loss surfaces are rarely simple bowls. They often have:\n- **Local minima**: Points that look like minima locally but aren't the global best\n- **Saddle points**: Points where gradient is zero but it's neither min nor max\n- **Plateaus**: Flat regions where gradient is tiny\n\nNeural networks have extremely complex loss landscapes. Fortunately:\n1. In high dimensions, true local minima are rare (saddle points are more common)\n2. Many local minima have similar loss values\n3. Modern optimizers (Adam, etc.) can escape shallow local minima",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualize local minima, saddle points, and the challenges they pose\n\ndef visualize_local_minima_problem():\n    \"\"\"\n    Visualize a loss landscape with multiple local minima\n    and show how gradient descent can get stuck.\n    \"\"\"\n    \n    # Create a function with multiple local minima\n    # f(x) = sin(x) + 0.1*x² (creates multiple valleys)\n    def f_1d(x):\n        return np.sin(3*x) + 0.1*x**2\n    \n    def df_1d(x):\n        return 3*np.cos(3*x) + 0.2*x\n    \n    # 1D visualization\n    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n    \n    x = np.linspace(-4, 4, 200)\n    y = f_1d(x)\n    \n    axes[0].plot(x, y, 'b-', linewidth=2)\n    axes[0].set_xlabel('x (parameter)')\n    axes[0].set_ylabel('Loss')\n    axes[0].set_title('Loss Landscape with Multiple Minima')\n    axes[0].grid(True, alpha=0.3)\n    \n    # Mark local minima (where derivative crosses zero from - to +)\n    for xi in np.linspace(-4, 4, 1000):\n        if abs(df_1d(xi)) < 0.05 and f_1d(xi-0.01) > f_1d(xi) < f_1d(xi+0.01):\n            axes[0].scatter([xi], [f_1d(xi)], color='red', s=100, marker='v', zorder=5)\n    \n    axes[0].annotate('Global\\nminimum', xy=(-2.1, f_1d(-2.1)), xytext=(-3, 1),\n                    arrowprops=dict(arrowstyle='->', color='green'), fontsize=10, color='green')\n    axes[0].annotate('Local\\nminimum', xy=(0.0, f_1d(0.0)), xytext=(1, 1.5),\n                    arrowprops=dict(arrowstyle='->', color='red'), fontsize=10, color='red')\n    \n    # Run GD from different starting points\n    def gd_1d(x0, lr=0.1, n_steps=50):\n        x = x0\n        history = [x]\n        for _ in range(n_steps):\n            x = x - lr * df_1d(x)\n            history.append(x)\n        return np.array(history)\n    \n    starts = [-3.5, -1.0, 1.5, 3.0]\n    colors = ['green', 'red', 'orange', 'purple']\n    \n    axes[1].plot(x, y, 'b-', linewidth=2, alpha=0.5)\n    for start, color in zip(starts, colors):\n        hist = gd_1d(start, lr=0.05, n_steps=100)\n        y_hist = f_1d(hist)\n        axes[1].plot(hist, y_hist, '.-', color=color, markersize=4, \n                    linewidth=1, label=f'Start at x={start}')\n        axes[1].scatter([start], [f_1d(start)], color=color, s=100, marker='o', zorder=5)\n    \n    axes[1].set_xlabel('x')\n    axes[1].set_ylabel('Loss')\n    axes[1].set_title('GD from Different Starting Points')\n    axes[1].legend(fontsize=9)\n    axes[1].grid(True, alpha=0.3)\n    \n    # Show final loss values\n    final_losses = []\n    for start in starts:\n        hist = gd_1d(start, lr=0.05, n_steps=100)\n        final_losses.append(f_1d(hist[-1]))\n    \n    axes[2].bar(range(len(starts)), final_losses, color=colors)\n    axes[2].set_xticks(range(len(starts)))\n    axes[2].set_xticklabels([f'x={s}' for s in starts])\n    axes[2].set_xlabel('Starting point')\n    axes[2].set_ylabel('Final loss')\n    axes[2].set_title('Final Loss Depends on Start!\\n(Different starting points = different results)')\n    axes[2].grid(True, alpha=0.3, axis='y')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"Key insight: Gradient descent finds LOCAL minima, not necessarily GLOBAL minima.\")\n    print(\"The solution depends on where you start!\")\n    print(\"\\nStrategies to address this:\")\n    print(\"1. Run from multiple random starting points\")\n    print(\"2. Use momentum to escape shallow local minima\")\n    print(\"3. Add noise (stochastic gradient descent)\")\n    print(\"4. Use learning rate schedules\")\n\nvisualize_local_minima_problem()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize saddle points in 2D\n# A saddle point is a critical point that is a minimum in one direction but maximum in another\n\ndef visualize_saddle_point():\n    \"\"\"\n    Visualize a saddle point and why it's problematic for gradient descent.\n    \"\"\"\n    # Classic saddle: f(x,y) = x² - y²\n    def f_saddle(x, y):\n        return x**2 - y**2\n    \n    def grad_saddle(p):\n        return np.array([2*p[0], -2*p[1]])\n    \n    fig = plt.figure(figsize=(16, 5))\n    \n    # Create grid\n    x = np.linspace(-2, 2, 100)\n    y = np.linspace(-2, 2, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = f_saddle(X, Y)\n    \n    # 3D surface\n    ax1 = fig.add_subplot(131, projection='3d')\n    ax1.plot_surface(X, Y, Z, cmap=cm.coolwarm, alpha=0.8)\n    ax1.scatter([0], [0], [0], color='black', s=200, marker='o')\n    ax1.set_xlabel('x')\n    ax1.set_ylabel('y')\n    ax1.set_zlabel('f(x,y)')\n    ax1.set_title('Saddle Point: f(x,y) = x² - y²\\nMinimum in x, Maximum in y')\n    \n    # Contour plot\n    ax2 = fig.add_subplot(132)\n    contour = ax2.contour(X, Y, Z, levels=20, cmap=cm.coolwarm)\n    ax2.clabel(contour, inline=True, fontsize=8)\n    ax2.scatter([0], [0], color='black', s=200, marker='o', label='Saddle point')\n    \n    # Draw gradient arrows around saddle point\n    for px, py in [(0.5, 0), (-0.5, 0), (0, 0.5), (0, -0.5)]:\n        grad = grad_saddle([px, py])\n        ax2.arrow(px, py, -grad[0]*0.15, -grad[1]*0.15, head_width=0.05, \n                 head_length=0.02, fc='green', ec='green')\n    \n    ax2.set_xlabel('x')\n    ax2.set_ylabel('y')\n    ax2.set_title('Contour Plot\\nGreen arrows: negative gradient direction')\n    ax2.legend()\n    ax2.set_aspect('equal')\n    \n    # 1D slices through saddle point\n    ax3 = fig.add_subplot(133)\n    x_slice = np.linspace(-2, 2, 100)\n    ax3.plot(x_slice, x_slice**2, 'b-', linewidth=2, label='Slice at y=0: f = x² (minimum)')\n    ax3.plot(x_slice, -x_slice**2, 'r-', linewidth=2, label='Slice at x=0: f = -y² (maximum)')\n    ax3.axhline(y=0, color='k', linewidth=0.5)\n    ax3.axvline(x=0, color='k', linewidth=0.5)\n    ax3.scatter([0], [0], color='black', s=100, zorder=5)\n    ax3.set_xlabel('x or y')\n    ax3.set_ylabel('f')\n    ax3.set_title('1D Slices Through Saddle\\nSame point is min AND max!')\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"At a saddle point:\")\n    print(\"- Gradient is zero (looks like a minimum/maximum)\")\n    print(\"- But it's a minimum in some directions, maximum in others\")\n    print(\"- GD can get stuck here if approaching from certain directions\")\n    print(\"\\nIn high-dimensional neural network loss landscapes:\")\n    print(\"- Saddle points are MUCH more common than local minima\")\n    print(\"- This is because you need ALL directions to curve upward for a minimum\")\n    print(\"- Momentum helps escape saddle points by building up velocity\")\n\nvisualize_saddle_point()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Rule in Neural Networks\n",
    "\n",
    "Consider a simple network:\n",
    "\n",
    "$$\\text{Input } x \\rightarrow z = wx + b \\rightarrow a = \\sigma(z) \\rightarrow L = (a - y)^2$$\n",
    "\n",
    "To find $\\frac{\\partial L}{\\partial w}$, we apply the chain rule:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computational graph example\n",
    "# Forward pass: x -> z = wx + b -> a = sigmoid(z) -> L = (a - y)²\n",
    "\n",
    "def forward_and_backward(x, y_true, w, b):\n",
    "    \"\"\"Compute forward pass and gradients using chain rule.\"\"\"\n",
    "    \n",
    "    # Forward pass\n",
    "    z = w * x + b\n",
    "    a = sigmoid(z)\n",
    "    L = (a - y_true)**2\n",
    "    \n",
    "    print(\"=== Forward Pass ===\")\n",
    "    print(f\"x = {x}\")\n",
    "    print(f\"z = w*x + b = {w}*{x} + {b} = {z}\")\n",
    "    print(f\"a = sigmoid(z) = {a:.6f}\")\n",
    "    print(f\"L = (a - y)² = ({a:.6f} - {y_true})² = {L:.6f}\")\n",
    "    \n",
    "    # Backward pass (chain rule)\n",
    "    print(\"\\n=== Backward Pass (Chain Rule) ===\")\n",
    "    \n",
    "    # dL/da\n",
    "    dL_da = 2 * (a - y_true)\n",
    "    print(f\"∂L/∂a = 2(a - y) = {dL_da:.6f}\")\n",
    "    \n",
    "    # da/dz (sigmoid derivative)\n",
    "    da_dz = a * (1 - a)\n",
    "    print(f\"∂a/∂z = σ(z)(1 - σ(z)) = {da_dz:.6f}\")\n",
    "    \n",
    "    # dz/dw\n",
    "    dz_dw = x\n",
    "    print(f\"∂z/∂w = x = {dz_dw}\")\n",
    "    \n",
    "    # dz/db\n",
    "    dz_db = 1\n",
    "    print(f\"∂z/∂b = 1\")\n",
    "    \n",
    "    # Chain rule\n",
    "    dL_dz = dL_da * da_dz\n",
    "    dL_dw = dL_dz * dz_dw\n",
    "    dL_db = dL_dz * dz_db\n",
    "    \n",
    "    print(f\"\\n∂L/∂w = ∂L/∂a · ∂a/∂z · ∂z/∂w = {dL_dw:.6f}\")\n",
    "    print(f\"∂L/∂b = ∂L/∂a · ∂a/∂z · ∂z/∂b = {dL_db:.6f}\")\n",
    "    \n",
    "    return L, dL_dw, dL_db\n",
    "\n",
    "# Example\n",
    "x = 2.0\n",
    "y_true = 1.0\n",
    "w = 0.5\n",
    "b = 0.1\n",
    "\n",
    "L, dL_dw, dL_db = forward_and_backward(x, y_true, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify with numerical gradient\n",
    "h = 1e-5\n",
    "\n",
    "def loss(w, b, x=2.0, y=1.0):\n",
    "    z = w * x + b\n",
    "    a = sigmoid(z)\n",
    "    return (a - y)**2\n",
    "\n",
    "# Numerical gradients\n",
    "dL_dw_numerical = (loss(w + h, b) - loss(w - h, b)) / (2 * h)\n",
    "dL_db_numerical = (loss(w, b + h) - loss(w, b - h)) / (2 * h)\n",
    "\n",
    "print(\"Verification with numerical gradients:\")\n",
    "print(f\"∂L/∂w: analytical = {dL_dw:.6f}, numerical = {dL_dw_numerical:.6f}\")\n",
    "print(f\"∂L/∂b: analytical = {dL_db:.6f}, numerical = {dL_db_numerical:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Chain Rule\n",
    "\n",
    "When a variable affects the output through multiple paths:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x} = \\sum_{i} \\frac{\\partial L}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial x}$$\n",
    "\n",
    "This is why we **sum** gradients when a variable is used multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: f = x*y + x*z where y and z both depend on x\n",
    "# Actually, let's do: f(x) = x² + x (x is used twice)\n",
    "\n",
    "# Computational graph:\n",
    "# x --> a = x  --\\\n",
    "#                 +--> c = a * b --> f = c + d\n",
    "# x --> b = x  --/                      |\n",
    "#                                       |\n",
    "# x --> d = x  -------------------------/\n",
    "\n",
    "# This is: f = x*x + x = x² + x\n",
    "# df/dx = 2x + 1 (by calculus)\n",
    "\n",
    "# But through the graph:\n",
    "# df/dx = df/dc * dc/da * da/dx + df/dc * dc/db * db/dx + df/dd * dd/dx\n",
    "#       = 1 * b * 1 + 1 * a * 1 + 1 * 1\n",
    "#       = x + x + 1 = 2x + 1 ✓\n",
    "\n",
    "x = 3.0\n",
    "print(f\"f(x) = x² + x at x = {x}\")\n",
    "print(f\"f({x}) = {x**2 + x}\")\n",
    "print(f\"df/dx (analytical) = 2x + 1 = {2*x + 1}\")\n",
    "\n",
    "# Through computational graph\n",
    "a = x\n",
    "b = x  \n",
    "c = a * b  # = x²\n",
    "d = x\n",
    "f = c + d  # = x² + x\n",
    "\n",
    "# Backward\n",
    "df_dc = 1\n",
    "df_dd = 1\n",
    "dc_da = b  # = x\n",
    "dc_db = a  # = x\n",
    "da_dx = 1\n",
    "db_dx = 1\n",
    "dd_dx = 1\n",
    "\n",
    "# Sum all paths from f to x\n",
    "df_dx = df_dc * dc_da * da_dx + df_dc * dc_db * db_dx + df_dd * dd_dx\n",
    "print(f\"df/dx (computational graph) = {df_dx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Gradient Descent\n",
    "\n",
    "**Gradient descent** is the optimization algorithm that powers deep learning:\n",
    "\n",
    "$$\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\alpha \\nabla L(\\theta)$$\n",
    "\n",
    "Where:\n",
    "- $\\theta$: Parameters (weights)\n",
    "- $\\alpha$: Learning rate (step size)\n",
    "- $\\nabla L$: Gradient of loss with respect to parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_1d(f, df, x0, learning_rate=0.1, n_steps=50):\n",
    "    \"\"\"Gradient descent for 1D function.\"\"\"\n",
    "    x = x0\n",
    "    history = [(x, f(x))]\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        grad = df(x)\n",
    "        x = x - learning_rate * grad\n",
    "        history.append((x, f(x)))\n",
    "        \n",
    "    return x, history\n",
    "\n",
    "# Minimize f(x) = (x - 3)²\n",
    "f = lambda x: (x - 3)**2\n",
    "df = lambda x: 2 * (x - 3)\n",
    "\n",
    "x_final, history = gradient_descent_1d(f, df, x0=10.0, learning_rate=0.1, n_steps=30)\n",
    "\n",
    "print(f\"Minimum found at x = {x_final:.6f}\")\n",
    "print(f\"f(x) = {f(x_final):.6f}\")\n",
    "print(f\"True minimum at x = 3\")\n",
    "\n",
    "# Visualize\n",
    "x_range = np.linspace(-2, 12, 100)\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_range, f(x_range), 'b-', linewidth=2, label='f(x) = (x-3)²')\n",
    "xs, ys = zip(*history)\n",
    "plt.scatter(xs, ys, c=range(len(xs)), cmap='Reds', s=50, zorder=5)\n",
    "plt.plot(xs, ys, 'r--', alpha=0.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Gradient Descent Path')\n",
    "plt.legend()\n",
    "plt.colorbar(label='Iteration')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([h[1] for h in history], 'b-o')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Loss over Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_2d(f, grad_f, start, learning_rate=0.1, n_steps=50):\n",
    "    \"\"\"Gradient descent for 2D function.\"\"\"\n",
    "    point = np.array(start, dtype=float)\n",
    "    history = [point.copy()]\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        grad = grad_f(point)\n",
    "        point = point - learning_rate * grad\n",
    "        history.append(point.copy())\n",
    "        \n",
    "    return point, np.array(history)\n",
    "\n",
    "# Minimize f(x, y) = x² + y²\n",
    "def f(p):\n",
    "    return p[0]**2 + p[1]**2\n",
    "\n",
    "def grad_f(p):\n",
    "    return np.array([2*p[0], 2*p[1]])\n",
    "\n",
    "start = [4.0, 3.0]\n",
    "final, history = gradient_descent_2d(f, grad_f, start, learning_rate=0.1, n_steps=30)\n",
    "\n",
    "print(f\"Start: {start}\")\n",
    "print(f\"Final: {final}\")\n",
    "print(f\"f(final) = {f(final):.10f}\")\n",
    "\n",
    "# Visualize\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-5, 5, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = X**2 + Y**2\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contour(X, Y, Z, levels=20, cmap=cm.viridis)\n",
    "plt.plot(history[:, 0], history[:, 1], 'r.-', markersize=10, linewidth=2)\n",
    "plt.scatter([start[0]], [start[1]], color='green', s=200, marker='o', label='Start', zorder=5)\n",
    "plt.scatter([final[0]], [final[1]], color='red', s=200, marker='*', label='End', zorder=5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Gradient Descent on f(x,y) = x² + y²')\n",
    "plt.legend()\n",
    "plt.colorbar(label='f(x,y)')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Calculus Concepts and Their ML Applications\n\n| Calculus Concept | What it Means | ML Application |\n|------------------|---------------|----------------|\n| **Derivative** | Rate of change of output w.r.t. input | How loss changes when we change one weight |\n| **Partial Derivative** | Rate of change w.r.t. one variable (others fixed) | Gradient component for one parameter |\n| **Gradient** | Vector of all partial derivatives | Direction to update ALL weights at once |\n| **Chain Rule** | Derivative of composed functions = product of derivatives | Backpropagation through network layers |\n| **Gradient Descent** | Iteratively move opposite to gradient | Core training algorithm for neural networks |\n| **Learning Rate** | Step size in gradient descent | Hyperparameter controlling training speed |\n| **Local Minimum** | Point where gradient = 0 and function curves up | Where training might get stuck |\n| **Saddle Point** | Point where gradient = 0 but not min or max | Common in high-dim; momentum helps escape |\n\n### The Full Picture: How a Neural Network Learns\n\n1. **Forward pass**: Input flows through network, computing activations layer by layer\n2. **Loss computation**: Compare output to target, get a single number (the loss)\n3. **Backward pass**: Use chain rule to compute gradient of loss w.r.t. every weight\n4. **Parameter update**: Use gradient descent to update all weights\n5. **Repeat**: Until loss is small enough",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different learning rates\n",
    "learning_rates = [0.01, 0.1, 0.5, 0.95]\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Contour plot with paths\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-5, 5, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = X**2 + Y**2\n",
    "\n",
    "axes[0].contour(X, Y, Z, levels=20, cmap=cm.viridis, alpha=0.5)\n",
    "\n",
    "for lr, color in zip(learning_rates, colors):\n",
    "    final, history = gradient_descent_2d(f, grad_f, [4.0, 3.0], learning_rate=lr, n_steps=20)\n",
    "    axes[0].plot(history[:, 0], history[:, 1], '.-', color=color, markersize=8, \n",
    "                 linewidth=2, label=f'lr={lr}')\n",
    "    \n",
    "    # Loss curve\n",
    "    losses = [f(p) for p in history]\n",
    "    axes[1].plot(losses, color=color, linewidth=2, label=f'lr={lr}')\n",
    "\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Gradient Descent Paths')\n",
    "axes[0].legend()\n",
    "axes[0].axis('equal')\n",
    "\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Loss over Time')\n",
    "axes[1].legend()\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- Too small (0.01): Slow convergence\")\n",
    "print(\"- Good (0.1): Steady progress\")\n",
    "print(\"- Larger (0.5): Faster but oscillates\")\n",
    "print(\"- Too large (0.95): Oscillates wildly, may not converge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A More Challenging Function: Rosenbrock\n",
    "\n",
    "The Rosenbrock function is a classic optimization test:\n",
    "\n",
    "$$f(x, y) = (1 - x)^2 + 100(y - x^2)^2$$\n",
    "\n",
    "Minimum at $(1, 1)$. Famous for its narrow, curved valley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(p):\n",
    "    x, y = p\n",
    "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
    "\n",
    "def rosenbrock_grad(p):\n",
    "    x, y = p\n",
    "    dx = -2*(1 - x) - 400*x*(y - x**2)\n",
    "    dy = 200*(y - x**2)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "# Visualize the function\n",
    "x = np.linspace(-2, 2, 200)\n",
    "y = np.linspace(-1, 3, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = (1 - X)**2 + 100 * (Y - X**2)**2\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contour(X, Y, Z, levels=np.logspace(0, 3, 30), cmap=cm.viridis)\n",
    "plt.scatter([1], [1], color='red', s=200, marker='*', label='Minimum (1,1)', zorder=5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Rosenbrock Function\\nNotice the narrow curved valley')\n",
    "plt.colorbar(label='f(x,y)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent on Rosenbrock (challenging!)\n",
    "start = [-1.0, 1.0]\n",
    "final, history = gradient_descent_2d(rosenbrock, rosenbrock_grad, start, \n",
    "                                      learning_rate=0.001, n_steps=5000)\n",
    "\n",
    "print(f\"Start: {start}\")\n",
    "print(f\"Final: {final}\")\n",
    "print(f\"f(final) = {rosenbrock(final):.6f}\")\n",
    "print(f\"True minimum: (1, 1), f = 0\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.contour(X, Y, Z, levels=np.logspace(0, 3, 30), cmap=cm.viridis, alpha=0.5)\n",
    "plt.plot(history[::50, 0], history[::50, 1], 'r.-', markersize=5, linewidth=1)  # Every 50th point\n",
    "plt.scatter([start[0]], [start[1]], color='green', s=100, marker='o', label='Start', zorder=5)\n",
    "plt.scatter([final[0]], [final[1]], color='red', s=100, marker='*', label='End', zorder=5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Gradient Descent on Rosenbrock')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "losses = [rosenbrock(p) for p in history[::10]]\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration (x10)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Time')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Simple gradient descent struggles with this function!\")\n",
    "print(\"More advanced optimizers (Adam, etc.) handle this better.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Putting It Together: Training a Linear Model\n",
    "\n",
    "Let's train a simple linear regression model using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "# True parameters\n",
    "w_true = 2.5\n",
    "b_true = 1.0\n",
    "\n",
    "# Generate data: y = w*x + b + noise\n",
    "X = np.random.uniform(-3, 3, n_samples)\n",
    "y = w_true * X + b_true + np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.6, label='Data')\n",
    "plt.plot(X, w_true * X + b_true, 'r-', linewidth=2, label=f'True: y = {w_true}x + {b_true}')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression Dataset')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(X, y, learning_rate=0.01, n_epochs=100):\n",
    "    \"\"\"\n",
    "    Train linear regression using gradient descent.\n",
    "    \n",
    "    Model: y_pred = w * x + b\n",
    "    Loss: MSE = (1/n) * sum((y_pred - y)^2)\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    \n",
    "    # Initialize parameters\n",
    "    w = 0.0\n",
    "    b = 0.0\n",
    "    \n",
    "    history = {'loss': [], 'w': [], 'b': []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Forward pass\n",
    "        y_pred = w * X + b\n",
    "        \n",
    "        # Compute loss (MSE)\n",
    "        loss = np.mean((y_pred - y)**2)\n",
    "        \n",
    "        # Compute gradients\n",
    "        # d(loss)/dw = (2/n) * sum((y_pred - y) * x)\n",
    "        # d(loss)/db = (2/n) * sum(y_pred - y)\n",
    "        dw = (2/n) * np.sum((y_pred - y) * X)\n",
    "        db = (2/n) * np.sum(y_pred - y)\n",
    "        \n",
    "        # Update parameters\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        # Record history\n",
    "        history['loss'].append(loss)\n",
    "        history['w'].append(w)\n",
    "        history['b'].append(b)\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch:3d}: loss = {loss:.4f}, w = {w:.4f}, b = {b:.4f}\")\n",
    "    \n",
    "    return w, b, history\n",
    "\n",
    "# Train\n",
    "w_learned, b_learned, history = train_linear_regression(X, y, learning_rate=0.1, n_epochs=100)\n",
    "\n",
    "print(f\"\\nLearned: w = {w_learned:.4f}, b = {b_learned:.4f}\")\n",
    "print(f\"True:    w = {w_true:.4f}, b = {b_true:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(history['loss'])\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "\n",
    "# Parameter trajectory\n",
    "axes[1].plot(history['w'], label='w')\n",
    "axes[1].plot(history['b'], label='b')\n",
    "axes[1].axhline(y=w_true, color='blue', linestyle='--', alpha=0.5, label=f'w_true={w_true}')\n",
    "axes[1].axhline(y=b_true, color='orange', linestyle='--', alpha=0.5, label=f'b_true={b_true}')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Parameter Value')\n",
    "axes[1].set_title('Parameter Convergence')\n",
    "axes[1].legend()\n",
    "\n",
    "# Final fit\n",
    "axes[2].scatter(X, y, alpha=0.6, label='Data')\n",
    "x_line = np.linspace(-3, 3, 100)\n",
    "axes[2].plot(x_line, w_true * x_line + b_true, 'g-', linewidth=2, label=f'True: y = {w_true}x + {b_true}')\n",
    "axes[2].plot(x_line, w_learned * x_line + b_learned, 'r--', linewidth=2, \n",
    "             label=f'Learned: y = {w_learned:.2f}x + {b_learned:.2f}')\n",
    "axes[2].set_xlabel('x')\n",
    "axes[2].set_ylabel('y')\n",
    "axes[2].set_title('Final Fit')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Implement Gradient Checking\n",
    "\n",
    "Gradient checking is crucial for debugging backpropagation. Compare analytical gradients with numerical gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(f, grad_f, point, h=1e-5, threshold=1e-5):\n",
    "    \"\"\"\n",
    "    Compare analytical gradient with numerical gradient.\n",
    "    Returns True if they match within threshold.\n",
    "    \"\"\"\n",
    "    point = np.array(point, dtype=float)\n",
    "    analytical_grad = grad_f(point)\n",
    "    numerical_grad = compute_gradient(f, point, h)\n",
    "    \n",
    "    # Compute relative error\n",
    "    diff = np.abs(analytical_grad - numerical_grad)\n",
    "    denom = np.maximum(np.abs(analytical_grad) + np.abs(numerical_grad), 1e-10)\n",
    "    relative_error = diff / denom\n",
    "    \n",
    "    print(f\"Point: {point}\")\n",
    "    print(f\"Analytical gradient: {analytical_grad}\")\n",
    "    print(f\"Numerical gradient:  {numerical_grad}\")\n",
    "    print(f\"Relative error: {relative_error}\")\n",
    "    print(f\"Max relative error: {np.max(relative_error):.2e}\")\n",
    "    \n",
    "    return np.all(relative_error < threshold)\n",
    "\n",
    "# Test on f(x,y) = x³ + 2xy + y²\n",
    "def f(p):\n",
    "    x, y = p\n",
    "    return x**3 + 2*x*y + y**2\n",
    "\n",
    "def grad_f(p):\n",
    "    x, y = p\n",
    "    return np.array([3*x**2 + 2*y, 2*x + 2*y])\n",
    "\n",
    "passed = gradient_check(f, grad_f, [2.0, 3.0])\n",
    "print(f\"\\nGradient check passed: {passed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Softmax and Its Gradient\n",
    "\n",
    "Softmax is critical for classification. Implement it and its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute softmax: softmax(x)_i = exp(x_i) / sum(exp(x_j))\n",
    "    Subtract max for numerical stability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement softmax\n",
    "    x_shifted = x - np.max(x)  # For numerical stability\n",
    "    exp_x = np.exp(x_shifted)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "def softmax_jacobian(x):\n",
    "    \"\"\"\n",
    "    Compute Jacobian of softmax.\n",
    "    J[i,j] = d(softmax_i)/d(x_j)\n",
    "    \n",
    "    Formula: J[i,j] = softmax_i * (delta_ij - softmax_j)\n",
    "    where delta_ij = 1 if i==j, else 0\n",
    "    \"\"\"\n",
    "    s = softmax(x)\n",
    "    n = len(s)\n",
    "    jacobian = np.zeros((n, n))\n",
    "    \n",
    "    # TODO: Implement Jacobian\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                jacobian[i, j] = s[i] * (1 - s[j])\n",
    "            else:\n",
    "                jacobian[i, j] = -s[i] * s[j]\n",
    "    \n",
    "    return jacobian\n",
    "\n",
    "# Test\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Softmax: {softmax(x)}\")\n",
    "print(f\"Sum (should be 1): {softmax(x).sum():.6f}\")\n",
    "print(f\"\\nJacobian:\\n{softmax_jacobian(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Gradient Descent with Momentum\n",
    "\n",
    "Momentum helps accelerate gradient descent. Implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_momentum(f, grad_f, start, learning_rate=0.01, momentum=0.9, n_steps=100):\n",
    "    \"\"\"\n",
    "    Gradient descent with momentum.\n",
    "    \n",
    "    v = momentum * v - learning_rate * gradient\n",
    "    x = x + v\n",
    "    \"\"\"\n",
    "    point = np.array(start, dtype=float)\n",
    "    velocity = np.zeros_like(point)\n",
    "    history = [point.copy()]\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        grad = grad_f(point)\n",
    "        velocity = momentum * velocity - learning_rate * grad\n",
    "        point = point + velocity\n",
    "        history.append(point.copy())\n",
    "        \n",
    "    return point, np.array(history)\n",
    "\n",
    "# Compare regular GD vs GD with momentum on Rosenbrock\n",
    "start = [-1.0, 1.0]\n",
    "n_steps = 1000\n",
    "\n",
    "final_gd, history_gd = gradient_descent_2d(rosenbrock, rosenbrock_grad, start, \n",
    "                                            learning_rate=0.001, n_steps=n_steps)\n",
    "final_mom, history_mom = gradient_descent_momentum(rosenbrock, rosenbrock_grad, start,\n",
    "                                                    learning_rate=0.001, momentum=0.9, n_steps=n_steps)\n",
    "\n",
    "print(f\"Regular GD final loss: {rosenbrock(final_gd):.6f}\")\n",
    "print(f\"Momentum GD final loss: {rosenbrock(final_mom):.6f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.contour(X, Y, Z, levels=np.logspace(0, 3, 30), cmap=cm.viridis, alpha=0.5)\n",
    "plt.plot(history_gd[::20, 0], history_gd[::20, 1], 'b.-', markersize=3, label='Regular GD')\n",
    "plt.plot(history_mom[::20, 0], history_mom[::20, 1], 'r.-', markersize=3, label='With Momentum')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Optimization Paths')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "losses_gd = [rosenbrock(p) for p in history_gd]\n",
    "losses_mom = [rosenbrock(p) for p in history_mom]\n",
    "plt.plot(losses_gd, 'b-', label='Regular GD')\n",
    "plt.plot(losses_mom, 'r-', label='With Momentum')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Comparison')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Derivatives** measure rate of change - essential for optimization\n",
    "2. **Partial derivatives** handle functions of multiple variables\n",
    "3. **The gradient** points in the direction of steepest ascent\n",
    "4. **Chain rule** lets us compute gradients through composed functions (backprop!)\n",
    "5. **Gradient descent** minimizes loss by following the negative gradient\n",
    "\n",
    "### Connection to Deep Learning\n",
    "\n",
    "- **Forward pass**: Compute function values through the network\n",
    "- **Loss**: Scalar measuring prediction quality\n",
    "- **Backward pass**: Apply chain rule to compute gradients\n",
    "- **Update**: Move parameters in negative gradient direction\n",
    "\n",
    "### Checklist\n",
    "- [ ] I can compute derivatives of common functions\n",
    "- [ ] I understand partial derivatives and gradients\n",
    "- [ ] I can apply the chain rule to composite functions\n",
    "- [ ] I can implement gradient descent from scratch\n",
    "- [ ] I understand the effect of learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to **Part 1.3: Probability & Statistics** where we'll cover:\n",
    "- Probability distributions\n",
    "- Bayes' theorem\n",
    "- Maximum likelihood estimation\n",
    "- Information theory (entropy, KL divergence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}