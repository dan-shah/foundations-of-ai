{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Part 2.2: NumPy Deep Dive — The Formula 1 Edition\n\nNumPy is the foundation of scientific computing in Python. Understanding it deeply will help you:\n- Write faster, more efficient code\n- Understand how PyTorch tensors work (they're very similar!)\n- Debug shape mismatches in neural networks\n\n**F1 analogy:** Think of NumPy as the telemetry processing engine that every F1 team relies on. During a single Grand Prix, each car generates gigabytes of sensor data — speed, throttle position, brake pressure, tire temperatures, steering angle — sampled hundreds of times per second. NumPy's array operations are how engineers process all 20 cars' data simultaneously rather than looping through each reading one at a time. The difference between vectorized and loop-based telemetry processing is the difference between getting strategy calls right *during* the race versus figuring them out the next morning.\n\n## Learning Objectives\n- [ ] Master NumPy broadcasting rules\n- [ ] Use advanced indexing effectively\n- [ ] Vectorize operations for performance\n- [ ] Understand memory layout and views\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Array Creation and Basics\n\n### Creating Arrays\n\n**F1 analogy:** Every array is a telemetry channel. A 1D array is one sensor over time (e.g., speed readings lap by lap). A 2D array is multiple sensors stacked together — each row is a lap, each column is a different channel (speed, throttle, brake, steering). When the pit wall displays live data, they're rendering NumPy-style arrays in real time."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# From Python lists — like recording telemetry from sensors\nlap_speeds = np.array([310, 295, 302])  # Top speeds in km/h for 3 laps\ntelemetry = np.array([[310, 0.95, 0.0, 0.02],   # Lap 1: [speed, throttle, brake, steering]\n                      [295, 0.88, 0.15, 0.08]])  # Lap 2\n\nprint(f\"1D array (lap speeds): {lap_speeds}, shape: {lap_speeds.shape}\")\nprint(f\"2D array (telemetry):\\n{telemetry}\\nshape: {telemetry.shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Common creation functions\nprint(\"np.zeros((2, 3)) — blank telemetry (all sensors read zero before session starts):\")\nprint(np.zeros((2, 3)))\n\nprint(\"\\nnp.ones((2, 3)) — baseline reference (all channels at 1.0):\")\nprint(np.ones((2, 3)))\n\nprint(\"\\nnp.eye(3) (identity matrix) — no transformation applied to data:\")\nprint(np.eye(3))\n\nprint(\"\\nnp.arange(0, 10, 2) — sample every 2nd data point:\")\nprint(np.arange(0, 10, 2))\n\nprint(\"\\nnp.linspace(0, 1, 5) — 5 evenly-spaced points from 0 to 1 (like normalizing a lap):\")\nprint(np.linspace(0, 1, 5))\n\nprint(\"\\nnp.random.randn(2, 3) — simulated sensor noise (standard normal):\")\nprint(np.random.randn(2, 3))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Array Attributes\n\n**F1 analogy:** Knowing an array's shape is like knowing the structure of your telemetry log. A `(57, 300, 6)` array means 57 laps, 300 samples per lap, and 6 sensor channels. If you confuse the axes, you'll be reading brake pressure where you expected speed — the data engineering equivalent of fitting medium tires when you ordered hards."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imagine a 3D telemetry block: (laps, samples_per_lap, channels)\nrace_telemetry = np.random.randn(3, 4, 5)\n\nprint(f\"Shape: {race_telemetry.shape}\")      # (3 laps, 4 samples, 5 channels)\nprint(f\"Ndim: {race_telemetry.ndim}\")        # Number of dimensions\nprint(f\"Size: {race_telemetry.size}\")        # Total number of elements\nprint(f\"Dtype: {race_telemetry.dtype}\")      # Data type\nprint(f\"Itemsize: {race_telemetry.itemsize} bytes\")  # Bytes per element\nprint(f\"Total bytes: {race_telemetry.nbytes}\")       # Total memory"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 2. Reshaping and Manipulating Arrays\n\n### Understanding Shape\n\n**F1 analogy:** Reshaping is how you convert between different views of the same telemetry data. A flat stream of 3600 readings from a sensor might need to be reshaped into `(60 laps, 60 samples_per_lap)` for lap-by-lap analysis, or `(12 stints, 300 readings)` for stint-level strategy. The data doesn't change — just how you organize it. This is exactly what happens when a CNN's output gets flattened before a fully connected layer."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Reshape - VERY common in deep learning\n# Imagine 12 consecutive speed readings from a car\nspeed_readings = np.arange(12)\nprint(f\"Original flat telemetry: {speed_readings}, shape: {speed_readings.shape}\")\n\n# Reshape to (laps, samples_per_lap)\nby_lap = speed_readings.reshape(3, 4)\nprint(f\"\\nReshaped to (3 laps, 4 samples):\\n{by_lap}\")\n\nby_stint = speed_readings.reshape(4, 3)\nprint(f\"\\nReshaped to (4 stints, 3 samples):\\n{by_stint}\")\n\n# Use -1 to infer dimension — \"I know 2 stints, figure out the rest\"\nauto_shape = speed_readings.reshape(2, -1)\nprint(f\"\\nReshaped to (2, -1) -> {auto_shape.shape}:\\n{auto_shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Deep Dive: Flatten vs Ravel vs Reshape(-1)\n\n**F1 analogy:** Sometimes you need to take a structured telemetry log (laps x channels) and flatten it into a single stream for transmission back to the factory. The question is: do you want a *copy* of the data (safe but uses more memory) or a *view* (efficient but changes propagate back)?\n\n| Method | Returns | Memory | F1 Parallel |\n|--------|---------|--------|-------------|\n| `flatten()` | Copy | Always new array | Saving a snapshot to the archive — safe, independent |\n| `ravel()` | View if possible | Shares memory when possible | Live dashboard view — changes to the source update instantly |\n| `reshape(-1)` | View if possible | Same as ravel | Same as ravel |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "sector_times = np.array([[91.2, 28.5, 33.1], [90.8, 28.9, 32.7]])  # (2 laps, 3 sectors)\n\nflat = sector_times.flatten()\nravel = sector_times.ravel()\nreshape = sector_times.reshape(-1)\n\nprint(f\"Original sector times:\\n{sector_times}\")\nprint(f\"\\nflat: {flat}\")\nprint(f\"ravel: {ravel}\")\nprint(f\"reshape(-1): {reshape}\")\n\n# Modify original — simulate a timing correction\nsector_times[0, 0] = 999\nprint(f\"\\nAfter correcting sector_times[0,0] = 999:\")\nprint(f\"flat: {flat}  (unchanged - it's a copy)\")\nprint(f\"ravel: {ravel}  (changed - it's a view!)\")"
  },
  {
   "cell_type": "markdown",
   "source": "**What this means:** Computer memory is linear (1D), so 2D arrays must be \"flattened\" when stored. C-order stores row-by-row (natural for Python/C), while F-order stores column-by-column (natural for Fortran/MATLAB). This affects performance: accessing data along the \"fast\" axis is much quicker because it uses contiguous memory. In NumPy, iterating over rows is typically faster than columns.\n\n**F1 analogy:** Think of memory layout like reading a telemetry log. C-order reads all channels for lap 1, then all channels for lap 2 — \"read one lap at a time.\" F-order reads all laps for channel 1 (speed), then all laps for channel 2 (throttle) — \"read one sensor at a time.\" If your analysis is lap-focused, C-order is faster; if it's sensor-focused, F-order wins. F1 teams typically store data lap-first (C-order), because race engineers analyze lap by lap.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# VISUALIZATION: Memory Layout - C-order vs F-order\nfig, axes = plt.subplots(1, 3, figsize=(14, 5))\n\n# Create a 2D array\narr_2d = np.array([[1, 2, 3], [4, 5, 6]])\n\n# Left: The conceptual 2D array\nax = axes[0]\nax.set_title('Conceptual 2D Array\\n(how we think about it)', fontsize=11)\nfor i in range(2):\n    for j in range(3):\n        color = plt.cm.viridis(arr_2d[i, j] / 7)\n        ax.add_patch(plt.Rectangle((j, 1-i), 0.9, 0.9, facecolor=color, edgecolor='black', lw=2))\n        ax.text(j + 0.45, 1.45 - i, str(arr_2d[i, j]), ha='center', va='center', fontsize=14, fontweight='bold', color='white')\nax.text(1.5, -0.5, 'rows', ha='center', fontsize=10)\nax.text(-0.5, 1, 'cols', ha='center', fontsize=10, rotation=90)\nax.set_xlim(-0.8, 3.5)\nax.set_ylim(-1, 2.5)\nax.axis('off')\n\n# Middle: C-order (row-major)\nax = axes[1]\nax.set_title('C-order (Row-major)\\nDefault in NumPy', fontsize=11)\nc_order = arr_2d.ravel(order='C')\nfor i, val in enumerate(c_order):\n    color = plt.cm.viridis(val / 7)\n    ax.add_patch(plt.Rectangle((i, 0), 0.9, 0.9, facecolor=color, edgecolor='black', lw=2))\n    ax.text(i + 0.45, 0.45, str(val), ha='center', va='center', fontsize=14, fontweight='bold', color='white')\nax.text(2.5, -0.5, 'Memory addresses →', ha='center', fontsize=10)\nax.annotate('Row 0', xy=(1, 1), xytext=(1, 1.5), fontsize=9, ha='center')\nax.annotate('Row 1', xy=(4, 1), xytext=(4, 1.5), fontsize=9, ha='center')\nax.plot([2.95, 2.95], [0, 0.9], 'r--', lw=2)\nax.set_xlim(-0.5, 6.5)\nax.set_ylim(-1, 2)\nax.axis('off')\n\n# Right: F-order (column-major)\nax = axes[2]\nax.set_title('F-order (Column-major)\\nUsed in Fortran, MATLAB', fontsize=11)\nf_order = arr_2d.ravel(order='F')\nfor i, val in enumerate(f_order):\n    color = plt.cm.viridis(val / 7)\n    ax.add_patch(plt.Rectangle((i, 0), 0.9, 0.9, facecolor=color, edgecolor='black', lw=2))\n    ax.text(i + 0.45, 0.45, str(val), ha='center', va='center', fontsize=14, fontweight='bold', color='white')\nax.text(2.5, -0.5, 'Memory addresses →', ha='center', fontsize=10)\nax.annotate('Col 0', xy=(0.5, 1), xytext=(0.5, 1.5), fontsize=9, ha='center')\nax.annotate('Col 1', xy=(2.5, 1), xytext=(2.5, 1.5), fontsize=9, ha='center')\nax.annotate('Col 2', xy=(4.5, 1), xytext=(4.5, 1.5), fontsize=9, ha='center')\nax.plot([1.95, 1.95], [0, 0.9], 'r--', lw=2)\nax.plot([3.95, 3.95], [0, 0.9], 'r--', lw=2)\nax.set_xlim(-0.5, 6.5)\nax.set_ylim(-1, 2)\nax.axis('off')\n\nplt.tight_layout()\nplt.suptitle('Memory Layout: How 2D Arrays are Stored in 1D Memory', y=1.02, fontsize=13, fontweight='bold')\nplt.show()\n\nprint(\"C-order traverses rows first: \", arr_2d.ravel(order='C'))\nprint(\"F-order traverses columns first:\", arr_2d.ravel(order='F'))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Adding and Removing Dimensions\n\nCommon in deep learning when you need to:\n- Add batch dimension: `(H, W)` → `(1, H, W)`\n- Add channel dimension: `(B, H, W)` → `(B, 1, H, W)`\n\n**F1 analogy:** Adding a dimension is like going from \"one car's telemetry\" to \"a batch of all cars' telemetry.\" A single car's speed trace has shape `(300,)` — 300 samples in a lap. But the race director's system needs to process all 20 cars at once, so it adds a car dimension: `(20, 300)`. In deep learning, this is exactly how batching works — you take one sample and add a batch axis so the network can process many samples simultaneously."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# np.newaxis (same as None) adds a dimension\nlap_times = np.array([91.2, 90.8, 91.5])  # Shape: (3,) — times for 3 laps\nprint(f\"Original shape: {lap_times.shape}\")\n\n# Add dimension at front (batch/car dimension)\nlap_times_batch = lap_times[np.newaxis, :]  # or x[None, :] or x.reshape(1, -1)\nprint(f\"With batch dim: {lap_times_batch.shape}\")\n\n# Add dimension at end (turn into a column — one lap per row)\nlap_times_col = lap_times[:, np.newaxis]  # or x[:, None] or x.reshape(-1, 1)\nprint(f\"As column: {lap_times_col.shape}\")\n\n# np.expand_dims is more explicit\nprint(f\"\\nnp.expand_dims(lap_times, axis=0): {np.expand_dims(lap_times, axis=0).shape}\")\nprint(f\"np.expand_dims(lap_times, axis=1): {np.expand_dims(lap_times, axis=1).shape}\")\n\n# np.squeeze removes dimensions of size 1\ny = np.zeros((1, 3, 1, 4))\nprint(f\"\\nOriginal: {y.shape}\")\nprint(f\"Squeezed: {np.squeeze(y).shape}\")\nprint(f\"Squeeze axis 0 only: {np.squeeze(y, axis=0).shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Transpose and Swapaxes\n\n**F1 analogy:** Transposing is changing your perspective on the same data. If your telemetry is stored as `(laps, channels)` — one row per lap — transposing gives you `(channels, laps)` — one row per sensor. This is exactly what happens when converting between TensorFlow's NHWC and PyTorch's NCHW image formats."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 2D transpose — switch from (laps, channels) to (channels, laps)\nstint_data = np.array([[310, 0.95, 0.0],   # Lap 1: speed, throttle, brake\n                       [295, 0.88, 0.15]])  # Lap 2\nprint(f\"Original (laps, channels) (2, 3):\\n{stint_data}\")\nprint(f\"\\nTransposed (channels, laps) (3, 2):\\n{stint_data.T}\")\n\n# For higher dimensions, use transpose with axis order\n# Example: Convert (batch, height, width, channels) to (batch, channels, height, width)\nimg_nhwc = np.random.randn(32, 28, 28, 3)  # TensorFlow format\nimg_nchw = img_nhwc.transpose(0, 3, 1, 2)  # PyTorch format\n\nprint(f\"\\nNHWC (TensorFlow): {img_nhwc.shape}\")\nprint(f\"NCHW (PyTorch): {img_nchw.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 3. Broadcasting\n\n**Broadcasting** allows NumPy to perform operations on arrays of different shapes. This is crucial for writing efficient, vectorized code.\n\n**F1 analogy:** Broadcasting is like applying a fuel correction factor to every lap in every stint, or subtracting the track baseline temperature from every sensor on every car. You have one correction value (or one row of corrections), and NumPy automatically applies it across all the laps and cars without you writing a single loop. When the team says \"add 3 kg of fuel load to all strategy simulations,\" that single number gets broadcast across every scenario.\n\n### Broadcasting Rules\n\nWhen operating on two arrays, NumPy compares shapes element-wise from the **trailing dimensions**:\n\n1. If dimensions are equal, they're compatible\n2. If one dimension is 1, it's \"stretched\" to match the other\n3. If neither condition is met, error!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple example: apply a fuel-weight correction to all lap times\nlap_times = np.array([91.2, 90.8, 91.5])\nprint(f\"lap_times + 0.3 (fuel correction) = {lap_times + 0.3}\")\n# 0.3 is \"broadcast\" to [0.3, 0.3, 0.3]\n\n# 2D + 1D: Apply per-channel calibration offsets to all laps\ntelemetry_2lap = np.array([[310, 95, 0],     # Lap 1: [speed, throttle%, brake%]\n                           [295, 88, 15]])    # Lap 2\ncalibration_offset = np.array([2, -1, 3])     # Sensor calibration per channel\n\nprint(f\"\\nTelemetry (shape {telemetry_2lap.shape}):\\n{telemetry_2lap}\")\nprint(f\"Calibration offset (shape {calibration_offset.shape}): {calibration_offset}\")\nprint(f\"\\nCorrected telemetry (offset broadcast across laps):\\n{telemetry_2lap + calibration_offset}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Deep Dive: Visualizing Broadcasting\n\n**F1 analogy:** The three broadcast cases below mirror common F1 data operations:\n- **Scalar + Array**: Adding a fuel correction to every lap time\n- **2D + 1D**: Applying per-sensor calibration offsets to every lap's telemetry\n- **2D + Column**: Applying per-lap tire degradation to every sensor channel"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_broadcast(a, b):\n",
    "    \"\"\"Visualize how two arrays are broadcast together.\"\"\"\n",
    "    print(f\"Array A shape: {a.shape}\")\n",
    "    print(f\"Array B shape: {b.shape}\")\n",
    "    \n",
    "    try:\n",
    "        result = a + b\n",
    "        print(f\"Result shape: {result.shape}\")\n",
    "        print(f\"\\nA:\\n{a}\")\n",
    "        print(f\"\\nB:\\n{b}\")\n",
    "        print(f\"\\nA + B:\\n{result}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "\n",
    "# Case 1: (3,) + (3,) - same shape\n",
    "print(\"=\" * 40)\n",
    "print(\"Case 1: Same shapes\")\n",
    "show_broadcast(np.array([1, 2, 3]), np.array([10, 20, 30]))\n",
    "\n",
    "# Case 2: (2, 3) + (3,) - trailing dimensions match\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"Case 2: Trailing dimensions match\")\n",
    "show_broadcast(\n",
    "    np.array([[1, 2, 3], [4, 5, 6]]),\n",
    "    np.array([10, 20, 30])\n",
    ")\n",
    "\n",
    "# Case 3: (2, 3) + (2, 1) - one dimension is 1\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"Case 3: Dimension of 1 gets stretched\")\n",
    "show_broadcast(\n",
    "    np.array([[1, 2, 3], [4, 5, 6]]),\n",
    "    np.array([[10], [20]])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this means:** Broadcasting is NumPy's way of \"stretching\" smaller arrays to match larger ones during arithmetic operations. Instead of manually copying data to match shapes, NumPy virtually expands the smaller array. This happens automatically and uses no extra memory - it's just a clever indexing trick under the hood.\n\n**F1 analogy:** Broadcasting is the reason an F1 strategist can say \"apply a 0.1s tire degradation penalty per lap to all 20 cars' projected race times\" and have it happen instantly. The single degradation vector `(57,)` — one value per lap — gets broadcast across all 20 cars' `(20, 57)` projected lap time matrix without creating 20 copies. The same principle powers batch normalization in neural networks: one mean and variance per channel gets broadcast across the entire batch.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# VISUALIZATION: Broadcasting - How shapes expand\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\n\n# Case 1: (3,) + scalar\nax = axes[0]\nax.set_title('Scalar + Array\\n(3,) + () → (3,)', fontsize=11)\n# Draw original array\nfor i, val in enumerate([1, 2, 3]):\n    ax.add_patch(plt.Rectangle((i, 1), 0.9, 0.9, facecolor='steelblue', edgecolor='black'))\n    ax.text(i + 0.45, 1.45, str(val), ha='center', va='center', fontsize=12, color='white', fontweight='bold')\n# Draw scalar being broadcast\nfor i in range(3):\n    ax.add_patch(plt.Rectangle((i, 0), 0.9, 0.9, facecolor='coral', edgecolor='black', alpha=0.7 if i > 0 else 1))\n    ax.text(i + 0.45, 0.45, '10', ha='center', va='center', fontsize=12)\nax.annotate('', xy=(1.5, 0.95), xytext=(1.5, 0.05), arrowprops=dict(arrowstyle='->', color='green', lw=2))\nax.text(2.2, 0.5, 'broadcast', fontsize=9, color='green')\nax.set_xlim(-0.5, 4)\nax.set_ylim(-0.5, 2.5)\nax.axis('off')\n\n# Case 2: (2, 3) + (3,)\nax = axes[1]\nax.set_title('2D + 1D\\n(2,3) + (3,) → (2,3)', fontsize=11)\n# Draw 2D array\nfor i in range(2):\n    for j in range(3):\n        ax.add_patch(plt.Rectangle((j, 1-i), 0.9, 0.9, facecolor='steelblue', edgecolor='black'))\n        ax.text(j + 0.45, 1.45 - i, f'{i*3+j+1}', ha='center', va='center', fontsize=12, color='white', fontweight='bold')\n# Draw 1D array being broadcast\nfor j in range(3):\n    ax.add_patch(plt.Rectangle((j, -1), 0.9, 0.9, facecolor='coral', edgecolor='black'))\n    ax.text(j + 0.45, -0.55, f'{(j+1)*10}', ha='center', va='center', fontsize=11)\n# Arrows showing broadcast\nfor i in range(2):\n    ax.annotate('', xy=(1.5, 1-i), xytext=(1.5, -0.5), arrowprops=dict(arrowstyle='->', color='green', lw=1.5, alpha=0.5))\nax.text(3.3, 0.2, 'broadcast\\nto rows', fontsize=9, color='green')\nax.set_xlim(-0.5, 4.5)\nax.set_ylim(-1.8, 2.5)\nax.axis('off')\n\n# Case 3: (2, 3) + (2, 1)\nax = axes[2]\nax.set_title('2D + Column\\n(2,3) + (2,1) → (2,3)', fontsize=11)\n# Draw 2D array\nfor i in range(2):\n    for j in range(3):\n        ax.add_patch(plt.Rectangle((j+1, 1-i), 0.9, 0.9, facecolor='steelblue', edgecolor='black'))\n        ax.text(j + 1.45, 1.45 - i, f'{i*3+j+1}', ha='center', va='center', fontsize=12, color='white', fontweight='bold')\n# Draw column array being broadcast\nfor i in range(2):\n    ax.add_patch(plt.Rectangle((0, 1-i), 0.9, 0.9, facecolor='coral', edgecolor='black'))\n    ax.text(0.45, 1.45 - i, f'{(i+1)*10}', ha='center', va='center', fontsize=11)\n# Arrows showing broadcast\nfor i in range(2):\n    ax.annotate('', xy=(1, 1.45-i), xytext=(0.95, 1.45-i), arrowprops=dict(arrowstyle='->', color='green', lw=1.5))\nax.text(0.2, -0.8, 'broadcast\\nto columns', fontsize=9, color='green')\nax.set_xlim(-0.5, 4.5)\nax.set_ylim(-1.2, 2.5)\nax.axis('off')\n\nplt.tight_layout()\nplt.suptitle('Broadcasting: How NumPy Expands Shapes', y=1.05, fontsize=13, fontweight='bold')\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# INTERACTIVE: Show effect of different broadcasting shapes\n# Experiment with broadcasting behavior\n\nprint(\"Broadcasting Shape Combinations\")\nprint(\"=\" * 60)\n\ntest_cases = [\n    ((3,), (3,), \"Same shapes - element-wise\"),\n    ((3, 4), (4,), \"2D + 1D - broadcast along rows\"),\n    ((3, 4), (3, 1), \"2D + column - broadcast along columns\"),\n    ((3, 1), (1, 4), \"Column + row - outer product pattern\"),\n    ((5, 3, 4), (4,), \"3D + 1D - broadcast to all batches\"),\n    ((5, 3, 4), (3, 1), \"3D + 2D - broadcast channel-wise\"),\n]\n\nfor shape_a, shape_b, description in test_cases:\n    a = np.ones(shape_a)\n    b = np.ones(shape_b)\n    try:\n        result = a + b\n        print(f\"{str(shape_a):>12} + {str(shape_b):<12} -> {str(result.shape):<12} | {description}\")\n    except ValueError as e:\n        print(f\"{str(shape_a):>12} + {str(shape_b):<12} -> ERROR | {e}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Broadcasting Failures (incompatible shapes):\")\nprint(\"=\" * 60)\n\nfailure_cases = [\n    ((3,), (4,), \"Different sizes, neither is 1\"),\n    ((3, 4), (3,), \"Trailing dims don't match\"),\n    ((2, 3, 4), (2, 4), \"Middle dimension mismatch\"),\n]\n\nfor shape_a, shape_b, description in failure_cases:\n    a = np.ones(shape_a)\n    b = np.ones(shape_b)\n    try:\n        result = a + b\n        print(f\"{str(shape_a):>12} + {str(shape_b):<12} -> {str(result.shape)}\")\n    except ValueError as e:\n        print(f\"{str(shape_a):>12} + {str(shape_b):<12} -> FAIL | {description}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Classic use case: outer product via broadcasting\n# Imagine comparing 4 drivers' base pace against 3 different fuel loads\ndriver_pace = np.array([1, 2, 3, 4])     # Shape (4,) — relative pace of 4 drivers\nfuel_penalty = np.array([10, 20, 30])     # Shape (3,) — time penalty for 3 fuel loads\n\n# Make shapes compatible for broadcasting\n# (4, 1) * (3,) -> (4, 1) * (1, 3) -> (4, 3)\npace_fuel_matrix = driver_pace[:, np.newaxis] * fuel_penalty[np.newaxis, :]\n\nprint(f\"driver_pace (shape {driver_pace.shape}): {driver_pace}\")\nprint(f\"fuel_penalty (shape {fuel_penalty.shape}): {fuel_penalty}\")\nprint(f\"\\ndriver_pace[:, None] shape: {driver_pace[:, np.newaxis].shape}\")\nprint(f\"fuel_penalty[None, :] shape: {fuel_penalty[np.newaxis, :].shape}\")\nprint(f\"\\nOuter product — pace x fuel grid (4, 3):\\n{pace_fuel_matrix}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Broadcasting in Deep Learning\n\n| Operation | Shapes | Use Case | F1 Parallel |\n|-----------|--------|----------|-------------|\n| Add bias | `(batch, features) + (features,)` | FC layer output + bias | Adding sensor calibration offsets to all laps |\n| Normalize | `(B, C, H, W) - (C, 1, 1)` | Subtract channel means | Subtracting track baseline from all telemetry channels |\n| Scale | `(B, C, H, W) * (C, 1, 1)` | Batch normalization | Applying tire degradation factor per stint |\n| Attention mask | `(B, H, L, L) + (1, 1, L, L)` | Causal mask | \"Only use past lap data to predict future laps\" |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Practical example: Batch normalization-style operation\n# Think of this as normalizing telemetry across all cars and laps per sensor channel\n# Input: (cars, sensors, time_steps_x, time_steps_y)\ntelemetry_batch = np.random.randn(32, 64, 8, 8)  # 32 cars, 64 sensor channels, 8x8 grid\n\n# Compute per-channel mean and std (average across cars and spatial dims)\nchannel_mean = telemetry_batch.mean(axis=(0, 2, 3), keepdims=True)  # (1, 64, 1, 1)\nchannel_std = telemetry_batch.std(axis=(0, 2, 3), keepdims=True)    # (1, 64, 1, 1)\n\n# Normalize (broadcasts automatically!)\ntelemetry_normalized = (telemetry_batch - channel_mean) / (channel_std + 1e-5)\n\nprint(f\"Input shape: {telemetry_batch.shape}\")\nprint(f\"Channel mean shape: {channel_mean.shape}\")\nprint(f\"Normalized shape: {telemetry_normalized.shape}\")\nprint(f\"\\nPer-channel mean after normalization: {telemetry_normalized.mean(axis=(0, 2, 3))[:5].round(6)}\")\nprint(f\"Per-channel std after normalization: {telemetry_normalized.std(axis=(0, 2, 3))[:5].round(4)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 4. Advanced Indexing\n\nNumPy offers powerful ways to select elements from arrays.\n\n**F1 analogy:** Advanced indexing is how you extract exactly the data you need from a massive telemetry archive. Basic slicing is like saying \"give me laps 10-20.\" Boolean indexing is \"give me all laps where top speed exceeded 320 km/h.\" Fancy indexing is \"give me specifically laps 3, 17, and 42 — the ones where we had DRS failures.\" These are the same patterns used in neural networks for masking padded tokens, selecting class probabilities, and implementing attention.\n\n### Basic Slicing"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Speed readings across 10 time samples\nspeed_trace = np.arange(10)\nprint(f\"speed_trace: {speed_trace}\")\nprint(f\"speed_trace[2:7] (samples 2-6): {speed_trace[2:7]}\")\nprint(f\"speed_trace[::2] (every 2nd sample): {speed_trace[::2]}\")\nprint(f\"speed_trace[::-1] (reversed): {speed_trace[::-1]}\")\n\n# 2D slicing: telemetry grid (laps x sensors)\nrace_grid = np.arange(20).reshape(4, 5)\nprint(f\"\\nRace telemetry grid (4 laps, 5 sensors):\\n{race_grid}\")\nprint(f\"\\nrace_grid[1:3, 2:4] (laps 1-2, sensors 2-3):\\n{race_grid[1:3, 2:4]}\")\nprint(f\"\\nrace_grid[:, 0] (sensor 0 across all laps): {race_grid[:, 0]}\")\nprint(f\"race_grid[0, :] (all sensors for lap 0): {race_grid[0, :]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Boolean Indexing\n\nSelect elements based on conditions. Extremely useful!\n\n**F1 analogy:** Boolean indexing is the engineer's filter tool. \"Show me only the laps where brake temperature exceeded the safety threshold.\" \"Give me every sector where the driver lifted off the throttle.\" In ML, the exact same pattern implements ReLU (zero out negatives) and attention masking (ignore padded positions)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tire temperature deltas — positive means overheating, negative means underheating\ntire_temp_delta = np.array([5, -2, 8, -4, 3, -6])\n\n# Create boolean mask: which readings show overheating?\noverheating = tire_temp_delta > 0\nprint(f\"tire_temp_delta: {tire_temp_delta}\")\nprint(f\"overheating mask (> 0): {overheating}\")\nprint(f\"overheating values: {tire_temp_delta[overheating]}\")\n\n# Directly in one line\nprint(f\"tire_temp_delta[tire_temp_delta > 0]: {tire_temp_delta[tire_temp_delta > 0]}\")\n\n# Combine conditions: moderate overheating only\nprint(f\"Moderate (> 0 and < 5): {tire_temp_delta[(tire_temp_delta > 0) & (tire_temp_delta < 5)]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Practical: Apply ReLU using boolean indexing\n# In F1 terms: clamp negative g-force readings to zero (sensor floor)\ndef relu_boolean(x):\n    result = x.copy()\n    result[result < 0] = 0\n    return result\n\ng_force_readings = np.array([-2, -1, 0, 1, 2])\nprint(f\"Raw g-force: {g_force_readings}\")\nprint(f\"ReLU(g-force): {relu_boolean(g_force_readings)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Integer Array Indexing (Fancy Indexing)\n\nUse arrays of indices to select specific elements.\n\n**F1 analogy:** Fancy indexing is cherry-picking specific laps from the race log. \"Give me lap 1, lap 15, and lap 42 — those were the three laps right after each pit stop.\" In ML, this is exactly how embedding lookup works: you have a vocabulary of 50,000 word vectors, and you select specific ones by their indices."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sector times for 5 laps\nsector1_times = np.array([28.1, 27.9, 28.5, 27.6, 28.0])\npit_stop_laps = np.array([0, 2, 4])  # Laps right after pit stops\n\nprint(f\"All sector 1 times: {sector1_times}\")\nprint(f\"Pit stop lap indices: {pit_stop_laps}\")\nprint(f\"Post-pit sector times: {sector1_times[pit_stop_laps]}\")\n\n# Can repeat indices — useful for duplicating data\nprint(f\"sector1_times[[0, 0, 1, 1]]: {sector1_times[[0, 0, 1, 1]]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 2D fancy indexing — select specific (lap, sensor) pairs\n# Like pulling specific data points from the telemetry grid\ntelemetry_grid = np.arange(12).reshape(3, 4)\nprint(f\"Telemetry grid (3 laps, 4 sensors):\\n{telemetry_grid}\")\n\nlap_indices = np.array([0, 1, 2])\nsensor_indices = np.array([0, 2, 3])\n\n# This selects grid[0,0], grid[1,2], grid[2,3]\nprint(f\"\\nlap_indices: {lap_indices}\")\nprint(f\"sensor_indices: {sensor_indices}\")\nprint(f\"telemetry_grid[laps, sensors]: {telemetry_grid[lap_indices, sensor_indices]}\")"
  },
  {
   "cell_type": "markdown",
   "source": "**What this means:** NumPy indexing lets you select subsets of arrays without copying data. Basic slicing creates \"views\" (same memory), while boolean and fancy indexing create copies. Understanding this distinction matters for both performance and avoiding bugs when modifying arrays.\n\n**F1 analogy:** Basic slicing is like the race engineer pointing at a section of the scrolling telemetry display — they're looking at the *same live data*, not a copy. If the data updates, they see the update. Boolean and fancy indexing are like exporting specific laps to a separate report — those are independent copies. Knowing which is which prevents the F1 equivalent of \"I updated the strategy sheet but the pit wall still shows the old numbers.\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# VISUALIZATION: Indexing Patterns - Highlight selected elements\nfig, axes = plt.subplots(2, 3, figsize=(14, 8))\n\n# Create a sample 4x5 array for visualization (4 laps, 5 sensors)\nA = np.arange(20).reshape(4, 5)\n\ndef visualize_selection(ax, A, mask, title):\n    \"\"\"Visualize array with selected elements highlighted.\"\"\"\n    rows, cols = A.shape\n    for i in range(rows):\n        for j in range(cols):\n            selected = mask[i, j] if mask.ndim == 2 else False\n            color = 'coral' if selected else 'lightgray'\n            edgecolor = 'darkred' if selected else 'gray'\n            lw = 3 if selected else 1\n            ax.add_patch(plt.Rectangle((j, rows-1-i), 0.9, 0.9, \n                                        facecolor=color, edgecolor=edgecolor, lw=lw))\n            ax.text(j + 0.45, rows - 0.55 - i, str(A[i, j]), \n                   ha='center', va='center', fontsize=11, fontweight='bold')\n    ax.set_xlim(-0.2, cols + 0.2)\n    ax.set_ylim(-0.2, rows + 0.2)\n    ax.set_title(title, fontsize=11, fontweight='bold')\n    ax.axis('off')\n\n# 1. Basic slicing: A[1:3, 2:4] — \"Laps 1-2, sensors 2-3\"\nax = axes[0, 0]\nmask = np.zeros_like(A, dtype=bool)\nmask[1:3, 2:4] = True\nvisualize_selection(ax, A, mask, 'A[1:3, 2:4]\\n(laps 1-2, sensors 2-3)')\n\n# 2. Row selection: A[2, :] — \"All sensors for lap 2\"\nax = axes[0, 1]\nmask = np.zeros_like(A, dtype=bool)\nmask[2, :] = True\nvisualize_selection(ax, A, mask, 'A[2, :]\\n(all sensors, lap 2)')\n\n# 3. Column selection: A[:, 1] — \"Sensor 1 across all laps\"\nax = axes[0, 2]\nmask = np.zeros_like(A, dtype=bool)\nmask[:, 1] = True\nvisualize_selection(ax, A, mask, 'A[:, 1]\\n(sensor 1, all laps)')\n\n# 4. Boolean indexing: A > 10 — \"All readings above threshold\"\nax = axes[1, 0]\nmask = A > 10\nvisualize_selection(ax, A, mask, 'A[A > 10]\\n(above threshold)')\n\n# 5. Fancy indexing: A[[0, 2, 3], [1, 3, 4]] — \"Specific (lap, sensor) pairs\"\nax = axes[1, 1]\nmask = np.zeros_like(A, dtype=bool)\nmask[0, 1] = True\nmask[2, 3] = True\nmask[3, 4] = True\nvisualize_selection(ax, A, mask, 'A[[0,2,3], [1,3,4]]\\n(cherry-picked readings)')\n\n# 6. Step slicing: A[::2, ::2] — \"Every other lap, every other sensor\"\nax = axes[1, 2]\nmask = np.zeros_like(A, dtype=bool)\nmask[::2, ::2] = True\nvisualize_selection(ax, A, mask, 'A[::2, ::2]\\n(downsampled telemetry)')\n\nplt.suptitle('Telemetry Indexing Patterns: Selected Readings in Red', y=1.02, fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"Telemetry grid A (4 laps x 5 sensors):\")\nprint(A)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Practical: Selecting Class Probabilities\n\nIn classification, you often need to select the probability of the true class for each sample.\n\n**F1 analogy:** Imagine a tire strategy model that predicts probabilities for each compound (soft, medium, hard) for each stint. The team knows which compound was actually used — fancy indexing picks out the predicted probability for the *actual* choice, which is exactly what cross-entropy loss needs."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tire compound prediction: model outputs probabilities for [Soft, Medium, Hard]\ntire_probs = np.array([\n    [0.1, 0.7, 0.2],  # Stint 0: model says Medium most likely\n    [0.8, 0.1, 0.1],  # Stint 1: model says Soft most likely\n    [0.3, 0.3, 0.4],  # Stint 2: model says Hard most likely\n])\n\n# Actual compounds used (0=Soft, 1=Medium, 2=Hard)\nactual_compound = np.array([1, 0, 2])\n\n# Get probability of actual compound for each stint\nstint_indices = np.arange(len(actual_compound))\npredicted_prob_of_actual = tire_probs[stint_indices, actual_compound]\n\nprint(f\"Tire compound probabilities [Soft, Medium, Hard]:\\n{tire_probs}\")\nprint(f\"\\nActual compounds used: {actual_compound}\")\nprint(f\"Stint indices: {stint_indices}\")\nprint(f\"\\nPredicted probability of actual compound: {predicted_prob_of_actual}\")\n\n# Cross-entropy loss — how wrong was the model?\nloss = -np.log(predicted_prob_of_actual).mean()\nprint(f\"Cross-entropy loss: {loss:.4f}\")"
  },
  {
   "cell_type": "code",
   "source": "# INTERACTIVE: Vary array sizes and show timing differences\n# Like processing telemetry for increasingly long races\n\nsizes = [100, 1000, 10000, 100000, 1000000]\nloop_times_by_size = []\nvec_times_by_size = []\n\nprint(\"Timing element-wise telemetry processing at different data sizes...\")\nprint(\"-\" * 60)\n\nfor size in sizes:\n    speed_data = np.random.randn(size)\n    throttle_data = np.random.randn(size)\n    \n    # Only run loop version for smaller sizes (it's too slow otherwise)\n    if size <= 100000:\n        def loop_op(a, b):\n            result = np.empty(len(a))\n            for i in range(len(a)):\n                result[i] = a[i] * b[i]\n            return result\n        t_loop, _ = time_function(loop_op, speed_data, throttle_data, n_runs=3)\n    else:\n        # Estimate based on linear scaling\n        t_loop = loop_times_by_size[-1] * (size / sizes[sizes.index(size)-1])\n    \n    t_vec, _ = time_function(lambda a, b: a * b, speed_data, throttle_data, n_runs=10)\n    \n    loop_times_by_size.append(t_loop * 1000)\n    vec_times_by_size.append(t_vec * 1000)\n    \n    speedup = t_loop / t_vec if t_vec > 0 else float('inf')\n    print(f\"Size {size:>10,}: Loop={t_loop*1000:>10.3f}ms, Vec={t_vec*1000:>8.4f}ms, Speedup={speedup:>6.0f}x\")\n\n# Plot the scaling behavior\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\nax = axes[0]\nax.loglog(sizes, loop_times_by_size, 'o-', color='coral', label='Python Loop', linewidth=2, markersize=8)\nax.loglog(sizes, vec_times_by_size, 's-', color='steelblue', label='Vectorized (NumPy)', linewidth=2, markersize=8)\nax.set_xlabel('Telemetry Samples', fontsize=11)\nax.set_ylabel('Time (ms)', fontsize=11)\nax.set_title('Telemetry Processing Time vs Data Size\\n(log-log scale)', fontsize=12, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nax = axes[1]\nspeedups_by_size = [l/v for l, v in zip(loop_times_by_size, vec_times_by_size)]\nax.semilogx(sizes, speedups_by_size, 'o-', color='green', linewidth=2, markersize=8)\nax.set_xlabel('Telemetry Samples', fontsize=11)\nax.set_ylabel('Speedup Factor (x)', fontsize=11)\nax.set_title('Vectorization Speedup vs Data Size', fontsize=12, fontweight='bold')\nax.grid(True, alpha=0.3)\nax.axhline(y=100, color='red', linestyle='--', alpha=0.5, label='100x reference')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nNote: Speedup tends to stabilize or increase with larger arrays due to better cache utilization.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**What this means:** Vectorization is the single most important optimization in NumPy. When you write `a * b` instead of a loop, NumPy executes optimized C code that processes data in chunks, uses CPU cache efficiently, and leverages SIMD (Single Instruction, Multiple Data) parallelism. This is why NumPy can be 100x faster than pure Python.\n\n**F1 analogy:** This is the difference between a race engineer manually calculating tire degradation for each lap one at a time versus the telemetry system processing all 57 laps simultaneously. During a live race, decisions happen in seconds — you simply cannot afford to loop through each data point in Python. The pit wall's real-time analytics depend on vectorized computation, just like training a neural network depends on batch-level matrix operations instead of processing one sample at a time.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 5. Vectorization: The Key to Fast NumPy\n\n**Vectorization** means replacing explicit loops with array operations. This is MUCH faster because:\n1. Operations are implemented in C\n2. Can use SIMD instructions\n3. Better memory access patterns\n\n**F1 analogy:** Consider processing telemetry for all 20 cars on the grid. The loop approach is like having one engineer read each car's data sequentially — car 1, then car 2, then car 3... The vectorized approach is like having the telemetry system process all 20 cars' speed traces in a single operation. In a 2-hour race with 300Hz sampling, that's over 40 million data points. The vectorized version finishes in milliseconds; the loop version might take minutes."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def time_function(func, *args, n_runs=10):\n    \"\"\"Time a function — like benchmarking pit stop duration.\"\"\"\n    times = []\n    for _ in range(n_runs):\n        start = time.time()\n        result = func(*args)\n        times.append(time.time() - start)\n    return np.mean(times), result\n\n\n# Compare: Element-wise multiplication of telemetry channels\nn = 1000000\nspeed_channel = np.random.randn(n)      # 1M speed readings\nthrottle_channel = np.random.randn(n)   # 1M throttle readings\n\ndef loop_multiply(a, b):\n    \"\"\"Process telemetry one sample at a time (slow!).\"\"\"\n    result = np.empty(len(a))\n    for i in range(len(a)):\n        result[i] = a[i] * b[i]\n    return result\n\ndef vectorized_multiply(a, b):\n    \"\"\"Process all telemetry samples simultaneously (fast!).\"\"\"\n    return a * b\n\nloop_time, _ = time_function(loop_multiply, speed_channel, throttle_channel, n_runs=3)\nvec_time, _ = time_function(vectorized_multiply, speed_channel, throttle_channel, n_runs=3)\n\nprint(f\"Telemetry samples: {n:,}\")\nprint(f\"Loop time: {loop_time*1000:.2f} ms\")\nprint(f\"Vectorized time: {vec_time*1000:.4f} ms\")\nprint(f\"Speedup: {loop_time/vec_time:.0f}x\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Vectorization Examples\n\n**F1 analogy:** Below we compare loop vs. vectorized versions of two operations that mirror real F1 analytics:\n1. **Pairwise distance** — comparing every car's telemetry profile against every other car's (used to find similar setups or driving styles)\n2. **Softmax** — converting raw strategy scores into probabilities across compound choices (the same function that powers attention in transformers)"
  },
  {
   "cell_type": "code",
   "source": "# VISUALIZATION: Performance Comparison Bar Charts\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Store results for visualization\noperations = ['Element-wise\\nMultiply', 'Pairwise\\nDistance', 'Softmax']\nloop_times = []\nvec_times = []\n\n# Re-run timing for visualization (smaller sizes for quick demo)\n# 1. Element-wise multiply (speed * throttle for 100k samples)\nn = 100000\nspeed_small = np.random.randn(n)\nthrottle_small = np.random.randn(n)\n\ndef loop_mult(a, b):\n    result = np.empty(len(a))\n    for i in range(len(a)):\n        result[i] = a[i] * b[i]\n    return result\n\nt1, _ = time_function(loop_mult, speed_small, throttle_small, n_runs=3)\nt2, _ = time_function(lambda a, b: a * b, speed_small, throttle_small, n_runs=3)\nloop_times.append(t1 * 1000)\nvec_times.append(t2 * 1000)\n\n# 2. Pairwise distance (comparing 50 cars' telemetry profiles)\ncar_profiles = np.random.randn(50, 10)\nt1, _ = time_function(pairwise_distance_loops, car_profiles, n_runs=3)\nt2, _ = time_function(pairwise_distance_vectorized, car_profiles, n_runs=3)\nloop_times.append(t1 * 1000)\nvec_times.append(t2 * 1000)\n\n# 3. Softmax (strategy probabilities for 500 scenarios, 50 options each)\nstrategy_scores = np.random.randn(500, 50)\nt1, _ = time_function(softmax_loops, strategy_scores, n_runs=3)\nt2, _ = time_function(softmax_vectorized, strategy_scores, n_runs=3)\nloop_times.append(t1 * 1000)\nvec_times.append(t2 * 1000)\n\n# Left plot: Absolute times (log scale)\nax = axes[0]\nx_pos = np.arange(len(operations))\nwidth = 0.35\nbars1 = ax.bar(x_pos - width/2, loop_times, width, label='Python Loop', color='coral', edgecolor='darkred')\nbars2 = ax.bar(x_pos + width/2, vec_times, width, label='Vectorized', color='steelblue', edgecolor='darkblue')\nax.set_ylabel('Time (ms)', fontsize=11)\nax.set_title('Telemetry Processing Time\\n(log scale)', fontsize=12, fontweight='bold')\nax.set_xticks(x_pos)\nax.set_xticklabels(operations)\nax.legend()\nax.set_yscale('log')\nax.grid(axis='y', alpha=0.3)\n\n# Right plot: Speedup factors\nax = axes[1]\nspeedups = [l/v for l, v in zip(loop_times, vec_times)]\ncolors = ['green' if s > 10 else 'orange' for s in speedups]\nbars = ax.bar(operations, speedups, color=colors, edgecolor='black')\nax.set_ylabel('Speedup Factor (x)', fontsize=11)\nax.set_title('Vectorization Speedup\\n(higher = faster pit stop)', fontsize=12, fontweight='bold')\nax.axhline(y=1, color='red', linestyle='--', alpha=0.5, label='Break-even')\n\n# Add speedup labels on bars\nfor bar, speedup in zip(bars, speedups):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n            f'{speedup:.0f}x', ha='center', va='bottom', fontweight='bold')\n\nax.grid(axis='y', alpha=0.3)\nax.set_ylim(0, max(speedups) * 1.2)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey Insight: Vectorization typically provides 10-100x+ speedup — the difference between\")\nprint(\"a strategy call during the race vs. one that arrives after the checkered flag!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example 1: Pairwise distance between all cars' telemetry profiles\n# Like comparing every driver's driving style against every other driver\n\ndef pairwise_distance_loops(X):\n    \"\"\"Compute pairwise distances using loops — the slow way.\"\"\"\n    n = len(X)\n    D = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            D[i, j] = np.sqrt(np.sum((X[i] - X[j])**2))\n    return D\n\ndef pairwise_distance_vectorized(X):\n    \"\"\"Compute pairwise distances using broadcasting — the fast way.\"\"\"\n    # X: (n_cars, n_features)\n    # X[:, None, :] - X[None, :, :] gives (n_cars, n_cars, n_features) differences\n    diff = X[:, np.newaxis, :] - X[np.newaxis, :, :]\n    return np.sqrt(np.sum(diff**2, axis=2))\n\n# Test: Compare telemetry profiles of 100 cars across 10 features\ncar_telemetry = np.random.randn(100, 10)  # 100 cars, 10 telemetry features\n\nloop_time, D_loop = time_function(pairwise_distance_loops, car_telemetry, n_runs=3)\nvec_time, D_vec = time_function(pairwise_distance_vectorized, car_telemetry, n_runs=3)\n\nprint(f\"Results match: {np.allclose(D_loop, D_vec)}\")\nprint(f\"Loop time: {loop_time*1000:.2f} ms\")\nprint(f\"Vectorized time: {vec_time*1000:.4f} ms\")\nprint(f\"Speedup: {loop_time/vec_time:.0f}x\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example 2: Softmax — converting raw strategy scores into probabilities\n# In F1: \"Given 100 possible strategies, what's the probability each one is optimal?\"\n\ndef softmax_loops(x):\n    \"\"\"Softmax with loops — one strategy scenario at a time.\"\"\"\n    result = np.empty_like(x)\n    for i in range(len(x)):\n        max_val = x[i].max()\n        exp_x = np.exp(x[i] - max_val)\n        result[i] = exp_x / exp_x.sum()\n    return result\n\ndef softmax_vectorized(x):\n    \"\"\"Softmax vectorized — all scenarios at once.\"\"\"\n    max_val = x.max(axis=1, keepdims=True)\n    exp_x = np.exp(x - max_val)\n    return exp_x / exp_x.sum(axis=1, keepdims=True)\n\n# Test: 1000 race scenarios, 100 possible strategies each\nstrategy_scores = np.random.randn(1000, 100)\n\nloop_time, s_loop = time_function(softmax_loops, strategy_scores)\nvec_time, s_vec = time_function(softmax_vectorized, strategy_scores)\n\nprint(f\"Results match: {np.allclose(s_loop, s_vec)}\")\nprint(f\"Loop time: {loop_time*1000:.2f} ms\")\nprint(f\"Vectorized time: {vec_time*1000:.4f} ms\")\nprint(f\"Speedup: {loop_time/vec_time:.0f}x\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Vectorization Patterns\n\n| Loop Pattern | Vectorized Version | F1 Example |\n|--------------|--------------------|------------|\n| `for i: result[i] = a[i] + b[i]` | `result = a + b` | Add fuel correction to all lap times |\n| `for i: result[i] = f(a[i])` | `result = f(a)` (if f is ufunc) | Apply tire degradation model to all readings |\n| `for i: total += a[i]` | `total = a.sum()` | Sum sector times for total lap time |\n| `for i: if cond: result[i] = x` | `result[cond] = x` | Zero out invalid sensor readings |\n| `for i,j: C[i,j] = A[i,:] @ B[:,j]` | `C = A @ B` | Batch matrix multiply for all cars' telemetry transforms |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 6. Useful NumPy Functions\n\n### Aggregation Functions\n\n**F1 analogy:** Aggregations are the race engineer's summary statistics. \"What was the average lap time? The fastest sector? The total race time? Which lap had the highest top speed?\" These are the exact same operations you use in ML to compute loss, find the best-performing model, or calculate batch statistics."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Telemetry summary: 3 stints x 4 metrics (lap_time, top_speed, avg_throttle, avg_brake)\nstint_metrics = np.random.randn(3, 4)\nprint(f\"Stint metrics (3 stints x 4 sensors):\\n{stint_metrics.round(2)}\")\n\nprint(f\"\\nsum (total across everything): {stint_metrics.sum():.2f}\")\nprint(f\"sum(axis=0) (total per metric): {stint_metrics.sum(axis=0).round(2)}\")\nprint(f\"sum(axis=1) (total per stint): {stint_metrics.sum(axis=1).round(2)}\")\n\nprint(f\"\\nmean: {stint_metrics.mean():.2f}\")\nprint(f\"std: {stint_metrics.std():.2f}\")\nprint(f\"min: {stint_metrics.min():.2f}\")\nprint(f\"max: {stint_metrics.max():.2f}\")\n\nprint(f\"\\nargmax (index of best reading overall): {stint_metrics.argmax()}\")\nprint(f\"argmax(axis=1) (best metric index per stint): {stint_metrics.argmax(axis=1)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### np.where - Conditional Selection\n\n**F1 analogy:** `np.where` is the engineer's \"if this sensor reads negative, clamp it to zero\" rule — applied to millions of readings at once. It's how ReLU activation works, and it's how F1 telemetry systems handle invalid sensor data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sensor readings that might have invalid negatives\nsensor_reading = np.array([-2, -1, 0, 1, 2])\n\n# np.where(condition, value_if_true, value_if_false)\ncleaned = np.where(sensor_reading > 0, sensor_reading, 0)  # ReLU — clamp negatives!\nprint(f\"Raw sensor: {sensor_reading}\")\nprint(f\"np.where(reading > 0, reading, 0) — ReLU: {cleaned}\")\n\n# Just get indices where condition is true — \"which laps had positive delta?\"\nvalid_indices = np.where(sensor_reading > 0)[0]\nprint(f\"Indices where reading > 0: {valid_indices}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### np.clip - Limit Values\n\n**F1 analogy:** Clipping is like the ECU's rev limiter or the FIA's fuel flow cap. No matter what the raw signal says, the output stays within safe bounds. In ML, this is gradient clipping — preventing exploding gradients from destabilizing training."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Raw throttle signal — might overshoot 0-100% range\nthrottle_raw = np.array([-5, -1, 0, 1, 5, 10])\nprint(f\"Raw throttle: {throttle_raw}\")\nprint(f\"np.clip(throttle, 0, 6) — clamped: {np.clip(throttle_raw, 0, 6)}\")\n\n# Gradient clipping — prevent exploding gradients (or wild strategy adjustments)\ngradients = np.random.randn(5) * 10\nclipped_gradients = np.clip(gradients, -1, 1)\nprint(f\"\\nGradients (raw): {gradients.round(2)}\")\nprint(f\"Clipped to [-1, 1]: {clipped_gradients.round(2)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Stacking and Concatenating\n\n**F1 analogy:** Concatenation is joining two stints of telemetry end-to-end (same sensors, more laps). Stacking is combining telemetry from two different cars side-by-side (same laps, new car axis). In ML, `concatenate` builds skip connections and `stack` creates batches."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Lap times from two stints\nstint_1_laps = np.array([91.2, 90.8, 91.5])\nstint_2_laps = np.array([92.1, 91.9, 92.4])\n\n# Concatenate — join stints end-to-end (all laps in one array)\nprint(f\"np.concatenate (all laps): {np.concatenate([stint_1_laps, stint_2_laps])}\")\n\n# Stack — create new axis (stints as separate rows)\nprint(f\"np.stack (stints as rows):\")\nprint(np.stack([stint_1_laps, stint_2_laps]))\n\nprint(f\"\\nnp.vstack (vertical — same as stack for 1D):\")\nprint(np.vstack([stint_1_laps, stint_2_laps]))\n\nprint(f\"\\nnp.hstack (horizontal — same as concatenate for 1D):\")\nprint(np.hstack([stint_1_laps, stint_2_laps]))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercises\n\n### Exercise 1: Batch Telemetry Transform\n\nIn F1, each car's telemetry goes through a transformation matrix (sensor calibration, coordinate rotation, etc.). Given telemetry for a batch of cars `A` of shape `(batch, m, n)` and a transform matrix `B` of shape `(batch, n, p)`, compute the batch matrix product.\n\nThis is the same operation used in transformer attention heads — each head applies its own learned projection to the input batch."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def batch_telemetry_transform(telemetry, transform):\n    \"\"\"\n    Batch matrix multiplication — apply per-car transforms to telemetry.\n    telemetry: (n_cars, sensors_in, time_steps)\n    transform: (n_cars, time_steps, features_out)\n    Returns: (n_cars, sensors_in, features_out)\n    \"\"\"\n    # TODO: Implement (hint: use np.einsum or @ with proper broadcasting)\n    return np.einsum('bmn,bnp->bmp', telemetry, transform)\n    # Or: return telemetry @ transform  # NumPy handles batch dimension!\n\n# Test: 32 cars, 64 sensor inputs, 128 time steps -> 32 output features\nn_cars, sensors_in, time_steps, features_out = 32, 64, 128, 32\ntelemetry_batch = np.random.randn(n_cars, sensors_in, time_steps)\ntransform_batch = np.random.randn(n_cars, time_steps, features_out)\n\nresult = batch_telemetry_transform(telemetry_batch, transform_batch)\nprint(f\"Telemetry shape: {telemetry_batch.shape}\")\nprint(f\"Transform shape: {transform_batch.shape}\")\nprint(f\"Result shape: {result.shape}\")\n\n# Verify with loop (one car at a time)\nexpected = np.stack([telemetry_batch[i] @ transform_batch[i] for i in range(n_cars)])\nprint(f\"Correct: {np.allclose(result, expected)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2: One-Hot Encode Tire Compounds\n\nIn F1 strategy models, tire compounds (Soft=0, Medium=1, Hard=2, Inter=3) need to be one-hot encoded before feeding into a neural network. Convert an array of compound labels into a one-hot matrix — without loops!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def one_hot_compound(compound_labels, num_compounds):\n    \"\"\"\n    Convert tire compound labels to one-hot encoding.\n    compound_labels: (n_stints,) array of integers (0=Soft, 1=Medium, 2=Hard, 3=Inter)\n    num_compounds: number of compound types\n    Returns: (n_stints, num_compounds) one-hot encoded\n    \"\"\"\n    # TODO: Implement without loops!\n    n = len(compound_labels)\n    result = np.zeros((n, num_compounds))\n    result[np.arange(n), compound_labels] = 1\n    return result\n\n# Test: 5 stints with different compounds\ncompound_labels = np.array([0, 2, 1, 0, 3])  # Soft, Hard, Medium, Soft, Inter\ncompound_names = ['Soft', 'Medium', 'Hard', 'Inter']\none_hot_encoded = one_hot_compound(compound_labels, num_compounds=4)\nprint(f\"Compound labels: {compound_labels}\")\nprint(f\"Compound names: {[compound_names[i] for i in compound_labels]}\")\nprint(f\"One-hot encoded:\\n{one_hot_encoded}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 3: Implement Conv2D (Naive) — Track Surface Analysis\n\nImplement a simple 2D convolution. In F1, this is like running a filter over a track surface map to detect bumps, elevation changes, or grip variations. A Sobel filter detects edges — in our case, abrupt changes in grip level across the circuit surface.\n\nThis is the fundamental operation behind Convolutional Neural Networks (CNNs)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def conv2d_track_surface(surface_map, kernel):\n    \"\"\"\n    Simple 2D convolution (no padding, stride=1).\n    surface_map: (H, W) — track surface grip levels\n    kernel: (kH, kW) — detection filter (e.g., Sobel for edge detection)\n    Returns: (H-kH+1, W-kW+1) — filtered output\n    \"\"\"\n    H, W = surface_map.shape\n    kH, kW = kernel.shape\n    out_H = H - kH + 1\n    out_W = W - kW + 1\n    \n    # TODO: Implement convolution\n    output = np.zeros((out_H, out_W))\n    for i in range(out_H):\n        for j in range(out_W):\n            output[i, j] = np.sum(surface_map[i:i+kH, j:j+kW] * kernel)\n    return output\n\n# Test with Sobel edge detection kernel on a simulated track surface\ntrack_surface = np.random.randn(10, 10)  # 10x10 grip level grid\nsobel_x = np.array([[-1, 0, 1],\n                    [-2, 0, 2],\n                    [-1, 0, 1]])  # Detects horizontal grip changes\n\ngrip_edges = conv2d_track_surface(track_surface, sobel_x)\nprint(f\"Track surface shape: {track_surface.shape}\")\nprint(f\"Sobel kernel shape: {sobel_x.shape}\")\nprint(f\"Grip edge map shape: {grip_edges.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Summary\n\n### Key Concepts\n\n| Concept | Description | Example | F1 Parallel |\n|---------|-------------|---------|-------------|\n| **Shape manipulation** | Reshape, transpose, add dims | `x.reshape(-1)`, `x.T` | Converting flat telemetry to (laps x channels) |\n| **Broadcasting** | Auto-expand dimensions | `(3,4) + (4,)` works | Applying fuel correction to all laps at once |\n| **Boolean indexing** | Select by condition | `x[x > 0]` | Filtering laps where top speed > 320 km/h |\n| **Fancy indexing** | Select by indices | `x[[0, 2, 4]]` | Cherry-picking specific pit stop laps |\n| **Vectorization** | Replace loops with array ops | 100x+ speedup | Processing all 20 cars' telemetry simultaneously |\n| **keepdims** | Preserve dimensions for broadcasting | `x.sum(axis=1, keepdims=True)` | Per-stint averages that broadcast back to all laps |\n\n### Checklist\n- [ ] I can reshape and transpose arrays (reorganize telemetry from flat to structured)\n- [ ] I understand broadcasting rules (apply corrections across all cars/laps)\n- [ ] I can use boolean and fancy indexing (filter laps, select specific stints)\n- [ ] I can vectorize loop-based code (process the whole grid at once, not car by car)"
  },
  {
   "cell_type": "markdown",
   "source": "### Connection to Deep Learning\n\n| NumPy Concept | PyTorch Equivalent | ML Application | F1 Parallel |\n|---------------|-------------------|----------------|-------------|\n| `np.array()` | `torch.tensor()` | Creating weight matrices, input data | Building telemetry arrays from sensor feeds |\n| `x.reshape()` | `x.view()` / `x.reshape()` | Flattening CNN output before FC layer | Flat telemetry stream to (laps x samples x channels) |\n| `x.T` / `x.transpose()` | `x.T` / `x.transpose()` | Converting between NHWC and NCHW formats | Switching from (laps, channels) to (channels, laps) |\n| Broadcasting | Same rules | Adding biases, batch normalization | Fuel correction to all laps, tire deg to all stints |\n| `x[x > 0]` (boolean indexing) | `x[x > 0]` | ReLU activation, masking padded tokens | Filtering laps above a threshold |\n| `x[indices]` (fancy indexing) | `x[indices]` | Embedding lookup, selecting class probs | Selecting specific pit stop laps from the log |\n| `np.sum(x, axis=1, keepdims=True)` | `x.sum(dim=1, keepdim=True)` | Softmax normalization | Per-stint totals that broadcast to all laps |\n| `np.matmul()` / `@` | `torch.matmul()` / `@` | Linear layers, attention scores | Batch telemetry transforms across all cars |\n| `np.concatenate()` | `torch.cat()` | Skip connections, feature fusion | Joining stint telemetry end-to-end |\n| `np.stack()` | `torch.stack()` | Batching sequences | Stacking all cars' data into one batch |\n| `np.where(cond, x, y)` | `torch.where(cond, x, y)` | Conditional operations, masking | Clamping invalid sensor readings |\n| `np.clip()` | `torch.clamp()` | Gradient clipping | ECU rev limiter, fuel flow cap |\n| `np.einsum()` | `torch.einsum()` | Attention mechanisms, tensor contractions | Multi-head telemetry transforms |\n| `x.mean(axis=(0,2,3))` | `x.mean(dim=(0,2,3))` | Batch normalization statistics | Per-channel sensor averages across all cars |\n\n**Key insight:** If you master NumPy, you already know 90% of PyTorch tensor operations. The main differences are: (1) PyTorch tracks gradients automatically, (2) PyTorch can run on GPU, and (3) some method names differ slightly (`axis` vs `dim`, `keepdims` vs `keepdim`). Every telemetry operation you've practiced here translates directly to building and training neural networks.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Next Steps\n\nYou've completed the Python Foundations! Next up: **Part 3: Neural Network Fundamentals**\n- Building perceptrons from scratch\n- Implementing backpropagation\n- Introduction to PyTorch\n\n**F1 connection ahead:** In Part 3, you'll build neural networks that could predict lap times from telemetry, classify tire degradation patterns, and optimize pit stop strategy — all using the NumPy skills you just mastered, extended with automatic differentiation."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}