{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Part 3.1: Perceptrons & Basic Networks â€” The Formula 1 Edition\n\nWelcome to the beginning of neural networks! In this notebook, we'll build up from a single artificial neuron (the perceptron) to a complete multi-layer network that can solve non-linear problems. By the end, you'll implement a neural network from scratch using only NumPy.\n\n**Why this matters:** Every deep learning model - from GPT to image classifiers - is built from these fundamental building blocks. Understanding perceptrons, activation functions, and forward propagation is essential before tackling backpropagation and modern architectures.\n\n**F1 analogy:** Think of a perceptron as a single sensor threshold decision on an F1 car. The tire temperature sensor reads data, applies a threshold, and outputs a binary decision: is the tire too hot? Yes or no. A full neural network is the chain from dozens of sensors through feature extraction layers to the pit wall's strategy decision. In this notebook, we build that chain from a single sensor up.\n\n---\n\n## Learning Objectives\n\nBy the end of this notebook, you should be able to:\n\n- [ ] Explain how a perceptron computes a weighted sum and applies an activation\n- [ ] Visualize decision boundaries for single neurons\n- [ ] Compare activation functions (sigmoid, tanh, ReLU, Leaky ReLU, GELU) and know when to use each\n- [ ] Explain the vanishing gradient problem and why ReLU helps\n- [ ] Trace data through forward propagation in a multi-layer network\n- [ ] Choose appropriate loss functions for regression vs classification\n- [ ] Build a 2-layer neural network from scratch in NumPy\n- [ ] Train a network to solve the XOR problem"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 1. The Biological Inspiration\n\n### From Biology to Math\n\nNeural networks are loosely inspired by how biological neurons work in the brain:\n\n| Biological Neuron | Artificial Neuron | F1 Analogy |\n|-------------------|-------------------|------------|\n| Dendrites receive signals | Input features $x_1, x_2, ..., x_n$ | Sensor readings: speed, tire temp, throttle position |\n| Synapses have varying strengths | Weights $w_1, w_2, ..., w_n$ | How much each sensor matters for the decision |\n| Cell body sums inputs | Weighted sum $z = \\sum w_i x_i + b$ | Combining all telemetry into a single risk score |\n| Axon fires if threshold exceeded | Activation function $a = f(z)$ | Engineer decides: pit now or stay out |\n\n**Important caveat:** While inspired by biology, artificial neural networks are primarily a mathematical framework. We won't dwell on the biology - let's focus on the math that actually makes them work!\n\n**F1 analogy:** A single neuron is like a single sensor threshold decision. The tire temperature sensor reads 105C (input), multiplies by a calibration weight, adds a bias offset, and if the result exceeds a threshold, it fires a warning: \"tire degradation critical.\" That is exactly what a perceptron does."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Biological vs Artificial Neuron\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Biological neuron (simplified diagram)\n",
    "ax = axes[0]\n",
    "ax.set_xlim(-2, 6)\n",
    "ax.set_ylim(-2, 4)\n",
    "\n",
    "# Dendrites (inputs)\n",
    "for i, y in enumerate([3, 2, 1, 0]):\n",
    "    ax.annotate('', xy=(1, 1.5), xytext=(-1, y),\n",
    "                arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "    ax.text(-1.5, y, f'Input {i+1}', fontsize=10, va='center')\n",
    "\n",
    "# Cell body\n",
    "circle = plt.Circle((2, 1.5), 0.8, color='green', alpha=0.5)\n",
    "ax.add_patch(circle)\n",
    "ax.text(2, 1.5, 'Sum', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Axon (output)\n",
    "ax.annotate('', xy=(5, 1.5), xytext=(2.8, 1.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=3))\n",
    "ax.text(5.2, 1.5, 'Output', fontsize=10, va='center')\n",
    "\n",
    "ax.set_title('Biological Neuron (Simplified)', fontsize=14, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "# Right: Artificial neuron (mathematical)\n",
    "ax = axes[1]\n",
    "ax.set_xlim(-2, 8)\n",
    "ax.set_ylim(-1, 5)\n",
    "\n",
    "# Inputs with weights\n",
    "inputs = ['$x_1$', '$x_2$', '$x_3$', '$x_n$']\n",
    "weights = ['$w_1$', '$w_2$', '$w_3$', '$w_n$']\n",
    "y_positions = [4, 3, 2, 0.5]\n",
    "\n",
    "for i, (inp, w, y) in enumerate(zip(inputs, weights, y_positions)):\n",
    "    ax.annotate('', xy=(2.5, 2.25), xytext=(0, y),\n",
    "                arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "    ax.text(-0.5, y, inp, fontsize=12, va='center')\n",
    "    ax.text(1.2, (y + 2.25)/2 + 0.2, w, fontsize=10, color='purple')\n",
    "\n",
    "# Dots for \"...\"\n",
    "ax.text(-0.3, 1.2, '...', fontsize=14, va='center')\n",
    "\n",
    "# Summation node\n",
    "circle = plt.Circle((3, 2.25), 0.5, color='green', alpha=0.5)\n",
    "ax.add_patch(circle)\n",
    "ax.text(3, 2.25, '$\\Sigma$', ha='center', va='center', fontsize=16)\n",
    "\n",
    "# Bias\n",
    "ax.annotate('', xy=(3, 1.75), xytext=(3, 0),\n",
    "            arrowprops=dict(arrowstyle='->', color='orange', lw=2))\n",
    "ax.text(3, -0.3, '$b$ (bias)', ha='center', fontsize=10, color='orange')\n",
    "\n",
    "# Linear output\n",
    "ax.annotate('', xy=(5, 2.25), xytext=(3.5, 2.25),\n",
    "            arrowprops=dict(arrowstyle='->', color='gray', lw=2))\n",
    "ax.text(4.3, 2.6, '$z$', fontsize=12)\n",
    "\n",
    "# Activation function\n",
    "rect = plt.Rectangle((5, 1.75), 1, 1, color='red', alpha=0.3)\n",
    "ax.add_patch(rect)\n",
    "ax.text(5.5, 2.25, '$f$', ha='center', va='center', fontsize=14)\n",
    "\n",
    "# Output\n",
    "ax.annotate('', xy=(7.5, 2.25), xytext=(6, 2.25),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=3))\n",
    "ax.text(7.7, 2.25, '$a = f(z)$', fontsize=12, va='center')\n",
    "\n",
    "ax.set_title('Artificial Neuron (Perceptron)', fontsize=14, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key equation: z = w1*x1 + w2*x2 + ... + wn*xn + b = w . x + b\")\n",
    "print(\"Output:       a = f(z)  where f is the activation function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 2. Single Neuron (Perceptron)\n\n### The Fundamental Computation\n\nA single neuron performs two operations:\n\n1. **Linear transformation:** Compute a weighted sum of inputs plus a bias\n2. **Non-linear activation:** Apply a function to produce the output\n\n### The Weighted Sum\n\n$$z = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b = \\mathbf{w} \\cdot \\mathbf{x} + b$$\n\n#### Breaking down the formula:\n\n| Component | Meaning | Role | F1 Analogy |\n|-----------|---------|------|------------|\n| $\\mathbf{x}$ | Input features | The data we're processing | Sensor readings: `[tire_temp, brake_temp]` |\n| $\\mathbf{w}$ | Weights | How much each input matters (learned) | Calibration factors for each sensor |\n| $b$ | Bias | Shifts the decision boundary (learned) | Baseline threshold offset |\n| $z$ | Pre-activation | The raw linear combination | Combined risk score before the go/no-go decision |\n\n**What this means:** The weighted sum tells you \"how strongly does this input match the pattern I'm looking for?\" Large positive $z$ means strong match, large negative means opposite of pattern, near zero means uncertain.\n\n**F1 analogy:** Imagine computing a \"pit stop urgency score.\" You take tire temperature (weighted heavily), brake wear (weighted moderately), and fuel load (weighted lightly), combine them into a single number. A high score means \"pit now,\" a low score means \"stay out.\" The weights encode which factors matter most for this decision."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(x, w, b):\n",
    "    \"\"\"\n",
    "    Compute the weighted sum (pre-activation).\n",
    "    \n",
    "    Args:\n",
    "        x: Input vector (n,)\n",
    "        w: Weight vector (n,)\n",
    "        b: Bias scalar\n",
    "    \n",
    "    Returns:\n",
    "        z: Pre-activation value\n",
    "    \"\"\"\n",
    "    return np.dot(w, x) + b\n",
    "\n",
    "# Example: A simple 2-input neuron\n",
    "x = np.array([2, 3])       # Input features\n",
    "w = np.array([0.5, -0.3])  # Weights\n",
    "b = 0.1                     # Bias\n",
    "\n",
    "z = weighted_sum(x, w, b)\n",
    "\n",
    "print(\"Input x:\", x)\n",
    "print(\"Weights w:\", w)\n",
    "print(\"Bias b:\", b)\n",
    "print(f\"\\nWeighted sum z = w . x + b\")\n",
    "print(f\"z = ({w[0]} * {x[0]}) + ({w[1]} * {x[1]}) + {b}\")\n",
    "print(f\"z = {w[0]*x[0]} + {w[1]*x[1]} + {b}\")\n",
    "print(f\"z = {z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Decision Boundary\n",
    "\n",
    "A single neuron with 2 inputs creates a **linear decision boundary** - a line that separates two regions in the input space.\n",
    "\n",
    "The boundary occurs where $z = 0$, which means:\n",
    "$$w_1 x_1 + w_2 x_2 + b = 0$$\n",
    "\n",
    "Solving for $x_2$:\n",
    "$$x_2 = -\\frac{w_1}{w_2} x_1 - \\frac{b}{w_2}$$\n",
    "\n",
    "This is just a line with slope $-w_1/w_2$ and intercept $-b/w_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Decision boundary of a single neuron\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Different weight/bias combinations\n",
    "configs = [\n",
    "    {'w': np.array([1, 1]), 'b': 0, 'title': 'w=[1,1], b=0'},\n",
    "    {'w': np.array([1, 1]), 'b': -1, 'title': 'w=[1,1], b=-1 (shifted)'},\n",
    "    {'w': np.array([2, 1]), 'b': 0, 'title': 'w=[2,1], b=0 (rotated)'}\n",
    "]\n",
    "\n",
    "x_range = np.linspace(-3, 3, 100)\n",
    "\n",
    "for ax, config in zip(axes, configs):\n",
    "    w, b = config['w'], config['b']\n",
    "    \n",
    "    # Create a grid of points\n",
    "    xx, yy = np.meshgrid(x_range, x_range)\n",
    "    Z = w[0] * xx + w[1] * yy + b\n",
    "    \n",
    "    # Plot the regions\n",
    "    ax.contourf(xx, yy, Z, levels=[-100, 0, 100], colors=['lightblue', 'lightsalmon'], alpha=0.5)\n",
    "    \n",
    "    # Plot the decision boundary (z = 0)\n",
    "    ax.contour(xx, yy, Z, levels=[0], colors='black', linewidths=2)\n",
    "    \n",
    "    # Labels\n",
    "    ax.text(2, 2, 'z > 0', fontsize=12, color='red', fontweight='bold')\n",
    "    ax.text(-2, -2, 'z < 0', fontsize=12, color='blue', fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('$x_1$', fontsize=12)\n",
    "    ax.set_ylabel('$x_2$', fontsize=12)\n",
    "    ax.set_title(config['title'], fontsize=12, fontweight='bold')\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-3, 3)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: Weights control the orientation, bias shifts the boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Why We Need Activation Functions\n\nWithout activation functions, stacking multiple layers would be pointless:\n\n$$\\text{Layer 1: } z_1 = W_1 x + b_1$$\n$$\\text{Layer 2: } z_2 = W_2 z_1 + b_2 = W_2(W_1 x + b_1) + b_2 = (W_2 W_1)x + (W_2 b_1 + b_2)$$\n\nThis is just another linear transformation! We could replace it with a single layer.\n\n**What this means:** Without non-linearity, no matter how many layers we stack, we can only learn linear relationships. Activation functions break this limitation.\n\n**F1 analogy:** Tire grip versus temperature is not linear. At low temperatures, grip increases as the rubber warms up. At optimal temperature, grip is at its peak. Beyond that, grip drops off sharply as the tire overheats. This nonlinear S-curve relationship is exactly the kind of pattern that activation functions let neural networks capture. A purely linear model would draw a straight line through this curve and miss the critical \"cliff edge\" where grip falls off.\n\n| Problem Type | Linear Model Can Solve? | Needs Non-linearity? |\n|--------------|------------------------|---------------------|\n| AND gate | Yes | No |\n| OR gate | Yes | No |\n| XOR gate | **No** | **Yes** |\n| Image classification | No | Yes |\n| Language understanding | No | Yes |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Why XOR needs non-linearity\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# AND gate (linearly separable)\n",
    "ax = axes[0]\n",
    "and_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "and_labels = np.array([0, 0, 0, 1])  # Only (1,1) is True\n",
    "\n",
    "ax.scatter(and_data[and_labels==0, 0], and_data[and_labels==0, 1], \n",
    "           c='blue', s=200, marker='o', label='0 (False)', edgecolors='black')\n",
    "ax.scatter(and_data[and_labels==1, 0], and_data[and_labels==1, 1], \n",
    "           c='red', s=200, marker='s', label='1 (True)', edgecolors='black')\n",
    "\n",
    "# Decision boundary\n",
    "x_line = np.linspace(-0.5, 1.5, 100)\n",
    "ax.plot(x_line, 1.5 - x_line, 'g--', lw=2, label='Decision boundary')\n",
    "ax.fill_between(x_line, 1.5 - x_line, 2, alpha=0.2, color='red')\n",
    "\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "ax.set_xlabel('$x_1$', fontsize=12)\n",
    "ax.set_ylabel('$x_2$', fontsize=12)\n",
    "ax.set_title('AND Gate (Linearly Separable)', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# OR gate (linearly separable)\n",
    "ax = axes[1]\n",
    "or_labels = np.array([0, 1, 1, 1])  # Only (0,0) is False\n",
    "\n",
    "ax.scatter(and_data[or_labels==0, 0], and_data[or_labels==0, 1], \n",
    "           c='blue', s=200, marker='o', label='0 (False)', edgecolors='black')\n",
    "ax.scatter(and_data[or_labels==1, 0], and_data[or_labels==1, 1], \n",
    "           c='red', s=200, marker='s', label='1 (True)', edgecolors='black')\n",
    "\n",
    "ax.plot(x_line, 0.5 - x_line, 'g--', lw=2, label='Decision boundary')\n",
    "ax.fill_between(x_line, 0.5 - x_line, 2, alpha=0.2, color='red')\n",
    "\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "ax.set_xlabel('$x_1$', fontsize=12)\n",
    "ax.set_ylabel('$x_2$', fontsize=12)\n",
    "ax.set_title('OR Gate (Linearly Separable)', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# XOR gate (NOT linearly separable)\n",
    "ax = axes[2]\n",
    "xor_labels = np.array([0, 1, 1, 0])  # (0,1) and (1,0) are True\n",
    "\n",
    "ax.scatter(and_data[xor_labels==0, 0], and_data[xor_labels==0, 1], \n",
    "           c='blue', s=200, marker='o', label='0 (False)', edgecolors='black')\n",
    "ax.scatter(and_data[xor_labels==1, 0], and_data[xor_labels==1, 1], \n",
    "           c='red', s=200, marker='s', label='1 (True)', edgecolors='black')\n",
    "\n",
    "# Show that no single line works\n",
    "ax.plot(x_line, 0.5 - x_line, 'gray', lw=1, alpha=0.5, linestyle='--')\n",
    "ax.plot(x_line, 1.5 - x_line, 'gray', lw=1, alpha=0.5, linestyle='--')\n",
    "ax.text(0.5, 1.3, 'No single line\\ncan separate!', ha='center', fontsize=10, color='darkred')\n",
    "\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "ax.set_xlabel('$x_1$', fontsize=12)\n",
    "ax.set_ylabel('$x_2$', fontsize=12)\n",
    "ax.set_title('XOR Gate (NOT Linearly Separable)', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"XOR is the classic example showing why neural networks need non-linear activations.\")\n",
    "print(\"We'll solve XOR at the end of this notebook!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 3. Activation Functions\n\n### Intuition\n\nActivation functions introduce non-linearity, allowing neural networks to learn complex patterns. Think of them as \"decision makers\" that determine how strongly a neuron should \"fire\" based on its input.\n\nDifferent activation functions have different properties that make them suitable for different situations.\n\n**F1 analogy:** Each activation function is like a different type of nonlinear response in a race car. Sigmoid is like tire grip as a function of temperature -- it saturates at both extremes (cold = no grip, overheated = no grip, with a smooth transition between). ReLU is like the throttle response: below a threshold nothing happens, above it the response is proportional. Leaky ReLU is like engine braking -- even when you lift off the throttle, there is a small negative force (the \"leak\")."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all major activation functions\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation: squashes to (0, 1)\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Derivative of sigmoid\"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh_activation(z):\n",
    "    \"\"\"Tanh activation: squashes to (-1, 1)\"\"\"\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    \"\"\"Derivative of tanh\"\"\"\n",
    "    return 1 - np.tanh(z)**2\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation: max(0, z)\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"Derivative of ReLU\"\"\"\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def leaky_relu(z, alpha=0.01):\n",
    "    \"\"\"Leaky ReLU: allows small negative values\"\"\"\n",
    "    return np.where(z > 0, z, alpha * z)\n",
    "\n",
    "def leaky_relu_derivative(z, alpha=0.01):\n",
    "    \"\"\"Derivative of Leaky ReLU\"\"\"\n",
    "    return np.where(z > 0, 1, alpha)\n",
    "\n",
    "def gelu(z):\n",
    "    \"\"\"GELU activation: smooth approximation used in transformers\"\"\"\n",
    "    return 0.5 * z * (1 + np.tanh(np.sqrt(2/np.pi) * (z + 0.044715 * z**3)))\n",
    "\n",
    "def gelu_derivative(z):\n",
    "    \"\"\"Approximate derivative of GELU\"\"\"\n",
    "    # Numerical derivative for simplicity\n",
    "    eps = 1e-7\n",
    "    return (gelu(z + eps) - gelu(z - eps)) / (2 * eps)\n",
    "\n",
    "print(\"Activation functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: All activation functions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "activations = [\n",
    "    ('Sigmoid', sigmoid, sigmoid_derivative, 'Squashes to (0,1)\\nUsed for binary output'),\n",
    "    ('Tanh', tanh_activation, tanh_derivative, 'Squashes to (-1,1)\\nZero-centered'),\n",
    "    ('ReLU', relu, relu_derivative, 'max(0, z)\\nMost popular hidden layer'),\n",
    "    ('Leaky ReLU', leaky_relu, leaky_relu_derivative, 'Allows small negatives\\nFixes \"dying ReLU\"'),\n",
    "    ('GELU', gelu, gelu_derivative, 'Smooth ReLU variant\\nUsed in transformers')\n",
    "]\n",
    "\n",
    "for ax, (name, func, deriv, desc) in zip(axes.flat[:5], activations):\n",
    "    # Plot activation\n",
    "    ax.plot(z, func(z), 'b-', lw=2.5, label='Activation')\n",
    "    # Plot derivative\n",
    "    ax.plot(z, deriv(z), 'r--', lw=2, label='Derivative')\n",
    "    \n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "    ax.set_xlabel('z (input)', fontsize=11)\n",
    "    ax.set_ylabel('f(z) / f\\'(z)', fontsize=11)\n",
    "    ax.set_title(f'{name}', fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='best')\n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.set_ylim(-1.5, 2)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add description\n",
    "    ax.text(0.02, 0.02, desc, transform=ax.transAxes, fontsize=9,\n",
    "            verticalalignment='bottom', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Remove the 6th subplot\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Activation Function Comparison\n\n| Activation | Formula | Range | Derivative Range | Pros | Cons | When to Use |\n|------------|---------|-------|------------------|------|------|-------------|\n| **Sigmoid** | $\\frac{1}{1+e^{-z}}$ | (0, 1) | (0, 0.25] | Output as probability | Vanishing gradient | Output layer (binary) |\n| **Tanh** | $\\frac{e^z - e^{-z}}{e^z + e^{-z}}$ | (-1, 1) | (0, 1] | Zero-centered | Vanishing gradient | RNNs, hidden layers |\n| **ReLU** | $\\max(0, z)$ | [0, inf) | {0, 1} | Fast, no vanishing grad | Dead neurons | Default for hidden |\n| **Leaky ReLU** | $\\max(\\alpha z, z)$ | (-inf, inf) | {alpha, 1} | No dead neurons | Extra hyperparameter | When ReLU fails |\n| **GELU** | $z \\cdot \\Phi(z)$ | approx (-0.17, inf) | smooth | Smooth, probabilistic | Slower to compute | Transformers |\n\n**F1 analogy:** Choosing an activation function is like choosing a tire compound. Soft compounds (sigmoid/tanh) give nuanced response but degrade quickly at extremes (vanishing gradients). Hard compounds (ReLU) are durable and fast but binary -- they either grip or they do not. Medium compounds (GELU, Leaky ReLU) try to find the best compromise for the conditions."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Deep Dive: The Vanishing Gradient Problem\n\nWhen training deep networks with sigmoid or tanh activations, gradients can become extremely small as they propagate backward through layers.\n\n#### Why it happens:\n\n1. **Sigmoid derivative is small:** Maximum value is 0.25 (at z=0)\n2. **Gradients multiply:** In backpropagation, gradients from each layer multiply together\n3. **Exponential decay:** With n layers: gradient ~ $(0.25)^n$ in worst case\n\n| Layers | Max Gradient (sigmoid) | Practical Impact |\n|--------|------------------------|------------------|\n| 2 | 0.0625 | Manageable |\n| 5 | ~0.001 | Learning slows |\n| 10 | ~0.0000001 | Almost no learning |\n\n**F1 analogy:** Imagine tracing a lap time problem back through the car's systems. The pit wall sees a 0.3s time loss. They attribute it to lower corner exit speed, which came from reduced rear grip, which came from tire overheating, which came from an aggressive differential setting. At each step in this chain, the \"blame signal\" gets weaker. By the time you trace it back to the differential, the signal is so diluted you cannot tell if it mattered. That is the vanishing gradient problem -- the feedback signal dies before it reaches the parameters that need adjusting.\n\n#### Key Insight\n\nReLU solves this because its derivative is exactly 1 for positive inputs - gradients flow unchanged! This is why ReLU became the default activation for deep networks.\n\n#### Common Misconceptions\n\n| Misconception | Reality |\n|---------------|--------|\n| \"Sigmoid is bad\" | Sigmoid is fine for output layers in binary classification |\n| \"ReLU has no problems\" | ReLU can cause \"dead neurons\" that never activate |\n| \"Deeper is always better\" | Without proper activations, deeper can be worse |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Vanishing gradient demonstration\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Show how sigmoid derivative is small\n",
    "ax = axes[0]\n",
    "z = np.linspace(-6, 6, 200)\n",
    "\n",
    "ax.plot(z, sigmoid(z), 'b-', lw=2, label='Sigmoid')\n",
    "ax.plot(z, sigmoid_derivative(z), 'r-', lw=2, label='Sigmoid derivative')\n",
    "ax.axhline(y=0.25, color='r', linestyle='--', alpha=0.5)\n",
    "ax.text(3, 0.27, 'Max derivative = 0.25', color='r', fontsize=10)\n",
    "\n",
    "# Shade the \"saturation regions\"\n",
    "ax.fill_between(z[z < -3], 0, sigmoid_derivative(z[z < -3]), alpha=0.3, color='gray')\n",
    "ax.fill_between(z[z > 3], 0, sigmoid_derivative(z[z > 3]), alpha=0.3, color='gray')\n",
    "ax.text(-4.5, 0.1, 'Saturated\\n(gradient~0)', ha='center', fontsize=9, color='gray')\n",
    "ax.text(4.5, 0.1, 'Saturated\\n(gradient~0)', ha='center', fontsize=9, color='gray')\n",
    "\n",
    "ax.set_xlabel('z', fontsize=12)\n",
    "ax.set_ylabel('Value', fontsize=12)\n",
    "ax.set_title('Sigmoid Saturation Problem', fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Show gradient decay through layers\n",
    "ax = axes[1]\n",
    "layers = np.arange(1, 11)\n",
    "\n",
    "# Sigmoid gradient decay (assuming derivative ~ 0.25 at each layer)\n",
    "sigmoid_grad = 0.25 ** layers\n",
    "# ReLU gradient (stays at 1 for positive activations)\n",
    "relu_grad = 1.0 ** layers\n",
    "\n",
    "ax.semilogy(layers, sigmoid_grad, 'b-o', lw=2, markersize=8, label='Sigmoid (worst case)')\n",
    "ax.semilogy(layers, relu_grad, 'r-s', lw=2, markersize=8, label='ReLU (positive path)')\n",
    "\n",
    "ax.axhline(y=1e-6, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.text(8, 2e-6, 'Effectively zero', fontsize=9, color='gray')\n",
    "\n",
    "ax.set_xlabel('Number of Layers', fontsize=12)\n",
    "ax.set_ylabel('Gradient Magnitude (log scale)', fontsize=12)\n",
    "ax.set_title('Gradient Flow Through Layers', fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.set_xlim(1, 10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"This is why deep networks struggled until ReLU became popular around 2012!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive: Compare activations on the same input\n",
    "z_values = np.array([-3, -1, 0, 1, 3])\n",
    "\n",
    "print(\"Comparing activation outputs for different inputs:\\n\")\n",
    "print(f\"{'z':>6} | {'Sigmoid':>8} | {'Tanh':>8} | {'ReLU':>8} | {'LeakyReLU':>10} | {'GELU':>8}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for z_val in z_values:\n",
    "    print(f\"{z_val:>6} | {sigmoid(z_val):>8.4f} | {tanh_activation(z_val):>8.4f} | {relu(z_val):>8.4f} | {leaky_relu(z_val):>10.4f} | {gelu(z_val):>8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Why This Matters in Machine Learning\n\n| Application | Activation Choice | Reason | F1 Parallel |\n|-------------|-------------------|--------|-------------|\n| Binary classification output | Sigmoid | Outputs probability in (0,1) | Pit/no-pit probability |\n| Multi-class output | Softmax | Outputs probability distribution | Tire compound selection probabilities |\n| Hidden layers (CNNs) | ReLU | Fast, no vanishing gradient | Quick sensor threshold checks |\n| Hidden layers (RNNs) | Tanh | Zero-centered, bounded | Smoothly varying telemetry signals |\n| Transformers (BERT, GPT) | GELU | Smooth, probabilistic | Nuanced strategy weighting |\n| GANs | Leaky ReLU | Prevents dead neurons | Never fully ignoring a sensor reading |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 4. Forward Propagation\n\n### Intuition\n\nForward propagation is simply passing data through the network layer by layer:\n\n**Input** --> **Layer 1** --> **Layer 2** --> ... --> **Output**\n\nEach layer performs: **Linear transformation** --> **Activation function**\n\n**F1 analogy:** Forward propagation is data flowing from car sensors through the telemetry pipeline to the pit wall. Raw sensor data (speed, tire temps, fuel load) enters the system. Layer 1 extracts features: \"is the car understeering?\", \"is the tire degrading faster than expected?\" Layer 2 combines those features into higher-level assessments: \"should we change strategy?\" The final output is the decision the pit wall communicates to the driver. Each layer adds understanding, just like each stage in the telemetry pipeline.\n\n### Single Layer Forward Pass\n\nFor one layer with input $\\mathbf{x}$, weights $W$, and bias $\\mathbf{b}$:\n\n1. **Linear step:** $\\mathbf{z} = W\\mathbf{x} + \\mathbf{b}$\n2. **Activation step:** $\\mathbf{a} = f(\\mathbf{z})$\n\nwhere $f$ is the activation function."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_layer(x, W, b, activation_fn):\n",
    "    \"\"\"\n",
    "    Forward pass through a single layer.\n",
    "    \n",
    "    Args:\n",
    "        x: Input vector/matrix (features, ) or (features, samples)\n",
    "        W: Weight matrix (output_size, input_size)\n",
    "        b: Bias vector (output_size, )\n",
    "        activation_fn: Activation function to apply\n",
    "    \n",
    "    Returns:\n",
    "        a: Activated output\n",
    "        z: Pre-activation (for backprop, stored in cache)\n",
    "    \"\"\"\n",
    "    z = np.dot(W, x) + b.reshape(-1, 1) if x.ndim > 1 else np.dot(W, x) + b\n",
    "    a = activation_fn(z)\n",
    "    return a, z\n",
    "\n",
    "# Example: Single layer with 3 inputs, 2 outputs\n",
    "x = np.array([1.0, 2.0, 3.0])  # 3 input features\n",
    "W = np.array([[0.1, 0.2, 0.3],   # 2 output neurons\n",
    "              [0.4, 0.5, 0.6]])\n",
    "b = np.array([0.1, 0.2])\n",
    "\n",
    "a, z = forward_layer(x, W, b, relu)\n",
    "\n",
    "print(\"Single Layer Forward Pass:\")\n",
    "print(f\"Input x: {x}\")\n",
    "print(f\"\\nWeights W:\\n{W}\")\n",
    "print(f\"\\nBias b: {b}\")\n",
    "print(f\"\\nPre-activation z = Wx + b: {z}\")\n",
    "print(f\"Activated output a = ReLU(z): {a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Data flowing through a network\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Network architecture: 3 -> 4 -> 2\n",
    "layer_sizes = [3, 4, 2]\n",
    "layer_names = ['Input\\n(3 features)', 'Hidden\\n(4 neurons)', 'Output\\n(2 neurons)']\n",
    "x_positions = [0, 2.5, 5]\n",
    "\n",
    "# Draw neurons\n",
    "neuron_positions = []\n",
    "for layer_idx, (n_neurons, x_pos) in enumerate(zip(layer_sizes, x_positions)):\n",
    "    y_positions = np.linspace(0, 6, n_neurons + 2)[1:-1]  # Evenly space neurons\n",
    "    positions = [(x_pos, y) for y in y_positions]\n",
    "    neuron_positions.append(positions)\n",
    "    \n",
    "    for (x, y) in positions:\n",
    "        color = ['lightblue', 'lightgreen', 'lightsalmon'][layer_idx]\n",
    "        circle = plt.Circle((x, y), 0.3, color=color, ec='black', linewidth=2)\n",
    "        ax.add_patch(circle)\n",
    "\n",
    "# Draw connections\n",
    "for layer_idx in range(len(layer_sizes) - 1):\n",
    "    for (x1, y1) in neuron_positions[layer_idx]:\n",
    "        for (x2, y2) in neuron_positions[layer_idx + 1]:\n",
    "            ax.annotate('', xy=(x2 - 0.3, y2), xytext=(x1 + 0.3, y1),\n",
    "                       arrowprops=dict(arrowstyle='->', color='gray', lw=0.5, alpha=0.5))\n",
    "\n",
    "# Labels\n",
    "for layer_idx, (x_pos, name) in enumerate(zip(x_positions, layer_names)):\n",
    "    ax.text(x_pos, -0.8, name, ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Show the math\n",
    "ax.text(1.25, 7, r'$z^{[1]} = W^{[1]}x + b^{[1]}$', fontsize=12, ha='center')\n",
    "ax.text(1.25, 6.5, r'$a^{[1]} = \\text{ReLU}(z^{[1]})$', fontsize=12, ha='center')\n",
    "\n",
    "ax.text(3.75, 7, r'$z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$', fontsize=12, ha='center')\n",
    "ax.text(3.75, 6.5, r'$a^{[2]} = \\sigma(z^{[2]})$', fontsize=12, ha='center')\n",
    "\n",
    "# Arrows for flow\n",
    "ax.annotate('', xy=(1.5, 3), xytext=(0.5, 3),\n",
    "           arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "ax.annotate('', xy=(4.0, 3), xytext=(3.0, 3),\n",
    "           arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "\n",
    "ax.set_xlim(-1, 6)\n",
    "ax.set_ylim(-1.5, 8)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Forward Propagation: Data Flows Left to Right', fontsize=14, fontweight='bold', y=1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Show actual values flowing through\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize a small network: 2 inputs -> 3 hidden -> 1 output\n",
    "W1 = np.array([[0.5, -0.5],\n",
    "               [0.3, 0.3],\n",
    "               [-0.2, 0.6]])\n",
    "b1 = np.array([0.1, 0.0, -0.1])\n",
    "\n",
    "W2 = np.array([[0.4, -0.3, 0.5]])\n",
    "b2 = np.array([0.1])\n",
    "\n",
    "# Input\n",
    "x = np.array([1.0, 0.5])\n",
    "\n",
    "# Forward pass - Layer 1\n",
    "z1 = np.dot(W1, x) + b1\n",
    "a1 = relu(z1)\n",
    "\n",
    "# Forward pass - Layer 2\n",
    "z2 = np.dot(W2, a1) + b2\n",
    "a2 = sigmoid(z2)  # Sigmoid for final output\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FORWARD PROPAGATION - Tracing Values Through the Network\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nINPUT LAYER:\")\n",
    "print(f\"  x = {x}\")\n",
    "\n",
    "print(f\"\\nHIDDEN LAYER (ReLU activation):\")\n",
    "print(f\"  z1 = W1 @ x + b1\")\n",
    "print(f\"  z1 = {z1}\")\n",
    "print(f\"  a1 = ReLU(z1) = {a1}\")\n",
    "\n",
    "print(f\"\\nOUTPUT LAYER (Sigmoid activation):\")\n",
    "print(f\"  z2 = W2 @ a1 + b2\")\n",
    "print(f\"  z2 = {z2}\")\n",
    "print(f\"  a2 = sigmoid(z2) = {a2}\")\n",
    "\n",
    "print(f\"\\nFINAL OUTPUT: {a2[0]:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: Notation Conventions\n",
    "\n",
    "In neural network literature, you'll see consistent notation:\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|--------|\n",
    "| $L$ | Number of layers (not counting input) |\n",
    "| $n^{[l]}$ | Number of neurons in layer $l$ |\n",
    "| $W^{[l]}$ | Weight matrix for layer $l$, shape $(n^{[l]}, n^{[l-1]})$ |\n",
    "| $b^{[l]}$ | Bias vector for layer $l$, shape $(n^{[l]},)$ |\n",
    "| $z^{[l]}$ | Pre-activation at layer $l$ |\n",
    "| $a^{[l]}$ | Activation (output) at layer $l$ |\n",
    "| $a^{[0]}$ | Input $x$ (by convention) |\n",
    "\n",
    "**Key insight:** The superscript $[l]$ denotes the layer number. This is different from superscript $(i)$ which denotes the training example number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 5. Loss Functions\n\n### Intuition\n\nA loss function measures \"how wrong\" the network's predictions are. During training, we minimize this loss to improve predictions.\n\n**Key principle:** Different problems need different loss functions.\n\n**F1 analogy:** The loss function is the lap time delta from the theoretical best. If your car crosses the line 0.3s behind the ideal lap, your \"loss\" is 0.3. The entire engineering effort -- setup changes, strategy adjustments, driver coaching -- is aimed at minimizing this delta. Lower is always better. For regression problems (predicting lap times), we use MSE. For classification problems (will it rain? yes/no), we use cross-entropy.\n\n| Problem Type | Example | Loss Function | F1 Parallel |\n|--------------|---------|---------------|-------------|\n| Regression | Predict house price | Mean Squared Error (MSE) | Predicting lap time delta |\n| Binary Classification | Cat vs Dog | Binary Cross-Entropy | Rain or dry? Pit or stay? |\n| Multi-class Classification | Digit recognition | Categorical Cross-Entropy | Which tire compound to choose? |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Mean Squared Error (MSE) for Regression\n\n**Intuition:** Penalize predictions that are far from the true value. Squaring makes large errors hurt more.\n\n$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n\n#### Breaking down the formula:\n\n| Component | Meaning | F1 Analogy |\n|-----------|--------|------------|\n| $y_i$ | True value for example $i$ | Actual lap time |\n| $\\hat{y}_i$ | Predicted value for example $i$ | Model's predicted lap time |\n| $(y_i - \\hat{y}_i)^2$ | Squared error (always positive) | Squared time delta |\n| $\\frac{1}{n}$ | Average over all examples | Average over all laps |\n\n**What this means:** MSE = 0 means perfect predictions. Larger MSE means worse predictions.\n\n**F1 analogy:** If your model predicts lap times, MSE penalizes big misses much more than small ones. Predicting a 1:32.0 when the actual time is 1:32.1 costs you $0.1^2 = 0.01$. But predicting 1:34.0 costs $2.0^2 = 4.0$. That large error is 400 times more costly, which forces the model to eliminate the big prediction mistakes first."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Squared Error loss.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True values\n",
    "        y_pred: Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        MSE loss value\n",
    "    \"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    \"\"\"Derivative of MSE with respect to predictions.\"\"\"\n",
    "    n = len(y_true)\n",
    "    return 2 * (y_pred - y_true) / n\n",
    "\n",
    "# Example\n",
    "y_true = np.array([3.0, 5.0, 2.0, 7.0])\n",
    "y_pred = np.array([2.5, 5.5, 2.1, 6.0])\n",
    "\n",
    "print(\"MSE Loss Example:\")\n",
    "print(f\"True values:      {y_true}\")\n",
    "print(f\"Predicted values: {y_pred}\")\n",
    "print(f\"Errors:           {y_true - y_pred}\")\n",
    "print(f\"Squared errors:   {(y_true - y_pred)**2}\")\n",
    "print(f\"MSE Loss:         {mse_loss(y_true, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: MSE penalizes large errors more\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Show quadratic penalty\n",
    "ax = axes[0]\n",
    "errors = np.linspace(-3, 3, 100)\n",
    "squared_errors = errors ** 2\n",
    "\n",
    "ax.plot(errors, squared_errors, 'b-', lw=2.5)\n",
    "ax.scatter([0.5, 1, 2], [0.25, 1, 4], c='red', s=100, zorder=5)\n",
    "ax.annotate('Small error: 0.5 -> 0.25', xy=(0.5, 0.25), xytext=(1.5, 1.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'), fontsize=10)\n",
    "ax.annotate('Large error: 2 -> 4', xy=(2, 4), xytext=(0, 5),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'), fontsize=10)\n",
    "\n",
    "ax.set_xlabel('Error (y - y_pred)', fontsize=12)\n",
    "ax.set_ylabel('Squared Error', fontsize=12)\n",
    "ax.set_title('MSE Penalty: Large Errors Hurt More', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# Right: Show predictions vs true values\n",
    "ax = axes[1]\n",
    "x_plot = np.arange(len(y_true))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x_plot - width/2, y_true, width, label='True', color='blue', alpha=0.7)\n",
    "ax.bar(x_plot + width/2, y_pred, width, label='Predicted', color='red', alpha=0.7)\n",
    "\n",
    "# Draw error lines\n",
    "for i in range(len(y_true)):\n",
    "    ax.plot([i, i], [y_true[i], y_pred[i]], 'k--', lw=1)\n",
    "    error = abs(y_true[i] - y_pred[i])\n",
    "    ax.text(i + 0.15, (y_true[i] + y_pred[i])/2, f'{error:.1f}', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Sample', fontsize=12)\n",
    "ax.set_ylabel('Value', fontsize=12)\n",
    "ax.set_title(f'Predictions vs True Values (MSE = {mse_loss(y_true, y_pred):.4f})', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.set_xticks(x_plot)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Cross-Entropy for Classification\n",
    "\n",
    "**Intuition:** Measure how \"surprised\" we are by the prediction. If we predict 0.99 for a true positive, low surprise (low loss). If we predict 0.01 for a true positive, high surprise (high loss).\n",
    "\n",
    "$$\\text{BCE} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$$\n",
    "\n",
    "#### Breaking down the formula:\n",
    "\n",
    "| Component | When Active | Meaning |\n",
    "|-----------|-------------|--------|\n",
    "| $y_i \\log(\\hat{y}_i)$ | When $y_i = 1$ | Penalize low probability for true class |\n",
    "| $(1-y_i) \\log(1-\\hat{y}_i)$ | When $y_i = 0$ | Penalize high probability for wrong class |\n",
    "| $-$ sign | Always | Make loss positive (log of probability is negative) |\n",
    "\n",
    "**What this means:** Cross-entropy heavily penalizes confident wrong predictions. Predicting 0.01 when truth is 1 gives $-\\log(0.01) = 4.6$. Predicting 0.5 gives only $-\\log(0.5) = 0.69$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred, epsilon=1e-15):\n",
    "    \"\"\"\n",
    "    Binary Cross-Entropy loss.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True binary labels (0 or 1)\n",
    "        y_pred: Predicted probabilities (0 to 1)\n",
    "        epsilon: Small value to avoid log(0)\n",
    "    \n",
    "    Returns:\n",
    "        BCE loss value\n",
    "    \"\"\"\n",
    "    # Clip predictions to avoid log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def bce_derivative(y_true, y_pred, epsilon=1e-15):\n",
    "    \"\"\"Derivative of BCE with respect to predictions.\"\"\"\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return (y_pred - y_true) / (y_pred * (1 - y_pred)) / len(y_true)\n",
    "\n",
    "# Example\n",
    "y_true_class = np.array([1, 0, 1, 1])\n",
    "y_pred_prob = np.array([0.9, 0.1, 0.8, 0.3])  # Note: 0.3 is a bad prediction for y=1\n",
    "\n",
    "print(\"Binary Cross-Entropy Loss Example:\")\n",
    "print(f\"True labels:       {y_true_class}\")\n",
    "print(f\"Predicted probs:   {y_pred_prob}\")\n",
    "print(f\"\\nPer-sample loss breakdown:\")\n",
    "for i in range(len(y_true_class)):\n",
    "    if y_true_class[i] == 1:\n",
    "        loss_i = -np.log(y_pred_prob[i])\n",
    "        print(f\"  Sample {i}: y=1, pred={y_pred_prob[i]:.1f}, loss = -log({y_pred_prob[i]:.1f}) = {loss_i:.4f}\")\n",
    "    else:\n",
    "        loss_i = -np.log(1 - y_pred_prob[i])\n",
    "        print(f\"  Sample {i}: y=0, pred={y_pred_prob[i]:.1f}, loss = -log(1-{y_pred_prob[i]:.1f}) = {loss_i:.4f}\")\n",
    "\n",
    "print(f\"\\nBCE Loss: {binary_cross_entropy(y_true_class, y_pred_prob):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Cross-entropy loss behavior\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Loss vs predicted probability for y=1\n",
    "ax = axes[0]\n",
    "probs = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "# When true label is 1\n",
    "loss_y1 = -np.log(probs)\n",
    "ax.plot(probs, loss_y1, 'b-', lw=2.5, label='True label = 1')\n",
    "\n",
    "# When true label is 0\n",
    "loss_y0 = -np.log(1 - probs)\n",
    "ax.plot(probs, loss_y0, 'r-', lw=2.5, label='True label = 0')\n",
    "\n",
    "# Mark some key points\n",
    "ax.scatter([0.9, 0.1], [-np.log(0.9), -np.log(1-0.1)], c='green', s=100, zorder=5)\n",
    "ax.annotate('Good prediction\\n(low loss)', xy=(0.9, -np.log(0.9)), \n",
    "            xytext=(0.6, 1.5), arrowprops=dict(arrowstyle='->', color='green'))\n",
    "\n",
    "ax.scatter([0.1, 0.9], [-np.log(0.1), -np.log(1-0.9)], c='red', s=100, zorder=5)\n",
    "ax.annotate('Bad prediction\\n(high loss)', xy=(0.1, -np.log(0.1)), \n",
    "            xytext=(0.3, 3), arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "ax.set_xlabel('Predicted Probability', fontsize=12)\n",
    "ax.set_ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "ax.set_title('Binary Cross-Entropy: Confident Mistakes Hurt', fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Compare MSE vs BCE for classification\n",
    "ax = axes[1]\n",
    "\n",
    "# For y_true = 1\n",
    "mse_loss_y1 = (1 - probs) ** 2\n",
    "bce_loss_y1 = -np.log(probs)\n",
    "\n",
    "ax.plot(probs, mse_loss_y1, 'b--', lw=2, label='MSE (for y=1)')\n",
    "ax.plot(probs, bce_loss_y1 / 5, 'b-', lw=2, label='BCE / 5 (for y=1)')  # Scaled for comparison\n",
    "\n",
    "ax.axvline(x=0.5, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.text(0.52, 0.8, 'Decision\\nboundary', fontsize=9, color='gray')\n",
    "\n",
    "ax.set_xlabel('Predicted Probability', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('MSE vs BCE for Classification', fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 1)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: BCE has steeper gradients near 0 and 1, leading to faster learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Loss Function Comparison Table\n\n| Loss Function | Formula | Problem Type | Output Activation | Gradient Behavior |\n|---------------|---------|--------------|-------------------|-------------------|\n| **MSE** | $\\frac{1}{n}\\sum(y-\\hat{y})^2$ | Regression | Linear (none) | Proportional to error |\n| **MAE** | $\\frac{1}{n}\\sum|y-\\hat{y}|$ | Regression (robust) | Linear | Constant magnitude |\n| **Binary CE** | $-[y\\log\\hat{y} + (1-y)\\log(1-\\hat{y})]$ | Binary classification | Sigmoid | Strong at confident errors |\n| **Categorical CE** | $-\\sum y_c \\log \\hat{y}_c$ | Multi-class | Softmax | Strong at confident errors |\n\n### Why This Matters in Machine Learning\n\n| Scenario | Recommended Loss | Reason | F1 Parallel |\n|----------|------------------|--------|-------------|\n| Predicting continuous values | MSE | Penalizes large errors | Predicting lap time, tire wear rate |\n| Binary yes/no decisions | Binary Cross-Entropy | Probabilistic interpretation | Will it rain? Should we pit? |\n| Classifying into categories | Categorical Cross-Entropy | Works with softmax | Which tire compound is optimal? |\n| When outliers exist | MAE (or Huber) | Less sensitive to outliers | Safety car laps distorting time data |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 6. Putting It Together: Building a Neural Network from Scratch\n\nNow let's build a complete 2-layer neural network and train it to solve the XOR problem!\n\n**F1 analogy:** Building this network from scratch is like building a telemetry system from individual components. We wire sensors (inputs) through processing stages (hidden layers) to a strategy output. The XOR problem below is a simplified version of a real F1 challenge: sometimes conditions that are individually fine (warm tires AND low fuel) combine in unexpected ways. A single-layer system cannot capture these interactions, just as a single threshold cannot capture nonlinear sensor interactions.\n\n### The XOR Problem\n\nXOR (exclusive or) is the classic non-linear problem:\n\n| $x_1$ | $x_2$ | XOR |\n|-------|-------|-----|\n| 0 | 0 | 0 |\n| 0 | 1 | 1 |\n| 1 | 0 | 1 |\n| 1 | 1 | 0 |\n\nA single perceptron cannot solve this, but a 2-layer network can!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR dataset\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T  # Shape: (2, 4) - 2 features, 4 samples\n",
    "y_xor = np.array([[0, 1, 1, 0]])  # Shape: (1, 4)\n",
    "\n",
    "print(\"XOR Dataset:\")\n",
    "print(f\"X (inputs):\\n{X_xor}\")\n",
    "print(f\"\\ny (outputs): {y_xor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    A simple 2-layer neural network built from scratch.\n",
    "    \n",
    "    Architecture: Input -> Hidden (with ReLU) -> Output (with Sigmoid)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize the network with random weights.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_size: Number of neurons in hidden layer\n",
    "            output_size: Number of output neurons\n",
    "        \"\"\"\n",
    "        # Initialize weights with small random values\n",
    "        # Using Xavier/Glorot initialization for better convergence\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W2 = np.random.randn(output_size, hidden_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "        \n",
    "        # Store architecture info\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation through the network.\n",
    "        \n",
    "        Args:\n",
    "            X: Input data, shape (input_size, n_samples)\n",
    "        \n",
    "        Returns:\n",
    "            A2: Output predictions, shape (output_size, n_samples)\n",
    "        \"\"\"\n",
    "        # Layer 1: Linear + ReLU\n",
    "        self.Z1 = np.dot(self.W1, X) + self.b1\n",
    "        self.A1 = np.maximum(0, self.Z1)  # ReLU\n",
    "        \n",
    "        # Layer 2: Linear + Sigmoid\n",
    "        self.Z2 = np.dot(self.W2, self.A1) + self.b2\n",
    "        self.A2 = 1 / (1 + np.exp(-self.Z2))  # Sigmoid\n",
    "        \n",
    "        # Store input for backprop\n",
    "        self.X = X\n",
    "        \n",
    "        return self.A2\n",
    "    \n",
    "    def compute_loss(self, y):\n",
    "        \"\"\"\n",
    "        Compute binary cross-entropy loss.\n",
    "        \n",
    "        Args:\n",
    "            y: True labels, shape (output_size, n_samples)\n",
    "        \n",
    "        Returns:\n",
    "            loss: Scalar loss value\n",
    "        \"\"\"\n",
    "        m = y.shape[1]  # Number of samples\n",
    "        epsilon = 1e-15\n",
    "        A2_clipped = np.clip(self.A2, epsilon, 1 - epsilon)\n",
    "        loss = -np.mean(y * np.log(A2_clipped) + (1 - y) * np.log(1 - A2_clipped))\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, y):\n",
    "        \"\"\"\n",
    "        Backward propagation to compute gradients.\n",
    "        \n",
    "        Args:\n",
    "            y: True labels, shape (output_size, n_samples)\n",
    "        \"\"\"\n",
    "        m = y.shape[1]  # Number of samples\n",
    "        \n",
    "        # Output layer gradient\n",
    "        # dL/dZ2 = A2 - y (for BCE + sigmoid)\n",
    "        dZ2 = self.A2 - y\n",
    "        self.dW2 = (1/m) * np.dot(dZ2, self.A1.T)\n",
    "        self.db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        \n",
    "        # Hidden layer gradient\n",
    "        # dL/dA1 = W2.T @ dZ2\n",
    "        dA1 = np.dot(self.W2.T, dZ2)\n",
    "        # dL/dZ1 = dL/dA1 * ReLU'(Z1)\n",
    "        dZ1 = dA1 * (self.Z1 > 0)  # ReLU derivative\n",
    "        self.dW1 = (1/m) * np.dot(dZ1, self.X.T)\n",
    "        self.db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    def update_weights(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update weights using gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            learning_rate: Step size for gradient descent\n",
    "        \"\"\"\n",
    "        self.W1 -= learning_rate * self.dW1\n",
    "        self.b1 -= learning_rate * self.db1\n",
    "        self.W2 -= learning_rate * self.dW2\n",
    "        self.b2 -= learning_rate * self.db2\n",
    "    \n",
    "    def train(self, X, y, learning_rate=1.0, epochs=10000, print_every=1000):\n",
    "        \"\"\"\n",
    "        Train the network using gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            X: Training inputs\n",
    "            y: Training labels\n",
    "            learning_rate: Step size\n",
    "            epochs: Number of training iterations\n",
    "            print_every: Print loss every N epochs\n",
    "        \n",
    "        Returns:\n",
    "            losses: List of loss values during training\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(y)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(y)\n",
    "            \n",
    "            # Update weights\n",
    "            self.update_weights(learning_rate)\n",
    "            \n",
    "            # Print progress\n",
    "            if epoch % print_every == 0:\n",
    "                print(f\"Epoch {epoch:5d}: Loss = {loss:.6f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Make binary predictions.\n",
    "        \n",
    "        Args:\n",
    "            X: Input data\n",
    "            threshold: Classification threshold\n",
    "        \n",
    "        Returns:\n",
    "            Binary predictions (0 or 1)\n",
    "        \"\"\"\n",
    "        probs = self.forward(X)\n",
    "        return (probs > threshold).astype(int)\n",
    "\n",
    "print(\"NeuralNetwork class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the network on XOR\n",
    "np.random.seed(42)\n",
    "\n",
    "# Network architecture: 2 inputs -> 4 hidden -> 1 output\n",
    "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
    "\n",
    "print(\"Training Neural Network on XOR Problem\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Architecture: {nn.input_size} -> {nn.hidden_size} -> {nn.output_size}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Train\n",
    "losses = nn.train(X_xor, y_xor, learning_rate=1.0, epochs=10001, print_every=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained network\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TESTING TRAINED NETWORK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "predictions = nn.forward(X_xor)\n",
    "\n",
    "print(f\"\\n{'Input':^15} | {'True':^8} | {'Predicted':^10} | {'Rounded':^8}\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(X_xor.shape[1]):\n",
    "    x_str = f\"({X_xor[0,i]}, {X_xor[1,i]})\"\n",
    "    print(f\"{x_str:^15} | {y_xor[0,i]:^8} | {predictions[0,i]:^10.4f} | {int(predictions[0,i] > 0.5):^8}\")\n",
    "\n",
    "accuracy = np.mean((predictions > 0.5) == y_xor) * 100\n",
    "print(f\"\\nAccuracy: {accuracy:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Training progress and decision boundary\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Left: Loss curve\n",
    "ax = axes[0]\n",
    "ax.plot(losses, 'b-', lw=1)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss (BCE)', fontsize=12)\n",
    "ax.set_title('Training Loss Over Time', fontsize=13, fontweight='bold')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Middle: Decision boundary\n",
    "ax = axes[1]\n",
    "\n",
    "# Create a grid of points\n",
    "xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 100), np.linspace(-0.5, 1.5, 100))\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()].T  # Shape: (2, 10000)\n",
    "\n",
    "# Get predictions for all grid points\n",
    "Z = nn.forward(grid_points)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "ax.contourf(xx, yy, Z, levels=np.linspace(0, 1, 11), cmap='RdBu_r', alpha=0.7)\n",
    "ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "\n",
    "# Plot data points\n",
    "for i in range(4):\n",
    "    color = 'red' if y_xor[0, i] == 1 else 'blue'\n",
    "    marker = 's' if y_xor[0, i] == 1 else 'o'\n",
    "    ax.scatter(X_xor[0, i], X_xor[1, i], c=color, s=200, marker=marker, \n",
    "               edgecolors='black', linewidths=2, zorder=5)\n",
    "\n",
    "ax.set_xlabel('$x_1$', fontsize=12)\n",
    "ax.set_ylabel('$x_2$', fontsize=12)\n",
    "ax.set_title('Learned Decision Boundary for XOR', fontsize=13, fontweight='bold')\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(ax.contourf(xx, yy, Z, levels=np.linspace(0, 1, 11), cmap='RdBu_r', alpha=0.7), ax=ax)\n",
    "cbar.set_label('P(y=1)', fontsize=10)\n",
    "\n",
    "# Right: Network architecture visualization\n",
    "ax = axes[2]\n",
    "ax.set_xlim(-1, 5)\n",
    "ax.set_ylim(-0.5, 4.5)\n",
    "\n",
    "# Draw neurons\n",
    "layer_x = [0, 2, 4]\n",
    "layer_neurons = [[1, 3], [0.5, 1.5, 2.5, 3.5], [2]]  # y positions\n",
    "layer_colors = ['lightblue', 'lightgreen', 'lightsalmon']\n",
    "layer_labels = ['Input', 'Hidden (ReLU)', 'Output (Sigmoid)']\n",
    "\n",
    "for layer_idx, (x, neurons, color) in enumerate(zip(layer_x, layer_neurons, layer_colors)):\n",
    "    for y in neurons:\n",
    "        circle = plt.Circle((x, y), 0.25, color=color, ec='black', linewidth=2)\n",
    "        ax.add_patch(circle)\n",
    "    ax.text(x, -0.3, layer_labels[layer_idx], ha='center', fontsize=9)\n",
    "\n",
    "# Draw connections (simplified - just show some)\n",
    "for y1 in layer_neurons[0]:\n",
    "    for y2 in layer_neurons[1]:\n",
    "        ax.plot([0.25, 1.75], [y1, y2], 'gray', alpha=0.3, lw=0.5)\n",
    "\n",
    "for y1 in layer_neurons[1]:\n",
    "    for y2 in layer_neurons[2]:\n",
    "        ax.plot([2.25, 3.75], [y1, y2], 'gray', alpha=0.3, lw=0.5)\n",
    "\n",
    "ax.set_title('Network Architecture', fontsize=13, fontweight='bold')\n",
    "ax.axis('off')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe network learned a NON-LINEAR decision boundary to solve XOR!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Deep Dive: How the Network Solves XOR\n\nThe hidden layer transforms the input space so that XOR becomes linearly separable!\n\n#### Key Insight\n\nThe 4 hidden neurons learn to create a new representation where:\n- Points (0,0) and (1,1) map to one region\n- Points (0,1) and (1,0) map to another region\n\nThe output layer then just needs to draw a line in this new space.\n\n**F1 analogy:** This is exactly what a multi-layer telemetry system does. Raw sensor data (tire temps, wind speed) are hard to interpret directly. But the hidden layer transforms them into meaningful features like \"effective downforce\" and \"tire degradation rate.\" In this new feature space, the strategy decision becomes simple. The hidden layer is the feature engineering that the network learns automatically."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the hidden layer representations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Original space\n",
    "ax = axes[0]\n",
    "for i in range(4):\n",
    "    color = 'red' if y_xor[0, i] == 1 else 'blue'\n",
    "    marker = 's' if y_xor[0, i] == 1 else 'o'\n",
    "    ax.scatter(X_xor[0, i], X_xor[1, i], c=color, s=300, marker=marker, \n",
    "               edgecolors='black', linewidths=2)\n",
    "    ax.annotate(f'({int(X_xor[0,i])},{int(X_xor[1,i])})', \n",
    "                xy=(X_xor[0, i], X_xor[1, i]),\n",
    "                xytext=(X_xor[0, i] + 0.15, X_xor[1, i] + 0.15), fontsize=11)\n",
    "\n",
    "ax.set_xlabel('$x_1$', fontsize=12)\n",
    "ax.set_ylabel('$x_2$', fontsize=12)\n",
    "ax.set_title('Original Input Space (Not Separable)', fontsize=13, fontweight='bold')\n",
    "ax.set_xlim(-0.3, 1.5)\n",
    "ax.set_ylim(-0.3, 1.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Right: Hidden layer representation (first 2 dimensions)\n",
    "ax = axes[1]\n",
    "nn.forward(X_xor)  # Make sure A1 is computed\n",
    "A1 = nn.A1  # Hidden layer activations\n",
    "\n",
    "# Use first 2 hidden neurons for visualization\n",
    "for i in range(4):\n",
    "    color = 'red' if y_xor[0, i] == 1 else 'blue'\n",
    "    marker = 's' if y_xor[0, i] == 1 else 'o'\n",
    "    ax.scatter(A1[0, i], A1[1, i], c=color, s=300, marker=marker, \n",
    "               edgecolors='black', linewidths=2)\n",
    "    ax.annotate(f'({int(X_xor[0,i])},{int(X_xor[1,i])})', \n",
    "                xy=(A1[0, i], A1[1, i]),\n",
    "                xytext=(A1[0, i] + 0.05, A1[1, i] + 0.05), fontsize=11)\n",
    "\n",
    "ax.set_xlabel('Hidden Neuron 1 Activation', fontsize=12)\n",
    "ax.set_ylabel('Hidden Neuron 2 Activation', fontsize=12)\n",
    "ax.set_title('Hidden Layer Space (Separable!)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The hidden layer 'untangles' the XOR problem!\")\n",
    "print(\"Red squares (XOR=1) and blue circles (XOR=0) are now separable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Implement a Perceptron\n",
    "\n",
    "Implement a single perceptron that can learn the AND gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 1: Implement a simple perceptron for AND gate\n",
    "def perceptron(x, w, b):\n",
    "    \"\"\"\n",
    "    Single perceptron with step activation.\n",
    "    \n",
    "    Args:\n",
    "        x: Input features (n,)\n",
    "        w: Weights (n,)\n",
    "        b: Bias\n",
    "    \n",
    "    Returns:\n",
    "        Output (0 or 1)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this!\n",
    "    # 1. Compute weighted sum z = w . x + b\n",
    "    # 2. Apply step function: return 1 if z >= 0, else 0\n",
    "    \n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "# Test: AND gate with known weights\n",
    "# For AND: both inputs must be 1 for output to be 1\n",
    "# Try w = [1, 1] and b = -1.5 (why does this work?)\n",
    "\n",
    "w_and = np.array([1.0, 1.0])\n",
    "b_and = -1.5\n",
    "\n",
    "print(\"Testing AND gate perceptron:\")\n",
    "test_inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "expected = [0, 0, 0, 1]\n",
    "\n",
    "for inp, exp in zip(test_inputs, expected):\n",
    "    result = perceptron(np.array(inp), w_and, b_and)\n",
    "    status = \"Correct\" if result == exp else \"WRONG\"\n",
    "    print(f\"AND({inp[0]}, {inp[1]}) = {result}, expected = {exp}, {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Compare Activation Functions\n",
    "\n",
    "Create a function that applies different activations to the same input and visualizes the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 2: Create a comparison visualization\n",
    "def compare_activations(z_values):\n",
    "    \"\"\"\n",
    "    Apply all activation functions to z_values and print comparison.\n",
    "    \n",
    "    Args:\n",
    "        z_values: Array of input values\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with activation name -> output array\n",
    "    \"\"\"\n",
    "    # TODO: Apply sigmoid, tanh, relu, and leaky_relu to z_values\n",
    "    # Return a dictionary like {'sigmoid': [...], 'tanh': [...], ...}\n",
    "    \n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "# Test\n",
    "z_test = np.array([-2, -1, 0, 1, 2])\n",
    "results = compare_activations(z_test)\n",
    "\n",
    "if results:\n",
    "    print(f\"Input z:       {z_test}\")\n",
    "    for name, values in results.items():\n",
    "        print(f\"{name:12s}: {np.round(values, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Extend the Network\n",
    "\n",
    "Modify the neural network to solve a slightly harder problem: the circle classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 3: Train on circle dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate circle data: points inside circle (r < 1.0) are class 1\n",
    "n_samples = 200\n",
    "X_circle = np.random.randn(2, n_samples)\n",
    "radii = np.sqrt(X_circle[0]**2 + X_circle[1]**2)\n",
    "y_circle = (radii < 1.0).astype(int).reshape(1, -1)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X_circle[0, y_circle[0]==0], X_circle[1, y_circle[0]==0], \n",
    "            c='blue', label='Class 0 (outside)', alpha=0.6)\n",
    "plt.scatter(X_circle[0, y_circle[0]==1], X_circle[1, y_circle[0]==1], \n",
    "            c='red', label='Class 1 (inside)', alpha=0.6)\n",
    "circle = plt.Circle((0, 0), 1.0, fill=False, color='black', linestyle='--', linewidth=2)\n",
    "plt.gca().add_patch(circle)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('Circle Classification Problem')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# TODO: Create a neural network and train it on this data\n",
    "# Hint: You may need more hidden neurons (try 8 or 16)\n",
    "# nn_circle = NeuralNetwork(input_size=2, hidden_size=?, output_size=1)\n",
    "# losses = nn_circle.train(X_circle, y_circle, learning_rate=?, epochs=?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Summary\n\n### Key Concepts\n\n| Concept | Definition | F1 Parallel |\n|---------|-----------|-------------|\n| **Perceptron** | A single neuron: weighted sum + activation | A single sensor threshold decision (is the tire too hot? yes/no) |\n| **Weighted sum** | $z = \\mathbf{w} \\cdot \\mathbf{x} + b$ | Combining telemetry readings into a single urgency score |\n| **Activation functions** | Non-linear functions enabling complex patterns | Nonlinear sensor responses (tire grip vs temp is not linear) |\n| **Forward propagation** | Data flows through layers: linear + activation | Sensor data flowing through the telemetry pipeline to the pit wall |\n| **Loss functions** | Measure prediction error (MSE, cross-entropy) | Lap time delta from the theoretical best (lower is better) |\n| **Hidden layers** | Transform inputs into separable representations | Feature extraction: raw data to \"understeer detected\" |\n\n- **Activation function specifics:**\n  - Sigmoid: $(0, 1)$ range, good for binary output\n  - Tanh: $(-1, 1)$ range, zero-centered\n  - ReLU: Default for hidden layers, avoids vanishing gradients\n  - Leaky ReLU: Fixes \"dying ReLU\" problem\n  - GELU: Smooth version used in transformers\n\n### Connection to Deep Learning\n\n| Concept | Where You'll See It | F1 Parallel |\n|---------|--------------------|-------------|\n| Weighted sums | Every layer in every network | Every telemetry aggregation step |\n| ReLU activation | CNNs, MLPs, most architectures | Binary threshold decisions on sensor data |\n| GELU activation | Transformers (BERT, GPT) | Smooth strategy weighting in modern F1 analytics |\n| Sigmoid output | Binary classification tasks | Pit stop probability: go or stay |\n| Cross-entropy loss | Classification training | Penalizing confident wrong strategy calls |\n| Forward propagation | Inference in all networks | The full telemetry pipeline from sensor to pit wall |\n| Hidden representations | Feature learning, embeddings | Intermediate features like \"effective downforce\" |\n\n### Checklist\n\nBefore moving on, make sure you can:\n\n- [ ] Compute a weighted sum by hand for 2-3 inputs\n- [ ] Sketch the shape of sigmoid, tanh, and ReLU\n- [ ] Explain why ReLU helps with vanishing gradients\n- [ ] Trace values through a 2-layer network\n- [ ] Explain why XOR requires a hidden layer\n- [ ] Choose the right loss function for a given problem"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand forward propagation and how networks compute predictions, you're ready to learn **how they learn**!\n",
    "\n",
    "**Next: Part 3.2 - Backpropagation & Training**\n",
    "\n",
    "In the next notebook, we'll cover:\n",
    "- The chain rule of calculus (essential for understanding backprop)\n",
    "- Backpropagation: computing gradients efficiently\n",
    "- Gradient descent: updating weights to minimize loss\n",
    "- Training loops and batch processing\n",
    "\n",
    "**Recommended preparation:**\n",
    "- Review the chain rule from calculus: $(f \\circ g)'(x) = f'(g(x)) \\cdot g'(x)$\n",
    "- Re-run this notebook's XOR example and trace how the loss decreases\n",
    "- Try Exercise 3 to get practice with training networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}