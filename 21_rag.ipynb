{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Part 7.1: Retrieval-Augmented Generation (RAG) — The Formula 1 Edition\n\nLanguage models are powerful, but they have a fundamental limitation: their knowledge is frozen at training time. Ask about yesterday's news, your company's internal docs, or a recently published paper, and the model can only hallucinate. **Retrieval-Augmented Generation (RAG)** solves this by giving LLMs access to external knowledge at inference time.\n\n**F1 analogy:** Think of RAG like a race engineer's access to the team's historical database. During a race, the engineer doesn't rely solely on memory — they query past race data for similar conditions (retrieval), then combine that historical context with live telemetry to make strategy calls (augmented generation). The vector store is the team's indexed archive of thousands of past races, searchable by situation similarity rather than date or keyword.\n\nRAG is the most widely deployed LLM pattern in production today — it's how ChatGPT plugins, enterprise search, and AI assistants work with custom data. Understanding RAG means understanding how to build practical AI systems.\n\n## Learning Objectives\n\n- [ ] Understand why RAG exists and when to use it vs. fine-tuning\n- [ ] Implement document chunking strategies and understand their tradeoffs\n- [ ] Build a vector store with similarity search from scratch\n- [ ] Implement a complete RAG pipeline: chunk → embed → retrieve → generate\n- [ ] Understand and implement reranking for improved retrieval quality\n- [ ] Evaluate RAG systems with retrieval and generation metrics\n- [ ] Recognize common RAG failure modes and how to fix them\n- [ ] Connect RAG to the embedding concepts from Notebook 15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import hashlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Part 7.1: Retrieval-Augmented Generation (RAG)\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 1. Why RAG?\n\n### The Problem with Parametric Knowledge\n\nLLMs store knowledge in their parameters (weights). This has three major limitations:\n\n| Problem | Description | Example | F1 Parallel |\n|---------|------------|----------|-------------|\n| **Staleness** | Knowledge frozen at training cutoff | \"Who won the 2025 Super Bowl?\" | A strategy model trained on 2023 data doesn't know about 2024 regulation changes |\n| **Hallucination** | Generates plausible but wrong facts | Citing non-existent papers | The model confidently predicting tire life based on data from a track it's never seen |\n| **No private data** | Can't access your specific documents | Company policy questions | Can't access your team's proprietary telemetry or confidential race debriefs |\n\n### RAG vs. Fine-Tuning\n\n| Approach | When to Use | Pros | Cons |\n|----------|------------|------|------|\n| **RAG** | Dynamic knowledge, factual accuracy | No training needed, always up-to-date, verifiable | Latency overhead, retrieval errors |\n| **Fine-tuning** | Behavior change, domain style | Faster inference, no retrieval | Expensive, knowledge goes stale |\n| **RAG + Fine-tuning** | Best of both | Accurate + well-behaved | Most complex |\n\n### Intuitive Explanation\n\nThink of RAG like an open-book exam: instead of memorizing everything (fine-tuning), you bring your textbooks (retrieved documents) and look up the answer. The LLM's job shifts from \"know everything\" to \"read well and synthesize.\"\n\n**F1 analogy:** Fine-tuning is like training a driver on a simulator until they memorize every corner — great until the track layout changes. RAG is like giving the driver a radio connection to the pit wall, where engineers look up relevant data from past races in real-time and relay it. The driver (LLM) still makes the final call, but with the latest information at hand."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: The RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(14, 7))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 9)\n",
    "ax.axis('off')\n",
    "ax.set_title('The RAG Pipeline', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Indexing pipeline (top)\n",
    "ax.text(7, 8.5, 'Indexing Pipeline (offline)', ha='center', fontsize=12,\n",
    "        fontweight='bold', color='gray', style='italic')\n",
    "\n",
    "index_steps = [\n",
    "    (0.5, 7, 2.5, 1, 'Documents', '#95a5a6', 'Raw text, PDFs,\\nweb pages'),\n",
    "    (3.5, 7, 2.5, 1, 'Chunker', '#3498db', 'Split into\\nmanageable pieces'),\n",
    "    (6.5, 7, 2.5, 1, 'Embedder', '#9b59b6', 'Convert chunks\\nto vectors'),\n",
    "    (10, 7, 3, 1, 'Vector Store', '#2ecc71', 'Index vectors\\nfor fast search'),\n",
    "]\n",
    "\n",
    "for x, y, w, h, label, color, desc in index_steps:\n",
    "    box = mpatches.FancyBboxPatch((x, y), w, h, boxstyle=\"round,pad=0.2\",\n",
    "                                   facecolor=color, edgecolor='black', linewidth=2, alpha=0.9)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x + w/2, y + h/2, label, ha='center', va='center',\n",
    "            fontsize=10, fontweight='bold', color='white')\n",
    "    ax.text(x + w/2, y - 0.4, desc, ha='center', va='center', fontsize=8, color='gray')\n",
    "\n",
    "for i in range(len(index_steps) - 1):\n",
    "    x1 = index_steps[i][0] + index_steps[i][2]\n",
    "    x2 = index_steps[i+1][0]\n",
    "    ax.annotate('', xy=(x2, 7.5), xytext=(x1, 7.5),\n",
    "               arrowprops=dict(arrowstyle='->', lw=2, color='gray'))\n",
    "\n",
    "# Query pipeline (bottom)\n",
    "ax.text(7, 5, 'Query Pipeline (online)', ha='center', fontsize=12,\n",
    "        fontweight='bold', color='gray', style='italic')\n",
    "\n",
    "query_steps = [\n",
    "    (0.5, 3, 2.5, 1, 'User Query', '#e74c3c', '\"What is...?\"'),\n",
    "    (3.5, 3, 2.5, 1, 'Embed Query', '#9b59b6', 'Same embedder\\nas indexing'),\n",
    "    (6.5, 3, 2.5, 1, 'Retrieve', '#2ecc71', 'Find top-k\\nnearest chunks'),\n",
    "    (10, 3, 3, 1, 'Generate', '#f39c12', 'LLM synthesizes\\nanswer + context'),\n",
    "]\n",
    "\n",
    "for x, y, w, h, label, color, desc in query_steps:\n",
    "    box = mpatches.FancyBboxPatch((x, y), w, h, boxstyle=\"round,pad=0.2\",\n",
    "                                   facecolor=color, edgecolor='black', linewidth=2, alpha=0.9)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x + w/2, y + h/2, label, ha='center', va='center',\n",
    "            fontsize=10, fontweight='bold', color='white')\n",
    "    ax.text(x + w/2, y - 0.4, desc, ha='center', va='center', fontsize=8, color='gray')\n",
    "\n",
    "for i in range(len(query_steps) - 1):\n",
    "    x1 = query_steps[i][0] + query_steps[i][2]\n",
    "    x2 = query_steps[i+1][0]\n",
    "    ax.annotate('', xy=(x2, 3.5), xytext=(x1, 3.5),\n",
    "               arrowprops=dict(arrowstyle='->', lw=2, color='gray'))\n",
    "\n",
    "# Connection from vector store to retrieve\n",
    "ax.annotate('', xy=(7.75, 4), xytext=(11.5, 7),\n",
    "           arrowprops=dict(arrowstyle='->', lw=2, color='#2ecc71',\n",
    "                          connectionstyle='arc3,rad=-0.3'))\n",
    "\n",
    "# Answer\n",
    "box = mpatches.FancyBboxPatch((4, 0.5), 6, 1, boxstyle=\"round,pad=0.3\",\n",
    "                               facecolor='#2c3e50', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(box)\n",
    "ax.text(7, 1, 'Grounded Answer (with citations)', ha='center', va='center',\n",
    "        fontsize=11, fontweight='bold', color='white')\n",
    "ax.annotate('', xy=(7, 1.5), xytext=(11.5, 3),\n",
    "           arrowprops=dict(arrowstyle='->', lw=2, color='#f39c12'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 2. Document Chunking\n\nBefore we can search documents, we need to split them into chunks. Chunking strategy dramatically affects RAG quality.\n\n### Why Chunk?\n\n- LLMs have limited context windows\n- Embedding models work best on shorter texts\n- Retrieval is more precise with smaller, focused chunks\n\n### Key Tradeoff\n\n- **Too small**: Loses context, retrieves fragments without enough information\n- **Too large**: Dilutes relevance, wastes context window space\n\n**F1 analogy:** Chunking is like breaking race reports into queryable sections. A full 50-page post-race debrief is too big to search effectively — you need it split into sections: \"tire strategy,\" \"weather conditions,\" \"overtaking analysis,\" \"pit stop timing.\" Too granular (individual sentences) and you lose context; too coarse (the entire report) and the retrieval becomes imprecise. The art is finding the right granularity, just like choosing how to organize the team's knowledge base so engineers can find what they need mid-race."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample document corpus for our RAG system\n",
    "DOCUMENTS = {\n",
    "    \"neural_networks\": \"\"\"Neural networks are computing systems inspired by biological neural networks. \n",
    "They consist of layers of interconnected nodes called neurons. Each connection has a weight that \n",
    "adjusts during training. The input layer receives data, hidden layers process it through weighted \n",
    "connections and activation functions, and the output layer produces predictions.\n",
    "\n",
    "Backpropagation is the key algorithm for training neural networks. It computes gradients of the loss \n",
    "function with respect to each weight by applying the chain rule. These gradients tell us how to \n",
    "adjust weights to minimize the loss. Stochastic gradient descent (SGD) and its variants like Adam \n",
    "are commonly used optimizers.\n",
    "\n",
    "Deep learning refers to neural networks with many hidden layers. Deep networks can learn hierarchical \n",
    "representations — early layers detect simple features like edges, while deeper layers combine them \n",
    "into complex patterns like faces or objects. This hierarchical feature learning is what makes deep \n",
    "learning so powerful for tasks like image recognition and natural language processing.\"\"\",\n",
    "\n",
    "    \"transformers\": \"\"\"The Transformer architecture was introduced in the 2017 paper 'Attention Is All \n",
    "You Need' by Vaswani et al. It replaced recurrent networks with self-attention mechanisms, enabling \n",
    "parallel processing of sequences. The key innovation is the attention mechanism, which allows each \n",
    "token to attend to all other tokens in the sequence.\n",
    "\n",
    "Self-attention computes three matrices: Query (Q), Key (K), and Value (V) from the input. The \n",
    "attention score between positions is computed as the dot product of Q and K, scaled by the square \n",
    "root of the dimension, then softmaxed to get weights for the V matrix. Multi-head attention runs \n",
    "multiple attention operations in parallel, each learning different relationship patterns.\n",
    "\n",
    "Transformers use positional encodings since they have no inherent notion of sequence order. The \n",
    "original paper used sinusoidal encodings, but modern models often use learned positional embeddings \n",
    "or relative position encodings like RoPE. Layer normalization and residual connections are critical \n",
    "for training stability in deep transformer models.\"\"\",\n",
    "\n",
    "    \"rlhf\": \"\"\"Reinforcement Learning from Human Feedback (RLHF) is the technique used to align \n",
    "language models with human preferences. The process has three stages: supervised fine-tuning (SFT) \n",
    "on human demonstrations, training a reward model on preference comparisons, and optimizing the \n",
    "policy with PPO using the reward model.\n",
    "\n",
    "The reward model is trained using the Bradley-Terry preference model. Given pairs of responses \n",
    "where humans indicated a preference, the model learns to assign higher scores to preferred \n",
    "responses. The loss function is the negative log likelihood of the human preferences under \n",
    "the model's scoring.\n",
    "\n",
    "PPO optimization with a KL penalty is critical to prevent reward hacking. Without the KL \n",
    "constraint, the model finds degenerate solutions that exploit the reward model's weaknesses. \n",
    "The KL penalty keeps the policy close to the SFT reference model, ensuring the model remains \n",
    "coherent and fluent while improving on the reward signal. Direct Preference Optimization (DPO) \n",
    "is a newer alternative that skips the reward model entirely.\"\"\",\n",
    "\n",
    "    \"embeddings\": \"\"\"Word embeddings represent words as dense vectors in a continuous space. \n",
    "Word2Vec introduced two key approaches: Skip-gram, which predicts context words from a target \n",
    "word, and CBOW, which predicts the target from context. These models learn that semantically \n",
    "similar words end up close together in the embedding space.\n",
    "\n",
    "Modern embedding models like sentence transformers produce embeddings for entire sentences or \n",
    "paragraphs. These are crucial for RAG systems because they enable semantic search — finding \n",
    "documents by meaning rather than keyword matching. Cosine similarity is the standard metric \n",
    "for comparing embeddings.\n",
    "\n",
    "Contextual embeddings from models like BERT produce different vectors for the same word \n",
    "depending on context. The word 'bank' gets different embeddings in 'river bank' versus \n",
    "'bank account'. This context-sensitivity makes them far more powerful than static embeddings \n",
    "for understanding natural language.\"\"\"\n",
    "}\n",
    "\n",
    "print(f\"Document corpus: {len(DOCUMENTS)} documents\")\n",
    "for name, text in DOCUMENTS.items():\n",
    "    words = len(text.split())\n",
    "    print(f\"  {name}: {words} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentChunker:\n",
    "    \"\"\"Multiple chunking strategies for RAG.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def fixed_size(text, chunk_size=200, overlap=50):\n",
    "        \"\"\"Split into fixed-size character chunks with overlap.\"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            chunk = text[start:end].strip()\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "            start += chunk_size - overlap\n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def sentence_based(text, max_sentences=3):\n",
    "        \"\"\"Split into groups of sentences.\"\"\"\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "        chunks = []\n",
    "        for i in range(0, len(sentences), max_sentences):\n",
    "            chunk = ' '.join(sentences[i:i + max_sentences]).strip()\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def paragraph_based(text):\n",
    "        \"\"\"Split on paragraph boundaries.\"\"\"\n",
    "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "        return paragraphs\n",
    "    \n",
    "    @staticmethod\n",
    "    def semantic_window(text, window_size=3):\n",
    "        \"\"\"Sliding window over sentences with overlap.\"\"\"\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "        chunks = []\n",
    "        for i in range(len(sentences)):\n",
    "            window = sentences[max(0, i - window_size // 2):i + window_size // 2 + 1]\n",
    "            chunk = ' '.join(window).strip()\n",
    "            if chunk and chunk not in chunks:\n",
    "                chunks.append(chunk)\n",
    "        return chunks\n",
    "\n",
    "\n",
    "# Demonstrate chunking strategies\n",
    "sample_text = DOCUMENTS['transformers']\n",
    "chunker = DocumentChunker()\n",
    "\n",
    "strategies = {\n",
    "    'Fixed (200 chars, 50 overlap)': chunker.fixed_size(sample_text, 200, 50),\n",
    "    'Sentence-based (3 per chunk)': chunker.sentence_based(sample_text, 3),\n",
    "    'Paragraph-based': chunker.paragraph_based(sample_text),\n",
    "}\n",
    "\n",
    "for name, chunks in strategies.items():\n",
    "    print(f\"\\n{name}: {len(chunks)} chunks\")\n",
    "    for i, chunk in enumerate(chunks[:3]):\n",
    "        preview = chunk[:80].replace('\\n', ' ') + ('...' if len(chunk) > 80 else '')\n",
    "        print(f\"  [{i}] ({len(chunk)} chars) {preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Chunking Strategy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax, (name, chunks) in zip(axes, strategies.items()):\n",
    "    sizes = [len(c) for c in chunks]\n",
    "    bars = ax.bar(range(len(sizes)), sizes, color='#3498db', edgecolor='black', alpha=0.8)\n",
    "    ax.set_xlabel('Chunk index', fontsize=11)\n",
    "    ax.set_ylabel('Chunk size (chars)', fontsize=11)\n",
    "    ax.set_title(f'{name}\\n({len(chunks)} chunks)', fontsize=11, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Annotate mean\n",
    "    mean_size = np.mean(sizes)\n",
    "    ax.axhline(y=mean_size, color='red', linestyle='--', alpha=0.7)\n",
    "    ax.text(len(sizes) - 1, mean_size + 10, f'μ={mean_size:.0f}', color='red', fontsize=9)\n",
    "\n",
    "plt.suptitle('Chunk Size Distribution by Strategy', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Fixed-size: Uniform chunks but may split mid-sentence\")\n",
    "print(\"Sentence-based: Respects sentence boundaries, variable size\")\n",
    "print(\"Paragraph-based: Respects semantic boundaries, fewest chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 3. Embeddings and Vector Search\n\nOnce we have chunks, we convert them to vectors using an embedding model, then store them for similarity search. We covered embeddings in Notebook 15 — now we'll use them in practice.\n\n**F1 analogy:** The vector store is the team's knowledge base of past races, indexed by situation similarity. Instead of filing race reports by date or circuit name, you encode each report section as a vector that captures its *meaning*. When the engineer asks \"What happened last time we faced tire degradation on a hot street circuit?\", the system finds the most semantically similar past situations — even if they used completely different words in the original report. This is the difference between keyword search (\"tire degradation AND street circuit\") and semantic search (\"situations like this one\").\n\n### Building a Simple Embedding Model\n\nIn production, you'd use a pre-trained model (e.g., OpenAI `text-embedding-3-small` or an open-source sentence transformer). Here we'll build a simplified TF-IDF + learned projection to demonstrate the mechanics."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEmbedder:\n",
    "    \"\"\"TF-IDF based embedding model for demonstration.\n",
    "    \n",
    "    In production, use sentence-transformers or API-based embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim=64):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab = {}\n",
    "        self.idf = {}\n",
    "        self.projection = None  # Random projection for dimensionality reduction\n",
    "    \n",
    "    def _tokenize(self, text):\n",
    "        \"\"\"Simple whitespace + lowercase tokenization.\"\"\"\n",
    "        return re.findall(r'\\b[a-zA-Z]{2,}\\b', text.lower())\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"Build vocabulary and IDF scores from a corpus.\"\"\"\n",
    "        doc_freq = Counter()\n",
    "        all_tokens = set()\n",
    "        \n",
    "        for doc in documents:\n",
    "            tokens = set(self._tokenize(doc))\n",
    "            for token in tokens:\n",
    "                doc_freq[token] += 1\n",
    "            all_tokens.update(tokens)\n",
    "        \n",
    "        # Build vocabulary (top tokens by document frequency)\n",
    "        sorted_tokens = sorted(all_tokens, key=lambda t: doc_freq[t], reverse=True)\n",
    "        self.vocab = {token: idx for idx, token in enumerate(sorted_tokens)}\n",
    "        \n",
    "        # Compute IDF\n",
    "        n_docs = len(documents)\n",
    "        self.idf = {token: np.log(n_docs / (1 + doc_freq[token]))\n",
    "                     for token in self.vocab}\n",
    "        \n",
    "        # Random projection matrix for dimensionality reduction\n",
    "        vocab_size = len(self.vocab)\n",
    "        np.random.seed(42)\n",
    "        self.projection = np.random.randn(vocab_size, self.embedding_dim) / np.sqrt(self.embedding_dim)\n",
    "        \n",
    "        print(f\"Embedder fitted: {len(self.vocab)} tokens, {self.embedding_dim}d embeddings\")\n",
    "    \n",
    "    def embed(self, text):\n",
    "        \"\"\"Convert text to a dense embedding vector.\"\"\"\n",
    "        tokens = self._tokenize(text)\n",
    "        \n",
    "        # TF-IDF vector\n",
    "        tf = Counter(tokens)\n",
    "        tfidf = np.zeros(len(self.vocab))\n",
    "        for token, count in tf.items():\n",
    "            if token in self.vocab:\n",
    "                idx = self.vocab[token]\n",
    "                tfidf[idx] = (count / len(tokens)) * self.idf.get(token, 0)\n",
    "        \n",
    "        # Project to lower dimension\n",
    "        embedding = tfidf @ self.projection\n",
    "        \n",
    "        # L2 normalize\n",
    "        norm = np.linalg.norm(embedding)\n",
    "        if norm > 0:\n",
    "            embedding = embedding / norm\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    def embed_batch(self, texts):\n",
    "        \"\"\"Embed multiple texts.\"\"\"\n",
    "        return np.array([self.embed(text) for text in texts])\n",
    "\n",
    "\n",
    "# Build embedder from our corpus\n",
    "all_text = list(DOCUMENTS.values())\n",
    "embedder = SimpleEmbedder(embedding_dim=64)\n",
    "embedder.fit(all_text)\n",
    "\n",
    "# Test it\n",
    "test_embedding = embedder.embed(\"How does attention work in transformers?\")\n",
    "print(f\"\\nQuery embedding shape: {test_embedding.shape}\")\n",
    "print(f\"L2 norm: {np.linalg.norm(test_embedding):.4f} (should be ~1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 4. Vector Store\n\nA vector store indexes embeddings for fast similarity search. The core operation is **nearest neighbor search**: given a query vector, find the $k$ most similar stored vectors.\n\n**F1 analogy:** The vector store is like the team's race strategy database — thousands of past race situations encoded as vectors. When the race engineer types \"We're on mediums, lap 25 of 55, gap to car ahead is closing,\" the vector store finds the three or four most similar historical situations. The cosine similarity score tells you *how* similar each past situation was. In production F1 teams, this kind of indexed retrieval needs to return results in milliseconds — you can't wait 10 seconds for an answer when racing at 300 km/h.\n\nIn production, you'd use FAISS, Pinecone, Weaviate, or Chroma. Here we build one from scratch to understand the mechanics."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Simple vector store with cosine similarity search.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedder):\n",
    "        self.embedder = embedder\n",
    "        self.embeddings = []  # List of vectors\n",
    "        self.documents = []   # Original text chunks\n",
    "        self.metadata = []    # Source document info\n",
    "    \n",
    "    def add_documents(self, chunks, source_name=\"\"):\n",
    "        \"\"\"Index a list of text chunks.\"\"\"\n",
    "        for chunk in chunks:\n",
    "            embedding = self.embedder.embed(chunk)\n",
    "            self.embeddings.append(embedding)\n",
    "            self.documents.append(chunk)\n",
    "            self.metadata.append({'source': source_name, 'length': len(chunk)})\n",
    "    \n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"Find the top-k most similar chunks to a query.\"\"\"\n",
    "        query_embedding = self.embedder.embed(query)\n",
    "        \n",
    "        # Cosine similarity (embeddings are already normalized)\n",
    "        similarities = np.array([\n",
    "            np.dot(query_embedding, emb) for emb in self.embeddings\n",
    "        ])\n",
    "        \n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'chunk': self.documents[idx],\n",
    "                'score': similarities[idx],\n",
    "                'metadata': self.metadata[idx],\n",
    "                'index': idx\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.documents)\n",
    "\n",
    "\n",
    "# Build the vector store\n",
    "store = VectorStore(embedder)\n",
    "\n",
    "for doc_name, doc_text in DOCUMENTS.items():\n",
    "    chunks = DocumentChunker.paragraph_based(doc_text)\n",
    "    store.add_documents(chunks, source_name=doc_name)\n",
    "\n",
    "print(f\"Vector store: {len(store)} chunks indexed\")\n",
    "print(f\"Sources: {set(m['source'] for m in store.metadata)}\")\n",
    "\n",
    "# Test search\n",
    "query = \"How does attention work?\"\n",
    "results = store.search(query, top_k=3)\n",
    "\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "print(\"\\nTop-3 results:\")\n",
    "for i, r in enumerate(results):\n",
    "    preview = r['chunk'][:100].replace('\\n', ' ') + '...'\n",
    "    print(f\"  [{i+1}] Score: {r['score']:.4f} | Source: {r['metadata']['source']}\")\n",
    "    print(f\"       {preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the embedding space with PCA\n",
    "from numpy.linalg import svd\n",
    "\n",
    "embeddings_matrix = np.array(store.embeddings)\n",
    "# Simple PCA via SVD\n",
    "mean = embeddings_matrix.mean(axis=0)\n",
    "centered = embeddings_matrix - mean\n",
    "U, S, Vt = svd(centered, full_matrices=False)\n",
    "pca_2d = centered @ Vt[:2].T\n",
    "\n",
    "# Color by source document\n",
    "source_colors = {'neural_networks': '#3498db', 'transformers': '#e74c3c',\n",
    "                 'rlhf': '#2ecc71', 'embeddings': '#f39c12'}\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "for i, (point, meta) in enumerate(zip(pca_2d, store.metadata)):\n",
    "    color = source_colors[meta['source']]\n",
    "    ax.scatter(point[0], point[1], c=color, s=100, edgecolor='black', zorder=5)\n",
    "    ax.annotate(f\"{meta['source'][:5]}_{i}\", (point[0], point[1]),\n",
    "               textcoords=\"offset points\", xytext=(5, 5), fontsize=7)\n",
    "\n",
    "# Plot query point\n",
    "query_emb = embedder.embed(query)\n",
    "query_pca = (query_emb - mean) @ Vt[:2].T\n",
    "ax.scatter(query_pca[0], query_pca[1], c='black', s=200, marker='*', zorder=10, label='Query')\n",
    "\n",
    "# Draw lines to top results\n",
    "for r in results[:3]:\n",
    "    idx = r['index']\n",
    "    ax.plot([query_pca[0], pca_2d[idx, 0]], [query_pca[1], pca_2d[idx, 1]],\n",
    "            'k--', alpha=0.3, linewidth=1.5)\n",
    "\n",
    "# Legend\n",
    "for name, color in source_colors.items():\n",
    "    ax.scatter([], [], c=color, s=80, label=name, edgecolor='black')\n",
    "ax.legend(fontsize=10, loc='upper left')\n",
    "\n",
    "ax.set_xlabel('PCA Component 1', fontsize=12)\n",
    "ax.set_ylabel('PCA Component 2', fontsize=12)\n",
    "ax.set_title('Chunk Embeddings in 2D (PCA)\\nDashed lines = retrieved chunks',\n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 5. The Complete RAG Pipeline\n\nNow let's put it all together into a complete RAG system.\n\n**F1 analogy:** The full RAG pipeline mirrors what happens on the pit wall during a race. The engineer's question (query) gets embedded and matched against the team's historical database (retrieval). The top-k most relevant past race situations are pulled together with the current live data (augmented context). Then the strategy model (LLM) synthesizes all of this into a recommendation: \"Based on similar situations at Silverstone 2022 and Barcelona 2023, we recommend pitting on lap 32 for hards.\" The key insight: the model doesn't need to *remember* every past race — it just needs to *read* the relevant ones and reason well."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    \"\"\"Complete Retrieval-Augmented Generation pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store, top_k=3):\n",
    "        self.store = vector_store\n",
    "        self.top_k = top_k\n",
    "    \n",
    "    def build_prompt(self, query, retrieved_chunks):\n",
    "        \"\"\"Build the prompt with retrieved context.\"\"\"\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[Source: {r['metadata']['source']}] {r['chunk']}\"\n",
    "            for r in retrieved_chunks\n",
    "        ])\n",
    "        \n",
    "        prompt = f\"\"\"Answer the question based on the provided context. If the context doesn't \n",
    "contain enough information, say so. Cite the source documents.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def query(self, question):\n",
    "        \"\"\"Full RAG pipeline: retrieve + build prompt.\"\"\"\n",
    "        # Step 1: Retrieve relevant chunks\n",
    "        results = self.store.search(question, top_k=self.top_k)\n",
    "        \n",
    "        # Step 2: Build augmented prompt\n",
    "        prompt = self.build_prompt(question, results)\n",
    "        \n",
    "        # Step 3: In production, send to LLM. Here we return the prompt.\n",
    "        return {\n",
    "            'prompt': prompt,\n",
    "            'retrieved': results,\n",
    "            'n_chunks': len(results),\n",
    "            'total_context_chars': sum(len(r['chunk']) for r in results),\n",
    "        }\n",
    "\n",
    "\n",
    "# Build and test the RAG pipeline\n",
    "rag = RAGPipeline(store, top_k=3)\n",
    "\n",
    "test_queries = [\n",
    "    \"How does backpropagation work?\",\n",
    "    \"What is the attention mechanism?\",\n",
    "    \"How is RLHF used to train language models?\",\n",
    "    \"What are word embeddings?\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    result = rag.query(query)\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(f\"  Retrieved {result['n_chunks']} chunks ({result['total_context_chars']} chars)\")\n",
    "    for r in result['retrieved']:\n",
    "        print(f\"    [{r['score']:.3f}] {r['metadata']['source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show what the LLM would see\n",
    "result = rag.query(\"What is the attention mechanism?\")\n",
    "print(\"=\" * 60)\n",
    "print(\"PROMPT SENT TO LLM:\")\n",
    "print(\"=\" * 60)\n",
    "print(result['prompt'][:1000])\n",
    "if len(result['prompt']) > 1000:\n",
    "    print(f\"\\n... ({len(result['prompt'])} total characters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 6. Reranking: Improving Retrieval Quality\n\nEmbedding-based retrieval is fast but approximate. A **reranker** takes the top-k results and re-scores them using a more powerful (but slower) model. This two-stage approach is the standard in production RAG:\n\n1. **Stage 1 (Retriever)**: Fast approximate search over millions of chunks → top-k candidates\n2. **Stage 2 (Reranker)**: Accurate scoring of k candidates → final ranked list\n\nThe reranker sees the query and chunk together (cross-attention), unlike the embedder which encodes them independently.\n\n**F1 analogy:** This is exactly how F1 teams filter data during a race. Stage 1 is the quick database scan: \"Pull all past situations involving tire degradation on a hot track.\" That might return 50 results. Stage 2 is the senior strategist reviewing those 50 and saying, \"Actually, only these 5 are truly comparable — the others were wet races or used different tire compounds.\" The reranker applies deeper analysis to a smaller candidate set, just like the experienced engineer applying judgment to narrow down the relevant precedents."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEncoderReranker:\n",
    "    \"\"\"Simplified cross-encoder reranker.\n",
    "    \n",
    "    In production, use a fine-tuned cross-encoder like ms-marco-MiniLM.\n",
    "    Here we simulate with a learned relevance scorer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedder):\n",
    "        self.embedder = embedder\n",
    "    \n",
    "    def score(self, query, chunk):\n",
    "        \"\"\"Score query-chunk relevance.\n",
    "        \n",
    "        Simulates a cross-encoder by computing multiple similarity features\n",
    "        and combining them.\n",
    "        \"\"\"\n",
    "        q_emb = self.embedder.embed(query)\n",
    "        c_emb = self.embedder.embed(chunk)\n",
    "        \n",
    "        # Feature 1: Cosine similarity (same as retriever)\n",
    "        cosine_sim = np.dot(q_emb, c_emb)\n",
    "        \n",
    "        # Feature 2: Term overlap (keyword matching)\n",
    "        q_tokens = set(re.findall(r'\\b[a-zA-Z]{2,}\\b', query.lower()))\n",
    "        c_tokens = set(re.findall(r'\\b[a-zA-Z]{2,}\\b', chunk.lower()))\n",
    "        if len(q_tokens) > 0:\n",
    "            term_overlap = len(q_tokens & c_tokens) / len(q_tokens)\n",
    "        else:\n",
    "            term_overlap = 0\n",
    "        \n",
    "        # Feature 3: Chunk length penalty (prefer focused chunks)\n",
    "        length_penalty = 1.0 / (1.0 + len(chunk) / 1000)\n",
    "        \n",
    "        # Combined score (weighted)\n",
    "        score = 0.5 * cosine_sim + 0.35 * term_overlap + 0.15 * length_penalty\n",
    "        return score\n",
    "    \n",
    "    def rerank(self, query, results):\n",
    "        \"\"\"Rerank a list of retrieval results.\"\"\"\n",
    "        scored = []\n",
    "        for r in results:\n",
    "            new_score = self.score(query, r['chunk'])\n",
    "            scored.append({**r, 'original_score': r['score'], 'rerank_score': new_score})\n",
    "        \n",
    "        scored.sort(key=lambda x: x['rerank_score'], reverse=True)\n",
    "        return scored\n",
    "\n",
    "\n",
    "# Demonstrate reranking\n",
    "reranker = CrossEncoderReranker(embedder)\n",
    "\n",
    "query = \"How does the reward model work in RLHF?\"\n",
    "initial_results = store.search(query, top_k=6)\n",
    "reranked_results = reranker.rerank(query, initial_results)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(f\"{'Rank':<5} {'Original':>10} {'Reranked':>10} {'Source':<20} {'Preview'}\")\n",
    "print(\"-\" * 80)\n",
    "for i, r in enumerate(reranked_results):\n",
    "    preview = r['chunk'][:50].replace('\\n', ' ') + '...'\n",
    "    print(f\"{i+1:<5} {r['original_score']:10.4f} {r['rerank_score']:10.4f} \"\n",
    "          f\"{r['metadata']['source']:<20} {preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 7. Evaluating RAG Systems\n\nRAG evaluation has two components:\n\n### Retrieval Metrics\n- **Precision@k**: Of the k retrieved chunks, how many are relevant?\n- **Recall@k**: Of all relevant chunks, how many were retrieved?\n- **MRR** (Mean Reciprocal Rank): Where does the first relevant result appear?\n\n### Generation Metrics\n- **Faithfulness**: Does the answer stick to the retrieved context? (No hallucination)\n- **Relevance**: Does the answer address the question?\n- **Groundedness**: Can every claim be traced to a source chunk?\n\n**F1 analogy:** Evaluating a RAG system is like evaluating the pit wall's information delivery. **Precision@k**: Of the 3 past race reports the system pulled up, how many were actually relevant to the current situation? **Recall@k**: Were there other critical past situations the system missed? **MRR**: Was the most relevant historical race the first one shown, or buried at position 5? On the generation side, **faithfulness** asks: did the strategy recommendation actually follow from the retrieved data, or did the model hallucinate a recommendation not supported by the evidence?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluator:\n",
    "    \"\"\"Evaluate RAG retrieval quality.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def precision_at_k(retrieved_sources, relevant_sources, k=None):\n",
    "        \"\"\"Fraction of retrieved items that are relevant.\"\"\"\n",
    "        if k:\n",
    "            retrieved_sources = retrieved_sources[:k]\n",
    "        if not retrieved_sources:\n",
    "            return 0.0\n",
    "        relevant_count = sum(1 for s in retrieved_sources if s in relevant_sources)\n",
    "        return relevant_count / len(retrieved_sources)\n",
    "    \n",
    "    @staticmethod\n",
    "    def recall_at_k(retrieved_sources, relevant_sources, k=None):\n",
    "        \"\"\"Fraction of relevant items that were retrieved.\"\"\"\n",
    "        if k:\n",
    "            retrieved_sources = retrieved_sources[:k]\n",
    "        if not relevant_sources:\n",
    "            return 0.0\n",
    "        relevant_count = sum(1 for s in retrieved_sources if s in relevant_sources)\n",
    "        return relevant_count / len(relevant_sources)\n",
    "    \n",
    "    @staticmethod\n",
    "    def mrr(retrieved_sources, relevant_sources):\n",
    "        \"\"\"Mean Reciprocal Rank: 1/rank of first relevant result.\"\"\"\n",
    "        for i, source in enumerate(retrieved_sources):\n",
    "            if source in relevant_sources:\n",
    "                return 1.0 / (i + 1)\n",
    "        return 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def context_relevance(query, chunks, embedder):\n",
    "        \"\"\"Average cosine similarity between query and retrieved chunks.\"\"\"\n",
    "        q_emb = embedder.embed(query)\n",
    "        sims = [np.dot(q_emb, embedder.embed(chunk)) for chunk in chunks]\n",
    "        return np.mean(sims)\n",
    "\n",
    "\n",
    "# Create evaluation dataset (query, expected relevant sources)\n",
    "eval_dataset = [\n",
    "    (\"How does backpropagation work?\", {\"neural_networks\"}),\n",
    "    (\"What is self-attention?\", {\"transformers\"}),\n",
    "    (\"How is RLHF used?\", {\"rlhf\"}),\n",
    "    (\"What are word embeddings?\", {\"embeddings\"}),\n",
    "    (\"How does PPO optimize language models?\", {\"rlhf\"}),\n",
    "    (\"What is the transformer architecture?\", {\"transformers\"}),\n",
    "    (\"How do neural networks learn?\", {\"neural_networks\"}),\n",
    "    (\"What is cosine similarity?\", {\"embeddings\"}),\n",
    "]\n",
    "\n",
    "evaluator = RAGEvaluator()\n",
    "\n",
    "# Evaluate retrieval\n",
    "all_p1, all_p3, all_mrr = [], [], []\n",
    "\n",
    "for query, relevant in eval_dataset:\n",
    "    results = store.search(query, top_k=5)\n",
    "    retrieved = [r['metadata']['source'] for r in results]\n",
    "    \n",
    "    p1 = evaluator.precision_at_k(retrieved, relevant, k=1)\n",
    "    p3 = evaluator.precision_at_k(retrieved, relevant, k=3)\n",
    "    mrr = evaluator.mrr(retrieved, relevant)\n",
    "    \n",
    "    all_p1.append(p1)\n",
    "    all_p3.append(p3)\n",
    "    all_mrr.append(mrr)\n",
    "\n",
    "print(\"RAG Retrieval Evaluation:\")\n",
    "print(f\"  Precision@1: {np.mean(all_p1):.3f}\")\n",
    "print(f\"  Precision@3: {np.mean(all_p3):.3f}\")\n",
    "print(f\"  MRR:         {np.mean(all_mrr):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-query retrieval performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Per-query metrics\n",
    "ax = axes[0]\n",
    "x = np.arange(len(eval_dataset))\n",
    "width = 0.25\n",
    "ax.bar(x - width, all_p1, width, label='P@1', color='#3498db', alpha=0.8)\n",
    "ax.bar(x, all_p3, width, label='P@3', color='#2ecc71', alpha=0.8)\n",
    "ax.bar(x + width, all_mrr, width, label='MRR', color='#e74c3c', alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([q[0][:20] + '...' for q in eval_dataset], rotation=45, ha='right', fontsize=8)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Per-Query Retrieval Metrics', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Summary metrics\n",
    "ax = axes[1]\n",
    "metrics = {'P@1': np.mean(all_p1), 'P@3': np.mean(all_p3), 'MRR': np.mean(all_mrr)}\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "bars = ax.bar(metrics.keys(), metrics.values(), color=colors, edgecolor='black', alpha=0.8)\n",
    "for bar, val in zip(bars, metrics.values()):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "            f'{val:.3f}', ha='center', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Average Retrieval Metrics', fontsize=13, fontweight='bold')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 8. Common RAG Failure Modes\n\nUnderstanding how RAG fails is as important as understanding how it works.\n\n| Failure Mode | Cause | Fix | F1 Parallel |\n|-------------|-------|-----|-------------|\n| **Missing context** | Relevant chunk not retrieved | Better chunking, more chunks, hybrid search | The database didn't have data from a comparable race — need to expand the archive |\n| **Wrong context** | Irrelevant chunks retrieved | Reranking, better embeddings, metadata filters | Pulling up a Monaco comparison when the current race is Monza — wrong track type entirely |\n| **Lost in the middle** | LLM ignores middle context | Put important context first/last | The critical data point was buried in the third report — the strategist only read the first and last |\n| **Hallucination** | LLM generates beyond context | Stronger prompting, faithfulness checks | The model recommends a three-stop strategy based on data that only supports two stops |\n| **Chunk boundary** | Answer spans two chunks | Overlapping chunks, larger windows | Tire degradation data is in one chunk, weather data in another — the full picture requires both |\n| **Embedding mismatch** | Query and doc use different vocabulary | Query expansion, hypothetical document embeddings | Engineer asks about \"graining\" but the report used \"front tire surface degradation\" |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the \"chunk boundary\" problem\n",
    "print(\"FAILURE MODE: Chunk Boundary Problem\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Information split across chunks\n",
    "text = \"\"\"The transformer uses multi-head attention with 8 heads. Each head has dimension 64.\n",
    "\n",
    "The total dimension is therefore 8 * 64 = 512, which is the model's hidden size.\"\"\"\n",
    "\n",
    "chunks = DocumentChunker.paragraph_based(text)\n",
    "print(f\"\\nOriginal text answers: 'What is the total dimension?'\")\n",
    "print(f\"But paragraph chunking splits it:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"  Chunk {i}: '{chunk.strip()}'\")\n",
    "print(f\"\\nChunk 0 has the components, Chunk 1 has the answer.\")\n",
    "print(f\"Retrieving only one chunk misses the complete picture.\")\n",
    "\n",
    "# Fix: overlapping chunks\n",
    "print(f\"\\nFIX: Sentence-based chunking with overlap\")\n",
    "chunks_fixed = DocumentChunker.sentence_based(text, max_sentences=4)\n",
    "for i, chunk in enumerate(chunks_fixed):\n",
    "    print(f\"  Chunk {i}: '{chunk.strip()[:100]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate hybrid search (keyword + semantic)\n",
    "class HybridSearch:\n",
    "    \"\"\"Combine keyword (BM25-like) and semantic search.\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store, alpha=0.5):\n",
    "        self.store = vector_store\n",
    "        self.alpha = alpha  # Weight for semantic vs keyword\n",
    "    \n",
    "    def keyword_score(self, query, chunk):\n",
    "        \"\"\"Simple keyword matching score (BM25 approximation).\"\"\"\n",
    "        q_tokens = set(re.findall(r'\\b[a-zA-Z]{3,}\\b', query.lower()))\n",
    "        c_tokens = re.findall(r'\\b[a-zA-Z]{3,}\\b', chunk.lower())\n",
    "        c_set = set(c_tokens)\n",
    "        \n",
    "        if not q_tokens:\n",
    "            return 0.0\n",
    "        \n",
    "        # Term frequency component\n",
    "        tf_score = 0\n",
    "        for token in q_tokens:\n",
    "            if token in c_set:\n",
    "                count = c_tokens.count(token)\n",
    "                tf_score += count / (count + 1.0)  # Saturating TF\n",
    "        \n",
    "        return tf_score / len(q_tokens)\n",
    "    \n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"Hybrid search combining semantic and keyword scores.\"\"\"\n",
    "        # Get all semantic scores\n",
    "        q_emb = self.store.embedder.embed(query)\n",
    "        \n",
    "        scored = []\n",
    "        for i, (emb, doc, meta) in enumerate(zip(\n",
    "            self.store.embeddings, self.store.documents, self.store.metadata\n",
    "        )):\n",
    "            semantic = np.dot(q_emb, emb)\n",
    "            keyword = self.keyword_score(query, doc)\n",
    "            combined = self.alpha * semantic + (1 - self.alpha) * keyword\n",
    "            scored.append({\n",
    "                'chunk': doc, 'score': combined,\n",
    "                'semantic_score': semantic, 'keyword_score': keyword,\n",
    "                'metadata': meta, 'index': i\n",
    "            })\n",
    "        \n",
    "        scored.sort(key=lambda x: x['score'], reverse=True)\n",
    "        return scored[:top_k]\n",
    "\n",
    "\n",
    "# Compare semantic vs hybrid\n",
    "hybrid = HybridSearch(store, alpha=0.6)\n",
    "\n",
    "query = \"Bradley-Terry preference model\"\n",
    "semantic_results = store.search(query, top_k=3)\n",
    "hybrid_results = hybrid.search(query, top_k=3)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Semantic search:\")\n",
    "for r in semantic_results:\n",
    "    print(f\"  [{r['score']:.3f}] {r['metadata']['source']}\")\n",
    "\n",
    "print(\"\\nHybrid search:\")\n",
    "for r in hybrid_results:\n",
    "    print(f\"  [{r['score']:.3f}] sem={r['semantic_score']:.3f} kw={r['keyword_score']:.3f} \"\n",
    "          f\"{r['metadata']['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 9. Advanced RAG Patterns\n\nProduction RAG systems go beyond basic retrieve-and-generate:\n\n| Pattern | Description | When to Use | F1 Parallel |\n|---------|------------|-------------|-------------|\n| **Query expansion** | Rewrite query for better retrieval | Ambiguous or short queries | Engineer says \"tires\" — expand to \"tire degradation compound temperature graining blistering\" |\n| **HyDE** | Generate hypothetical answer, embed that | Query-document vocabulary mismatch | Generate what a good race report *would* say, then find actual reports like it |\n| **Multi-step RAG** | Retrieve → reason → retrieve again | Complex multi-hop questions | \"Find races with similar weather, then from those find ones with similar tire strategy outcomes\" |\n| **Parent-child chunking** | Retrieve small chunks, return parent | Need precision + context | Find the exact sentence about tire life, but return the full strategy section for context |\n| **Metadata filtering** | Filter by date, source, category | Large corpora with structure | Only search races from this season, or only search data from this specific circuit |\n| **Agentic RAG** | Agent decides when/what to retrieve | Open-ended tasks | The strategy model decides whether it needs historical data or can answer from current telemetry alone |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement HyDE (Hypothetical Document Embeddings)\n",
    "class HyDE:\n",
    "    \"\"\"Hypothetical Document Embeddings.\n",
    "    \n",
    "    Instead of embedding the query directly, generate a hypothetical answer\n",
    "    and embed THAT. The hypothetical answer will use vocabulary closer to\n",
    "    the actual documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store):\n",
    "        self.store = vector_store\n",
    "    \n",
    "    def generate_hypothetical(self, query):\n",
    "        \"\"\"Simulate generating a hypothetical answer.\n",
    "        In production, this would use an LLM.\"\"\"\n",
    "        # Simple simulation: expand query with related terms\n",
    "        expansions = {\n",
    "            'attention': 'self-attention computes query key value matrices dot product softmax weights',\n",
    "            'backprop': 'backpropagation chain rule gradients loss function weights update',\n",
    "            'rlhf': 'reinforcement learning human feedback reward model PPO policy optimization',\n",
    "            'embedding': 'word embeddings dense vectors semantic similarity cosine distance',\n",
    "        }\n",
    "        \n",
    "        hypothetical = query\n",
    "        for key, expansion in expansions.items():\n",
    "            if key in query.lower():\n",
    "                hypothetical = f\"{query} {expansion}\"\n",
    "                break\n",
    "        \n",
    "        return hypothetical\n",
    "    \n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"Search using hypothetical document embedding.\"\"\"\n",
    "        hypothetical = self.generate_hypothetical(query)\n",
    "        \n",
    "        # Embed the hypothetical answer instead of the raw query\n",
    "        query_emb = self.store.embedder.embed(hypothetical)\n",
    "        \n",
    "        similarities = np.array([\n",
    "            np.dot(query_emb, emb) for emb in self.store.embeddings\n",
    "        ])\n",
    "        \n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'chunk': self.store.documents[idx],\n",
    "                'score': similarities[idx],\n",
    "                'metadata': self.store.metadata[idx],\n",
    "            })\n",
    "        return results\n",
    "\n",
    "\n",
    "hyde = HyDE(store)\n",
    "query = \"How does attention work?\"\n",
    "\n",
    "normal_results = store.search(query, top_k=3)\n",
    "hyde_results = hyde.search(query, top_k=3)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Standard retrieval:\")\n",
    "for r in normal_results:\n",
    "    print(f\"  [{r['score']:.3f}] {r['metadata']['source']}\")\n",
    "print(\"\\nHyDE retrieval:\")\n",
    "for r in hyde_results:\n",
    "    print(f\"  [{r['score']:.3f}] {r['metadata']['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercises\n\n### Exercise 1: Chunking Strategy Comparison — Race Report Optimization\n\nImagine you're building a RAG system for an F1 team's race report archive. Run the full RAG evaluation with each chunking strategy (fixed, sentence, paragraph). Which strategy gives the best retrieval precision? Think about how this applies to chunking race reports: would you split by fixed character count (potentially cutting mid-sentence about tire compounds), by sentence (natural boundaries), or by paragraph (one topic per chunk, like \"pit stop analysis\" or \"weather impact\")? Does the best chunking strategy depend on the query type?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Hint: Build separate vector stores for each chunking strategy,\n",
    "# run the same eval queries, and compare P@1, P@3, MRR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2: Multi-Hop RAG — Cross-Referencing Race Data\n\nImplement a two-step RAG system where the first retrieval provides context that helps reformulate the query for a second retrieval. This mirrors how an F1 strategist might first look up \"races with similar tire degradation patterns\" and then use those results to refine their search to \"what pit stop strategies worked in those specific races?\" Test with the question: \"What algorithm from NB20 uses the mechanism from NB12?\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Hint: Retrieve once, extract key terms from results,\n",
    "# reformulate query, retrieve again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 3: RAG with Metadata Filtering — Circuit-Specific Search\n\nExtend the VectorStore to support metadata filtering (e.g., only search within `source='transformers'`). In F1 terms, this is like filtering your race database to only search within a specific circuit or season — \"only show me data from Silverstone\" or \"only recent regulation era.\" Show how filtering improves precision when the user specifies a topic."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "# Hint: Add a filter parameter to VectorStore.search()\n",
    "# that skips chunks not matching the filter criteria\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Summary\n\n### Key Concepts\n\n| Concept | Definition | F1 Parallel |\n|---------|-----------|-------------|\n| **RAG** | Augments LLMs with external knowledge at inference time — no retraining needed | Race engineer querying the team's historical database mid-race instead of relying on memory |\n| **Chunking strategy** | Splitting documents into searchable pieces — too small loses context, too large dilutes relevance | Breaking race reports into sections: tire strategy, weather, pit stops — right granularity matters |\n| **Vector stores** | Index embeddings for fast similarity search using cosine similarity | The team's knowledge base indexed by situation similarity, not just date or circuit name |\n| **Reranking** | Rescoring candidates with a cross-encoder for improved precision | Senior strategist reviewing the initial search results and filtering to the truly relevant ones |\n| **Hybrid search** | Combining semantic + keyword search outperforms either alone | Matching both by meaning (\"similar conditions\") and keywords (\"medium compound, lap 25\") |\n| **Retrieval metrics** | P@k, Recall@k, MRR quantify how well the right chunks are found | Measuring if the pit wall is pulling up the right historical data when it matters |\n| **Advanced patterns** | HyDE, multi-hop RAG, and agentic RAG handle complex queries | Multi-step data lookups: find similar weather, then find strategies that worked in that weather |\n\n### Fundamental Insight\n\nRAG transforms LLMs from closed-book test-takers into open-book researchers. By separating knowledge storage (vector store) from reasoning (LLM), RAG systems stay current, reduce hallucination, and work with private data — making them the most practical pattern for production AI systems. In F1 terms, you're giving the strategy model access to the full team archive instead of asking it to memorize everything from training. The model's job is to *reason* over the retrieved data, not to *store* all the data in its weights."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Next Steps\n\nRAG retrieves knowledge, but sometimes an AI system needs to **take actions** — search the web, call APIs, write code, or interact with tools. In **Notebook 22: AI Agents & Tool Use**, we'll build agents that reason about *what to do* and *when to do it*, using the ReAct pattern and multi-step planning. In F1 terms, we're moving from the strategist who *looks up data* to the race engineer who *makes decisions and executes them* — querying the weather API, running the tire model, adjusting the fuel calculator, and radioing the driver with the call."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}