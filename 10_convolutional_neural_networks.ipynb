{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ds0lzkdtvn",
   "source": "# Part 4.1: Convolutional Neural Networks (CNNs) â€” The Formula 1 Edition\n\nImages have **spatial structure** -- pixels near each other are related. Fully connected networks ignore this structure entirely, treating each pixel as an independent input. Convolutional Neural Networks exploit spatial patterns by learning local filters that slide across the image, dramatically reducing parameters while capturing the features that matter: edges, textures, shapes, and objects.\n\nCNNs are the foundation of modern computer vision and have transformed everything from medical imaging to self-driving cars.\n\n**F1 analogy:** Think of a CNN the way an F1 engineer scans telemetry data. You don't stare at every single data point from a 78-lap race simultaneously -- you slide a window across the trace, looking for local patterns: a braking spike here, a traction event there, a temperature anomaly in sector 2. Convolution filters are exactly this: small pattern-detectors that sweep across data, flagging where interesting things happen. Pooling is how you zoom out from thousands of data points per lap to a handful of key sector metrics. And the feature hierarchy CNNs learn -- edges, textures, parts, objects -- mirrors how telemetry analysis progresses from raw sensor spikes to driving-style classification.\n\n---\n\n## Learning Objectives\n\nBy the end of this notebook, you should be able to:\n\n- [ ] Explain why fully connected networks are impractical for images and how CNNs solve this\n- [ ] Perform 2D convolution by hand and understand padding, stride, and output size\n- [ ] Describe how multiple filters create feature maps and what filters learn at different depths\n- [ ] Explain the purpose of pooling layers and compare max vs average pooling\n- [ ] Build a CNN in PyTorch using nn.Conv2d, nn.MaxPool2d, and linear layers\n- [ ] Describe the key innovations in LeNet-5, VGG, and ResNet\n- [ ] Implement skip connections and explain why they help training\n- [ ] Train a CNN on a real dataset with data augmentation and evaluate with a confusion matrix",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "knu0ph35kq8",
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\n\n%matplotlib inline\nplt.style.use('seaborn-v0_8-whitegrid')\ntorch.manual_seed(42)\nnp.random.seed(42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d84mt9cx1ka",
   "source": "---\n\n## 1. Why CNNs?\n\n### Intuitive Explanation\n\nImagine you have a 256x256 color image. As a flat vector, that is 256 x 256 x 3 = **196,608 input values**. If the first hidden layer has just 1,000 neurons, you need 196,608 x 1,000 = **~197 million** parameters in the first layer alone. This is absurd for several reasons:\n\n1. **Parameter explosion**: More parameters means more memory, more compute, and much more data needed to train\n2. **No spatial awareness**: A fully connected layer treats pixel (0,0) and pixel (255,255) identically -- it does not know they are far apart\n3. **No translation invariance**: If the network learns to detect a cat in the top-left corner, it cannot recognize the same cat in the bottom-right without learning entirely separate weights\n\nCNNs solve all three problems with three key ideas:\n\n| Key Idea | What It Means | Benefit | F1 Parallel |\n|----------|--------------|---------|-------------|\n| **Local connectivity** | Each neuron connects to only a small patch of the input | Massively fewer parameters | An engineer looks at a short window of telemetry, not the entire race at once |\n| **Weight sharing** | The same filter is applied everywhere across the image | Learns one detector, uses it everywhere | The same \"braking-zone detector\" works at every corner on every circuit |\n| **Translation invariance** | A feature detected anywhere produces the same response | Recognizes objects regardless of position | A lockup signature looks the same whether it happens at Turn 1 or Turn 15 |\n\n**F1 analogy:** Imagine analyzing telemetry from a 5.4 km track with sensors logging 300 Hz -- that is over 1.6 million data points per lap. A fully connected approach would try to learn separate weights for every single sample point. A CNN-style approach slides a small filter (say, a 50-sample window) across the trace, detecting patterns like braking events, throttle lifts, and traction spikes regardless of where on the circuit they occur.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "auwm881etam",
   "source": "### Visualization: FC vs CNN Parameter Comparison",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "3e74lp5f0lv",
   "source": "# Visualize the parameter explosion problem\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: Parameter count comparison\nimage_sizes = [28, 64, 128, 256, 512]\nhidden = 1000\nfc_params = [s * s * 3 * hidden for s in image_sizes]\n# CNN: 3 input channels, 32 filters, 3x3 kernel\ncnn_params = [3 * 32 * 3 * 3 + 32 for _ in image_sizes]  # Same regardless of image size!\n\nax = axes[0]\nx_pos = np.arange(len(image_sizes))\nwidth = 0.35\nbars1 = ax.bar(x_pos - width/2, [p / 1e6 for p in fc_params], width, \n               label='Fully Connected', color='red', alpha=0.7)\nbars2 = ax.bar(x_pos + width/2, [p / 1e6 for p in cnn_params], width, \n               label='Conv Layer (3x3, 32 filters)', color='blue', alpha=0.7)\nax.set_xlabel('Input Image Size')\nax.set_ylabel('Parameters (millions)')\nax.set_title('Parameters in First Layer: FC vs CNN')\nax.set_xticks(x_pos)\nax.set_xticklabels([f'{s}x{s}x3' for s in image_sizes], rotation=15)\nax.legend()\nax.set_yscale('log')\nax.grid(True, alpha=0.3)\n\n# Add value labels on FC bars\nfor bar, val in zip(bars1, fc_params):\n    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() * 1.1,\n            f'{val/1e6:.1f}M', ha='center', va='bottom', fontsize=8)\n\n# Right: Connectivity diagram\nax = axes[1]\nax.set_xlim(0, 10)\nax.set_ylim(0, 10)\n\n# Draw \"image\" grid (input)\nfor i in range(5):\n    for j in range(5):\n        color = 'lightblue' if not (1 <= i <= 3 and 1 <= j <= 3) else 'orange'\n        alpha = 0.3 if color == 'lightblue' else 0.7\n        rect = plt.Rectangle((0.5 + j * 0.7, 6.5 - i * 0.7), 0.6, 0.6, \n                              facecolor=color, edgecolor='black', alpha=alpha, linewidth=1)\n        ax.add_patch(rect)\n\n# Label\nax.text(2.3, 4.3, 'Input (5x5)', ha='center', fontsize=10, fontweight='bold')\nax.text(2.3, 3.8, 'Orange = local\\nreceptive field', ha='center', fontsize=8, color='darkorange')\n\n# Draw filter\nfor i in range(3):\n    for j in range(3):\n        rect = plt.Rectangle((6 + j * 0.7, 7.5 - i * 0.7), 0.6, 0.6,\n                              facecolor='green', edgecolor='black', alpha=0.6, linewidth=1)\n        ax.add_patch(rect)\n\nax.text(7.1, 5.7, 'Filter (3x3)', ha='center', fontsize=10, fontweight='bold')\nax.text(7.1, 5.2, '9 shared weights', ha='center', fontsize=8, color='green')\n\n# Arrow from receptive field to filter\nax.annotate('', xy=(6, 7.8), xytext=(4.2, 7.8),\n            arrowprops=dict(arrowstyle='->', color='black', lw=2))\nax.text(5.1, 8.2, 'Same filter\\nslides across\\nentire image', ha='center', fontsize=8)\n\n# Draw output neuron\ncircle = plt.Circle((8.5, 3.5), 0.4, facecolor='red', edgecolor='black', alpha=0.7)\nax.add_patch(circle)\nax.text(8.5, 2.7, 'Output\\nneuron', ha='center', fontsize=9)\n\n# Arrow from filter to output\nax.annotate('', xy=(8.5, 3.9), xytext=(7.5, 5.7),\n            arrowprops=dict(arrowstyle='->', color='black', lw=2))\n\nax.set_title('CNN: Local Connectivity + Weight Sharing')\nax.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Print the dramatic difference\nprint(\"Parameter comparison for 256x256x3 image:\")\nprint(f\"  Fully Connected (1000 neurons): {256*256*3*1000:>15,} parameters\")\nprint(f\"  Conv Layer (32 3x3 filters):    {3*32*3*3+32:>15,} parameters\")\nprint(f\"  Reduction factor:               {256*256*3*1000 / (3*32*3*3+32):>15,.0f}x fewer!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2t108gbg0a3",
   "source": "### Deep Dive: Why Spatial Structure Matters\n\nConsider how you actually look at an image. You do not process every pixel independently -- you see **local patterns**: edges, corners, textures. A vertical edge is defined by neighboring pixels being very different (dark on one side, light on the other). This is fundamentally a **local** property.\n\n**F1 analogy:** Telemetry has the same kind of local structure. A braking zone is defined by a sharp drop in speed and a spike in brake pressure within a short window. A traction event is a brief burst of wheelspin followed by recovery. These are local patterns -- you detect them by looking at neighboring data points, not by comparing the start of the lap to the end.\n\n#### Key Insight\n\nCNNs mirror how biological vision works: simple cells in the visual cortex respond to edges at specific locations and orientations, then more complex cells combine these into higher-level features. CNNs learn a similar hierarchy automatically.\n\n#### Common Misconceptions\n\n| Misconception | Reality |\n|---------------|--------|\n| CNNs only work on images | They work on any data with spatial/temporal structure (audio, time series, graphs) -- including 1D telemetry traces |\n| More parameters = better | CNNs show fewer parameters with weight sharing often works *better* due to regularization effect |\n| CNNs replace fully connected layers | CNNs typically end with FC layers for final classification |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "j6u55b8q6wj",
   "source": "---\n\n## 2. Convolution Operation\n\n### Intuitive Explanation\n\n**Convolution** is a mathematical operation that combines two functions to produce a third. In CNNs, think of it as a **sliding window** operation:\n\n1. Take a small filter (e.g., 3x3 grid of weights)\n2. Place it on the top-left corner of the image\n3. Multiply each filter weight by the corresponding pixel value\n4. Sum all the products to get one output value\n5. Slide the filter one position to the right and repeat\n6. When you reach the right edge, move down one row and start again from the left\n\nThe result is a **feature map** -- a new, smaller image where each value tells you \"how much does this local region match the filter pattern?\"\n\n**F1 analogy:** Picture scanning a speed trace from a race lap. You take a small template -- say, the signature shape of a heavy braking event (speed dropping sharply, then leveling) -- and slide it along the entire trace. At every position, you compute a similarity score. Where the trace matches the template, you get a big number. The output is a \"braking event map\" showing where on the circuit the driver braked hard. Different templates detect different events: throttle lifts, DRS activations, traction loss. Each template is a convolution filter.\n\n### 2.1 1D Convolution: The Simplest Case\n\nBefore tackling 2D images, let us see convolution in 1D -- it is just a sliding dot product.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "lhgc9rujcw",
   "source": "# 1D Convolution: sliding window visualization\nsignal = np.array([1, 3, 2, 5, 4, 1, 3, 2])\nkernel = np.array([1, 0, -1])  # Edge detector\n\n# Manual convolution\noutput_size = len(signal) - len(kernel) + 1\noutput = np.zeros(output_size)\nfor i in range(output_size):\n    output[i] = np.sum(signal[i:i+len(kernel)] * kernel)\n\nfig, axes = plt.subplots(3, 1, figsize=(12, 8))\n\n# Signal\nax = axes[0]\nax.bar(range(len(signal)), signal, color='blue', alpha=0.7, edgecolor='black')\nfor i, v in enumerate(signal):\n    ax.text(i, v + 0.15, str(v), ha='center', fontsize=12, fontweight='bold')\nax.set_title('Input Signal', fontsize=13)\nax.set_ylabel('Value')\nax.set_xticks(range(len(signal)))\nax.grid(True, alpha=0.3)\n\n# Kernel\nax = axes[1]\ncolors_k = ['green' if v >= 0 else 'red' for v in kernel]\nax.bar(range(len(kernel)), kernel, color=colors_k, alpha=0.7, edgecolor='black')\nfor i, v in enumerate(kernel):\n    ax.text(i, v + 0.1 * np.sign(v), str(v), ha='center', fontsize=12, fontweight='bold')\nax.set_title('Kernel [1, 0, -1] (detects changes/edges)', fontsize=13)\nax.set_ylabel('Value')\nax.set_xticks(range(len(kernel)))\nax.axhline(y=0, color='black', linewidth=0.5)\nax.grid(True, alpha=0.3)\n\n# Output with step-by-step\nax = axes[2]\ncolors_out = ['green' if v >= 0 else 'red' for v in output]\nax.bar(range(len(output)), output, color=colors_out, alpha=0.7, edgecolor='black')\nfor i, v in enumerate(output):\n    # Show computation\n    s = signal[i:i+len(kernel)]\n    comp = f'{s[0]}*1 + {s[1]}*0 + {s[2]}*(-1) = {int(v)}'\n    ax.text(i, v + 0.3 * np.sign(v) if v != 0 else 0.3, comp, \n            ha='center', fontsize=8, rotation=0)\nax.set_title('Output (Convolution Result)', fontsize=13)\nax.set_ylabel('Value')\nax.set_xlabel('Position')\nax.set_xticks(range(len(output)))\nax.axhline(y=0, color='black', linewidth=0.5)\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Positive output = signal is increasing (left < right)\")\nprint(\"Negative output = signal is decreasing (left > right)\")\nprint(\"Zero output = no change\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "q3j3rkz9q1p",
   "source": "### 2.2 2D Convolution Step-by-Step\n\nNow the same idea in 2D. The filter slides across both rows and columns of the image.\n\n$$\\text{Output}(i, j) = \\sum_{m=0}^{K-1} \\sum_{n=0}^{K-1} \\text{Input}(i+m, j+n) \\cdot \\text{Kernel}(m, n)$$\n\n#### Breaking down the formula:\n\n| Component | Meaning | Typical Values | F1 Analogy |\n|-----------|---------|----------------|------------|\n| $K$ | Kernel size (height and width) | 3, 5, 7 | Size of the telemetry window you scan |\n| $\\text{Input}(i+m, j+n)$ | Pixel value at position (i+m, j+n) | 0 to 255 (images) | Sensor reading at a given sample |\n| $\\text{Kernel}(m, n)$ | Filter weight at position (m, n) | Learned during training | Template shape you are looking for |\n| $\\text{Output}(i, j)$ | Feature map value at position (i, j) | Any real number | Match strength at that position |\n\n**What this means:** At each output position, we compute a weighted sum of the input pixels in the local neighborhood, using the kernel weights. If the local pattern matches the kernel, the output is large.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "sjll14gpquk",
   "source": "def conv2d_manual(image, kernel):\n    \"\"\"\n    Perform 2D convolution manually (valid mode, no padding).\n    \n    Args:\n        image: 2D numpy array (H x W)\n        kernel: 2D numpy array (K x K)\n    \n    Returns:\n        Output feature map\n    \"\"\"\n    H, W = image.shape\n    K = kernel.shape[0]\n    out_h = H - K + 1\n    out_w = W - K + 1\n    output = np.zeros((out_h, out_w))\n    \n    for i in range(out_h):\n        for j in range(out_w):\n            patch = image[i:i+K, j:j+K]\n            output[i, j] = np.sum(patch * kernel)\n    \n    return output\n\n# Step-by-step 2D convolution visualization\nimage = np.array([\n    [1, 2, 0, 1, 3],\n    [0, 1, 3, 2, 1],\n    [1, 3, 1, 0, 2],\n    [2, 1, 0, 3, 1],\n    [0, 2, 1, 1, 0]\n], dtype=float)\n\nkernel = np.array([\n    [1, 0, -1],\n    [1, 0, -1],\n    [1, 0, -1]\n], dtype=float)\n\noutput = conv2d_manual(image, kernel)\n\n# Visualize the sliding window at different positions\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\npositions = [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2)]\n\nfor ax, (pi, pj) in zip(axes.flat, positions):\n    # Show the full image\n    ax.imshow(np.ones_like(image) * 0.9, cmap='gray', vmin=0, vmax=1, \n              extent=[-0.5, 4.5, 4.5, -0.5])\n    \n    # Highlight the receptive field\n    for mi in range(3):\n        for mj in range(3):\n            r, c = pi + mi, pj + mj\n            # Color based on kernel weight\n            if kernel[mi, mj] > 0:\n                color = 'lightgreen'\n            elif kernel[mi, mj] < 0:\n                color = 'lightsalmon'\n            else:\n                color = 'lightyellow'\n            rect = plt.Rectangle((c - 0.5, r - 0.5), 1, 1, \n                                  facecolor=color, edgecolor='black', linewidth=2, alpha=0.7)\n            ax.add_patch(rect)\n    \n    # Show all pixel values\n    for i in range(5):\n        for j in range(5):\n            ax.text(j, i, f'{int(image[i, j])}', ha='center', va='center', fontsize=12)\n    \n    # Compute and show result\n    patch = image[pi:pi+3, pj:pj+3]\n    result = np.sum(patch * kernel)\n    \n    # Build computation string\n    terms = []\n    for mi in range(3):\n        for mj in range(3):\n            if kernel[mi, mj] != 0:\n                terms.append(f'{int(image[pi+mi, pj+mj])}*({int(kernel[mi, mj])})')\n    \n    ax.set_title(f'Position ({pi},{pj}): output = {int(result)}', fontsize=11, fontweight='bold')\n    ax.set_xlim(-0.5, 4.5)\n    ax.set_ylim(4.5, -0.5)\n    ax.set_xticks(range(5))\n    ax.set_yticks(range(5))\n    ax.grid(True, alpha=0.3)\n\nplt.suptitle('2D Convolution: Sliding a Vertical Edge Detector [1,0,-1; 1,0,-1; 1,0,-1]', \n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"Full output feature map:\")\nprint(output.astype(int))\nprint(\"\\nGreen = positive kernel weight, Red = negative, Yellow = zero\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bjctugfzp2",
   "source": "### 2.3 Kernels as Feature Detectors\n\nDifferent kernels detect different features. Here are classic examples applied to a real-looking image.\n\n**F1 analogy:** In telemetry analysis, you might have different \"kernels\" for different events: a `[1, 0, -1]` filter detects sudden changes (like a braking point), a `[1, 1, 1]` averaging filter smooths out noise to reveal underlying trends, and a `[-1, 2, -1]` filter highlights spikes (like a wheel lockup or a kerb strike). Each kernel is a specialized detector for a different type of on-track event.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "xeagzezpell",
   "source": "# Create a simple test image with clear features\ndef make_test_image(size=64):\n    \"\"\"Create a test image with edges, corners, and gradients.\"\"\"\n    img = np.zeros((size, size))\n    \n    # Bright rectangle\n    img[10:30, 10:30] = 1.0\n    \n    # Diagonal line\n    for i in range(size):\n        j = i\n        if 0 <= j < size:\n            img[max(0,i-1):min(size,i+2), max(0,j-1):min(size,j+2)] = max(\n                img[max(0,i-1), max(0,j-1)], 0.7)\n    \n    # Circle\n    cy, cx = 45, 45\n    for i in range(size):\n        for j in range(size):\n            if abs((i - cy)**2 + (j - cx)**2 - 100) < 40:\n                img[i, j] = 1.0\n    \n    # Gradient region\n    img[35:55, 5:25] = np.tile(np.linspace(0, 1, 20), (20, 1))\n    \n    return img\n\n# Define classic kernels\nkernels = {\n    'Vertical Edge': np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=float),  # Sobel X\n    'Horizontal Edge': np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=float),  # Sobel Y\n    'Sharpen': np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]], dtype=float),\n    'Blur (Box)': np.ones((3, 3), dtype=float) / 9.0,\n    'Emboss': np.array([[-2, -1, 0], [-1, 1, 1], [0, 1, 2]], dtype=float),\n    'Laplacian': np.array([[0, 1, 0], [1, -4, 1], [0, 1, 0]], dtype=float),\n}\n\ntest_img = make_test_image()\n\n# Show original + all kernel results\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\n\n# Original\naxes[0, 0].imshow(test_img, cmap='gray')\naxes[0, 0].set_title('Original Image', fontsize=11, fontweight='bold')\naxes[0, 0].axis('off')\n\n# Show kernel values in second position\nax = axes[0, 1]\nax.axis('off')\nax.set_title('Kernel Values', fontsize=11, fontweight='bold')\nkernel_text = \"Classic 3x3 kernels:\\n\\n\"\nfor name, k in list(kernels.items())[:3]:\n    kernel_text += f\"{name}:\\n{k}\\n\\n\"\nax.text(0.05, 0.95, kernel_text, transform=ax.transAxes, fontsize=7,\n        verticalalignment='top', fontfamily='monospace')\n\n# Apply each kernel\nfor idx, (name, kernel) in enumerate(kernels.items()):\n    row = (idx + 2) // 4\n    col = (idx + 2) % 4\n    result = conv2d_manual(test_img, kernel)\n    axes[row, col].imshow(result, cmap='RdBu', vmin=-result.max(), vmax=result.max())\n    axes[row, col].set_title(name, fontsize=11, fontweight='bold')\n    axes[row, col].axis('off')\n\nplt.suptitle('Different Kernels Detect Different Features', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c991wok8lmt",
   "source": "### Interactive: Apply Different Kernels to a Simple Image\n\nLet us create a custom kernel and see what it detects.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "czfsu0yf3lq",
   "source": "# Interactive: Vary kernel parameters and see results\n# Create a checkerboard + stripe image\nimg_size = 48\ninteractive_img = np.zeros((img_size, img_size))\n\n# Add horizontal stripes\nfor i in range(0, img_size, 8):\n    interactive_img[i:i+4, :24] = 1.0\n\n# Add vertical stripes\nfor j in range(24, img_size, 8):\n    interactive_img[:24, j:j+4] = 1.0\n\n# Add diagonal\nfor i in range(24, img_size):\n    j = i - 24 + 24\n    if j < img_size:\n        interactive_img[i, j] = 1.0\n        if j+1 < img_size:\n            interactive_img[i, j+1] = 1.0\n\n# Add a filled region\ninteractive_img[30:45, 28:43] = 0.8\n\n# Test multiple kernel orientations\nangles_kernels = {\n    'Vertical\\n[1,0,-1]': np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]]),\n    'Horizontal\\n[-1,-1,-1; 0,0,0; 1,1,1]': np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]]),\n    'Diagonal /\\n[0,0,1; 0,0,0; -1,0,0]': np.array([[0, 0, 1], [0, 0, 0], [-1, 0, 0]]),\n    'Diagonal \\\\\\n[1,0,0; 0,0,0; 0,0,-1]': np.array([[1, 0, 0], [0, 0, 0], [0, 0, -1]]),\n    'Corner\\n[1,-1; -1,1] (padded)': np.array([[1, -1, 0], [-1, 1, 0], [0, 0, 0]]),\n    'Gaussian Blur\\n(3x3)': np.array([[1, 2, 1], [2, 4, 2], [1, 2, 1]]) / 16.0,\n}\n\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\n\naxes[0, 0].imshow(interactive_img, cmap='gray')\naxes[0, 0].set_title('Original', fontsize=10, fontweight='bold')\naxes[0, 0].axis('off')\n\n# Empty slot\naxes[0, 1].axis('off')\naxes[0, 1].text(0.5, 0.5, 'Stripes respond\\nto edge detectors\\nin matching\\norientation', \n                ha='center', va='center', fontsize=10, transform=axes[0, 1].transAxes)\n\nfor idx, (name, kern) in enumerate(angles_kernels.items()):\n    row = (idx + 2) // 4\n    col = (idx + 2) % 4\n    result = conv2d_manual(interactive_img, kern)\n    axes[row, col].imshow(np.abs(result), cmap='hot')\n    axes[row, col].set_title(name, fontsize=8, fontweight='bold')\n    axes[row, col].axis('off')\n\nplt.suptitle('Different Kernel Orientations Detect Different Features', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "nhdg8ra3jk",
   "source": "### 2.4 Padding\n\nWithout padding, the output is smaller than the input (we lose border pixels). **Padding** adds extra pixels around the border to control the output size.\n\n| Padding Type | Description | Output Size |\n|-------------|-------------|-------------|\n| **Valid (no padding)** | No padding at all | Smaller than input |\n| **Same (zero padding)** | Pad so output = input size | Same as input |\n\n**Why same padding matters:** Without it, every layer shrinks the feature map. After many layers, you would have nothing left!\n\n**F1 analogy:** Imagine your telemetry trace starts at the pit exit and ends at the pit entry. Without padding, your braking-zone filter cannot fully analyze the very first and last corners because there is not enough data on either side. Padding is like extending the trace with zeros so you can properly analyze the edges of the data -- just as an engineer might pad the start/end of a session log to avoid edge artifacts.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "lz0hhl9kw1",
   "source": "# Visualize padding\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nsmall_img = np.array([\n    [1, 2, 3, 4, 5],\n    [6, 7, 8, 9, 10],\n    [11, 12, 13, 14, 15],\n    [16, 17, 18, 19, 20],\n    [21, 22, 23, 24, 25]\n], dtype=float)\n\nkern_3x3 = np.ones((3, 3)) / 9.0  # Average filter\n\n# No padding (valid)\nax = axes[0]\nout_valid = conv2d_manual(small_img, kern_3x3)\nax.imshow(np.ones((5, 5)) * 0.9, cmap='gray', vmin=0, vmax=1, extent=[-0.5, 4.5, 4.5, -0.5])\nfor i in range(5):\n    for j in range(5):\n        ax.text(j, i, f'{int(small_img[i,j])}', ha='center', va='center', fontsize=10)\n# Highlight valid output region\nrect = plt.Rectangle((0.5, 0.5), 3, 3, linewidth=3, edgecolor='red', facecolor='red', alpha=0.15)\nax.add_patch(rect)\nax.set_title(f'Valid (no padding)\\nInput: 5x5, Output: {out_valid.shape[0]}x{out_valid.shape[1]}', fontsize=11)\nax.set_xticks(range(5))\nax.set_yticks(range(5))\n\n# Same padding (pad=1)\nax = axes[1]\npadded = np.pad(small_img, 1, mode='constant', constant_values=0)\nax.imshow(np.ones((7, 7)) * 0.9, cmap='gray', vmin=0, vmax=1, extent=[-0.5, 6.5, 6.5, -0.5])\nfor i in range(7):\n    for j in range(7):\n        val = int(padded[i, j])\n        color = 'blue' if val > 0 else 'gray'\n        ax.text(j, i, f'{val}', ha='center', va='center', fontsize=9, color=color)\n# Highlight padding\nfor i in range(7):\n    for j in range(7):\n        if i == 0 or i == 6 or j == 0 or j == 6:\n            rect = plt.Rectangle((j-0.5, i-0.5), 1, 1, facecolor='lightyellow', \n                                  edgecolor='orange', linewidth=1, alpha=0.5)\n            ax.add_patch(rect)\n\nout_same = conv2d_manual(padded, kern_3x3)\nax.set_title(f'Same padding (P=1)\\nInput: 5x5 + pad, Output: {out_same.shape[0]}x{out_same.shape[1]}', fontsize=11)\nax.set_xticks(range(7))\nax.set_yticks(range(7))\n\n# Show both outputs\nax = axes[2]\nax.text(0.5, 0.85, 'Output Comparison', ha='center', fontsize=12, fontweight='bold',\n        transform=ax.transAxes)\nax.text(0.5, 0.7, f'Valid: {out_valid.shape[0]}x{out_valid.shape[1]} (shrinks!)', \n        ha='center', fontsize=11, color='red', transform=ax.transAxes)\nax.text(0.5, 0.55, f'Same:  {out_same.shape[0]}x{out_same.shape[1]} (preserved!)', \n        ha='center', fontsize=11, color='green', transform=ax.transAxes)\nax.text(0.5, 0.35, 'Formula for \"same\" padding:\\nP = (K - 1) / 2\\n\\nFor 3x3 kernel: P = 1\\nFor 5x5 kernel: P = 2', \n        ha='center', fontsize=10, transform=ax.transAxes)\nax.axis('off')\n\nplt.suptitle('Padding: Controlling Output Size', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "y87sfmc0d8l",
   "source": "### 2.5 Stride\n\n**Stride** controls how far the filter moves at each step. Stride=1 means move one pixel at a time. Stride=2 means skip every other position, producing an output half the size.\n\n**F1 analogy:** Stride=1 is like analyzing telemetry at full resolution (every millisecond). Stride=2 is like downsampling to every other sample -- you lose some granularity but process twice as fast. For a strategy overview you might use a large stride; for detailed corner analysis you want stride=1.\n\n### Output Size Formula\n\n$$O = \\left\\lfloor \\frac{W - K + 2P}{S} \\right\\rfloor + 1$$\n\n#### Breaking down the formula:\n\n| Component | Meaning | Example |\n|-----------|---------|---------|\n| $W$ | Input width (or height) | 32 |\n| $K$ | Kernel size | 3 |\n| $P$ | Padding | 1 |\n| $S$ | Stride | 1 |\n| $O$ | Output size | (32-3+2)/1 + 1 = 32 |\n\n**What this means:** This formula tells you exactly how big your output feature map will be. It is essential for designing CNN architectures -- you need each layer's output to match the next layer's expected input.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "mlnc7gxnjus",
   "source": "# Visualize stride effect\ndef output_size(W, K, P, S):\n    \"\"\"Compute convolution output size.\"\"\"\n    return (W - K + 2 * P) // S + 1\n\n# Show stride 1 vs stride 2\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Stride 1\nax = axes[0]\ngrid = np.arange(1, 50).reshape(7, 7)\nax.imshow(np.ones((7, 7)) * 0.9, cmap='gray', vmin=0, vmax=1, extent=[-0.5, 6.5, 6.5, -0.5])\nfor i in range(7):\n    for j in range(7):\n        ax.text(j, i, f'{grid[i,j]}', ha='center', va='center', fontsize=9)\n\n# Show stride=1 positions (all positions in first row)\ncolors_s1 = ['red', 'blue', 'green', 'orange', 'purple']\nfor pos, color in zip(range(5), colors_s1):\n    rect = plt.Rectangle((pos-0.45, -0.45), 2.9, 2.9, linewidth=2, \n                          edgecolor=color, facecolor='none', linestyle='--', alpha=0.6)\n    ax.add_patch(rect)\nout_s1 = output_size(7, 3, 0, 1)\nax.set_title(f'Stride = 1\\n7x7 input, 3x3 kernel -> {out_s1}x{out_s1} output', fontsize=11)\nax.set_xticks(range(7))\nax.set_yticks(range(7))\n\n# Stride 2\nax = axes[1]\nax.imshow(np.ones((7, 7)) * 0.9, cmap='gray', vmin=0, vmax=1, extent=[-0.5, 6.5, 6.5, -0.5])\nfor i in range(7):\n    for j in range(7):\n        ax.text(j, i, f'{grid[i,j]}', ha='center', va='center', fontsize=9)\n\n# Show stride=2 positions (first row)\nfor pos_idx, pos in enumerate([0, 2, 4]):\n    color = colors_s1[pos_idx]\n    rect = plt.Rectangle((pos-0.45, -0.45), 2.9, 2.9, linewidth=2, \n                          edgecolor=color, facecolor=color, alpha=0.15)\n    ax.add_patch(rect)\nout_s2 = output_size(7, 3, 0, 2)\nax.set_title(f'Stride = 2\\n7x7 input, 3x3 kernel -> {out_s2}x{out_s2} output', fontsize=11)\nax.set_xticks(range(7))\nax.set_yticks(range(7))\n\n# Output size calculator\nax = axes[2]\nax.axis('off')\n\n# Show table of output sizes for common configurations\nconfigs = [\n    (28, 3, 0, 1), (28, 3, 1, 1), (28, 3, 1, 2), (28, 5, 2, 1),\n    (32, 3, 1, 1), (32, 3, 1, 2), (224, 7, 3, 2), (224, 3, 1, 1),\n]\n\ntable_text = \"Output Size Calculator\\n\" + \"=\" * 40 + \"\\n\"\ntable_text += f\"{'W':>5} {'K':>3} {'P':>3} {'S':>3} {'Output':>8}\\n\"\ntable_text += \"-\" * 40 + \"\\n\"\nfor W, K, P, S in configs:\n    O = output_size(W, K, P, S)\n    table_text += f\"{W:>5} {K:>3} {P:>3} {S:>3} {O:>8}\\n\"\n\nax.text(0.1, 0.95, table_text, transform=ax.transAxes, fontsize=10,\n        verticalalignment='top', fontfamily='monospace',\n        bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n\nplt.suptitle('Stride: How Far the Filter Moves Each Step', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "vbemxeysb8g",
   "source": "---\n\n## 3. Multiple Channels and Filters\n\n### Intuitive Explanation\n\nSo far we have convolved a single grayscale image with a single filter. Real images have multiple **channels** (R, G, B), and we want to detect many different features. This leads to two extensions:\n\n1. **Multi-channel input**: A 3x3 filter on an RGB image is actually 3x3x3 = 27 weights (one 3x3 slice per channel). The filter produces a weighted sum across ALL channels at each position.\n\n2. **Multiple filters**: Each filter produces one feature map. If we use 32 filters, we get 32 feature maps -- each detecting a different pattern.\n\n**The full picture:** A convolutional layer with $C_{in}$ input channels and $C_{out}$ filters has weights of shape $(C_{out}, C_{in}, K, K)$.\n\n**F1 analogy:** An F1 car streams multiple telemetry channels simultaneously -- speed, throttle, brake pressure, steering angle, tire temperatures, fuel flow. A single \"braking event\" filter needs to look across all these channels at once (speed dropping AND brake pressure spiking AND throttle at zero). That is multi-channel convolution. And you want many filters: one for braking events, one for traction loss, one for DRS deployment, one for fuel-saving coasting. Each filter produces its own \"event map\" of the lap.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7tkmjc319lw",
   "source": "# Visualization: How multi-channel convolution works\nfig, ax = plt.subplots(figsize=(14, 8))\nax.set_xlim(0, 14)\nax.set_ylim(0, 10)\nax.axis('off')\n\n# Draw RGB input (3 channels)\nchannel_colors = ['#FF6B6B', '#69DB7C', '#74C0FC']\nchannel_names = ['R', 'G', 'B']\n\nfor c, (color, name) in enumerate(zip(channel_colors, channel_names)):\n    x_off = 0.5 + c * 0.3\n    y_off = 6.5 - c * 0.3\n    \n    # Draw channel grid\n    for i in range(5):\n        for j in range(5):\n            rect = plt.Rectangle((x_off + j * 0.4, y_off - i * 0.4), 0.38, 0.38,\n                                  facecolor=color, edgecolor='black', alpha=0.6, linewidth=0.5)\n            ax.add_patch(rect)\n    \n    ax.text(x_off + 1.0, y_off + 0.5, name, fontsize=10, fontweight='bold', color=color)\n\nax.text(1.5, 4.3, 'Input\\n(H x W x 3)', ha='center', fontsize=11, fontweight='bold')\n\n# Draw filter (3 channel slices)\nfor c, color in enumerate(channel_colors):\n    x_off = 4.5 + c * 0.2\n    y_off = 7.5 - c * 0.2\n    \n    for i in range(3):\n        for j in range(3):\n            rect = plt.Rectangle((x_off + j * 0.4, y_off - i * 0.4), 0.38, 0.38,\n                                  facecolor=color, edgecolor='black', alpha=0.6, linewidth=0.5)\n            ax.add_patch(rect)\n\nax.text(5.3, 6.0, 'One Filter\\n(3 x 3 x 3)\\n= 27 weights', ha='center', fontsize=10, fontweight='bold')\n\n# Arrow\nax.annotate('', xy=(4.3, 6.5), xytext=(3.0, 6.5),\n            arrowprops=dict(arrowstyle='->', color='black', lw=2))\nax.text(3.6, 7.0, 'convolve', ha='center', fontsize=9)\n\n# Arrow to output\nax.annotate('', xy=(7.5, 6.5), xytext=(6.3, 6.5),\n            arrowprops=dict(arrowstyle='->', color='black', lw=2))\n\n# Single feature map output\nfor i in range(3):\n    for j in range(3):\n        rect = plt.Rectangle((7.7 + j * 0.5, 7.5 - i * 0.5), 0.48, 0.48,\n                              facecolor='gold', edgecolor='black', alpha=0.7, linewidth=1)\n        ax.add_patch(rect)\nax.text(8.5, 6.0, 'Feature Map\\n(1 channel)', ha='center', fontsize=10, fontweight='bold')\n\n# Now show multiple filters -> multiple feature maps\nax.text(7, 3.5, 'With N filters, we get N feature maps:', fontsize=12, fontweight='bold', ha='center')\n\nfilter_colors = ['#FF6B6B', '#69DB7C', '#74C0FC', '#FAB005']\nfor f in range(4):\n    x_off = 2 + f * 3\n    \n    # Small filter icon\n    for i in range(2):\n        for j in range(2):\n            rect = plt.Rectangle((x_off + j * 0.3, 2.5 - i * 0.3), 0.28, 0.28,\n                                  facecolor=filter_colors[f], edgecolor='black', alpha=0.6, linewidth=0.5)\n            ax.add_patch(rect)\n    \n    ax.text(x_off + 0.3, 1.7, f'Filter {f+1}', ha='center', fontsize=9, fontweight='bold')\n    \n    # Arrow down\n    ax.annotate('', xy=(x_off + 0.3, 1.3), xytext=(x_off + 0.3, 1.6),\n                arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n    \n    # Feature map\n    for i in range(2):\n        for j in range(2):\n            rect = plt.Rectangle((x_off + j * 0.3, 0.5 - i * 0.3), 0.28, 0.28,\n                                  facecolor=filter_colors[f], edgecolor='black', alpha=0.4, linewidth=0.5)\n            ax.add_patch(rect)\n    \n    ax.text(x_off + 0.3, -0.2, f'Map {f+1}', ha='center', fontsize=9)\n\nax.text(7, -0.7, 'Output: H\\' x W\\' x N_filters', ha='center', fontsize=11, fontweight='bold')\n\nax.set_title('Multi-Channel Convolution: RGB Input with Multiple Filters', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Parameter count example\nC_in, C_out, K = 3, 32, 3\nparams = C_out * (C_in * K * K + 1)  # +1 for bias per filter\nprint(f\"Conv layer: {C_in} input channels, {C_out} filters, {K}x{K} kernels\")\nprint(f\"  Weight shape: ({C_out}, {C_in}, {K}, {K})\")\nprint(f\"  Parameters: {C_out} * ({C_in} * {K} * {K} + 1) = {params}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "wq7l381w62",
   "source": "### Deep Dive: What Filters Learn at Different Depths\n\nOne of the most fascinating discoveries in deep learning is the **feature hierarchy** that CNNs learn automatically. Early layers learn simple patterns, and deeper layers combine them into increasingly complex ones.\n\n| Layer Depth | What Filters Detect | Example | F1 Telemetry Parallel |\n|-------------|--------------------|---------|-----------------------|\n| Layer 1 (shallow) | Edges, colors, simple gradients | Horizontal edge, red blob | Individual sensor spikes -- a brake pressure jump, a single temperature reading |\n| Layer 2-3 | Textures, corners, simple shapes | Brick pattern, corner of a box | Short event patterns -- a braking zone (speed drop + brake spike), a traction event (wheelspin + TC intervention) |\n| Layer 4-5 | Parts of objects | Eye, wheel, window | Corner profiles -- the full sequence of braking, turn-in, apex, exit for a single corner |\n| Deep layers | Entire objects or scenes | Face, car, building | Driving style -- aggressive late braking vs smooth early braking across an entire sector |\n\n#### Key Insight\n\nThis hierarchy emerges naturally from training -- nobody designs these filters by hand. The network discovers that edges are useful building blocks for textures, textures for parts, and parts for objects. Similarly, an F1 telemetry CNN would discover that individual sensor spikes compose into event patterns, events compose into corner profiles, and corner profiles compose into driving style signatures.\n\nThis is why **transfer learning** works: the early layers of a CNN trained on one task (e.g., ImageNet) learn general features (edges, textures) that are useful for almost any vision task.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "xjmgr821os",
   "source": "# Simulate what filters learn at different depths\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\n\n# Layer 1: Simple edge detectors (hand-crafted examples of what networks learn)\nlayer1_filters = [\n    np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]),    # Vertical edge\n    np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]]),     # Horizontal edge\n    np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]]),    # Spot detector\n    np.array([[-1, 0, 1], [0, 0, 0], [1, 0, -1]]),      # Diagonal\n]\nlayer1_names = ['Vertical\\nEdge', 'Horizontal\\nEdge', 'Spot\\nDetector', 'Diagonal\\nEdge']\n\nfor idx, (filt, name) in enumerate(zip(layer1_filters, layer1_names)):\n    ax = axes[0, idx]\n    im = ax.imshow(filt, cmap='RdBu', vmin=-2, vmax=2)\n    ax.set_title(f'Layer 1: {name}', fontsize=10, fontweight='bold')\n    for i in range(3):\n        for j in range(3):\n            ax.text(j, i, f'{filt[i,j]:+d}', ha='center', va='center', fontsize=12,\n                    color='white' if abs(filt[i,j]) > 1 else 'black')\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n# Layer 2+: Show what happens when you compose filters (simulated)\n# Generate synthetic \"deeper\" feature responses\nnp.random.seed(42)\ntest_img = make_test_image(64)\n\n# Apply successive convolutions to show increasing complexity\nlayer_outputs = [test_img]\ncurrent = test_img\nfor filters in [layer1_filters[:2], layer1_filters[2:]]:\n    responses = []\n    for f in filters:\n        resp = conv2d_manual(current, f)\n        responses.append(np.abs(resp))\n    # Combine responses (like what a deeper layer sees)\n    min_shape = min(r.shape[0] for r in responses)\n    combined = sum(r[:min_shape, :min_shape] for r in responses) / len(responses)\n    current = combined\n    layer_outputs.append(combined)\n\n# Show progressive feature extraction\ntitles_bottom = ['Original\\nImage', 'After Layer 1\\n(edges)', \n                 'After Layer 2\\n(combinations)', 'Feature\\nHierarchy']\nfor idx in range(3):\n    ax = axes[1, idx]\n    ax.imshow(layer_outputs[idx], cmap='hot')\n    ax.set_title(titles_bottom[idx], fontsize=10, fontweight='bold')\n    ax.axis('off')\n\n# Summary in last panel\nax = axes[1, 3]\nax.axis('off')\nhierarchy = \"Feature Hierarchy:\\n\\n\"\nhierarchy += \"Layer 1: Edges\\n      |\\n\"\nhierarchy += \"Layer 2: Textures\\n      |\\n\"\nhierarchy += \"Layer 3: Parts\\n      |\\n\"\nhierarchy += \"Layer 4: Objects\\n      |\\n\"\nhierarchy += \"Output:  Classification\"\nax.text(0.5, 0.5, hierarchy, ha='center', va='center', fontsize=11,\n        fontfamily='monospace', transform=ax.transAxes,\n        bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n\nplt.suptitle('What CNN Filters Learn at Different Depths', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "eqv6zog626l",
   "source": "---\n\n## 4. Pooling Layers\n\n### Intuitive Explanation\n\nAfter convolution, we often want to **reduce the spatial dimensions** while keeping the most important information. Pooling does this by summarizing small regions of the feature map.\n\nThink of it like creating a thumbnail of an image -- you lose fine detail but keep the big picture. This provides:\n\n1. **Dimensionality reduction**: Fewer values to process in later layers\n2. **Translation invariance**: Small shifts in the input do not change the output\n3. **Larger receptive field**: Each neuron in later layers \"sees\" more of the original image\n\n**F1 analogy:** Pooling is how you go from thousands of data points per sector to a handful of key metrics. Max pooling is like reporting the peak speed in each sector -- you keep the most extreme value. Average pooling is like reporting average speed through each sector. Global average pooling is like summarizing an entire lap into one number per metric (average speed, average tire temp, etc.). You lose the moment-by-moment detail but gain a compact summary that captures the essence of performance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "pps90e2glkq",
   "source": "# Visualize Max Pooling and Average Pooling\npool_input = np.array([\n    [1, 3, 2, 4],\n    [5, 6, 1, 2],\n    [3, 2, 7, 8],\n    [4, 1, 3, 5]\n], dtype=float)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Original\nax = axes[0]\nax.imshow(pool_input, cmap='YlOrRd', vmin=0, vmax=8, extent=[-0.5, 3.5, 3.5, -0.5])\nfor i in range(4):\n    for j in range(4):\n        ax.text(j, i, f'{int(pool_input[i,j])}', ha='center', va='center', fontsize=16, fontweight='bold')\n# Draw 2x2 regions\nfor i in range(0, 4, 2):\n    for j in range(0, 4, 2):\n        colors_pool = ['#FF6B6B', '#69DB7C', '#74C0FC', '#FAB005']\n        idx = (i // 2) * 2 + (j // 2)\n        rect = plt.Rectangle((j-0.5, i-0.5), 2, 2, linewidth=3, \n                              edgecolor=colors_pool[idx], facecolor='none')\n        ax.add_patch(rect)\nax.set_title('Input (4x4)', fontsize=12, fontweight='bold')\nax.set_xticks(range(4))\nax.set_yticks(range(4))\n\n# Max pooling\nax = axes[1]\nmax_pool = np.array([\n    [max(pool_input[0:2, 0:2].flat), max(pool_input[0:2, 2:4].flat)],\n    [max(pool_input[2:4, 0:2].flat), max(pool_input[2:4, 2:4].flat)]\n])\nax.imshow(max_pool, cmap='YlOrRd', vmin=0, vmax=8, extent=[-0.5, 1.5, 1.5, -0.5])\nfor i in range(2):\n    for j in range(2):\n        idx = i * 2 + j\n        ax.text(j, i, f'{int(max_pool[i,j])}', ha='center', va='center', fontsize=20, fontweight='bold')\n        rect = plt.Rectangle((j-0.5, i-0.5), 1, 1, linewidth=3, \n                              edgecolor=colors_pool[idx], facecolor='none')\n        ax.add_patch(rect)\n\n# Show which values were selected\nax.text(0, -0.8, 'max(1,3,5,6)=6', ha='center', fontsize=8, color=colors_pool[0])\nax.text(1, -0.8, 'max(2,4,1,2)=4', ha='center', fontsize=8, color=colors_pool[1])\n\nax.set_title('Max Pooling 2x2\\n(keeps strongest activation)', fontsize=12, fontweight='bold')\nax.set_xticks(range(2))\nax.set_yticks(range(2))\n\n# Average pooling\nax = axes[2]\navg_pool = np.array([\n    [np.mean(pool_input[0:2, 0:2]), np.mean(pool_input[0:2, 2:4])],\n    [np.mean(pool_input[2:4, 0:2]), np.mean(pool_input[2:4, 2:4])]\n])\nax.imshow(avg_pool, cmap='YlOrRd', vmin=0, vmax=8, extent=[-0.5, 1.5, 1.5, -0.5])\nfor i in range(2):\n    for j in range(2):\n        idx = i * 2 + j\n        ax.text(j, i, f'{avg_pool[i,j]:.1f}', ha='center', va='center', fontsize=18, fontweight='bold')\n        rect = plt.Rectangle((j-0.5, i-0.5), 1, 1, linewidth=3, \n                              edgecolor=colors_pool[idx], facecolor='none')\n        ax.add_patch(rect)\n\nax.set_title('Average Pooling 2x2\\n(keeps average activation)', fontsize=12, fontweight='bold')\nax.set_xticks(range(2))\nax.set_yticks(range(2))\n\nplt.suptitle('Pooling: Reducing Spatial Dimensions', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9bxip61dso",
   "source": "### Global Average Pooling (Modern Approach)\n\nModern architectures increasingly use **Global Average Pooling (GAP)** instead of flattening + fully connected layers. GAP takes the average of each entire feature map, producing one value per channel.\n\nFor example, if the final conv layer outputs 512 feature maps of size 7x7, GAP produces a vector of length 512 (averaging each 7x7 map into a single number).\n\n### Pooling Comparison Table\n\n| Pooling Type | Operation | Parameters | Use Case | F1 Parallel |\n|-------------|-----------|------------|----------|-------------|\n| **Max Pooling** | Take maximum in window | None (no learnable params!) | Classic CNNs, preserves strong activations | Peak speed per sector, maximum g-force per corner |\n| **Average Pooling** | Take mean in window | None | Smoother downsampling | Average sector speed, mean tire temperature per stint |\n| **Global Average Pooling** | Average entire feature map | None | Modern replacement for FC layers | One summary stat per channel for the whole lap |\n| **Strided Convolution** | Conv with stride > 1 | Learned | Modern alternative to pooling | Learned downsampling -- let the network decide what to keep |\n\n### Why This Matters in Machine Learning\n\n| Application | Pooling Strategy |\n|-------------|-----------------|\n| Image classification | Max pool between conv blocks, GAP before classifier |\n| Object detection | Feature pyramid with different pooling scales |\n| Semantic segmentation | Avoid pooling (need full resolution output) |\n| Modern architectures | Strided convolutions replacing pooling |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "6rsdqhstxn6",
   "source": "---\n\n## 5. Building a CNN in PyTorch\n\n### nn.Conv2d Parameters Explained\n\n```python\nnn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)\n```\n\n| Parameter | Meaning | Example |\n|-----------|---------|---------|\n| `in_channels` | Number of input channels | 3 (RGB) or 1 (grayscale) |\n| `out_channels` | Number of filters (output channels) | 32, 64, 128 |\n| `kernel_size` | Size of the filter | 3 (means 3x3) |\n| `stride` | Step size | 1 (default) or 2 (halves output) |\n| `padding` | Zero-padding | 1 (for 'same' with 3x3 kernel) |\n\n### nn.MaxPool2d\n\n```python\nnn.MaxPool2d(kernel_size, stride=None)\n```\n\nIf `stride` is not specified, it defaults to `kernel_size`. So `MaxPool2d(2)` means 2x2 windows with stride 2, halving the spatial dimensions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "uy7qx5w77or",
   "source": "# Building a simple CNN step by step\nclass SimpleCNN(nn.Module):\n    \"\"\"\n    A simple CNN for 28x28 grayscale images (e.g., MNIST).\n    \n    Architecture:\n        Conv(1->16, 3x3) -> ReLU -> MaxPool(2x2)   [28x28 -> 13x13]\n        Conv(16->32, 3x3) -> ReLU -> MaxPool(2x2)   [13x13 -> 5x5]\n        Flatten -> FC(800->128) -> ReLU -> FC(128->10)\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        \n        # Convolutional layers\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n        \n        # Pooling\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        # Fully connected layers\n        # After 2 pooling layers: 28 -> 14 -> 7, so 32 * 7 * 7 = 1568\n        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n        self.fc2 = nn.Linear(128, 10)\n    \n    def forward(self, x):\n        # Block 1: Conv -> ReLU -> Pool\n        x = self.pool(F.relu(self.conv1(x)))    # (B, 1, 28, 28) -> (B, 16, 14, 14)\n        \n        # Block 2: Conv -> ReLU -> Pool\n        x = self.pool(F.relu(self.conv2(x)))    # (B, 16, 14, 14) -> (B, 32, 7, 7)\n        \n        # Flatten\n        x = x.view(x.size(0), -1)              # (B, 32, 7, 7) -> (B, 1568)\n        \n        # Classifier\n        x = F.relu(self.fc1(x))                 # (B, 1568) -> (B, 128)\n        x = self.fc2(x)                         # (B, 128) -> (B, 10)\n        \n        return x\n\nmodel = SimpleCNN()\nprint(model)\n\n# Test with a dummy input\ndummy = torch.randn(1, 1, 28, 28)\noutput = model(dummy)\nprint(f\"\\nInput shape:  {dummy.shape}\")\nprint(f\"Output shape: {output.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ip8vp7eq3j",
   "source": "### Visualizing Feature Maps at Each Layer",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "e4ilbnrij",
   "source": "# Create a simple digit-like image and visualize feature maps\ndef create_digit_image():\n    \"\"\"Create a simple '7' digit image.\"\"\"\n    img = np.zeros((28, 28))\n    # Horizontal bar at top\n    img[4:7, 6:22] = 1.0\n    # Diagonal bar\n    for i in range(20):\n        j = 20 - i\n        if 4 <= j <= 22 and 6 <= i + 6 <= 25:\n            img[i + 6, max(6, j-1):j+2] = 1.0\n    return img\n\ndigit_img = create_digit_image()\n\n# Pass through the model and capture intermediate outputs\nmodel.eval()\nx = torch.tensor(digit_img, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Add batch and channel dims\n\n# Get intermediate activations\nwith torch.no_grad():\n    after_conv1 = F.relu(model.conv1(x))\n    after_pool1 = model.pool(after_conv1)\n    after_conv2 = F.relu(model.conv2(after_pool1))\n    after_pool2 = model.pool(after_conv2)\n\n# Plot feature maps\nfig, axes = plt.subplots(3, 6, figsize=(15, 8))\n\n# Original image\naxes[0, 0].imshow(digit_img, cmap='gray')\naxes[0, 0].set_title('Input (28x28)', fontsize=10, fontweight='bold')\naxes[0, 0].axis('off')\nfor j in range(1, 6):\n    axes[0, j].axis('off')\naxes[0, 1].text(0.5, 0.5, f'After Conv1:\\n16 feature maps\\nSize: {after_conv1.shape[2]}x{after_conv1.shape[3]}',\n                ha='center', va='center', fontsize=11, transform=axes[0, 1].transAxes)\n\n# After conv1 - show first 6 feature maps\nfor j in range(6):\n    if j < after_conv1.shape[1]:\n        axes[1, j].imshow(after_conv1[0, j].numpy(), cmap='viridis')\n        axes[1, j].set_title(f'Conv1 filter {j}', fontsize=8)\n    axes[1, j].axis('off')\n\n# After conv2 - show first 6 feature maps\nfor j in range(6):\n    if j < after_conv2.shape[1]:\n        axes[2, j].imshow(after_conv2[0, j].numpy(), cmap='viridis')\n        axes[2, j].set_title(f'Conv2 filter {j}', fontsize=8)\n    axes[2, j].axis('off')\n\naxes[1, 0].set_ylabel('After Conv1\\n(16 maps)', fontsize=10, fontweight='bold')\naxes[2, 0].set_ylabel('After Conv2\\n(32 maps)', fontsize=10, fontweight='bold')\n\nplt.suptitle('Feature Maps at Each Layer (random weights)', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"Note: These feature maps use random (untrained) weights.\")\nprint(\"After training, each filter map would detect meaningful patterns.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6rvi1kzip54",
   "source": "### Parameter Counting: CNN vs Fully Connected",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2airwxhq00e",
   "source": "# Parameter counting comparison\ndef count_parameters(model):\n    \"\"\"Count total and per-layer parameters.\"\"\"\n    total = 0\n    layer_counts = {}\n    for name, param in model.named_parameters():\n        count = param.numel()\n        total += count\n        layer_counts[name] = count\n    return total, layer_counts\n\n# CNN parameters\ncnn = SimpleCNN()\ncnn_total, cnn_layers = count_parameters(cnn)\n\n# Equivalent FC network for 28x28 images\nclass EquivalentFC(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(28 * 28, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 10)\n    \n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\nfc = EquivalentFC()\nfc_total, fc_layers = count_parameters(fc)\n\n# Print comparison\nprint(\"=\" * 55)\nprint(\"CNN Parameter Breakdown:\")\nprint(\"=\" * 55)\nfor name, count in cnn_layers.items():\n    print(f\"  {name:25s}: {count:>8,}\")\nprint(f\"  {'TOTAL':25s}: {cnn_total:>8,}\")\n\nprint(f\"\\n{'=' * 55}\")\nprint(\"FC Network Parameter Breakdown:\")\nprint(\"=\" * 55)\nfor name, count in fc_layers.items():\n    print(f\"  {name:25s}: {count:>8,}\")\nprint(f\"  {'TOTAL':25s}: {fc_total:>8,}\")\n\nprint(f\"\\n{'=' * 55}\")\nprint(f\"CNN parameters:  {cnn_total:>8,}\")\nprint(f\"FC parameters:   {fc_total:>8,}\")\nprint(f\"CNN/FC ratio:    {cnn_total/fc_total:.2f}x\")\nprint(f\"\\nNote: The CNN has MORE parameters here because of the FC layers\")\nprint(f\"at the end. Most CNN params are in fc1 ({cnn_layers['fc1.weight']:,} weights).\")\nprint(f\"The conv layers themselves use very few: conv1={cnn_layers['conv1.weight']:,}, conv2={cnn_layers['conv2.weight']:,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6xvsfk0sg2m",
   "source": "---\n\n## 6. Classic CNN Architectures\n\n### Intuitive Explanation\n\nThe history of CNNs is a story of going deeper and finding clever ways to make depth work. Each architecture introduced a key innovation that changed the field.\n\n**F1 analogy:** Think of it as the evolution of F1 car design. LeNet-5 is the early 1950s cars -- simple, functional, proved the concept. AlexNet is the ground-effect era -- a breakthrough in raw performance. VGG is the 1990s approach of refining a simple formula. And ResNet is the double-diffuser or blown-diffuser moment -- a clever trick (skip connections) that unlocked performance levels nobody thought were possible.\n\n### Architecture Comparison Table\n\n| Architecture | Year | Depth | Parameters | Key Innovation | Top-5 Error (ImageNet) |\n|-------------|------|-------|-----------|----------------|----------------------|\n| **LeNet-5** | 1998 | 5 layers | ~60K | First practical CNN | N/A (MNIST) |\n| **AlexNet** | 2012 | 8 layers | 60M | ReLU, dropout, GPU training | 15.3% |\n| **VGG-16** | 2014 | 16 layers | 138M | Small 3x3 filters throughout | 7.3% |\n| **GoogLeNet** | 2014 | 22 layers | 6.8M | Inception modules (multi-scale) | 6.7% |\n| **ResNet-50** | 2015 | 50 layers | 25.6M | Skip connections | 3.6% |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "pd8qou6c5zq",
   "source": "# Visualize architecture evolution\nfig, ax = plt.subplots(figsize=(14, 6))\n\narchitectures = [\n    ('LeNet-5\\n(1998)', 5, 0.06, 'First CNN'),\n    ('AlexNet\\n(2012)', 8, 60, 'ReLU + GPU'),\n    ('VGG-16\\n(2014)', 16, 138, '3x3 filters'),\n    ('GoogLeNet\\n(2014)', 22, 6.8, 'Inception'),\n    ('ResNet-50\\n(2015)', 50, 25.6, 'Skip connections'),\n    ('ResNet-152\\n(2015)', 152, 60, 'Very deep'),\n]\n\nx_pos = range(len(architectures))\ndepths = [a[1] for a in architectures]\nparams = [a[2] for a in architectures]\nnames = [a[0] for a in architectures]\ninnovations = [a[3] for a in architectures]\n\n# Bar chart for depth\nbars = ax.bar(x_pos, depths, color=['#74C0FC', '#FF6B6B', '#69DB7C', '#FAB005', '#DA77F2', '#DA77F2'],\n              alpha=0.8, edgecolor='black', linewidth=1)\n\n# Add parameter count as text\nfor i, (bar, p, innov) in enumerate(zip(bars, params, innovations)):\n    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 2,\n            f'{p}M params', ha='center', va='bottom', fontsize=9, fontweight='bold')\n    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height()/2,\n            innov, ha='center', va='center', fontsize=8, color='white', fontweight='bold',\n            bbox=dict(boxstyle='round,pad=0.2', facecolor='black', alpha=0.5))\n\nax.set_xticks(x_pos)\nax.set_xticklabels(names)\nax.set_ylabel('Number of Layers', fontsize=12)\nax.set_title('Evolution of CNN Architectures', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1fqo2sx7k7y",
   "source": "### 6.1 LeNet-5 (1998): The Original CNN\n\nYann LeCun's **LeNet-5** was the first CNN to achieve practical success, reading handwritten digits on checks. Its architecture is simple by modern standards but established the Conv-Pool-Conv-Pool-FC pattern that dominated for years.\n\n### 6.2 VGG (2014): Deeper is Better\n\nVGG's key insight was elegantly simple: **use only 3x3 filters and go deeper**. Two stacked 3x3 conv layers have the same receptive field as one 5x5 layer, but with fewer parameters and more nonlinearity (two ReLU activations instead of one).\n\n### 6.3 ResNet (2015): Skip Connections Solve Everything\n\nResNet introduced the most important architectural innovation in deep learning: **skip connections** (also called residual connections). Instead of learning $H(x)$, each block learns $F(x) = H(x) - x$, the \"residual.\"\n\n$$\\text{output} = F(x) + x$$\n\n**Why this is brilliant:** If a layer is not helpful, the network can easily learn $F(x) = 0$, making the block an identity function. This means adding more layers can never hurt -- at worst, they do nothing.\n\n**F1 analogy:** Skip connections are like having both the raw telemetry AND the processed telemetry available at every stage of analysis. Imagine your data pipeline has multiple processing stages: filtering, smoothing, feature extraction. Without skip connections, each stage only sees the output of the previous stage -- if any stage corrupts the signal, the information is lost forever. With skip connections, every stage also gets the original raw data. If your smoothing filter accidentally erases a brief but critical traction event, the raw trace still carries that information forward. This is exactly why modern F1 telemetry dashboards overlay raw and filtered data simultaneously.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "21livfhelyi",
   "source": "# Visualize the skip connection concept\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: Without skip connection (plain network)\nax = axes[0]\nax.set_xlim(0, 10)\nax.set_ylim(0, 8)\nax.axis('off')\n\n# Draw blocks\nblocks = [(2, 6, 'Conv + BN\\n+ ReLU'), (2, 4, 'Conv + BN'), (2, 2, 'ReLU')]\nfor x, y, label in blocks:\n    rect = plt.Rectangle((x, y - 0.4), 6, 0.8, facecolor='lightblue', \n                          edgecolor='black', linewidth=2)\n    ax.add_patch(rect)\n    ax.text(5, y, label, ha='center', va='center', fontsize=10, fontweight='bold')\n\n# Arrows between blocks\nax.annotate('', xy=(5, 5.6), xytext=(5, 5.0),\n            arrowprops=dict(arrowstyle='<-', color='black', lw=2))\nax.annotate('', xy=(5, 3.6), xytext=(5, 3.0),\n            arrowprops=dict(arrowstyle='<-', color='black', lw=2))\n\n# Input/Output labels\nax.text(5, 7.2, 'Input x', ha='center', fontsize=11, fontweight='bold')\nax.annotate('', xy=(5, 6.8), xytext=(5, 6.4),\n            arrowprops=dict(arrowstyle='->', color='black', lw=2))\nax.annotate('', xy=(5, 1.6), xytext=(5, 1.0),\n            arrowprops=dict(arrowstyle='->', color='black', lw=2))\nax.text(5, 0.6, 'Output H(x)', ha='center', fontsize=11, fontweight='bold')\n\nax.set_title('Plain Block\\nOutput = H(x)', fontsize=13, fontweight='bold')\n\n# Right: With skip connection (residual block)\nax = axes[1]\nax.set_xlim(0, 10)\nax.set_ylim(0, 8)\nax.axis('off')\n\n# Draw blocks\nfor x, y, label in blocks:\n    rect = plt.Rectangle((x, y - 0.4), 5, 0.8, facecolor='lightgreen', \n                          edgecolor='black', linewidth=2)\n    ax.add_patch(rect)\n    ax.text(4.5, y, label, ha='center', va='center', fontsize=10, fontweight='bold')\n\n# Arrows between blocks\nax.annotate('', xy=(4.5, 5.6), xytext=(4.5, 5.0),\n            arrowprops=dict(arrowstyle='<-', color='black', lw=2))\nax.annotate('', xy=(4.5, 3.6), xytext=(4.5, 3.0),\n            arrowprops=dict(arrowstyle='<-', color='black', lw=2))\n\n# Input/Output\nax.text(4.5, 7.2, 'Input x', ha='center', fontsize=11, fontweight='bold')\nax.annotate('', xy=(4.5, 6.8), xytext=(4.5, 6.4),\n            arrowprops=dict(arrowstyle='->', color='black', lw=2))\n\n# SKIP CONNECTION - the key part!\nax.annotate('', xy=(8.2, 2.0), xytext=(8.2, 6.0),\n            arrowprops=dict(arrowstyle='->', color='red', lw=3, \n                           connectionstyle='arc3,rad=0'))\nax.text(9.0, 4.0, 'Skip\\nconnection\\n(identity)', ha='center', fontsize=9, \n        color='red', fontweight='bold')\n\n# Plus symbol at output\ncircle = plt.Circle((7.5, 2.0), 0.25, facecolor='yellow', edgecolor='red', linewidth=2)\nax.add_patch(circle)\nax.text(7.5, 2.0, '+', ha='center', va='center', fontsize=16, fontweight='bold', color='red')\n\n# Output\nax.annotate('', xy=(5, 1.6), xytext=(5, 1.0),\n            arrowprops=dict(arrowstyle='->', color='black', lw=2))\nax.text(5, 0.6, 'Output = F(x) + x', ha='center', fontsize=11, fontweight='bold', color='red')\n\nax.set_title('Residual Block (ResNet)\\nOutput = F(x) + x', fontsize=13, fontweight='bold')\n\nplt.suptitle('Skip Connections: The Key Innovation of ResNet', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xxayjbg1i9l",
   "source": "### Implement a Mini-ResNet with Skip Connections",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "hx0wb0e0c49",
   "source": "class ResidualBlock(nn.Module):\n    \"\"\"\n    A single residual block with skip connection.\n    \n    output = ReLU(Conv(ReLU(Conv(x))) + x)\n    \n    If input and output channels differ, we use a 1x1 convolution \n    on the skip path to match dimensions.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        \n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n                               stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        \n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n                               stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        # Skip connection: if dimensions change, use 1x1 conv to match\n        self.skip = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.skip = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, \n                         stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        identity = x\n        \n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        \n        out += self.skip(identity)  # <-- The skip connection!\n        out = F.relu(out)\n        \n        return out\n\n\nclass MiniResNet(nn.Module):\n    \"\"\"\n    A small ResNet for 28x28 images (MNIST/FashionMNIST).\n    \n    Architecture:\n        Conv(1->16) -> ResBlock(16->16) -> ResBlock(16->32, stride=2) \n        -> ResBlock(32->64, stride=2) -> GAP -> FC(64->10)\n    \"\"\"\n    def __init__(self, num_classes=10):\n        super().__init__()\n        \n        # Initial convolution\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(16)\n        \n        # Residual blocks\n        self.layer1 = ResidualBlock(16, 16, stride=1)    # 28x28 -> 28x28\n        self.layer2 = ResidualBlock(16, 32, stride=2)    # 28x28 -> 14x14\n        self.layer3 = ResidualBlock(32, 64, stride=2)    # 14x14 -> 7x7\n        \n        # Global average pooling + classifier\n        self.gap = nn.AdaptiveAvgPool2d(1)               # 7x7 -> 1x1\n        self.fc = nn.Linear(64, num_classes)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.gap(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Create and inspect the model\nresnet = MiniResNet()\nprint(resnet)\n\n# Test with dummy input\ndummy = torch.randn(2, 1, 28, 28)\noutput = resnet(dummy)\nprint(f\"\\nInput shape:  {dummy.shape}\")\nprint(f\"Output shape: {output.shape}\")\n\n# Count parameters\ntotal_params = sum(p.numel() for p in resnet.parameters())\nprint(f\"Total parameters: {total_params:,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ipofxarg0xn",
   "source": "### Deep Dive: Why ResNets Work (Gradient Highways)\n\nThe fundamental problem with very deep networks is **vanishing gradients**. During backpropagation, gradients are multiplied through each layer. With many layers, these products can become astronomically small, so early layers learn almost nothing.\n\nSkip connections create **gradient highways** -- shortcuts for gradients to flow backward without being multiplied through many layers. The gradient of the skip connection is simply 1 (the derivative of the identity function), so gradients always have a direct path back.\n\n**F1 analogy:** Think of vanishing gradients like trying to relay a radio message from the pit wall through 50 intermediate stations around the track. By the time it arrives, it is garbled beyond recognition. Skip connections are like giving the pit wall a direct radio link to every station -- the message arrives intact regardless of how many intermediate points there are.\n\n#### Key Insight\n\nWithout skip connections, a 100-layer network performs *worse* than a 20-layer network (the degradation problem). With skip connections, a 152-layer ResNet outperforms everything before it. The difference is entirely due to trainability, not capacity.\n\n#### Common Misconceptions\n\n| Misconception | Reality |\n|---------------|--------|\n| Skip connections add parameters | A pure identity skip adds zero parameters |\n| ResNets learn residuals by choice | The architecture forces residual learning |\n| Deeper always needs skip connections | For < 20 layers, skip connections help less |\n| Skip connections are only for CNNs | They are used everywhere: Transformers, RNNs, MLPs |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "o4i5k942o7a",
   "source": "# Demonstrate the gradient flow advantage of skip connections\ndef simulate_gradient_flow(n_layers, has_skip=False):\n    \"\"\"\n    Simulate gradient magnitude through layers.\n    \n    Without skip: grad *= layer_factor at each layer\n    With skip:    grad = grad * layer_factor + grad_skip (identity = 1)\n    \"\"\"\n    np.random.seed(42)\n    gradient = 1.0\n    gradients = [gradient]\n    \n    for i in range(n_layers):\n        # Each layer multiplies gradient by a factor < 1 (slight vanishing)\n        layer_factor = np.random.uniform(0.7, 0.95)\n        \n        if has_skip and i % 2 == 1:  # Skip connection every 2 layers\n            # Gradient flows through both the layer path AND the skip path\n            gradient = gradient * layer_factor + gradients[-2] * 1.0\n        else:\n            gradient = gradient * layer_factor\n        \n        gradients.append(abs(gradient))\n    \n    return gradients\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nn_layers = 50\n\nno_skip = simulate_gradient_flow(n_layers, has_skip=False)\nwith_skip = simulate_gradient_flow(n_layers, has_skip=True)\n\nax.plot(no_skip, 'r-', linewidth=2, label='Without skip connections', alpha=0.8)\nax.plot(with_skip, 'g-', linewidth=2, label='With skip connections', alpha=0.8)\nax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5, label='Initial gradient')\n\nax.set_xlabel('Layer (from output to input)', fontsize=12)\nax.set_ylabel('Gradient Magnitude', fontsize=12)\nax.set_title('Gradient Flow: Skip Connections Prevent Vanishing Gradients', fontsize=14)\nax.set_yscale('log')\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"After {n_layers} layers:\")\nprint(f\"  Without skip: gradient = {no_skip[-1]:.6f} (vanished!)\")\nprint(f\"  With skip:    gradient = {with_skip[-1]:.4f} (healthy!)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "hw1sbwb3a2",
   "source": "---\n\n## 7. Practical CNN Training\n\n### 7.1 Data Augmentation\n\n**Data augmentation** artificially expands the training set by applying random transformations to existing images. This is one of the most effective regularization techniques for CNNs.\n\n| Transform | What It Does | When to Use | F1 Parallel |\n|-----------|-------------|-------------|-------------|\n| `RandomHorizontalFlip` | Mirror left-right | Most tasks (not text!) | Mirroring a clockwise circuit to simulate a counter-clockwise one |\n| `RandomRotation` | Rotate by random angle | When orientation varies | Small variations in track camber or car roll angle |\n| `RandomCrop` | Crop random region | Almost always | Analyzing a random subsection of a lap |\n| `ColorJitter` | Change brightness/contrast | Color-invariant tasks | Simulating different weather/lighting conditions |\n| `RandomAffine` | Scale, translate, shear | General robustness | Slight sensor calibration differences between sessions |\n| `Normalize` | Standardize pixel values | Always (not augmentation, but essential) | Normalizing telemetry to a common scale across cars |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "4c9ubuofaea",
   "source": "# Visualize data augmentation transforms\n# Define transforms\ntrain_transform = transforms.Compose([\n    transforms.RandomRotation(10),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\n# Load FashionMNIST\ntrain_dataset = torchvision.datasets.FashionMNIST(\n    root='./data', train=True, download=True, transform=train_transform\n)\ntest_dataset = torchvision.datasets.FashionMNIST(\n    root='./data', train=False, download=True, transform=test_transform\n)\n\n# Class names for FashionMNIST\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\n# Show augmented examples\nraw_dataset = torchvision.datasets.FashionMNIST(\n    root='./data', train=True, download=True, transform=transforms.ToTensor()\n)\n\nfig, axes = plt.subplots(3, 8, figsize=(16, 6))\n\n# First row: original images\nfor j in range(8):\n    img, label = raw_dataset[j]\n    axes[0, j].imshow(img.squeeze(), cmap='gray')\n    axes[0, j].set_title(class_names[label], fontsize=8)\n    axes[0, j].axis('off')\naxes[0, 0].set_ylabel('Original', fontsize=10, fontweight='bold')\n\n# Second and third rows: augmented versions of the same images\nfor row in range(1, 3):\n    for j in range(8):\n        img, label = train_dataset[j]  # Random augmentation applied each time\n        axes[row, j].imshow(img.squeeze(), cmap='gray')\n        axes[row, j].axis('off')\n    axes[row, 0].set_ylabel(f'Augmented {row}', fontsize=10, fontweight='bold')\n\nplt.suptitle('Data Augmentation: Same Images with Random Transforms', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(f\"Training samples: {len(train_dataset):,}\")\nprint(f\"Test samples:     {len(test_dataset):,}\")\nprint(f\"Classes:          {len(class_names)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ls4heyjfcne",
   "source": "### 7.2 Complete Training Pipeline\n\nLet us train our MiniResNet on FashionMNIST with all the best practices from the previous notebook.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "peko3wshlp",
   "source": "# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n\n# Initialize model, loss, optimizer, scheduler\ntorch.manual_seed(42)\nmodel = MiniResNet(num_classes=10).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\ndef train_one_epoch(model, loader, criterion, optimizer):\n    \"\"\"Train for one epoch and return average loss and accuracy.\"\"\"\n    model.train()\n    total_loss, correct, total = 0, 0, 0\n    \n    for images, labels in loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item() * images.size(0)\n        _, predicted = outputs.max(1)\n        correct += predicted.eq(labels).sum().item()\n        total += images.size(0)\n    \n    return total_loss / total, correct / total\n\ndef evaluate(model, loader, criterion):\n    \"\"\"Evaluate model and return loss and accuracy.\"\"\"\n    model.eval()\n    total_loss, correct, total = 0, 0, 0\n    \n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            total_loss += loss.item() * images.size(0)\n            _, predicted = outputs.max(1)\n            correct += predicted.eq(labels).sum().item()\n            total += images.size(0)\n    \n    return total_loss / total, correct / total\n\n# Training loop\nnum_epochs = 10\nhistory = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n\nprint(f\"Training MiniResNet on FashionMNIST ({sum(p.numel() for p in model.parameters()):,} parameters)\")\nprint(\"=\" * 65)\n\nfor epoch in range(num_epochs):\n    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n    test_loss, test_acc = evaluate(model, test_loader, criterion)\n    scheduler.step()\n    \n    history['train_loss'].append(train_loss)\n    history['test_loss'].append(test_loss)\n    history['train_acc'].append(train_acc)\n    history['test_acc'].append(test_acc)\n    \n    print(f\"Epoch {epoch+1:2d}/{num_epochs}: \"\n          f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f} | \"\n          f\"test_loss={test_loss:.4f}, test_acc={test_acc:.4f}\")\n\nprint(f\"\\nBest test accuracy: {max(history['test_acc']):.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "jgy6uqofdoj",
   "source": "# Plot training history\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Loss\nax = axes[0]\nax.plot(history['train_loss'], 'b-', linewidth=2, label='Train')\nax.plot(history['test_loss'], 'r-', linewidth=2, label='Test')\nax.set_xlabel('Epoch')\nax.set_ylabel('Loss')\nax.set_title('Training & Test Loss')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Accuracy\nax = axes[1]\nax.plot(history['train_acc'], 'b-', linewidth=2, label='Train')\nax.plot(history['test_acc'], 'r-', linewidth=2, label='Test')\nax.set_xlabel('Epoch')\nax.set_ylabel('Accuracy')\nax.set_title('Training & Test Accuracy')\nax.legend()\nax.grid(True, alpha=0.3)\nax.set_ylim(0.7, 1.0)\n\nplt.suptitle('MiniResNet Training on FashionMNIST', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "yob4utopm5l",
   "source": "### 7.3 Confusion Matrix and Per-Class Analysis",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vw7lhwmb4ml",
   "source": "# Generate predictions for confusion matrix\nmodel.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)\n        outputs = model(images)\n        _, predicted = outputs.max(1)\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.numpy())\n\nall_preds = np.array(all_preds)\nall_labels = np.array(all_labels)\n\n# Compute confusion matrix\nn_classes = 10\nconfusion = np.zeros((n_classes, n_classes), dtype=int)\nfor true, pred in zip(all_labels, all_preds):\n    confusion[true, pred] += 1\n\n# Plot confusion matrix\nfig, axes = plt.subplots(1, 2, figsize=(16, 7))\n\n# Confusion matrix\nax = axes[0]\nim = ax.imshow(confusion, cmap='Blues')\nplt.colorbar(im, ax=ax, fraction=0.046)\n\n# Add text annotations\nfor i in range(n_classes):\n    for j in range(n_classes):\n        val = confusion[i, j]\n        color = 'white' if val > confusion.max() / 2 else 'black'\n        ax.text(j, i, str(val), ha='center', va='center', fontsize=7, color=color)\n\nax.set_xticks(range(n_classes))\nax.set_yticks(range(n_classes))\nax.set_xticklabels(class_names, rotation=45, ha='right', fontsize=8)\nax.set_yticklabels(class_names, fontsize=8)\nax.set_xlabel('Predicted')\nax.set_ylabel('True')\nax.set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n\n# Per-class accuracy\nax = axes[1]\nper_class_acc = confusion.diagonal() / confusion.sum(axis=1)\ncolors_bar = ['green' if acc > 0.9 else 'orange' if acc > 0.8 else 'red' for acc in per_class_acc]\nbars = ax.barh(range(n_classes), per_class_acc, color=colors_bar, alpha=0.7, edgecolor='black')\nax.set_yticks(range(n_classes))\nax.set_yticklabels(class_names, fontsize=9)\nax.set_xlabel('Accuracy')\nax.set_title('Per-Class Accuracy', fontsize=12, fontweight='bold')\nax.set_xlim(0, 1)\nax.grid(True, alpha=0.3, axis='x')\n\n# Add value labels\nfor bar, acc in zip(bars, per_class_acc):\n    ax.text(acc + 0.01, bar.get_y() + bar.get_height()/2, f'{acc:.1%}', \n            va='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n# Print summary\noverall_acc = np.sum(all_preds == all_labels) / len(all_labels)\nprint(f\"Overall test accuracy: {overall_acc:.4f}\")\nprint(f\"\\nMost confused pair: \", end=\"\")\n\n# Find most confused pair (off-diagonal max)\nnp.fill_diagonal(confusion, 0)\nmax_idx = np.unravel_index(confusion.argmax(), confusion.shape)\nprint(f\"{class_names[max_idx[0]]} mistaken for {class_names[max_idx[1]]} \"\n      f\"({confusion[max_idx]} times)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "tjyb2pvqg3",
   "source": "# Show sample predictions with confidence\nmodel.eval()\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\n\n# Get a batch of test images\ntest_iter = iter(test_loader)\nimages, labels = next(test_iter)\n\nfor idx in range(10):\n    ax = axes[idx // 5, idx % 5]\n    \n    img = images[idx]\n    true_label = labels[idx].item()\n    \n    # Get prediction\n    with torch.no_grad():\n        output = model(img.unsqueeze(0).to(device))\n        probs = F.softmax(output, dim=1)\n        pred_label = probs.argmax().item()\n        confidence = probs.max().item()\n    \n    # Display\n    ax.imshow(img.squeeze(), cmap='gray')\n    \n    color = 'green' if pred_label == true_label else 'red'\n    ax.set_title(f'True: {class_names[true_label]}\\nPred: {class_names[pred_label]} ({confidence:.0%})',\n                 fontsize=8, color=color, fontweight='bold')\n    ax.axis('off')\n\nplt.suptitle('Sample Predictions (green=correct, red=wrong)', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "l0cf9h8eu9i",
   "source": "---\n\n## Exercises\n\n### Exercise 1: Manual Convolution\n\nImplement 2D convolution with padding and stride support.\n\n**F1 scenario:** Imagine you are building a telemetry analysis tool from scratch. Your first task is to implement the core operation -- sliding a pattern-detection filter across a 2D data grid (e.g., a time-frequency representation of engine audio). Get the padding and stride right so your output dimensions are exactly what you expect.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "3k5pe6h7yqa",
   "source": "# EXERCISE 1: Implement 2D convolution with padding and stride\ndef conv2d_full(image, kernel, padding=0, stride=1):\n    \"\"\"\n    Perform 2D convolution with padding and stride.\n    \n    Args:\n        image: 2D numpy array (H x W)\n        kernel: 2D numpy array (K x K)\n        padding: number of zero-padding pixels\n        stride: step size\n    \n    Returns:\n        Output feature map as numpy array\n    \"\"\"\n    # TODO: Implement this!\n    # Step 1: Pad the image with zeros if padding > 0\n    # Hint: Use np.pad(image, padding, mode='constant', constant_values=0)\n    \n    # Step 2: Compute output dimensions using the formula\n    # Hint: out_h = (H_padded - K) // stride + 1\n    \n    # Step 3: Slide the kernel with the given stride and compute dot products\n    \n    pass  # Replace with your implementation\n\n# Test cases\ntest_image = np.array([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12],\n    [13, 14, 15, 16]\n], dtype=float)\n\ntest_kernel = np.array([\n    [1, 0],\n    [0, -1]\n], dtype=float)\n\n# Test 1: No padding, stride 1\nresult1 = conv2d_full(test_image, test_kernel, padding=0, stride=1)\nexpected1 = np.array([[-5, -5, -5], [-5, -5, -5], [-5, -5, -5]])\nprint(\"Test 1 (pad=0, stride=1):\")\nprint(f\"  Your result:\\n{result1}\")\nprint(f\"  Expected:\\n{expected1}\")\nif result1 is not None:\n    print(f\"  Correct: {np.allclose(result1, expected1)}\")\n\n# Test 2: Padding 1, stride 1\nresult2 = conv2d_full(test_image, test_kernel, padding=1, stride=1)\nprint(f\"\\nTest 2 (pad=1, stride=1):\")\nprint(f\"  Output shape: {result2.shape if result2 is not None else 'None'}\")\nprint(f\"  Expected shape: (5, 5)\")\n\n# Test 3: No padding, stride 2\nresult3 = conv2d_full(test_image, test_kernel, padding=0, stride=2)\nexpected3 = np.array([[-5, -5], [-5, -5]])\nprint(f\"\\nTest 3 (pad=0, stride=2):\")\nprint(f\"  Your result:\\n{result3}\")\nprint(f\"  Expected:\\n{expected3}\")\nif result3 is not None:\n    print(f\"  Correct: {np.allclose(result3, expected3)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "enortdjwlya",
   "source": "### Exercise 2: Build a Custom CNN\n\nDesign a CNN for FashionMNIST and beat 88% test accuracy.\n\n**F1 scenario:** Think of each FashionMNIST image as a simplified telemetry snapshot -- a 28x28 grid encoding some pattern. Your task is to design a CNN architecture (choosing filter counts, kernel sizes, pooling strategy) that can reliably classify these patterns, much like an engineer would design a pipeline to classify different types of on-track events from telemetry spectrograms.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "1sjydgptf3f",
   "source": "# EXERCISE 2: Build your own CNN\nclass MyCNN(nn.Module):\n    \"\"\"\n    Design a CNN that achieves > 88% accuracy on FashionMNIST.\n    \n    Requirements:\n    - Input: (batch, 1, 28, 28) grayscale images\n    - Output: (batch, 10) class logits\n    - Use at least 2 conv layers\n    - Use at least 1 pooling layer\n    - Use BatchNorm\n    - Keep parameters under 500K\n    \n    Hints:\n    - Start with 16 or 32 filters, double at each block\n    - Use padding=1 with 3x3 kernels for 'same' convolutions\n    - Use MaxPool2d(2) to halve spatial dimensions\n    - Remember to flatten before the FC layers\n    - Use dropout for regularization\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        # TODO: Design your architecture!\n        # Hint: Conv -> BN -> ReLU -> Pool -> Conv -> BN -> ReLU -> Pool -> FC\n        \n        pass  # Replace with your implementation\n    \n    def forward(self, x):\n        # TODO: Implement forward pass\n        \n        pass  # Replace with your implementation\n\n# Test your architecture\n# my_model = MyCNN()\n# dummy = torch.randn(2, 1, 28, 28)\n# out = my_model(dummy)\n# print(f\"Output shape: {out.shape}\")\n# total_params = sum(p.numel() for p in my_model.parameters())\n# print(f\"Total parameters: {total_params:,}\")\n# assert out.shape == (2, 10), \"Output shape should be (batch, 10)\"\n# assert total_params < 500000, f\"Too many parameters: {total_params}\"\n# print(\"Architecture check passed!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "zpeakzxv6pl",
   "source": "### Exercise 3: Implement a Residual Block from Scratch\n\nImplement a residual block and verify the skip connection works correctly.\n\n**F1 scenario:** Build the \"raw + processed telemetry\" pipeline. Your residual block should process the input through two convolutional layers (the processed path) while also forwarding the raw input directly to the output. Verify that when the convolutional layers learn to output zero, the block acts as a perfect pass-through -- just like a telemetry system that defaults to showing raw data when no processing is applied.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "s5zun36iqhn",
   "source": "# EXERCISE 3: Implement a residual block\nclass MyResidualBlock(nn.Module):\n    \"\"\"\n    Implement a residual block:\n        output = ReLU( BN(Conv(ReLU(BN(Conv(x))))) + x )\n    \n    If in_channels != out_channels, use a 1x1 conv on the skip path.\n    \n    Args:\n        in_channels: Number of input channels\n        out_channels: Number of output channels\n    \"\"\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        # TODO: Define layers\n        # Hint: You need conv1, bn1, conv2, bn2\n        # Hint: If in_channels != out_channels, add a skip projection\n        \n        pass  # Replace with your implementation\n    \n    def forward(self, x):\n        # TODO: Implement forward with skip connection\n        # Hint: identity = x (or projected x)\n        # Hint: out = conv -> bn -> relu -> conv -> bn\n        # Hint: out = relu(out + identity)\n        \n        pass  # Replace with your implementation\n\n# Test\n# block = MyResidualBlock(16, 16)\n# x = torch.randn(2, 16, 14, 14)\n# out = block(x)\n# print(f\"Same channels:  input {x.shape} -> output {out.shape}\")\n# assert out.shape == x.shape, \"Output shape should match input when channels are the same\"\n\n# block2 = MyResidualBlock(16, 32)\n# out2 = block2(x)\n# print(f\"Diff channels:  input {x.shape} -> output {out2.shape}\")\n# assert out2.shape == (2, 32, 14, 14), \"Output should have 32 channels\"\n\n# # Verify skip connection: if conv weights are zero, output should equal input\n# block_zero = MyResidualBlock(16, 16)\n# with torch.no_grad():\n#     for param in block_zero.parameters():\n#         param.zero_()\n# # With zero weights and zero BN, output should be close to relu(x)\n# out_zero = block_zero(x)\n# print(f\"Zero weights test passed: output is non-trivial = {out_zero.abs().sum() > 0}\")\n# print(\"All tests passed!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5119duxpd2u",
   "source": "---\n\n## Summary\n\n### Key Concepts\n\n**Why CNNs:**\n- Fully connected networks waste parameters on images -- no spatial awareness\n- CNNs exploit local connectivity, weight sharing, and translation invariance\n- Dramatically fewer parameters while capturing spatial structure\n- **F1 parallel:** Just as an engineer scans telemetry with a sliding window rather than staring at every data point, CNNs slide learned filters across input data\n\n**Convolution Operation:**\n- A sliding window dot product between a filter and the input\n- Kernels detect features: edges, textures, patterns\n- Padding preserves spatial dimensions; stride reduces them\n- Output size: $O = \\lfloor(W - K + 2P) / S\\rfloor + 1$\n- **F1 parallel:** Each filter is a pattern template (braking signature, traction event) swept across the telemetry trace\n\n**Multiple Channels & Filters:**\n- Each filter slides across all input channels and produces one feature map\n- More filters = more features detected at each location\n- CNNs learn a hierarchy: edges -> textures -> parts -> objects\n- **F1 parallel:** Multi-channel convolution combines speed, brake, throttle, and steering simultaneously; the hierarchy goes from raw sensor spikes to event patterns to driving-style classification\n\n**Pooling:**\n- Max pooling keeps strongest activations; average pooling keeps means\n- Global average pooling replaces fully connected layers in modern architectures\n- No learnable parameters -- purely downsampling\n- **F1 parallel:** Summarizing a sector into key metrics -- peak speed (max pool), average pace (avg pool), one number per metric for the whole lap (GAP)\n\n**Classic Architectures:**\n- LeNet-5 (1998): Conv-Pool pattern, first practical CNN\n- VGG (2014): simple 3x3 filters stacked deep\n- ResNet (2015): skip connections enable hundreds of layers\n- **F1 parallel:** Skip connections = having both raw and processed telemetry available at every analysis stage, so no information is ever permanently lost\n\n**Practical Training:**\n- Data augmentation is critical for generalization\n- BatchNorm, proper initialization, and learning rate scheduling\n- Confusion matrices reveal per-class strengths and weaknesses\n\n### Connection to Deep Learning\n\n| Concept | Application | F1 Parallel |\n|---------|------------|-------------|\n| Convolution | Feature extraction from images, audio, and sequences | Scanning telemetry for braking zones, traction events |\n| Pooling | Dimensionality reduction, translation invariance | Summarizing sectors into key metrics |\n| Skip connections | Transformers, U-Nets, any deep architecture | Raw + processed telemetry overlay |\n| Feature hierarchy | Transfer learning (reuse early layers) | Sensor spikes -> events -> corner profiles -> driving style |\n| Data augmentation | Standard practice for all vision tasks | Simulating varied conditions (weather, calibration) |\n| Global average pooling | Modern classifier heads (fewer params than FC) | One summary stat per channel for a whole lap |\n| Confusion matrix | Diagnosing model weaknesses, class imbalance | Identifying which event types the model misclassifies |\n\n### Checklist\n\n- [ ] I can explain why CNNs are better than FC networks for images\n- [ ] I can compute 2D convolution by hand and calculate output sizes\n- [ ] I understand how multiple filters create feature maps\n- [ ] I know the difference between max pooling, average pooling, and GAP\n- [ ] I can build a CNN in PyTorch with Conv2d, MaxPool2d, and BatchNorm\n- [ ] I can explain skip connections and why they enable deeper networks\n- [ ] I can train a CNN with data augmentation and evaluate with a confusion matrix\n- [ ] I understand the key innovations of LeNet, VGG, and ResNet",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "a6larefwdhd",
   "source": "---\n\n## Next Steps\n\nWith CNNs under your belt, you are ready to tackle more advanced topics:\n\n1. **Recurrent Neural Networks (RNNs)**: Processing sequential data like text, time series, and audio -- or in F1 terms, modeling lap-by-lap tire degradation and fuel burn over a race distance\n2. **Transfer Learning**: Using pretrained CNNs (ResNet, EfficientNet) as feature extractors for new tasks with minimal data\n3. **Object Detection & Segmentation**: Going beyond classification to localize and segment objects (YOLO, Mask R-CNN)\n4. **Transformers for Vision**: Vision Transformers (ViT) that apply attention mechanisms to images, rivaling CNNs\n\n**Practical next steps:**\n- Train on CIFAR-10 (32x32 color images, 10 classes) -- a step up from FashionMNIST\n- Try transfer learning with `torchvision.models.resnet18(pretrained=True)`\n- Experiment with different augmentation strategies and measure their impact\n- Visualize learned filters of a trained network to see what it detects at each layer\n- Implement a deeper ResNet and compare with the MiniResNet from this notebook",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}