{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6.1: Reinforcement Learning Fundamentals\n",
    "\n",
    "We've spent 16 notebooks learning how to build models that learn from **labeled data** (supervised) or **unlabeled data** (self-supervised). But there's a third paradigm — one that learns from **experience and rewards**, just like a child learning to walk by falling down and getting back up.\n",
    "\n",
    "**Reinforcement Learning (RL)** is about an agent interacting with an environment, taking actions, receiving rewards, and learning a strategy to maximize long-term success. It's the foundation of game-playing AI (AlphaGo), robotics, and — critically — **RLHF**, the technique that makes language models like ChatGPT helpful and safe.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- [ ] Understand the agent-environment loop and how RL differs from supervised learning\n",
    "- [ ] Define Markov Decision Processes (MDPs) and their components\n",
    "- [ ] Derive and implement the Bellman equations for value functions\n",
    "- [ ] Distinguish between state-value functions V(s) and action-value functions Q(s,a)\n",
    "- [ ] Implement policy evaluation and policy iteration from scratch\n",
    "- [ ] Understand the exploration vs. exploitation tradeoff\n",
    "- [ ] Implement value iteration to solve a gridworld environment\n",
    "- [ ] Compare policy-based vs. value-based methods at a high level\n",
    "- [ ] Build intuition for temporal difference learning\n",
    "- [ ] Connect RL concepts to the RLHF pipeline introduced in Notebook 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Part 6.1: Reinforcement Learning Fundamentals\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The Reinforcement Learning Paradigm\n",
    "\n",
    "In supervised learning, we have input-output pairs and minimize a loss. In RL, there are no labels — the agent must **discover** which actions lead to rewards through trial and error.\n",
    "\n",
    "### The Agent-Environment Loop\n",
    "\n",
    "The core RL cycle works like this:\n",
    "\n",
    "1. The **agent** observes the current **state** $s_t$\n",
    "2. The agent selects an **action** $a_t$ based on its **policy** $\\pi$\n",
    "3. The **environment** transitions to a new state $s_{t+1}$\n",
    "4. The environment returns a **reward** $r_t$\n",
    "5. Repeat\n",
    "\n",
    "The goal: find a policy $\\pi^*$ that maximizes the **expected cumulative reward** over time.\n",
    "\n",
    "| Concept | Supervised Learning | Reinforcement Learning |\n",
    "|---------|-------------------|----------------------|\n",
    "| **Feedback** | Correct labels provided | Scalar reward signal |\n",
    "| **Timing** | Immediate | Can be delayed |\n",
    "| **Data** | Fixed dataset | Generated by agent's actions |\n",
    "| **Goal** | Minimize loss | Maximize cumulative reward |\n",
    "| **Exploration** | Not needed | Critical for learning |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: The RL Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "ax.set_title('The Reinforcement Learning Loop', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Agent box\n",
    "agent_box = mpatches.FancyBboxPatch((1, 5), 3, 2, boxstyle=\"round,pad=0.3\",\n",
    "                                     facecolor='#3498db', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(agent_box)\n",
    "ax.text(2.5, 6, 'AGENT', ha='center', va='center', fontsize=14,\n",
    "        fontweight='bold', color='white')\n",
    "ax.text(2.5, 5.4, 'Policy π(a|s)', ha='center', va='center', fontsize=10, color='white')\n",
    "\n",
    "# Environment box\n",
    "env_box = mpatches.FancyBboxPatch((6, 5), 3, 2, boxstyle=\"round,pad=0.3\",\n",
    "                                   facecolor='#2ecc71', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(env_box)\n",
    "ax.text(7.5, 6, 'ENVIRONMENT', ha='center', va='center', fontsize=14,\n",
    "        fontweight='bold', color='white')\n",
    "ax.text(7.5, 5.4, 'P(s\\'|s,a), R(s,a)', ha='center', va='center', fontsize=10, color='white')\n",
    "\n",
    "# Action arrow (agent -> environment)\n",
    "ax.annotate('', xy=(6, 6.5), xytext=(4, 6.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2.5, color='#e74c3c'))\n",
    "ax.text(5, 7.1, 'Action $a_t$', ha='center', va='center', fontsize=12,\n",
    "        color='#e74c3c', fontweight='bold')\n",
    "\n",
    "# State arrow (environment -> agent, bottom)\n",
    "ax.annotate('', xy=(4, 5.3), xytext=(6, 5.3),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2.5, color='#9b59b6'))\n",
    "ax.text(5, 4.6, 'State $s_{t+1}$', ha='center', va='center', fontsize=12,\n",
    "        color='#9b59b6', fontweight='bold')\n",
    "\n",
    "# Reward arrow (environment -> agent, further below)\n",
    "ax.annotate('', xy=(2.5, 3.5), xytext=(7.5, 3.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2.5, color='#f39c12',\n",
    "                           connectionstyle='arc3,rad=0.3'))\n",
    "ax.text(5, 2.5, 'Reward $r_t$', ha='center', va='center', fontsize=12,\n",
    "        color='#f39c12', fontweight='bold')\n",
    "\n",
    "# Time step indicator\n",
    "ax.text(5, 1.2, 'At each timestep t = 0, 1, 2, ...', ha='center', va='center',\n",
    "        fontsize=11, style='italic', color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key RL Terminology\n",
    "\n",
    "| Term | Symbol | Definition |\n",
    "|------|--------|------------|\n",
    "| **State** | $s \\in \\mathcal{S}$ | Current situation of the agent |\n",
    "| **Action** | $a \\in \\mathcal{A}$ | Decision the agent can make |\n",
    "| **Policy** | $\\pi(a|s)$ | Strategy mapping states to actions |\n",
    "| **Reward** | $r_t$ | Immediate feedback signal |\n",
    "| **Return** | $G_t$ | Cumulative discounted future reward |\n",
    "| **Discount factor** | $\\gamma \\in [0,1]$ | How much we value future vs. present rewards |\n",
    "| **Episode** | — | One complete sequence from start to terminal state |\n",
    "| **Trajectory** | $\\tau$ | Sequence of (state, action, reward) tuples |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Markov Decision Processes (MDPs)\n",
    "\n",
    "An MDP formalizes the RL problem mathematically. It's defined by the tuple $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$:\n",
    "\n",
    "- $\\mathcal{S}$: Set of states\n",
    "- $\\mathcal{A}$: Set of actions\n",
    "- $P(s'|s,a)$: **Transition probability** — probability of reaching state $s'$ from state $s$ after taking action $a$\n",
    "- $R(s,a)$: **Reward function** — expected reward for taking action $a$ in state $s$\n",
    "- $\\gamma$: **Discount factor** — balances immediate vs. future rewards\n",
    "\n",
    "### The Markov Property\n",
    "\n",
    "The key assumption: the future depends only on the **current state**, not the history:\n",
    "\n",
    "$$P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, \\ldots) = P(s_{t+1} | s_t, a_t)$$\n",
    "\n",
    "This is powerful because it means we can make optimal decisions using only the current state — no need to remember the entire history.\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "Think of chess: the current board position contains everything you need to make your next move. It doesn't matter *how* you got to that position — the optimal strategy depends only on where the pieces are *right now*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Gridworld MDP\n",
    "\n",
    "Let's build a simple gridworld — the \"hello world\" of RL. Our agent navigates a 4×4 grid trying to reach a goal while avoiding traps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"A simple gridworld MDP environment.\"\"\"\n",
    "    \n",
    "    # Cell types\n",
    "    EMPTY = 0\n",
    "    WALL = 1\n",
    "    GOAL = 2\n",
    "    TRAP = 3\n",
    "    \n",
    "    # Actions: up, down, left, right\n",
    "    ACTIONS = ['up', 'down', 'left', 'right']\n",
    "    ACTION_DELTAS = {\n",
    "        'up': (-1, 0),\n",
    "        'down': (1, 0),\n",
    "        'left': (0, -1),\n",
    "        'right': (0, 1)\n",
    "    }\n",
    "    \n",
    "    def __init__(self, grid_size=4, slip_prob=0.1):\n",
    "        self.grid_size = grid_size\n",
    "        self.slip_prob = slip_prob  # Probability of slipping to a random adjacent cell\n",
    "        \n",
    "        # Define the grid\n",
    "        self.grid = np.zeros((grid_size, grid_size), dtype=int)\n",
    "        self.grid[0, 3] = self.GOAL   # Goal at top-right\n",
    "        self.grid[1, 1] = self.WALL   # Wall\n",
    "        self.grid[2, 3] = self.TRAP   # Trap\n",
    "        \n",
    "        # State and action spaces\n",
    "        self.states = [(i, j) for i in range(grid_size) for j in range(grid_size)\n",
    "                       if self.grid[i, j] != self.WALL]\n",
    "        self.terminal_states = [(i, j) for i in range(grid_size) for j in range(grid_size)\n",
    "                                if self.grid[i, j] in [self.GOAL, self.TRAP]]\n",
    "        self.n_states = len(self.states)\n",
    "        self.n_actions = len(self.ACTIONS)\n",
    "        \n",
    "        # Starting position\n",
    "        self.start = (3, 0)\n",
    "        self.agent_pos = self.start\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset to starting position.\"\"\"\n",
    "        self.agent_pos = self.start\n",
    "        return self.agent_pos\n",
    "    \n",
    "    def _is_valid(self, pos):\n",
    "        \"\"\"Check if a position is valid (in bounds and not a wall).\"\"\"\n",
    "        r, c = pos\n",
    "        return (0 <= r < self.grid_size and 0 <= c < self.grid_size \n",
    "                and self.grid[r, c] != self.WALL)\n",
    "    \n",
    "    def get_transitions(self, state, action):\n",
    "        \"\"\"Return list of (probability, next_state, reward) for a state-action pair.\"\"\"\n",
    "        if state in self.terminal_states:\n",
    "            return [(1.0, state, 0.0)]  # Terminal states loop with zero reward\n",
    "        \n",
    "        transitions = []\n",
    "        intended_delta = self.ACTION_DELTAS[action]\n",
    "        intended_next = (state[0] + intended_delta[0], state[1] + intended_delta[1])\n",
    "        \n",
    "        # Intended action succeeds with probability (1 - slip_prob)\n",
    "        if self._is_valid(intended_next):\n",
    "            next_state = intended_next\n",
    "        else:\n",
    "            next_state = state  # Bounce off wall/boundary\n",
    "        \n",
    "        reward = self._get_reward(next_state)\n",
    "        transitions.append((1.0 - self.slip_prob, next_state, reward))\n",
    "        \n",
    "        # With slip_prob, agent moves in a random perpendicular direction\n",
    "        if self.slip_prob > 0:\n",
    "            perpendicular = []\n",
    "            if action in ['up', 'down']:\n",
    "                perpendicular = ['left', 'right']\n",
    "            else:\n",
    "                perpendicular = ['up', 'down']\n",
    "            \n",
    "            for perp_action in perpendicular:\n",
    "                perp_delta = self.ACTION_DELTAS[perp_action]\n",
    "                perp_next = (state[0] + perp_delta[0], state[1] + perp_delta[1])\n",
    "                if self._is_valid(perp_next):\n",
    "                    perp_state = perp_next\n",
    "                else:\n",
    "                    perp_state = state\n",
    "                perp_reward = self._get_reward(perp_state)\n",
    "                transitions.append((self.slip_prob / 2, perp_state, perp_reward))\n",
    "        \n",
    "        return transitions\n",
    "    \n",
    "    def _get_reward(self, state):\n",
    "        \"\"\"Reward function.\"\"\"\n",
    "        if self.grid[state[0], state[1]] == self.GOAL:\n",
    "            return +1.0\n",
    "        elif self.grid[state[0], state[1]] == self.TRAP:\n",
    "            return -1.0\n",
    "        else:\n",
    "            return -0.04  # Small step penalty to encourage efficiency\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action, return (next_state, reward, done).\"\"\"\n",
    "        transitions = self.get_transitions(self.agent_pos, action)\n",
    "        probs = [t[0] for t in transitions]\n",
    "        idx = np.random.choice(len(transitions), p=probs)\n",
    "        _, next_state, reward = transitions[idx]\n",
    "        \n",
    "        self.agent_pos = next_state\n",
    "        done = next_state in self.terminal_states\n",
    "        return next_state, reward, done\n",
    "\n",
    "\n",
    "# Create and display the gridworld\n",
    "env = GridWorld(grid_size=4, slip_prob=0.1)\n",
    "\n",
    "print(\"GridWorld MDP\")\n",
    "print(f\"States: {env.n_states} (excluding walls)\")\n",
    "print(f\"Actions: {env.n_actions} ({', '.join(env.ACTIONS)})\")\n",
    "print(f\"Terminal states: {env.terminal_states}\")\n",
    "print(f\"Slip probability: {env.slip_prob}\")\n",
    "print(f\"Start: {env.start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: The Gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gridworld(env, values=None, policy=None, title='GridWorld'):\n",
    "    \"\"\"Visualize the gridworld with optional value function and policy overlays.\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
    "    n = env.grid_size\n",
    "    \n",
    "    # Color map for cell types\n",
    "    colors = {\n",
    "        GridWorld.EMPTY: '#f0f0f0',\n",
    "        GridWorld.WALL: '#2c3e50',\n",
    "        GridWorld.GOAL: '#2ecc71',\n",
    "        GridWorld.TRAP: '#e74c3c'\n",
    "    }\n",
    "    \n",
    "    # Draw cells\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            cell_type = env.grid[i, j]\n",
    "            color = colors[cell_type]\n",
    "            \n",
    "            # If we have values, shade empty cells by value\n",
    "            if values is not None and cell_type == GridWorld.EMPTY:\n",
    "                v = values.get((i, j), 0)\n",
    "                # Normalize to [-1, 1] for coloring\n",
    "                intensity = np.clip(v, -1, 1)\n",
    "                if intensity >= 0:\n",
    "                    color = plt.cm.RdYlGn(0.5 + intensity * 0.5)\n",
    "                else:\n",
    "                    color = plt.cm.RdYlGn(0.5 + intensity * 0.5)\n",
    "            \n",
    "            rect = plt.Rectangle((j, n - 1 - i), 1, 1, facecolor=color,\n",
    "                                  edgecolor='black', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Labels\n",
    "            if cell_type == GridWorld.GOAL:\n",
    "                ax.text(j + 0.5, n - 1 - i + 0.7, 'GOAL', ha='center', va='center',\n",
    "                        fontsize=11, fontweight='bold', color='white')\n",
    "                ax.text(j + 0.5, n - 1 - i + 0.4, '+1.0', ha='center', va='center',\n",
    "                        fontsize=10, color='white')\n",
    "            elif cell_type == GridWorld.TRAP:\n",
    "                ax.text(j + 0.5, n - 1 - i + 0.7, 'TRAP', ha='center', va='center',\n",
    "                        fontsize=11, fontweight='bold', color='white')\n",
    "                ax.text(j + 0.5, n - 1 - i + 0.4, '-1.0', ha='center', va='center',\n",
    "                        fontsize=10, color='white')\n",
    "            elif cell_type == GridWorld.WALL:\n",
    "                ax.text(j + 0.5, n - 1 - i + 0.5, 'WALL', ha='center', va='center',\n",
    "                        fontsize=11, fontweight='bold', color='white')\n",
    "            \n",
    "            # Show values\n",
    "            if values is not None and (i, j) in values and cell_type not in [GridWorld.WALL]:\n",
    "                v = values[(i, j)]\n",
    "                ax.text(j + 0.5, n - 1 - i + 0.15, f'{v:.3f}', ha='center', va='center',\n",
    "                        fontsize=9, color='black', style='italic')\n",
    "            \n",
    "            # Show policy arrows\n",
    "            if policy is not None and (i, j) in policy and cell_type == GridWorld.EMPTY:\n",
    "                action = policy[(i, j)]\n",
    "                arrow_map = {\n",
    "                    'up': (0, 0.25),\n",
    "                    'down': (0, -0.25),\n",
    "                    'left': (-0.25, 0),\n",
    "                    'right': (0.25, 0)\n",
    "                }\n",
    "                dx, dy = arrow_map[action]\n",
    "                ax.annotate('', xy=(j + 0.5 + dx, n - 1 - i + 0.5 + dy),\n",
    "                           xytext=(j + 0.5, n - 1 - i + 0.5),\n",
    "                           arrowprops=dict(arrowstyle='->', lw=2.5, color='#2c3e50'))\n",
    "    \n",
    "    # Mark start\n",
    "    si, sj = env.start\n",
    "    ax.text(sj + 0.5, n - 1 - si + 0.85, 'START', ha='center', va='center',\n",
    "            fontsize=8, fontweight='bold', color='#3498db')\n",
    "    \n",
    "    ax.set_xlim(0, n)\n",
    "    ax.set_ylim(0, n)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(range(n))\n",
    "    ax.set_yticks(range(n))\n",
    "    ax.set_xticklabels(range(n))\n",
    "    ax.set_yticklabels(range(n-1, -1, -1))\n",
    "    ax.set_xlabel('Column')\n",
    "    ax.set_ylabel('Row')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_gridworld(env, title='4×4 GridWorld Environment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent starts at the bottom-left and must navigate to the GOAL (+1.0) while avoiding the TRAP (-1.0). Each non-terminal step costs -0.04 to encourage the agent to find the goal quickly. There's a 10% chance of slipping perpendicular to the intended direction — this stochasticity is what makes the problem interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Returns and Discounting\n",
    "\n",
    "The agent doesn't just want the next reward — it wants to maximize the **total reward over time**. We call this the **return**:\n",
    "\n",
    "$$G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}$$\n",
    "\n",
    "The **discount factor** $\\gamma$ determines how much we value future rewards:\n",
    "\n",
    "- $\\gamma = 0$: Only care about immediate reward (greedy)\n",
    "- $\\gamma = 1$: Value all future rewards equally (far-sighted)\n",
    "- $\\gamma = 0.9$: A reward 10 steps away is worth $0.9^{10} \\approx 0.35$ of an immediate reward\n",
    "\n",
    "### Why discount?\n",
    "\n",
    "1. **Mathematical convenience**: Makes infinite sums converge\n",
    "2. **Uncertainty**: The further into the future, the less certain we are\n",
    "3. **Human-like behavior**: We prefer rewards sooner rather than later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Effect of Discount Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Discount curves\n",
    "steps = np.arange(0, 20)\n",
    "gammas = [0.0, 0.5, 0.9, 0.99, 1.0]\n",
    "colors = plt.cm.viridis(np.linspace(0.1, 0.9, len(gammas)))\n",
    "\n",
    "for gamma, color in zip(gammas, colors):\n",
    "    weights = [gamma**k for k in steps]\n",
    "    axes[0].plot(steps, weights, 'o-', label=f'γ = {gamma}', color=color, markersize=4)\n",
    "\n",
    "axes[0].set_xlabel('Steps into the future (k)', fontsize=12)\n",
    "axes[0].set_ylabel('Discount weight (γᵏ)', fontsize=12)\n",
    "axes[0].set_title('How Much We Value Future Rewards', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Cumulative return example\n",
    "rewards = [0, -0.04, -0.04, -0.04, -0.04, 1.0]  # Path to goal\n",
    "gamma_vals = [0.5, 0.9, 0.99]\n",
    "bar_width = 0.25\n",
    "x = np.arange(len(rewards))\n",
    "\n",
    "for i, gamma in enumerate(gamma_vals):\n",
    "    discounted = [rewards[k] * gamma**k for k in range(len(rewards))]\n",
    "    axes[1].bar(x + i * bar_width, discounted, bar_width,\n",
    "               label=f'γ = {gamma} (Return = {sum(discounted):.3f})',\n",
    "               alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel('Time step', fontsize=12)\n",
    "axes[1].set_ylabel('Discounted reward', fontsize=12)\n",
    "axes[1].set_title('Same Path, Different Discount Factors', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xticks(x + bar_width)\n",
    "axes[1].set_xticklabels([f't={k}' for k in range(len(rewards))])\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Numeric example\n",
    "print(\"Example: Agent takes 5 steps then reaches goal\")\n",
    "print(f\"Rewards: {rewards}\")\n",
    "for gamma in [0.5, 0.9, 0.99]:\n",
    "    G = sum(r * gamma**k for k, r in enumerate(rewards))\n",
    "    print(f\"  γ = {gamma}: Return G₀ = {G:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Value Functions and the Bellman Equation\n",
    "\n",
    "Value functions answer the question: **\"How good is it to be in a particular state (or to take a particular action in a state)?\"**\n",
    "\n",
    "### State-Value Function V(s)\n",
    "\n",
    "The **state-value function** $V^\\pi(s)$ is the expected return starting from state $s$ and following policy $\\pi$:\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi[G_t | s_t = s] = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k r_{t+k} \\mid s_t = s\\right]$$\n",
    "\n",
    "### Action-Value Function Q(s, a)\n",
    "\n",
    "The **action-value function** $Q^\\pi(s, a)$ is the expected return starting from state $s$, taking action $a$, then following policy $\\pi$:\n",
    "\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}_\\pi[G_t | s_t = s, a_t = a]$$\n",
    "\n",
    "### The Bellman Equation\n",
    "\n",
    "The key insight: we can express the value of a state **recursively** in terms of the values of successor states:\n",
    "\n",
    "$$V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) \\left[R(s,a) + \\gamma V^\\pi(s')\\right]$$\n",
    "\n",
    "This is the **Bellman expectation equation** — the foundation of almost every RL algorithm.\n",
    "\n",
    "**In words**: The value of a state equals the expected immediate reward plus the discounted value of the next state, averaged over all actions and transitions.\n",
    "\n",
    "### Deep Dive: Why the Bellman Equation Matters\n",
    "\n",
    "The Bellman equation transforms an intractable problem (compute expected infinite sums) into a system of linear equations that can be solved iteratively. It's the RL equivalent of dynamic programming — breaking a hard problem into overlapping subproblems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Bellman Equation Backup Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: V(s) backup\n",
    "ax = axes[0]\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-1, 4)\n",
    "ax.axis('off')\n",
    "ax.set_title('V(s) Bellman Backup', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Root state\n",
    "ax.plot(0, 3.5, 'o', markersize=25, color='#3498db', zorder=5)\n",
    "ax.text(0, 3.5, 's', ha='center', va='center', fontsize=12, fontweight='bold', color='white')\n",
    "\n",
    "# Action nodes\n",
    "action_x = [-1.2, 0, 1.2]\n",
    "action_labels = ['a₁', 'a₂', 'a₃']\n",
    "for x, label in zip(action_x, action_labels):\n",
    "    ax.plot(x, 2, 's', markersize=15, color='#e74c3c', zorder=5)\n",
    "    ax.text(x, 2, label, ha='center', va='center', fontsize=9, color='white', fontweight='bold')\n",
    "    ax.plot([0, x], [3.2, 2.2], '-', color='gray', lw=1.5)\n",
    "    ax.text((0 + x)/2 - 0.15, 2.7, 'π(a|s)', fontsize=7, color='gray', ha='center')\n",
    "\n",
    "# Next states from action a2\n",
    "next_x = [-0.5, 0.5]\n",
    "for x in next_x:\n",
    "    ax.plot(x, 0.5, 'o', markersize=20, color='#2ecc71', zorder=5)\n",
    "    ax.text(x, 0.5, \"s'\", ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "    ax.plot([0, x], [1.8, 0.7], '-', color='gray', lw=1.5)\n",
    "\n",
    "ax.text(0.5, 1.3, 'P(s\\'|s,a)', fontsize=8, color='gray', ha='center')\n",
    "ax.text(0, -0.3, 'r + γV(s\\')', ha='center', fontsize=10, style='italic', color='#2c3e50')\n",
    "\n",
    "# Right: Q(s,a) backup\n",
    "ax = axes[1]\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-1, 4)\n",
    "ax.axis('off')\n",
    "ax.set_title('Q(s,a) Bellman Backup', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Root action\n",
    "ax.plot(0, 3.5, 's', markersize=20, color='#e74c3c', zorder=5)\n",
    "ax.text(0, 3.5, 'a', ha='center', va='center', fontsize=12, fontweight='bold', color='white')\n",
    "\n",
    "# Next states\n",
    "next_x = [-1, 0, 1]\n",
    "for x in next_x:\n",
    "    ax.plot(x, 2, 'o', markersize=22, color='#2ecc71', zorder=5)\n",
    "    ax.text(x, 2, \"s'\", ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "    ax.plot([0, x], [3.2, 2.2], '-', color='gray', lw=1.5)\n",
    "\n",
    "ax.text(0.7, 2.7, 'P(s\\'|s,a)', fontsize=8, color='gray', ha='center')\n",
    "\n",
    "# Next actions from s'\n",
    "for base_x in [-1, 1]:\n",
    "    offsets = [-0.3, 0.3]\n",
    "    for off in offsets:\n",
    "        x = base_x + off\n",
    "        ax.plot(x, 0.5, 's', markersize=12, color='#e74c3c', zorder=5)\n",
    "        ax.text(x, 0.5, \"a'\", ha='center', va='center', fontsize=7, color='white', fontweight='bold')\n",
    "        ax.plot([base_x, x], [1.8, 0.7], '-', color='gray', lw=1)\n",
    "\n",
    "ax.text(0.7, 1.3, 'π(a\\'|s\\')', fontsize=8, color='gray', ha='center')\n",
    "ax.text(0, -0.3, 'r + γ Σ π(a\\'|s\\')Q(s\\',a\\')', ha='center', fontsize=10, style='italic', color='#2c3e50')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Policy Evaluation\n",
    "\n",
    "Given a policy $\\pi$, **policy evaluation** computes $V^\\pi(s)$ for every state. We do this by repeatedly applying the Bellman equation until convergence:\n",
    "\n",
    "$$V_{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) \\left[R(s,a) + \\gamma V_k(s')\\right]$$\n",
    "\n",
    "Starting from $V_0(s) = 0$ for all states, this iterative process is guaranteed to converge to $V^\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, gamma=0.9, theta=1e-6, max_iterations=1000):\n",
    "    \"\"\"Evaluate a policy by iteratively applying the Bellman expectation equation.\n",
    "    \n",
    "    Args:\n",
    "        env: GridWorld environment\n",
    "        policy: dict mapping state -> action (deterministic policy)\n",
    "        gamma: discount factor\n",
    "        theta: convergence threshold\n",
    "        max_iterations: safety limit\n",
    "    \n",
    "    Returns:\n",
    "        V: dict mapping state -> value\n",
    "        history: list of value dicts at each iteration (for visualization)\n",
    "    \"\"\"\n",
    "    # Initialize values to zero\n",
    "    V = {s: 0.0 for s in env.states}\n",
    "    history = [V.copy()]\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        delta = 0\n",
    "        V_new = V.copy()\n",
    "        \n",
    "        for s in env.states:\n",
    "            if s in env.terminal_states:\n",
    "                # Terminal states have fixed values based on reward\n",
    "                if env.grid[s[0], s[1]] == GridWorld.GOAL:\n",
    "                    V_new[s] = 1.0\n",
    "                elif env.grid[s[0], s[1]] == GridWorld.TRAP:\n",
    "                    V_new[s] = -1.0\n",
    "                continue\n",
    "            \n",
    "            # Bellman expectation equation for deterministic policy\n",
    "            action = policy.get(s, 'right')  # Default action\n",
    "            transitions = env.get_transitions(s, action)\n",
    "            \n",
    "            v = sum(prob * (reward + gamma * V[s_next])\n",
    "                    for prob, s_next, reward in transitions)\n",
    "            \n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "            V_new[s] = v\n",
    "        \n",
    "        V = V_new\n",
    "        history.append(V.copy())\n",
    "        \n",
    "        if delta < theta:\n",
    "            print(f\"Policy evaluation converged after {iteration + 1} iterations (Δ < {theta})\")\n",
    "            break\n",
    "    \n",
    "    return V, history\n",
    "\n",
    "\n",
    "# Evaluate a simple policy: always go right\n",
    "simple_policy = {s: 'right' for s in env.states}\n",
    "V_simple, history_simple = policy_evaluation(env, simple_policy, gamma=0.9)\n",
    "\n",
    "print(\"\\nValues under 'always go right' policy:\")\n",
    "for i in range(env.grid_size):\n",
    "    row_vals = []\n",
    "    for j in range(env.grid_size):\n",
    "        if (i, j) in V_simple:\n",
    "            row_vals.append(f\"{V_simple[(i,j)]:7.3f}\")\n",
    "        else:\n",
    "            row_vals.append(\"  WALL \")\n",
    "    print(\" | \".join(row_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the values\n",
    "visualize_gridworld(env, values=V_simple, policy=simple_policy,\n",
    "                    title='Policy Evaluation: \"Always Go Right\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Convergence of Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how values converge over iterations\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "iterations_to_show = [0, 2, 10, len(history_simple) - 1]\n",
    "n = env.grid_size\n",
    "\n",
    "for ax, it in zip(axes, iterations_to_show):\n",
    "    V_it = history_simple[it]\n",
    "    grid_vals = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if (i, j) in V_it:\n",
    "                grid_vals[i, j] = V_it[(i, j)]\n",
    "            else:\n",
    "                grid_vals[i, j] = np.nan\n",
    "    \n",
    "    im = ax.imshow(grid_vals, cmap='RdYlGn', vmin=-1, vmax=1)\n",
    "    ax.set_title(f'Iteration {it}', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if (i, j) in V_it:\n",
    "                ax.text(j, i, f'{V_it[(i,j)]:.2f}', ha='center', va='center',\n",
    "                       fontsize=9, fontweight='bold')\n",
    "            elif env.grid[i, j] == GridWorld.WALL:\n",
    "                ax.text(j, i, 'W', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.set_xticks(range(n))\n",
    "    ax.set_yticks(range(n))\n",
    "\n",
    "plt.suptitle('Policy Evaluation Convergence', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the values propagate backward from the goal and trap states, iteration by iteration. This is the Bellman equation at work — each iteration, information about future rewards flows one more step backward through the state space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Policy Improvement and Policy Iteration\n",
    "\n",
    "Policy evaluation tells us *how good* a policy is. But we want the *best* policy. **Policy improvement** uses the value function to greedily select better actions:\n",
    "\n",
    "$$\\pi'(s) = \\arg\\max_a \\sum_{s'} P(s'|s,a) \\left[R(s,a) + \\gamma V^\\pi(s')\\right]$$\n",
    "\n",
    "**Policy iteration** alternates between:\n",
    "1. **Evaluate**: Compute $V^\\pi$ for the current policy\n",
    "2. **Improve**: Update the policy greedily with respect to $V^\\pi$\n",
    "\n",
    "This is guaranteed to converge to the optimal policy $\\pi^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, V, gamma=0.9):\n",
    "    \"\"\"Improve policy greedily based on value function.\"\"\"\n",
    "    policy = {}\n",
    "    \n",
    "    for s in env.states:\n",
    "        if s in env.terminal_states:\n",
    "            continue\n",
    "        \n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "        \n",
    "        for action in env.ACTIONS:\n",
    "            transitions = env.get_transitions(s, action)\n",
    "            q_sa = sum(prob * (reward + gamma * V[s_next])\n",
    "                       for prob, s_next, reward in transitions)\n",
    "            \n",
    "            if q_sa > best_value:\n",
    "                best_value = q_sa\n",
    "                best_action = action\n",
    "        \n",
    "        policy[s] = best_action\n",
    "    \n",
    "    return policy\n",
    "\n",
    "\n",
    "def policy_iteration(env, gamma=0.9):\n",
    "    \"\"\"Full policy iteration algorithm.\"\"\"\n",
    "    # Start with random policy\n",
    "    policy = {s: np.random.choice(env.ACTIONS) for s in env.states\n",
    "              if s not in env.terminal_states}\n",
    "    \n",
    "    iteration = 0\n",
    "    while True:\n",
    "        # Policy evaluation\n",
    "        V, _ = policy_evaluation(env, policy, gamma)\n",
    "        \n",
    "        # Policy improvement\n",
    "        new_policy = policy_improvement(env, V, gamma)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if new_policy == policy:\n",
    "            print(f\"\\nPolicy iteration converged after {iteration + 1} improvement steps!\")\n",
    "            break\n",
    "        \n",
    "        policy = new_policy\n",
    "        iteration += 1\n",
    "    \n",
    "    return policy, V\n",
    "\n",
    "\n",
    "optimal_policy, optimal_V = policy_iteration(env, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the optimal policy\n",
    "visualize_gridworld(env, values=optimal_V, policy=optimal_policy,\n",
    "                    title='Optimal Policy (Policy Iteration, γ=0.9)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arrows show the optimal action in each state. Notice how the agent learns to navigate around the wall, move toward the goal, and stay away from the trap. The values decrease as we move further from the goal, reflecting the discounted future reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Value Iteration\n",
    "\n",
    "**Value iteration** combines evaluation and improvement into a single step, updating values directly with the Bellman **optimality** equation:\n",
    "\n",
    "$$V_{k+1}(s) = \\max_a \\sum_{s'} P(s'|s,a) \\left[R(s,a) + \\gamma V_k(s')\\right]$$\n",
    "\n",
    "Instead of fully evaluating a policy before improving it, value iteration takes the max over actions at every step — essentially doing greedy improvement as part of the evaluation.\n",
    "\n",
    "| Method | Steps per Iteration | Convergence |\n",
    "|--------|-------------------|-------------|\n",
    "| **Policy Iteration** | Full evaluation + improvement | Fewer outer iterations |\n",
    "| **Value Iteration** | Single Bellman optimality update | More iterations but simpler |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.9, theta=1e-6, max_iterations=1000):\n",
    "    \"\"\"Value iteration: combine evaluation and improvement in one step.\"\"\"\n",
    "    V = {s: 0.0 for s in env.states}\n",
    "    history = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        delta = 0\n",
    "        V_new = V.copy()\n",
    "        \n",
    "        for s in env.states:\n",
    "            if s in env.terminal_states:\n",
    "                if env.grid[s[0], s[1]] == GridWorld.GOAL:\n",
    "                    V_new[s] = 1.0\n",
    "                elif env.grid[s[0], s[1]] == GridWorld.TRAP:\n",
    "                    V_new[s] = -1.0\n",
    "                continue\n",
    "            \n",
    "            # Bellman optimality equation: take the MAX over actions\n",
    "            action_values = []\n",
    "            for action in env.ACTIONS:\n",
    "                transitions = env.get_transitions(s, action)\n",
    "                q = sum(prob * (reward + gamma * V[s_next])\n",
    "                        for prob, s_next, reward in transitions)\n",
    "                action_values.append(q)\n",
    "            \n",
    "            best_value = max(action_values)\n",
    "            delta = max(delta, abs(best_value - V[s]))\n",
    "            V_new[s] = best_value\n",
    "        \n",
    "        V = V_new\n",
    "        history.append({'iteration': iteration, 'delta': delta, 'V': V.copy()})\n",
    "        \n",
    "        if delta < theta:\n",
    "            print(f\"Value iteration converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "    \n",
    "    # Extract policy from final values\n",
    "    policy = policy_improvement(env, V, gamma)\n",
    "    \n",
    "    return V, policy, history\n",
    "\n",
    "\n",
    "V_vi, policy_vi, history_vi = value_iteration(env, gamma=0.9)\n",
    "\n",
    "# Compare with policy iteration\n",
    "print(\"\\nValue Iteration vs Policy Iteration values match:\",\n",
    "      all(abs(V_vi[s] - optimal_V[s]) < 1e-4 for s in env.states))\n",
    "print(\"Policies match:\",\n",
    "      all(policy_vi.get(s) == optimal_policy.get(s) for s in env.states\n",
    "          if s not in env.terminal_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convergence speed\n",
    "deltas = [h['delta'] for h in history_vi]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.semilogy(range(len(deltas)), deltas, 'b-o', markersize=3)\n",
    "ax.axhline(y=1e-6, color='r', linestyle='--', label='Convergence threshold')\n",
    "ax.set_xlabel('Iteration', fontsize=12)\n",
    "ax.set_ylabel('Max value change (Δ)', fontsize=12)\n",
    "ax.set_title('Value Iteration Convergence', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Exploration vs. Exploitation\n",
    "\n",
    "One of the fundamental challenges in RL: should the agent **exploit** what it already knows works, or **explore** new actions that might lead to better outcomes?\n",
    "\n",
    "- **Exploitation**: Choose the action with the highest estimated value\n",
    "- **Exploration**: Try less-visited or uncertain actions\n",
    "\n",
    "Too much exploitation → stuck in local optima (never discovers the best path)  \n",
    "Too much exploration → wastes time on suboptimal actions\n",
    "\n",
    "### Common Exploration Strategies\n",
    "\n",
    "| Strategy | How it Works | Tradeoff |\n",
    "|----------|-------------|----------|\n",
    "| **ε-greedy** | With prob ε, random action; otherwise, best action | Simple, widely used |\n",
    "| **ε-decay** | Start with high ε, decrease over time | Explores early, exploits later |\n",
    "| **Softmax/Boltzmann** | Sample actions proportional to estimated values | Smooth exploration |\n",
    "| **UCB** | Bonus for under-explored actions | Principled, optimistic |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q, state, epsilon, actions):\n",
    "    \"\"\"Select action using epsilon-greedy strategy.\"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.choice(actions)  # Explore\n",
    "    else:\n",
    "        q_values = [Q.get((state, a), 0.0) for a in actions]\n",
    "        return actions[np.argmax(q_values)]  # Exploit\n",
    "\n",
    "\n",
    "def softmax_action(Q, state, temperature, actions):\n",
    "    \"\"\"Select action using softmax (Boltzmann) exploration.\"\"\"\n",
    "    q_values = np.array([Q.get((state, a), 0.0) for a in actions])\n",
    "    # Numerical stability\n",
    "    q_values = q_values - np.max(q_values)\n",
    "    probs = np.exp(q_values / temperature)\n",
    "    probs = probs / probs.sum()\n",
    "    return np.random.choice(actions, p=probs)\n",
    "\n",
    "\n",
    "# Demonstrate the multi-armed bandit problem — the simplest explore/exploit scenario\n",
    "class MultiArmedBandit:\n",
    "    \"\"\"A simple multi-armed bandit with Gaussian rewards.\"\"\"\n",
    "    def __init__(self, n_arms=5):\n",
    "        self.n_arms = n_arms\n",
    "        self.true_means = np.random.randn(n_arms)  # True reward means\n",
    "    \n",
    "    def pull(self, arm):\n",
    "        \"\"\"Pull an arm, get noisy reward.\"\"\"\n",
    "        return self.true_means[arm] + np.random.randn() * 0.5\n",
    "\n",
    "\n",
    "def run_bandit_experiment(n_steps=1000, n_runs=200):\n",
    "    \"\"\"Compare exploration strategies on a bandit problem.\"\"\"\n",
    "    strategies = {\n",
    "        'ε=0 (pure greedy)': 0.0,\n",
    "        'ε=0.01': 0.01,\n",
    "        'ε=0.1': 0.1,\n",
    "        'ε=0.5': 0.5,\n",
    "    }\n",
    "    \n",
    "    results = {name: np.zeros(n_steps) for name in strategies}\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        bandit = MultiArmedBandit(n_arms=10)\n",
    "        best_mean = np.max(bandit.true_means)\n",
    "        \n",
    "        for name, epsilon in strategies.items():\n",
    "            Q = np.zeros(10)  # Estimated values\n",
    "            N = np.zeros(10)  # Action counts\n",
    "            \n",
    "            for t in range(n_steps):\n",
    "                # Epsilon-greedy selection\n",
    "                if np.random.random() < epsilon:\n",
    "                    arm = np.random.randint(10)\n",
    "                else:\n",
    "                    arm = np.argmax(Q)\n",
    "                \n",
    "                reward = bandit.pull(arm)\n",
    "                N[arm] += 1\n",
    "                Q[arm] += (reward - Q[arm]) / N[arm]  # Running average\n",
    "                \n",
    "                results[name][t] += reward\n",
    "    \n",
    "    # Average over runs\n",
    "    for name in results:\n",
    "        results[name] /= n_runs\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "bandit_results = run_bandit_experiment()\n",
    "print(\"Bandit experiment complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Exploration vs. Exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Average reward over time\n",
    "colors = ['#e74c3c', '#f39c12', '#2ecc71', '#3498db']\n",
    "for (name, rewards), color in zip(bandit_results.items(), colors):\n",
    "    # Smooth with running average\n",
    "    window = 50\n",
    "    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    axes[0].plot(smoothed, label=name, color=color, linewidth=2)\n",
    "\n",
    "axes[0].set_xlabel('Step', fontsize=12)\n",
    "axes[0].set_ylabel('Average Reward', fontsize=12)\n",
    "axes[0].set_title('Multi-Armed Bandit: Exploration Strategies', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Epsilon decay schedule\n",
    "steps = np.arange(1000)\n",
    "decay_schedules = {\n",
    "    'Constant ε=0.1': np.ones(1000) * 0.1,\n",
    "    'Linear decay': np.maximum(0.01, 1.0 - steps / 500),\n",
    "    'Exponential decay': np.maximum(0.01, np.exp(-steps / 200)),\n",
    "}\n",
    "\n",
    "for (name, schedule), color in zip(decay_schedules.items(), ['#e74c3c', '#2ecc71', '#3498db']):\n",
    "    axes[1].plot(steps, schedule, label=name, color=color, linewidth=2)\n",
    "\n",
    "axes[1].set_xlabel('Step', fontsize=12)\n",
    "axes[1].set_ylabel('Epsilon (exploration rate)', fontsize=12)\n",
    "axes[1].set_title('Common ε-Decay Schedules', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: ε=0.1 finds a good balance — enough exploration to find\")\n",
    "print(\"the best arm, but not so much that it wastes pulls on bad arms.\")\n",
    "print(\"Pure greedy (ε=0) often gets stuck on a suboptimal arm early on.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Temporal Difference Learning\n",
    "\n",
    "So far, our methods require knowing the environment's transition dynamics $P(s'|s,a)$. In practice, the agent often doesn't have this information — it must learn from experience.\n",
    "\n",
    "**Temporal Difference (TD) learning** bridges the gap between dynamic programming (which requires a model) and Monte Carlo methods (which require complete episodes).\n",
    "\n",
    "### TD(0) Update Rule\n",
    "\n",
    "$$V(s_t) \\leftarrow V(s_t) + \\alpha \\left[r_t + \\gamma V(s_{t+1}) - V(s_t)\\right]$$\n",
    "\n",
    "The term in brackets is the **TD error** $\\delta_t$:\n",
    "\n",
    "$$\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "**Intuition**: The TD error measures how \"surprised\" the agent is. If the actual reward plus estimated future value is higher than expected, the TD error is positive, and we increase the value estimate.\n",
    "\n",
    "| Method | Updates | Requires |\n",
    "|--------|---------|----------|\n",
    "| **Dynamic Programming** | After full sweep of all states | Model of environment |\n",
    "| **Monte Carlo** | After complete episode | Complete episodes |\n",
    "| **TD Learning** | After each step | Only current transition |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_zero(env, n_episodes=5000, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    \"\"\"TD(0) prediction: learn V(s) from experience using an ε-greedy policy.\"\"\"\n",
    "    V = defaultdict(float)\n",
    "    visit_counts = defaultdict(int)\n",
    "    td_errors = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        episode_errors = []\n",
    "        \n",
    "        for _ in range(100):  # Max steps per episode\n",
    "            # ε-greedy action selection (using current V to estimate Q)\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.choice(env.ACTIONS)\n",
    "            else:\n",
    "                # Greedy: pick action that leads to highest-value next state\n",
    "                action_values = []\n",
    "                for a in env.ACTIONS:\n",
    "                    transitions = env.get_transitions(state, a)\n",
    "                    q = sum(p * (r + gamma * V[s_]) for p, s_, r in transitions)\n",
    "                    action_values.append(q)\n",
    "                action = env.ACTIONS[np.argmax(action_values)]\n",
    "            \n",
    "            next_state, reward, done = env.step(action)\n",
    "            visit_counts[state] += 1\n",
    "            \n",
    "            # TD(0) update\n",
    "            td_target = reward + gamma * V[next_state] * (0 if done else 1)\n",
    "            td_error = td_target - V[state]\n",
    "            V[state] += alpha * td_error\n",
    "            \n",
    "            episode_errors.append(abs(td_error))\n",
    "            \n",
    "            if done:\n",
    "                # Update terminal state values\n",
    "                if env.grid[next_state[0], next_state[1]] == GridWorld.GOAL:\n",
    "                    V[next_state] = 1.0\n",
    "                elif env.grid[next_state[0], next_state[1]] == GridWorld.TRAP:\n",
    "                    V[next_state] = -1.0\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        if episode_errors:\n",
    "            td_errors.append(np.mean(episode_errors))\n",
    "    \n",
    "    return dict(V), td_errors\n",
    "\n",
    "\n",
    "V_td, td_errors = td_zero(env, n_episodes=10000, alpha=0.1, gamma=0.9)\n",
    "\n",
    "# Compare TD-learned values with exact values\n",
    "print(\"TD(0) learned values vs. exact (value iteration):\")\n",
    "print(f\"{'State':<10} {'TD(0)':>8} {'Exact':>8} {'Diff':>8}\")\n",
    "print(\"-\" * 36)\n",
    "for s in sorted(env.states):\n",
    "    td_val = V_td.get(s, 0)\n",
    "    exact_val = V_vi.get(s, 0)\n",
    "    print(f\"{str(s):<10} {td_val:8.3f} {exact_val:8.3f} {abs(td_val - exact_val):8.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize TD learning convergence\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: TD error over episodes\n",
    "window = 100\n",
    "smoothed_errors = np.convolve(td_errors, np.ones(window)/window, mode='valid')\n",
    "axes[0].plot(smoothed_errors, color='#3498db', linewidth=1.5)\n",
    "axes[0].set_xlabel('Episode', fontsize=12)\n",
    "axes[0].set_ylabel('Average |TD Error|', fontsize=12)\n",
    "axes[0].set_title('TD(0) Learning: Error Convergence', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Comparison scatter plot\n",
    "td_vals = [V_td.get(s, 0) for s in env.states]\n",
    "exact_vals = [V_vi.get(s, 0) for s in env.states]\n",
    "axes[1].scatter(exact_vals, td_vals, s=100, color='#2ecc71', edgecolor='black', zorder=5)\n",
    "axes[1].plot([-1, 1], [-1, 1], 'r--', label='Perfect agreement', linewidth=2)\n",
    "axes[1].set_xlabel('Exact V(s) (Value Iteration)', fontsize=12)\n",
    "axes[1].set_ylabel('Learned V(s) (TD(0))', fontsize=12)\n",
    "axes[1].set_title('TD(0) vs. Exact Values', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"TD(0) learns values close to the exact solution, but from experience only!\")\n",
    "print(\"No knowledge of transition probabilities was needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Value-Based vs. Policy-Based Methods\n",
    "\n",
    "RL methods fall into two broad categories:\n",
    "\n",
    "### Value-Based Methods\n",
    "Learn a value function $V(s)$ or $Q(s,a)$, then derive a policy from it.\n",
    "- Examples: Q-learning, DQN, SARSA\n",
    "- **Pros**: Sample efficient, stable convergence\n",
    "- **Cons**: Can only handle discrete actions (without extensions)\n",
    "\n",
    "### Policy-Based Methods\n",
    "Learn the policy $\\pi(a|s)$ directly, without a value function.\n",
    "- Examples: REINFORCE, PPO, A2C\n",
    "- **Pros**: Handle continuous actions, can learn stochastic policies\n",
    "- **Cons**: Higher variance, less sample efficient\n",
    "\n",
    "### Actor-Critic Methods\n",
    "Combine both: an **actor** (policy) and a **critic** (value function).\n",
    "- Examples: A2C, PPO, SAC\n",
    "- **Pros**: Lower variance than pure policy methods, more flexible than pure value methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: taxonomy of RL methods\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "ax.set_title('Taxonomy of RL Methods', fontsize=16, fontweight='bold')\n",
    "\n",
    "# RL root\n",
    "root = mpatches.FancyBboxPatch((4.5, 6.5), 3, 1, boxstyle=\"round,pad=0.2\",\n",
    "                                facecolor='#2c3e50', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(root)\n",
    "ax.text(6, 7, 'Reinforcement Learning', ha='center', va='center',\n",
    "        fontsize=12, fontweight='bold', color='white')\n",
    "\n",
    "# Three branches\n",
    "branches = [\n",
    "    (1, 4, 'Value-Based', '#3498db', ['Q-Learning', 'DQN', 'SARSA']),\n",
    "    (4.5, 4, 'Actor-Critic', '#9b59b6', ['A2C/A3C', 'PPO', 'SAC']),\n",
    "    (8, 4, 'Policy-Based', '#e74c3c', ['REINFORCE', 'TRPO', 'ES']),\n",
    "]\n",
    "\n",
    "for x, y, label, color, methods in branches:\n",
    "    box = mpatches.FancyBboxPatch((x, y), 3, 1, boxstyle=\"round,pad=0.2\",\n",
    "                                   facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x + 1.5, y + 0.5, label, ha='center', va='center',\n",
    "            fontsize=11, fontweight='bold', color='white')\n",
    "    \n",
    "    # Connect to root\n",
    "    ax.plot([6, x + 1.5], [6.5, y + 1], '-', color='gray', lw=1.5)\n",
    "    \n",
    "    # Method labels\n",
    "    for i, method in enumerate(methods):\n",
    "        my = y - 0.7 - i * 0.6\n",
    "        ax.text(x + 1.5, my, f'• {method}', ha='center', va='center',\n",
    "                fontsize=10, color=color)\n",
    "\n",
    "# Annotations\n",
    "ax.text(2.5, 1.0, 'Learn Q(s,a)\\nDerive policy', ha='center', va='center',\n",
    "        fontsize=9, style='italic', color='gray',\n",
    "        bbox=dict(boxstyle='round', facecolor='#ecf0f1', alpha=0.8))\n",
    "ax.text(6, 1.0, 'Learn both V(s)\\nand π(a|s)', ha='center', va='center',\n",
    "        fontsize=9, style='italic', color='gray',\n",
    "        bbox=dict(boxstyle='round', facecolor='#ecf0f1', alpha=0.8))\n",
    "ax.text(9.5, 1.0, 'Learn π(a|s)\\ndirectly', ha='center', va='center',\n",
    "        fontsize=9, style='italic', color='gray',\n",
    "        bbox=dict(boxstyle='round', facecolor='#ecf0f1', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"In the next notebooks:\")\n",
    "print(\"  NB18: Q-Learning & DQN (value-based)\")\n",
    "print(\"  NB19: REINFORCE & Actor-Critic (policy-based)\")\n",
    "print(\"  NB20: PPO & Modern RL (actor-critic, RLHF)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Connecting RL to LLM Alignment\n",
    "\n",
    "In Notebook 16, we introduced **RLHF** (Reinforcement Learning from Human Feedback). Now you can see how it fits the RL framework:\n",
    "\n",
    "| RL Concept | RLHF for LLMs |\n",
    "|-----------|----------------|\n",
    "| **Agent** | The language model |\n",
    "| **State** | The prompt + tokens generated so far |\n",
    "| **Action** | Choosing the next token |\n",
    "| **Policy** | The model's probability distribution over tokens |\n",
    "| **Reward** | Score from a trained reward model (human preferences) |\n",
    "| **Environment** | The token generation process |\n",
    "\n",
    "The **PPO** algorithm (Notebook 20) is the standard method for this optimization — it updates the LLM's policy to maximize the reward model's scores while staying close to the original model (to prevent degradation).\n",
    "\n",
    "This is the bridge between everything we've learned about language models and the RL techniques we'll explore in this part of the curriculum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick simulation: how RL improves a \"language model\"\n",
    "# Simplified example with discrete token choices\n",
    "\n",
    "def simulate_rlhf_analogy():\n",
    "    \"\"\"Simulate how RL can steer a policy toward higher-reward outputs.\"\"\"\n",
    "    # Pretend we have 5 possible response \"styles\" with different reward scores\n",
    "    styles = ['Verbose & Vague', 'Concise & Clear', 'Rude & Brief', \n",
    "              'Helpful & Detailed', 'Off-topic']\n",
    "    true_rewards = [-0.2, 0.7, -0.8, 0.9, -0.5]  # Human preference scores\n",
    "    \n",
    "    # Initial policy: uniform over styles\n",
    "    policy = np.ones(5) / 5\n",
    "    policy_history = [policy.copy()]\n",
    "    \n",
    "    # Simple policy gradient update (simplified)\n",
    "    learning_rate = 0.3\n",
    "    for step in range(20):\n",
    "        # Sample an action from policy\n",
    "        action = np.random.choice(5, p=policy)\n",
    "        reward = true_rewards[action] + np.random.randn() * 0.1\n",
    "        \n",
    "        # Update: increase probability of rewarded actions\n",
    "        gradient = np.zeros(5)\n",
    "        gradient[action] = reward\n",
    "        \n",
    "        # Softmax update\n",
    "        logits = np.log(policy + 1e-8) + learning_rate * gradient\n",
    "        policy = np.exp(logits) / np.exp(logits).sum()\n",
    "        policy_history.append(policy.copy())\n",
    "    \n",
    "    return styles, true_rewards, np.array(policy_history)\n",
    "\n",
    "\n",
    "styles, rewards, policy_hist = simulate_rlhf_analogy()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "colors = ['#e74c3c', '#2ecc71', '#e67e22', '#3498db', '#95a5a6']\n",
    "for i, (style, color) in enumerate(zip(styles, colors)):\n",
    "    ax.plot(policy_hist[:, i], label=f'{style} (r={rewards[i]})', \n",
    "            color=color, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('RL Update Step', fontsize=12)\n",
    "ax.set_ylabel('Policy Probability', fontsize=12)\n",
    "ax.set_title('How RL Steers a Model Toward Better Outputs', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=9, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The policy gradually shifts probability toward 'Helpful & Detailed'\")\n",
    "print(\"and 'Concise & Clear' — the responses humans prefer.\")\n",
    "print(\"This is the core idea behind RLHF!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Custom Gridworld\n",
    "\n",
    "Create a 5×5 gridworld with multiple goals and traps. Run value iteration and visualize the optimal policy. Experiment with different discount factors (γ = 0.5, 0.9, 0.99) and observe how the policy changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Hint: Modify the GridWorld class to accept a custom grid layout\n",
    "# Then run value_iteration with different gamma values and compare\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Monte Carlo vs. TD(0)\n",
    "\n",
    "Implement first-visit Monte Carlo prediction alongside TD(0) for the same gridworld. Compare their convergence rates and final value estimates. Which converges faster? Which has lower variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Hint: Monte Carlo waits until the end of an episode to update V(s)\n",
    "# using the actual return G_t, while TD(0) updates after each step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: UCB Exploration\n",
    "\n",
    "Implement Upper Confidence Bound (UCB) exploration for the multi-armed bandit problem and compare it against ε-greedy. UCB selects:\n",
    "\n",
    "$$a_t = \\arg\\max_a \\left[Q(a) + c\\sqrt{\\frac{\\ln t}{N(a)}}\\right]$$\n",
    "\n",
    "where $c$ controls exploration strength and $N(a)$ is the number of times action $a$ has been selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "# Hint: The UCB bonus term goes to infinity for unvisited actions,\n",
    "# ensuring every action is tried at least once\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Reinforcement Learning** is about an agent learning to maximize cumulative reward through interaction with an environment\n",
    "- **MDPs** formalize the problem with states, actions, transitions, rewards, and a discount factor\n",
    "- The **Bellman equation** expresses values recursively — the foundation of all RL algorithms\n",
    "- **Policy evaluation** computes how good a policy is; **policy improvement** makes it better\n",
    "- **Value iteration** combines both into one step using the Bellman optimality equation\n",
    "- **Exploration vs. exploitation** is the fundamental tension: try new things or stick with what works?\n",
    "- **TD learning** enables learning from experience without knowing the environment's dynamics\n",
    "- RL methods split into **value-based** (learn Q), **policy-based** (learn π), and **actor-critic** (both)\n",
    "\n",
    "### Fundamental Insight\n",
    "\n",
    "The Bellman equation transforms the RL problem from \"predict the infinite future\" into \"look one step ahead and use your current estimate.\" This simple recursive trick — combined with sufficient exploration — is powerful enough to learn optimal behavior in complex environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that we understand the RL framework, value functions, and the Bellman equation, we're ready to build **practical RL agents**. In **Notebook 18: Q-Learning & Deep Q-Networks**, we'll:\n",
    "\n",
    "- Implement tabular Q-learning (model-free control)\n",
    "- Scale to function approximation with neural networks (DQN)\n",
    "- Learn key techniques: experience replay and target networks\n",
    "- Train a DQN agent to solve a control task from raw observations\n",
    "\n",
    "The journey from Bellman equations to DQN is one of the most elegant progressions in all of machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
