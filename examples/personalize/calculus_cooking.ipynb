{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Part 1.2: Calculus for Deep Learning — The Cooking Edition\n\nCalculus is essential for understanding how neural networks learn. The key insight: **learning = optimization**, and optimization requires derivatives.\n\nBut here's the thing: **cooks and bakers live and breathe calculus every time they step into the kitchen**. When a baker asks \"how much faster will the bread rise if I increase the oven temperature by ten degrees?\" — that's a derivative. When a chef asks \"which ingredient should I adjust to improve the flavor the most?\" — that's a gradient. And when a home cook iteratively tweaks a recipe between attempts to find the perfect version — that's gradient descent.\n\nEvery perfectly caramelized crust, every balanced seasoning, every ideal baking time is found through the same mathematical machinery that trains neural networks.\n\n## Learning Objectives\n- [ ] Compute partial derivatives of multivariate functions\n- [ ] Apply the chain rule to composite functions\n- [ ] Understand gradients as directions of steepest ascent\n- [ ] Implement gradient descent from scratch\n\n---",
   "id": "cell-0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)"
   ],
   "id": "cell-1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Derivatives: The Core Concept\n\nThe **derivative** measures the rate of change of a function:\n\n$$f'(x) = \\frac{df}{dx} = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}$$\n\n**Intuition**: The derivative tells you how much the output changes when you slightly change the input.\n\n**Cooking analogy:** The derivative is the thermometer of calculus. The rate at which your oven heats up is the derivative of temperature over time — it tells you how fast the temperature is changing at any given instant. The speed at which bread dough rises is the derivative of dough volume over time. The rate at which sugar caramelizes is the derivative of browning over temperature. Every \"rate of change\" question in the kitchen is a derivative question.\n\n### Why This Matters for Deep Learning\n\nIf `loss = f(weights)`, then the derivative tells us:\n- **Which direction** to change weights to reduce loss\n- **How much** each weight affects the loss",
   "id": "cell-2"
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: What Does a Derivative Really Mean?\n\nThe derivative answers a fundamental question: **\"If I wiggle the input a tiny bit, how much does the output wiggle?\"**\n\nThink of it like this:\n- You're turning a dial (input x)\n- A meter responds (output f(x))\n- The derivative tells you: \"For each unit I turn the dial, how many units does the meter move?\"\n\n**Cooking analogy:** Imagine you're adjusting the oven temperature while baking a soufflé. You nudge the temperature up by five degrees (the \"wiggle\"). The derivative tells you: \"For those five degrees of temperature change, how many minutes faster will the soufflé rise?\" or \"How much more browning will you get on top?\" The derivative is the sensitivity of your dish to your adjustment.\n\n#### The Formal Definition Unpacked\n\n$$f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}$$\n\n| Component | Meaning | Cooking Analogy |\n|-----------|---------|-----------------|\n| $f(x + h)$ | Output after nudging input by tiny amount h | Browning level after a small temperature increase |\n| $f(x)$ | Original output | Browning level at current temperature |\n| $f(x + h) - f(x)$ | Change in output (how much the meter moved) | Extra browning gained or lost from the change |\n| $h$ | Change in input (how much we turned the dial) | Size of the temperature adjustment |\n| $\\frac{f(x+h) - f(x)}{h}$ | **Rate of change** = output change per unit input change | Browning gained per degree of oven temperature |\n| $\\lim_{h \\to 0}$ | Take h infinitesimally small (instantaneous rate) | The exact sensitivity at this precise temperature |\n\n#### Why Do We Care About Rate of Change?\n\n| Context | What the Derivative Tells Us | Cooking Parallel |\n|---------|------------------------------|------------------|\n| **Physics** | Velocity = derivative of position. \"How fast am I moving right now?\" | How fast the oven temperature is climbing right now |\n| **Economics** | Marginal cost = derivative of total cost. \"Cost of making one more unit?\" | Cost of adding one more gram of saffron to the dish |\n| **Machine Learning** | Gradient of loss = derivative of loss w.r.t. weights. \"How does changing this weight affect the error?\" | \"How does changing oven temperature affect the bread's crust quality?\" |\n\nIn ML specifically: **Derivatives tell us which direction to adjust weights to reduce error.**\n\nIn cooking specifically: **Derivatives tell the chef which direction to adjust a recipe variable to improve the dish.**",
   "metadata": {},
   "id": "cell-3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Numerical derivative approximation\ndef numerical_derivative(f, x, h=1e-5):\n    \"\"\"Approximate derivative using finite differences.\"\"\"\n    return (f(x + h) - f(x - h)) / (2 * h)\n\n# Example: f(x) = x^2 — Think of x as oven temp, f(x) as browning intensity\noven_temp_curve = lambda x: x**2\noven_temp_derivative = lambda x: 2*x  # We know this analytically\n\ntemp = 3.0\nprint(f\"f(x) = x² at x = {temp}\")\nprint(f\"  (Think: browning intensity as a function of oven temperature)\")\nprint(f\"Numerical derivative: {numerical_derivative(oven_temp_curve, temp):.6f}\")\nprint(f\"Analytical derivative: {oven_temp_derivative(temp):.6f}\")",
   "id": "cell-4"
  },
  {
   "cell_type": "code",
   "source": "# Interactive visualization: Tangent lines at multiple points\n# Shows how the derivative (slope) changes across the function\n# Cooking context: Think of this as how the \"sensitivity\" of browning to \n# oven temperature changes depending on where you are in the temperature range\n\ndef plot_multiple_tangents(f, f_prime, x_range, points, title):\n    \"\"\"\n    Plot a function with tangent lines at multiple points.\n    This visualizes how the derivative changes across the function.\n    \"\"\"\n    x = np.linspace(x_range[0], x_range[1], 200)\n    y = f(x)\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Left plot: Function with tangent lines\n    axes[0].plot(x, y, 'b-', linewidth=2, label='f(x)')\n    \n    colors = plt.cm.Reds(np.linspace(0.3, 0.9, len(points)))\n    \n    for x0, color in zip(points, colors):\n        y0 = f(x0)\n        slope = f_prime(x0)\n        \n        # Tangent line: y = f(x0) + f'(x0)(x - x0)\n        x_tangent = np.linspace(x0 - 1.5, x0 + 1.5, 50)\n        y_tangent = y0 + slope * (x_tangent - x0)\n        \n        axes[0].plot(x_tangent, y_tangent, '--', color=color, linewidth=1.5, alpha=0.8)\n        axes[0].scatter([x0], [y0], color=color, s=80, zorder=5)\n        axes[0].annotate(f'slope={slope:.2f}', xy=(x0, y0), xytext=(x0+0.3, y0+0.5),\n                        fontsize=9, color=color)\n    \n    axes[0].set_xlabel('Recipe Parameter')\n    axes[0].set_ylabel('Dish Quality')\n    axes[0].set_title(f'{title}\\nTangent lines show instantaneous rate of change')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n    axes[0].set_ylim([min(y) - 1, max(y) + 2])\n    \n    # Right plot: The derivative function itself\n    y_prime = f_prime(x)\n    axes[1].plot(x, y_prime, 'r-', linewidth=2, label=\"f'(x) (derivative)\")\n    axes[1].axhline(y=0, color='k', linewidth=0.5)\n    \n    for x0, color in zip(points, colors):\n        slope = f_prime(x0)\n        axes[1].scatter([x0], [slope], color=color, s=80, zorder=5)\n    \n    axes[1].set_xlabel('Recipe Parameter')\n    axes[1].set_ylabel(\"f'(x) — Sensitivity\")\n    axes[1].set_title(\"The Derivative Function\\nShows the sensitivity at every point\")\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Example 1: f(x) = x^2 (parabola) — like browning vs oven temperature\nf = lambda x: x**2\nf_prime = lambda x: 2*x\nplot_multiple_tangents(f, f_prime, x_range=(-3, 3), \n                       points=[-2, -1, 0, 1, 2], \n                       title='f(x) = x² — Browning vs Oven Temperature')\n\nprint(\"Key observations for f(x) = x²:\")\nprint(\"- At x=0: slope is 0 (bottom of the parabola - minimum!)\")\nprint(\"  Cooking: At the ideal temperature, tiny changes barely affect browning\")\nprint(\"- Negative x: slope is negative (function decreasing)\")\nprint(\"- Positive x: slope is positive (function increasing)\")\nprint(\"- Slope magnitude increases as we move away from 0\")\nprint(\"  Cooking: The further from ideal temp, the more sensitive the dish becomes\")",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "cell-5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize derivative as slope of tangent line\ndef plot_tangent(f, f_prime, x0, title):\n    \"\"\"Plot function with tangent line at x0.\"\"\"\n    x = np.linspace(x0 - 2, x0 + 2, 100)\n    y = f(x)\n    \n    # Tangent line: y = f(x0) + f'(x0)(x - x0)\n    slope = f_prime(x0)\n    tangent = f(x0) + slope * (x - x0)\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(x, y, 'b-', linewidth=2, label='f(x)')\n    plt.plot(x, tangent, 'r--', linewidth=2, label=f'Tangent (slope = {slope:.2f})')\n    plt.scatter([x0], [f(x0)], color='red', s=100, zorder=5)\n    plt.xlabel('Oven Temperature (scaled)')\n    plt.ylabel('Browning Intensity')\n    plt.title(title)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\n# f(x) = x² at different points\nf = lambda x: x**2\nf_prime = lambda x: 2*x\n\nplot_tangent(f, f_prime, 1.5, \"Browning vs Oven Temp: f(x) = x² with tangent at x = 1.5\\nSlope tells us how sensitive browning is to temperature changes at this point\")",
   "id": "cell-6"
  },
  {
   "cell_type": "code",
   "source": "# Example 2: A more complex function - sine wave\n# Cooking context: Oscillating quality, like bread dough rising and falling\n# (volume cycles as yeast ferments, peaks, then over-proofs)\nf = lambda x: np.sin(x)\nf_prime = lambda x: np.cos(x)\nplot_multiple_tangents(f, f_prime, x_range=(-2*np.pi, 2*np.pi), \n                       points=[-np.pi, -np.pi/2, 0, np.pi/2, np.pi], \n                       title='f(x) = sin(x) — Oscillating Dough Rise')\n\nprint(\"\\nKey observations for f(x) = sin(x):\")\nprint(\"- At peaks/troughs (x = +/-pi/2): slope is 0 (maxima/minima)\")\nprint(\"  Cooking: When dough rise peaks or collapses, it's momentarily stable\")\nprint(\"- At zero crossings (x = 0, +/-pi): slope is +/-1 (steepest)\")\nprint(\"  Cooking: Dough volume is changing fastest during transitions\")\nprint(\"- The derivative of sin(x) is cos(x) - shifted by pi/2!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "cell-7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Common Derivatives\n\n| Function | Derivative | Cooking Analogy |\n|----------|------------|-----------------|\n| $x^n$ | $nx^{n-1}$ | Power-law relationships (caramelization rate scales with sugar concentration squared) |\n| $e^x$ | $e^x$ | Exponential yeast growth — rate of rising proportional to current yeast population |\n| $\\ln(x)$ | $1/x$ | Diminishing returns — each extra pinch of salt matters less as you add more |\n| $\\sin(x)$ | $\\cos(x)$ | Oscillating dough rise through proofing cycles |\n| $\\cos(x)$ | $-\\sin(x)$ | Phase-shifted oscillations in oven temperature regulation |\n\n### Activation Functions and Their Derivatives\n\nThese are critical for backpropagation!\n\n**Cooking analogy:** Activation functions are like cooking response curves. A sigmoid is like caramelization — gentle start, rapid change in the sweet spot, then saturating at burnt. ReLU is like a boiling threshold — nothing happens below 100C, then vigorous bubbling above it.",
   "id": "cell-8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sigmoid and its derivative — like caramelization: gentle at first, rapid in sweet spot, saturates at burnt\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    s = sigmoid(x)\n    return s * (1 - s)  # Nice property!\n\n# ReLU and its derivative — like a boiling threshold: nothing below 100C, vigorous above\ndef relu(x):\n    return np.maximum(0, x)\n\ndef relu_derivative(x):\n    return (x > 0).astype(float)\n\n# Tanh and its derivative — like seasoning balance: symmetric, saturates at extremes\ndef tanh(x):\n    return np.tanh(x)\n\ndef tanh_derivative(x):\n    return 1 - np.tanh(x)**2\n\n# Plot them all\nx = np.linspace(-5, 5, 200)\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 8))\n\nactivations = [\n    (sigmoid, sigmoid_derivative, 'Sigmoid\\n(Caramelization)'),\n    (relu, relu_derivative, 'ReLU\\n(Boiling Threshold)'),\n    (tanh, tanh_derivative, 'Tanh\\n(Seasoning Balance)')\n]\n\nfor i, (func, deriv, name) in enumerate(activations):\n    # Function\n    axes[0, i].plot(x, func(x), 'b-', linewidth=2)\n    axes[0, i].set_title(f'{name}')\n    axes[0, i].set_xlabel('Input Signal')\n    axes[0, i].axhline(y=0, color='k', linewidth=0.5)\n    axes[0, i].axvline(x=0, color='k', linewidth=0.5)\n    axes[0, i].grid(True, alpha=0.3)\n    \n    # Derivative\n    axes[1, i].plot(x, deriv(x), 'r-', linewidth=2)\n    axes[1, i].set_title(f'{name.split(chr(10))[0]} Derivative (Sensitivity)')\n    axes[1, i].set_xlabel('Input Signal')\n    axes[1, i].axhline(y=0, color='k', linewidth=0.5)\n    axes[1, i].axvline(x=0, color='k', linewidth=0.5)\n    axes[1, i].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key observations:\")\nprint(\"- Sigmoid derivative max is 0.25 (causes vanishing gradients)\")\nprint(\"  Cooking: Like over-caramelized sugar — can't tell the difference anymore\")\nprint(\"- ReLU derivative is 0 or 1 (no vanishing gradient for positive x)\")\nprint(\"  Cooking: Clean on/off response — either boiling or not\")\nprint(\"- Tanh derivative max is 1 (better than sigmoid)\")\nprint(\"  Cooking: Better seasoning feel, but still saturates at extremes\")",
   "id": "cell-9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 2. Partial Derivatives\n\nFor functions of multiple variables, **partial derivatives** measure the rate of change with respect to one variable while holding others constant.\n\n$$\\frac{\\partial f}{\\partial x} = \\lim_{h \\to 0} \\frac{f(x + h, y) - f(x, y)}{h}$$\n\n**Cooking analogy:** A recipe has many variables — flour amount, sugar amount, butter, eggs, oven temperature, baking time. A partial derivative answers the question: **\"If I change ONLY the sugar amount (holding everything else fixed), how much does the sweetness change?\"** This is exactly what experienced bakers do when perfecting a recipe — isolate one variable at a time to understand its individual effect.\n\n### Example\n\nFor $f(x, y) = x^2 + 3xy + y^2$:\n\n$$\\frac{\\partial f}{\\partial x} = 2x + 3y$$\n$$\\frac{\\partial f}{\\partial y} = 3x + 2y$$",
   "id": "cell-10"
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: Why Does the Gradient Point \"Uphill\"?\n\nThis is one of the most important insights in optimization. Let's build intuition for WHY the gradient points in the direction of steepest increase.\n\n#### The Gradient as a \"Which Way is Up?\" Detector\n\nImagine you're standing on a hilly surface (the function) and want to find the steepest uphill direction. The gradient is like a compass that always points uphill.\n\n**Cooking analogy:** Think of the recipe quality landscape as a terrain map, where altitude represents how far the dish is from perfection (the error). The gradient at your current recipe is like your taste buds telling you which combination of ingredient changes would make the dish worse the fastest. To make the dish **better**, you go in the **opposite** direction — down the gradient, toward lower error. This is exactly what gradient descent does.\n\n**Mathematical Intuition:**\n\nThe gradient $\\nabla f$ at a point gives you the direction where the function increases **most rapidly**.\n\nThink about it:\n- $\\frac{\\partial f}{\\partial x}$ tells you: \"If I move in the x-direction, how fast does f increase?\"\n- $\\frac{\\partial f}{\\partial y}$ tells you: \"If I move in the y-direction, how fast does f increase?\"\n- The gradient combines these: \"The optimal uphill direction is a blend of these, weighted by how steep each direction is\"\n\n**Cooking analogy:** If adding sugar (x) improves flavor score by 0.3 per gram, and adding salt (y) improves it by 0.5 per gram, the gradient tells you: \"The fastest way to improve is a blend of both, weighted by their individual sensitivities.\"\n\n#### Directional Derivatives: Movement in Any Direction\n\nIf you move in direction $\\mathbf{u}$ (a unit vector), the rate of change is:\n\n$$\\frac{\\partial f}{\\partial \\mathbf{u}} = \\nabla f \\cdot \\mathbf{u} = |\\nabla f| \\cos(\\theta)$$\n\nWhere $\\theta$ is the angle between gradient and movement direction.\n\n| Direction relative to gradient | $\\cos(\\theta)$ | Rate of change | Cooking Interpretation |\n|-------------------------------|----------------|----------------|------------------------|\n| Same as gradient ($\\theta = 0$) | 1 | Maximum increase | Worst possible recipe change (max error increase) |\n| Perpendicular ($\\theta = 90$) | 0 | No change (contour line) | A trade-off change — different but equally good |\n| Opposite ($\\theta = 180$) | -1 | Maximum decrease | Best possible recipe change (max error decrease) |\n\n**This is why gradient descent works!** Moving opposite to the gradient gives maximum decrease.",
   "metadata": {},
   "id": "cell-11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Numerical partial derivatives\ndef partial_derivative(f, point, var_index, h=1e-5):\n    \"\"\"\n    Compute partial derivative of f at point with respect to variable var_index.\n    \n    Cooking context: This is like measuring the sensitivity of flavor score to ONE \n    ingredient while holding all others fixed.\n    \n    Args:\n        f: Function taking array of variables (recipe ingredients)\n        point: Array of variable values (current recipe)\n        var_index: Which variable to differentiate (which ingredient to tweak)\n        h: Step size (how big a tweak)\n    \"\"\"\n    point = np.array(point, dtype=float)\n    point_plus = point.copy()\n    point_minus = point.copy()\n    point_plus[var_index] += h\n    point_minus[var_index] -= h\n    return (f(point_plus) - f(point_minus)) / (2 * h)\n\n# f(sugar_g, oven_temp) = sugar_g² + 3*sugar_g*oven_temp + oven_temp²\n# Think: flavor score as a function of two recipe parameters\ndef flavor_model(p):\n    sugar_g, oven_temp = p\n    return sugar_g**2 + 3*sugar_g*oven_temp + oven_temp**2\n\n# Analytical partial derivatives\ndef dflavor_d_sugar(sugar_g, oven_temp):\n    return 2*sugar_g + 3*oven_temp\n\ndef dflavor_d_temp(sugar_g, oven_temp):\n    return 3*sugar_g + 2*oven_temp\n\n# Test at point (2, 3) — sugar_g=2, oven_temp=3\nrecipe_point = [2, 3]\nprint(f\"At recipe point (sugar_g={recipe_point[0]}, oven_temp={recipe_point[1]}):\")\nprint(f\"  d(flavor)/d(sugar_g) numerical:  {partial_derivative(flavor_model, recipe_point, 0):.6f}\")\nprint(f\"  d(flavor)/d(sugar_g) analytical: {dflavor_d_sugar(*recipe_point):.6f}\")\nprint(f\"  d(flavor)/d(oven_temp) numerical:  {partial_derivative(flavor_model, recipe_point, 1):.6f}\")\nprint(f\"  d(flavor)/d(oven_temp) analytical: {dflavor_d_temp(*recipe_point):.6f}\")\nprint(f\"\\nInterpretation: oven_temp has a bigger derivative ({dflavor_d_temp(*recipe_point)}) than sugar_g ({dflavor_d_sugar(*recipe_point)})\")\nprint(\"=> Changing oven temperature would affect flavor more at this recipe point\")",
   "id": "cell-12"
  },
  {
   "cell_type": "code",
   "source": "# Enhanced gradient field visualization with interactive exploration\n# Shows gradient as arrows pointing uphill, with different movement directions\n# Cooking: Gradient field over the recipe landscape — arrows show \"which way makes the dish worse\"\n\ndef visualize_gradient_directions():\n    \"\"\"\n    Interactive visualization showing:\n    1. Gradient field (arrows pointing uphill / toward worse flavor)\n    2. How rate of change varies with direction\n    3. Why opposite-to-gradient is the best descent direction\n    \"\"\"\n    \n    fig = plt.figure(figsize=(16, 5))\n    \n    # Define function: f(x,y) = x² + 0.5*y² (elliptical paraboloid)\n    # Cooking: Flavor error surface — sugar² + 0.5*temp²\n    def f(x, y):\n        return x**2 + 0.5*y**2\n    \n    def grad_f(x, y):\n        return np.array([2*x, y])\n    \n    # Create grid\n    x = np.linspace(-3, 3, 100)\n    y = np.linspace(-3, 3, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = f(X, Y)\n    \n    # Plot 1: 3D surface\n    ax1 = fig.add_subplot(131, projection='3d')\n    ax1.plot_surface(X, Y, Z, cmap=cm.viridis, alpha=0.7)\n    ax1.set_xlabel('Sugar Amount')\n    ax1.set_ylabel('Oven Temp')\n    ax1.set_zlabel('Flavor Error')\n    ax1.set_title('Flavor Error Surface\\n(Bowl = perfect recipe at center)')\n    \n    # Plot 2: Contour with gradient field\n    ax2 = fig.add_subplot(132)\n    contour = ax2.contour(X, Y, Z, levels=15, cmap=cm.viridis)\n    ax2.clabel(contour, inline=True, fontsize=8)\n    \n    # Sparse grid for gradient arrows\n    x_sparse = np.linspace(-2.5, 2.5, 8)\n    y_sparse = np.linspace(-2.5, 2.5, 8)\n    X_s, Y_s = np.meshgrid(x_sparse, y_sparse)\n    \n    U = 2 * X_s  # df/d(sugar)\n    V = Y_s      # df/d(temp)\n    \n    # Normalize for visualization\n    mag = np.sqrt(U**2 + V**2) + 1e-10\n    U_norm = U / mag * 0.4\n    V_norm = V / mag * 0.4\n    \n    ax2.quiver(X_s, Y_s, U_norm, V_norm, mag, cmap=cm.Reds, alpha=0.8)\n    ax2.set_xlabel('Sugar Amount')\n    ax2.set_ylabel('Oven Temp')\n    ax2.set_title('Gradient Field Over Recipe Space\\nArrows point toward WORSE flavor')\n    ax2.set_aspect('equal')\n    ax2.plot([0], [0], 'k*', markersize=15, label='Perfect Recipe')\n    ax2.legend()\n    \n    # Plot 3: Directional derivative at a specific point\n    ax3 = fig.add_subplot(133)\n    \n    # Pick a point (current recipe)\n    px, py = 2.0, 1.0\n    grad = grad_f(px, py)\n    grad_mag = np.linalg.norm(grad)\n    \n    # Compute directional derivative for all directions\n    angles = np.linspace(0, 2*np.pi, 100)\n    dir_derivs = []\n    for theta in angles:\n        direction = np.array([np.cos(theta), np.sin(theta)])\n        dir_deriv = np.dot(grad, direction)\n        dir_derivs.append(dir_deriv)\n    \n    # Plot directional derivative vs angle\n    ax3.plot(np.degrees(angles), dir_derivs, 'b-', linewidth=2)\n    ax3.axhline(y=0, color='k', linewidth=0.5)\n    ax3.axhline(y=grad_mag, color='g', linestyle='--', label=f'Max = |grad| = {grad_mag:.2f}')\n    ax3.axhline(y=-grad_mag, color='r', linestyle='--', label=f'Min = -|grad| = {-grad_mag:.2f}')\n    \n    # Mark special directions\n    grad_angle = np.degrees(np.arctan2(grad[1], grad[0]))\n    ax3.axvline(x=grad_angle, color='g', alpha=0.5)\n    ax3.axvline(x=grad_angle + 180, color='r', alpha=0.5)\n    \n    ax3.set_xlabel('Recipe Change Direction (degrees)')\n    ax3.set_ylabel('Rate of Flavor Error Change')\n    ax3.set_title(f'Sensitivity vs Direction at Recipe ({px}, {py})\\nWhich way to adjust?')\n    ax3.legend(loc='lower right')\n    ax3.set_xticks([0, 90, 180, 270, 360])\n    ax3.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"At recipe (sugar={px}, temp={py}):\")\n    print(f\"  Gradient = {grad}\")\n    print(f\"  Gradient magnitude = {grad_mag:.2f}\")\n    print(f\"  Gradient direction = {grad_angle:.1f} degrees\")\n    print(f\"\\n  To make dish WORSE fastest: change recipe at {grad_angle:.1f} degrees (with gradient)\")\n    print(f\"  To make dish BETTER fastest: change recipe at {grad_angle + 180:.1f} degrees (against gradient)\")\n    print(f\"  To trade off (same quality): change at {grad_angle + 90:.1f} degrees or {grad_angle - 90:.1f} degrees\")\n\nvisualize_gradient_directions()",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "cell-13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize a 2D function and its partial derivatives\n# Cooking: Flavor error as a function of two recipe parameters\ndef flavor_error_surface(sugar_amount, oven_temp):\n    return sugar_amount**2 + oven_temp**2\n\n# Create meshgrid\nx = np.linspace(-3, 3, 50)\ny = np.linspace(-3, 3, 50)\nX, Y = np.meshgrid(x, y)\nZ = flavor_error_surface(X, Y)\n\nfig = plt.figure(figsize=(15, 5))\n\n# 3D surface\nax1 = fig.add_subplot(131, projection='3d')\nax1.plot_surface(X, Y, Z, cmap=cm.viridis, alpha=0.8)\nax1.set_xlabel('Sugar Amount')\nax1.set_ylabel('Oven Temp')\nax1.set_zlabel('Flavor Error')\nax1.set_title('Error = sugar² + temp²')\n\n# Contour plot with gradient vectors\nax2 = fig.add_subplot(132)\ncontour = ax2.contour(X, Y, Z, levels=15, cmap=cm.viridis)\nax2.clabel(contour, inline=True, fontsize=8)\n\n# Add gradient vectors at some recipe points\nrecipe_points = [(-2, -2), (-2, 0), (0, 2), (1, 1), (2, -1)]\nfor px, py in recipe_points:\n    grad_x = 2 * px  # d(error)/d(sugar) = 2*sugar\n    grad_y = 2 * py  # d(error)/d(temp) = 2*temp\n    ax2.arrow(px, py, grad_x*0.3, grad_y*0.3, head_width=0.15, head_length=0.1, fc='red', ec='red')\n\nax2.set_xlabel('Sugar Amount')\nax2.set_ylabel('Oven Temp')\nax2.set_title('Recipe Space with Gradient Vectors\\n(Red arrows = direction of increasing error)')\nax2.set_aspect('equal')\n\n# Slice at oven_temp=1 — what happens when we only vary sugar?\nax3 = fig.add_subplot(133)\ntemp_fixed = 1\nz_slice = flavor_error_surface(x, temp_fixed)\nax3.plot(x, z_slice, 'b-', linewidth=2)\nax3.set_xlabel('Sugar Amount')\nax3.set_ylabel(f'Flavor Error (temp={temp_fixed})')\nax3.set_title(f'Partial View: Vary Sugar Only (temp={temp_fixed})\\nd(error)/d(sugar) = 2*sugar')\nax3.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "id": "cell-14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 3. The Gradient\n\nThe **gradient** is the vector of all partial derivatives:\n\n$$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}$$\n\n**Cooking analogy:** The gradient is the chef's **complete sensitivity report**. Instead of asking about one ingredient at a time, the gradient bundles ALL partial derivatives into a single vector: \"Here's how sensitive the flavor score is to sugar, salt, butter, oven temp, baking time... all at once.\" It tells you which ingredient to change to get the biggest improvement, and by how much.\n\n### Key Properties\n\n1. **Direction**: Points in the direction of steepest **increase** (toward worse flavor)\n2. **Magnitude**: Tells how steep that increase is (how sensitive the dish is to changes)\n3. **To minimize**: Move in the **opposite** direction (negative gradient = toward better flavor)",
   "id": "cell-15"
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: Why the Chain Rule is CRITICAL for Deep Learning\n\nThe chain rule is not just another calculus rule - it's the **mathematical heart of backpropagation**. Without it, we couldn't train neural networks.\n\n**Cooking analogy:** The chain rule describes **sequential dependencies** in a system. In cooking, oven temperature affects crust formation, which affects moisture retention, which affects interior texture, which affects overall quality. If you want to know \"how does a 10-degree oven increase affect the final texture?\", you need to chain together the sensitivities at each link: (d_crust/d_temp) x (d_moisture/d_crust) x (d_texture/d_moisture) x (d_quality/d_texture). Each link multiplies the effect through the chain — that's the chain rule.\n\n#### The Core Insight\n\nWhen you have nested functions (f composed with g), changes **propagate** through the chain:\n\n$$\\text{small change in } x \\rightarrow \\text{change in } g(x) \\rightarrow \\text{change in } f(g(x))$$\n\nThe chain rule says: **multiply the rates of change at each step**.\n\n#### Breaking Down the Formula\n\nFor $y = f(g(x))$, let's call $u = g(x)$ (the intermediate value):\n\n$$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$$\n\n| Component | Meaning | Neural Network Terms | Cooking Analogy |\n|-----------|---------|----------------------|-----------------|\n| $\\frac{du}{dx}$ | How much does u change when x changes? | \"Local gradient\" of layer | How crust changes with oven temperature |\n| $\\frac{dy}{du}$ | How much does y change when u changes? | \"Upstream gradient\" from later layers | How overall quality changes with crust |\n| $\\frac{dy}{dx}$ | How much does y change when x changes? | \"Full gradient\" through the network | How overall quality changes with oven temperature |\n\n#### Visual: How Changes Propagate\n\n```\nInput x                        Output y\n   |                              |\n   v                              v\n   x ----[g]----> u = g(x) ----[f]----> y = f(u)\n   \n   Δx    ->     Δu = (dg/dx)·Δx   ->   Δy = (df/du)·Δu\n                                            = (df/du)·(dg/dx)·Δx\n```\n\nThe change in x gets **amplified (or diminished)** at each step, and the total effect is the product!\n\n**Cooking analogy:** A small oven temperature change ($\\Delta x$) produces a crust change, which produces a moisture change, which produces a texture change. If each step amplifies by 2x, the total effect is $2 \\times 2 \\times 2 = 8x$. But if one link attenuates (say the crust insulates the interior), the effect might be $2 \\times 0.1 \\times 2 = 0.4x$. This \"vanishing\" effect is exactly the **vanishing gradient problem** in deep networks.",
   "metadata": {},
   "id": "cell-16"
  },
  {
   "cell_type": "code",
   "source": "# Step-by-step example with ACTUAL NUMBERS\n# Cooking context: How oven temperature affects dish quality through a chain of dependencies\n# Let's trace through f(g(x)) = (2x + 1)³ at x = 2\n\nprint(\"=\" * 60)\nprint(\"CHAIN RULE: Step-by-Step with Actual Numbers\")\nprint(\"=\" * 60)\nprint(\"\\nFunction: y = (2x + 1)³\")\nprint(\"This is f(g(x)) where g(x) = 2x + 1 and f(u) = u³\")\nprint(\"\\nCooking context: x = oven temp setting, g(x) = crust formation,\")\nprint(\"f(g(x)) = effect on dish quality (cubed relationship)\")\nprint(\"\\n\" + \"-\" * 60)\n\nx = 2\nprint(f\"Evaluating at x = {x}\")\n\n# Step 1: Forward pass - compute intermediate and final values\nu = 2*x + 1  # g(x)\ny = u**3      # f(u)\n\nprint(f\"\\n1. FORWARD PASS:\")\nprint(f\"   u = g(x) = 2({x}) + 1 = {u}\")\nprint(f\"   y = f(u) = {u}³ = {y}\")\n\n# Step 2: Compute local derivatives\ndg_dx = 2          # derivative of g(x) = 2x + 1 is 2\ndf_du = 3 * u**2   # derivative of f(u) = u³ is 3u²\n\nprint(f\"\\n2. LOCAL DERIVATIVES (at this point):\")\nprint(f\"   dg/dx = d(2x+1)/dx = 2\")\nprint(f\"   df/du = d(u³)/du = 3u² = 3({u})² = {df_du}\")\n\n# Step 3: Apply chain rule\ndy_dx = df_du * dg_dx\n\nprint(f\"\\n3. CHAIN RULE:\")\nprint(f\"   dy/dx = (df/du) x (dg/dx)\")\nprint(f\"         = {df_du} x {dg_dx}\")\nprint(f\"         = {dy_dx}\")\n\n# Verify with the analytical derivative\n# y = (2x+1)³, so dy/dx = 3(2x+1)² × 2 = 6(2x+1)²\ndy_dx_analytical = 6 * (2*x + 1)**2\nprint(f\"\\n4. VERIFICATION:\")\nprint(f\"   Analytical formula: dy/dx = 6(2x+1)²\")\nprint(f\"   At x = {x}: dy/dx = 6({2*x+1})² = {dy_dx_analytical}\")\nprint(f\"   Match: {dy_dx == dy_dx_analytical}\")\n\n# What does this mean?\nprint(f\"\\n5. INTERPRETATION:\")\nprint(f\"   If we increase x by a tiny amount dx = 0.001:\")\nprint(f\"   y will increase by approximately {dy_dx} x 0.001 = {dy_dx * 0.001}\")\nprint(f\"   Cooking: A tiny oven temp tweak propagates and amplifies through the chain!\")\n\n# Verify numerically\nh = 0.001\ny_original = (2*x + 1)**3\ny_nudged = (2*(x + h) + 1)**3\nactual_change = y_nudged - y_original\nprint(f\"   Actual change: {y_nudged} - {y_original} = {actual_change:.6f}\")\nprint(f\"   Predicted change: {dy_dx * h:.6f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "cell-17"
  },
  {
   "cell_type": "code",
   "source": "# Visualization: Chain Rule as Signal Propagation\n# Shows how a small change propagates through composed functions\n# Cooking: Like tracing how an oven temperature change ripples through the baking process\n\ndef visualize_chain_rule_propagation():\n    \"\"\"\n    Visualize how changes propagate through composed functions.\n    f(g(x)) = sin(x²) \n    Cooking: Think of x as sugar amount, g(x) = caramelization level, f(g(x)) = flavor output\n    \"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n    \n    x = np.linspace(-2, 2, 200)\n    \n    # g(x) = x²  (caramelization response to sugar)\n    g = x**2\n    # f(u) = sin(u) where u = g(x)  (flavor oscillation from caramelization)\n    f = np.sin(g)\n    \n    # Plot g(x)\n    axes[0].plot(x, g, 'b-', linewidth=2)\n    axes[0].set_xlabel('Sugar Amount (x)')\n    axes[0].set_ylabel('Caramelization u = g(x) = x²')\n    axes[0].set_title('Step 1: Inner function\\nSugar -> Caramelization')\n    axes[0].grid(True, alpha=0.3)\n    axes[0].axhline(y=0, color='k', linewidth=0.5)\n    axes[0].axvline(x=0, color='k', linewidth=0.5)\n    \n    # Highlight a point\n    x0 = 1.5\n    u0 = x0**2\n    axes[0].scatter([x0], [u0], color='red', s=100, zorder=5)\n    axes[0].annotate(f'x={x0}\\nu={u0:.2f}', xy=(x0, u0), xytext=(x0+0.3, u0-0.5), fontsize=10)\n    \n    # Plot f(u) = sin(u)\n    u = np.linspace(0, 4, 200)\n    axes[1].plot(u, np.sin(u), 'g-', linewidth=2)\n    axes[1].set_xlabel('Caramelization (u)')\n    axes[1].set_ylabel('Flavor Output y = f(u) = sin(u)')\n    axes[1].set_title('Step 2: Outer function\\nCaramelization -> Flavor')\n    axes[1].grid(True, alpha=0.3)\n    axes[1].axhline(y=0, color='k', linewidth=0.5)\n    \n    y0 = np.sin(u0)\n    axes[1].scatter([u0], [y0], color='red', s=100, zorder=5)\n    axes[1].annotate(f'u={u0:.2f}\\ny={y0:.2f}', xy=(u0, y0), xytext=(u0+0.3, y0+0.2), fontsize=10)\n    \n    # Plot the composition f(g(x))\n    axes[2].plot(x, f, 'm-', linewidth=2)\n    axes[2].set_xlabel('Sugar Amount (x)')\n    axes[2].set_ylabel('Flavor Output y = sin(x²)')\n    axes[2].set_title('Result: Full Chain\\nSugar -> Flavor (composed)')\n    axes[2].grid(True, alpha=0.3)\n    axes[2].axhline(y=0, color='k', linewidth=0.5)\n    axes[2].axvline(x=0, color='k', linewidth=0.5)\n    \n    axes[2].scatter([x0], [y0], color='red', s=100, zorder=5)\n    axes[2].annotate(f'x={x0}\\ny={y0:.2f}', xy=(x0, y0), xytext=(x0+0.2, y0+0.3), fontsize=10)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Now show the gradient computation\n    print(\"\\n\" + \"=\" * 60)\n    print(\"CHAIN RULE COMPUTATION for y = sin(x²) at x = 1.5\")\n    print(\"Cooking: How does a small sugar change affect final flavor?\")\n    print(\"=\" * 60)\n    \n    # At x = 1.5\n    x_val = 1.5\n    u_val = x_val**2\n    y_val = np.sin(u_val)\n    \n    # Local gradients\n    dg_dx = 2 * x_val          # d(x²)/dx = 2x\n    df_du = np.cos(u_val)       # d(sin(u))/du = cos(u)\n    \n    # Chain rule\n    dy_dx = df_du * dg_dx\n    \n    print(f\"\\n1. Forward pass (sugar -> caramelization -> flavor):\")\n    print(f\"   x (sugar amount) = {x_val}\")\n    print(f\"   u (caramelization) = x² = {u_val}\")\n    print(f\"   y (flavor) = sin(u) = {y_val:.4f}\")\n    \n    print(f\"\\n2. Backward pass (computing sensitivities):\")\n    print(f\"   dy/du = cos(u) = cos({u_val}) = {df_du:.4f}\")\n    print(f\"   du/dx = 2x = 2({x_val}) = {dg_dx}\")\n    \n    print(f\"\\n3. Chain rule (total sensitivity):\")\n    print(f\"   dy/dx = (dy/du) x (du/dx)\")\n    print(f\"         = {df_du:.4f} x {dg_dx}\")\n    print(f\"         = {dy_dx:.4f}\")\n    \n    # Verify numerically\n    h = 1e-5\n    numerical_grad = (np.sin((x_val + h)**2) - np.sin((x_val - h)**2)) / (2*h)\n    print(f\"\\n4. Numerical verification: {numerical_grad:.4f}\")\n\nvisualize_chain_rule_propagation()",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "cell-18"
  },
  {
   "cell_type": "markdown",
   "source": "### Computational Graphs: The Key to Backpropagation\n\nA **computational graph** is a visual representation of how a function computes its output. Each node is an operation, and edges show data flow.\n\n**Why does this matter?** Neural networks are just big computational graphs, and backpropagation is just the chain rule applied systematically through the graph!\n\n**Cooking analogy:** A recipe itself is a computational graph. Inputs like flour, sugar, butter, and eggs flow through cooking operations (nodes) — mixing, heating, rising — to produce outputs like the final dish quality. When you want to know \"how does the amount of butter affect the final texture?\", you trace the graph backward — just like backpropagation.\n\n#### Example: Computing gradients through a simple graph\n\nConsider: $L = (wx + b - y)^2$ (squared error loss)\n\n```\n     w                              \n     |                              \n     v                              \nx -->[ * ]--> z1 -->[ + ]--> z2 -->[ - ]--> z3 -->[ ² ]--> L\n                      ^              ^\n                      |              |\n                      b              y (target)\n```\n\n**Forward pass (left to right):** Compute values at each node\n**Backward pass (right to left):** Compute gradients using chain rule",
   "metadata": {},
   "id": "cell-19"
  },
  {
   "cell_type": "code",
   "source": "# Detailed walkthrough of forward and backward pass through computational graph\n# L = (w*x + b - y)²\n# Cooking context: L = predicted flavor error, w = ingredient sensitivity, x = amount, b = baseline, y = target flavor\n\ndef computational_graph_example():\n    \"\"\"\n    Step-by-step forward and backward pass through a computational graph.\n    This is EXACTLY how neural network libraries compute gradients!\n    \n    Cooking parallel: Like tracing how ingredient amounts flow through a \n    recipe model to produce predicted flavor, then tracing back\n    to find which ingredient matters most.\n    \"\"\"\n    print(\"=\" * 70)\n    print(\"COMPUTATIONAL GRAPH: L = (w*x + b - y)²\")\n    print(\"Cooking: Predicted flavor error from a simple recipe model\")\n    print(\"=\" * 70)\n    \n    # Input values\n    x = 2.0   # input (e.g., sugar amount in grams)\n    y = 7.0   # target (e.g., target sweetness score)\n    w = 3.0   # weight (e.g., sweetness-per-gram coefficient)\n    b = 1.0   # bias (e.g., baseline sweetness from other ingredients)\n    \n    print(f\"\\nInputs: x={x} (sugar grams), y={y} (target sweetness), w={w} (sensitivity), b={b} (baseline)\")\n    print(\"\\n\" + \"-\" * 70)\n    print(\"FORWARD PASS (compute values left to right)\")\n    print(\"-\" * 70)\n    \n    # Forward pass - compute each node\n    z1 = w * x          # multiplication\n    print(f\"z1 = w * x = {w} * {x} = {z1}\")\n    \n    z2 = z1 + b         # addition\n    print(f\"z2 = z1 + b = {z1} + {b} = {z2}\")\n    \n    z3 = z2 - y         # subtraction (error)\n    print(f\"z3 = z2 - y = {z2} - {y} = {z3}\")\n    \n    L = z3 ** 2         # square (loss)\n    print(f\"L = z3² = {z3}² = {L}\")\n    \n    print(\"\\n\" + \"-\" * 70)\n    print(\"BACKWARD PASS (compute gradients right to left)\")\n    print(\"-\" * 70)\n    print(\"Starting from dL/dL = 1 (gradient of L with respect to itself)\\n\")\n    \n    # Backward pass - apply chain rule at each node\n    \n    # Start with gradient of loss w.r.t. itself\n    dL_dL = 1\n    print(f\"dL/dL = {dL_dL}\")\n    \n    # Node: L = z3² \n    # dL/dz3 = d(z3²)/dz3 = 2*z3\n    dL_dz3 = dL_dL * (2 * z3)\n    print(f\"\\nNode L = z3²:\")\n    print(f\"  Local gradient: d(z3²)/dz3 = 2*z3 = 2*{z3} = {2*z3}\")\n    print(f\"  dL/dz3 = dL/dL x d(z3²)/dz3 = {dL_dL} x {2*z3} = {dL_dz3}\")\n    \n    # Node: z3 = z2 - y\n    # dz3/dz2 = 1, dz3/dy = -1\n    dL_dz2 = dL_dz3 * 1\n    dL_dy = dL_dz3 * (-1)\n    print(f\"\\nNode z3 = z2 - y:\")\n    print(f\"  Local gradients: dz3/dz2 = 1, dz3/dy = -1\")\n    print(f\"  dL/dz2 = dL/dz3 x 1 = {dL_dz3} x 1 = {dL_dz2}\")\n    print(f\"  dL/dy = dL/dz3 x (-1) = {dL_dz3} x (-1) = {dL_dy}\")\n    \n    # Node: z2 = z1 + b\n    # dz2/dz1 = 1, dz2/db = 1\n    dL_dz1 = dL_dz2 * 1\n    dL_db = dL_dz2 * 1\n    print(f\"\\nNode z2 = z1 + b:\")\n    print(f\"  Local gradients: dz2/dz1 = 1, dz2/db = 1\")\n    print(f\"  dL/dz1 = dL/dz2 x 1 = {dL_dz2} x 1 = {dL_dz1}\")\n    print(f\"  dL/db = dL/dz2 x 1 = {dL_dz2} x 1 = {dL_db}\")\n    \n    # Node: z1 = w * x\n    # dz1/dw = x, dz1/dx = w\n    dL_dw = dL_dz1 * x\n    dL_dx = dL_dz1 * w\n    print(f\"\\nNode z1 = w * x:\")\n    print(f\"  Local gradients: dz1/dw = x = {x}, dz1/dx = w = {w}\")\n    print(f\"  dL/dw = dL/dz1 x x = {dL_dz1} x {x} = {dL_dw}\")\n    print(f\"  dL/dx = dL/dz1 x w = {dL_dz1} x {w} = {dL_dx}\")\n    \n    print(\"\\n\" + \"-\" * 70)\n    print(\"SUMMARY OF GRADIENTS\")\n    print(\"-\" * 70)\n    print(f\"dL/dw = {dL_dw}  <- How much changing sensitivity coefficient affects error\")\n    print(f\"dL/db = {dL_db}  <- How much changing baseline sweetness affects error\")\n    print(f\"dL/dx = {dL_dx}  <- How much changing sugar amount affects error\")\n    \n    print(\"\\n\" + \"-\" * 70)\n    print(\"VERIFICATION with numerical gradients\")\n    print(\"-\" * 70)\n    h = 1e-5\n    \n    loss = lambda w, b, x, y: (w*x + b - y)**2\n    \n    dL_dw_num = (loss(w+h, b, x, y) - loss(w-h, b, x, y)) / (2*h)\n    dL_db_num = (loss(w, b+h, x, y) - loss(w, b-h, x, y)) / (2*h)\n    \n    print(f\"dL/dw: analytical = {dL_dw}, numerical = {dL_dw_num:.4f}\")\n    print(f\"dL/db: analytical = {dL_db}, numerical = {dL_db_num:.4f}\")\n    \n    return dL_dw, dL_db\n\ndL_dw, dL_db = computational_graph_example()",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "cell-20"
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: Understanding Gradient Descent\n\nGradient descent is the **optimization engine** of deep learning. Let's build deep intuition.\n\n#### The Core Idea in Plain English\n\nYou're lost on a foggy mountainside and want to reach the lowest valley. What do you do?\n1. **Feel the slope** under your feet (compute gradient)\n2. **Take a step downhill** (move opposite to gradient)\n3. **Repeat** until you reach flat ground (gradient is zero)\n\nThat's gradient descent!\n\n**Cooking analogy:** Think of this as **iterative recipe perfection**. You bake your first attempt at a chocolate cake and taste it. Each baking attempt is a gradient descent step:\n1. **Taste the cake** and assess the result (compute loss)\n2. **Figure out which ingredients** to adjust to improve the most (compute gradient)\n3. **Make the changes** in the direction of improvement (update parameters)\n4. **Bake again** until you've found the perfect recipe (converged)\n\nYou can't test every possible ingredient combination — there are too many variables. Instead, you iteratively move \"downhill\" in recipe space toward the best flavor.\n\n#### The Update Rule Decoded\n\n$$\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\alpha \\nabla L(\\theta)$$\n\n| Component | Meaning | Mountain Analogy | Cooking Analogy |\n|-----------|---------|------------------|-----------------|\n| $\\theta$ | Parameters (weights) | Your position on the mountain | Current recipe |\n| $\\nabla L(\\theta)$ | Gradient of loss | Which way is uphill | Which recipe direction makes the dish worse |\n| $-\\nabla L(\\theta)$ | Negative gradient | Which way is downhill | Which recipe direction makes the dish better |\n| $\\alpha$ | Learning rate | Size of your steps | How aggressively you adjust the recipe |\n| $\\alpha \\nabla L(\\theta)$ | The actual step | How far you move | The actual recipe adjustment made |\n\n#### The Learning Rate $\\alpha$ is Critical\n\n| Learning rate | What happens | Problem | Cooking Parallel |\n|---------------|--------------|---------|------------------|\n| **Too small** | Tiny steps, very slow progress | Takes forever to converge | Timid cook: adding salt one grain at a time, takes forever to season |\n| **Too large** | Big steps, overshoots minimum | Oscillates or diverges | Heavy-handed cook: dumping in tablespoons of spice, dish oscillates between bland and overwhelming |\n| **Just right** | Steady progress, converges | Sweet spot (hard to find!) | Experienced chef: right-sized adjustments that converge to perfect seasoning |\n\nThis is why learning rate scheduling and adaptive optimizers (Adam) are important in practice.",
   "metadata": {},
   "id": "cell-21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_gradient(f, point, h=1e-5):\n    \"\"\"Compute gradient of f at point using numerical differentiation.\n    \n    Cooking context: Compute the full sensitivity vector — how does flavor\n    respond to changes in each recipe ingredient?\n    \"\"\"\n    point = np.array(point, dtype=float)\n    grad = np.zeros_like(point)\n    for i in range(len(point)):\n        grad[i] = partial_derivative(f, point, i, h)\n    return grad\n\n# Example: flavor_error(sugar, temp) = sugar² + temp²\n# The perfect recipe is at (0, 0)\ndef flavor_error(p):\n    return p[0]**2 + p[1]**2\n\nrecipe = np.array([3.0, 4.0])\ngrad = compute_gradient(flavor_error, recipe)\n\nprint(f\"Current recipe (sugar={recipe[0]}, temp={recipe[1]}):\")\nprint(f\"  Flavor error = {flavor_error(recipe)}\")\nprint(f\"  Gradient (sensitivity) = {grad}\")\nprint(f\"  Gradient magnitude = {np.linalg.norm(grad):.4f}\")\nprint(f\"\\nTo improve the recipe, adjust by: {-grad}\")\nprint(\"(Move opposite to gradient = toward better flavor)\")",
   "id": "cell-22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize gradient field over the recipe landscape\n# Cooking: Each arrow shows which direction makes the dish WORSE at that recipe point\ndef flavor_error_2d(sugar, temp):\n    return sugar**2 + temp**2\n\n# Create grid of recipe combinations\nsugar_range = np.linspace(-3, 3, 15)\ntemp_range = np.linspace(-3, 3, 15)\nS, T = np.meshgrid(sugar_range, temp_range)\n\n# Compute gradient at each recipe point\nU = 2 * S  # d(error)/d(sugar)\nV = 2 * T  # d(error)/d(temp)\n\n# Normalize for better visualization\nmagnitude = np.sqrt(U**2 + V**2)\nU_norm = U / (magnitude + 1e-10)\nV_norm = V / (magnitude + 1e-10)\n\nplt.figure(figsize=(10, 8))\n\n# Contour plot (iso-error lines)\nsugar_fine = np.linspace(-3, 3, 100)\ntemp_fine = np.linspace(-3, 3, 100)\nS_fine, T_fine = np.meshgrid(sugar_fine, temp_fine)\nZ_fine = flavor_error_2d(S_fine, T_fine)\nplt.contour(S_fine, T_fine, Z_fine, levels=15, cmap=cm.viridis, alpha=0.5)\n\n# Gradient vectors (pointing toward worse flavor)\nplt.quiver(S, T, U_norm, V_norm, magnitude, cmap=cm.Reds, alpha=0.8)\n\nplt.xlabel('Sugar Amount')\nplt.ylabel('Oven Temperature')\nplt.title('Gradient Field: Recipe Sensitivity Landscape\\nArrows point toward WORSE flavor — go OPPOSITE for better!')\nplt.colorbar(label='Gradient magnitude (sensitivity)')\nplt.axis('equal')\nplt.show()\n\nprint(\"Notice: Gradients point away from the perfect recipe (origin)\")\nprint(\"To find the best recipe, follow the NEGATIVE gradient (opposite direction)\")",
   "id": "cell-23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 4. The Chain Rule\n\nThe **chain rule** is the foundation of backpropagation. It tells us how to differentiate composite functions.\n\n### Single Variable\n\nIf $y = f(g(x))$, then:\n\n$$\\frac{dy}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}$$\n\n### Intuition\n\nIf $x$ changes by a small amount $\\Delta x$:\n- $g$ changes by $\\frac{dg}{dx} \\cdot \\Delta x$\n- This causes $f$ to change by $\\frac{df}{dg} \\cdot (\\frac{dg}{dx} \\cdot \\Delta x)$\n\nThe changes **multiply** through the chain!\n\n**Cooking analogy:** The chain rule is how chefs trace the effect of a low-level change (like oven temperature) through a sequence of dependencies: oven temperature affects crust formation, which affects moisture retention, which affects interior texture, which affects overall quality. Each link has its own sensitivity, and the total effect is the product of all of them.",
   "id": "cell-24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example: y = (3x + 2)²\n# Cooking: Like \"flavor_score = (3*sugar_amount + baseline)²\"\n# Let g(x) = 3x + 2, f(g) = g²\n# dy/dx = df/dg * dg/dx = 2g * 3 = 6(3x + 2)\n\ndef y(x):\n    return (3*x + 2)**2\n\ndef dy_dx_analytical(x):\n    return 6 * (3*x + 2)\n\nsugar_amount = 1.0\nprint(f\"y = (3x + 2)² at x = {sugar_amount}\")\nprint(f\"  (Cooking: Flavor score model with sugar_amount = {sugar_amount})\")\nprint(f\"y({sugar_amount}) = {y(sugar_amount)}\")\nprint(f\"dy/dx numerical:  {numerical_derivative(y, sugar_amount):.6f}\")\nprint(f\"dy/dx analytical: {dy_dx_analytical(sugar_amount):.6f}\")\nprint(f\"\\nCooking interpretation: At sugar_amount={sugar_amount}, each gram of sugar changes\")\nprint(f\"flavor score by {dy_dx_analytical(sugar_amount):.1f} units (chain rule in action)\")",
   "id": "cell-25"
  },
  {
   "cell_type": "code",
   "source": "# Comprehensive visualization of gradient descent behavior\n# Cooking: Like watching a baker iterate on a recipe across multiple attempts\n# Shows the path, learning rate effects, and convergence\n\ndef visualize_gradient_descent_comprehensive():\n    \"\"\"\n    Create a comprehensive visualization showing:\n    1. 3D view of the flavor error surface with recipe optimization path\n    2. Top-down view (contour) with path\n    3. Flavor error over iterations\n    4. Effect of different learning rates (recipe adjustment aggressiveness)\n    \"\"\"\n    \n    # Define a flavor error landscape: f(sugar,temp) = sugar² + 10*temp² (elongated bowl)\n    # Temperature is more sensitive than sugar — common in real baking\n    def f(p):\n        return p[0]**2 + 10*p[1]**2\n    \n    def grad_f(p):\n        return np.array([2*p[0], 20*p[1]])\n    \n    def gradient_descent(start, lr, n_steps):\n        point = np.array(start, dtype=float)\n        history = [point.copy()]\n        for _ in range(n_steps):\n            point = point - lr * grad_f(point)\n            history.append(point.copy())\n        return np.array(history)\n    \n    # Run recipe optimization with different \"aggressiveness\" levels\n    start = [3.0, 1.0]  # Initial recipe: sugar=3, temp=1\n    n_steps = 30\n    \n    lr_conservative = 0.01   # Very cautious baker\n    lr_good = 0.05           # Experienced baker\n    lr_aggressive = 0.09     # Bold baker\n    lr_reckless = 0.11       # Too aggressive — overshoots!\n    \n    hist_conservative = gradient_descent(start, lr_conservative, n_steps)\n    hist_good = gradient_descent(start, lr_good, n_steps)\n    hist_aggressive = gradient_descent(start, lr_aggressive, n_steps)\n    hist_reckless = gradient_descent(start, lr_reckless, n_steps)\n    \n    # Create figure\n    fig = plt.figure(figsize=(16, 10))\n    \n    # Create grid for surface plots\n    x = np.linspace(-4, 4, 100)\n    y = np.linspace(-2, 2, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = X**2 + 10*Y**2\n    \n    # Plot 1: 3D surface with recipe optimization path\n    ax1 = fig.add_subplot(221, projection='3d')\n    ax1.plot_surface(X, Y, Z, cmap=cm.viridis, alpha=0.6)\n    \n    # Add optimization path on surface\n    path_z = [f(p) for p in hist_good]\n    ax1.plot(hist_good[:, 0], hist_good[:, 1], path_z, 'r.-', \n             markersize=8, linewidth=2, label='Recipe path')\n    ax1.scatter([start[0]], [start[1]], [f(start)], color='green', s=100, marker='o')\n    ax1.scatter([0], [0], [0], color='red', s=100, marker='*')\n    \n    ax1.set_xlabel('Sugar Amount')\n    ax1.set_ylabel('Oven Temp')\n    ax1.set_zlabel('Flavor Error')\n    ax1.set_title('3D View: Recipe Optimization Path\\n(Learning rate = 0.05)')\n    \n    # Plot 2: Contour with all paths\n    ax2 = fig.add_subplot(222)\n    contour = ax2.contour(X, Y, Z, levels=20, cmap=cm.viridis)\n    ax2.clabel(contour, inline=True, fontsize=8)\n    \n    ax2.plot(hist_conservative[:, 0], hist_conservative[:, 1], 'b.-', markersize=5, \n             linewidth=1.5, label=f'lr={lr_conservative} (conservative)')\n    ax2.plot(hist_good[:, 0], hist_good[:, 1], 'g.-', markersize=5, \n             linewidth=1.5, label=f'lr={lr_good} (experienced)')\n    ax2.plot(hist_aggressive[:, 0], hist_aggressive[:, 1], 'orange', marker='.', markersize=5, \n             linewidth=1.5, label=f'lr={lr_aggressive} (aggressive)')\n    ax2.plot(hist_reckless[:, 0], hist_reckless[:, 1], 'r.-', markersize=5, \n             linewidth=1.5, label=f'lr={lr_reckless} (reckless)')\n    \n    ax2.scatter([start[0]], [start[1]], color='green', s=150, marker='o', zorder=5, label='First Attempt')\n    ax2.scatter([0], [0], color='red', s=150, marker='*', zorder=5, label='Perfect Recipe')\n    \n    ax2.set_xlabel('Sugar Amount')\n    ax2.set_ylabel('Oven Temp')\n    ax2.set_title('Top View: Different Baker Strategies')\n    ax2.legend(loc='upper right', fontsize=9)\n    ax2.set_aspect('equal')\n    \n    # Plot 3: Flavor error curves\n    ax3 = fig.add_subplot(223)\n    \n    losses_conservative = [f(p) for p in hist_conservative]\n    losses_good = [f(p) for p in hist_good]\n    losses_aggressive = [f(p) for p in hist_aggressive]\n    losses_reckless = [f(p) for p in hist_reckless]\n    \n    ax3.plot(losses_conservative, 'b-', linewidth=2, label=f'lr={lr_conservative}')\n    ax3.plot(losses_good, 'g-', linewidth=2, label=f'lr={lr_good}')\n    ax3.plot(losses_aggressive, color='orange', linewidth=2, label=f'lr={lr_aggressive}')\n    ax3.plot(losses_reckless, 'r-', linewidth=2, label=f'lr={lr_reckless}')\n    \n    ax3.set_xlabel('Baking Attempt (Iteration)')\n    ax3.set_ylabel('Flavor Error')\n    ax3.set_title('Flavor Error Improvement Over Attempts')\n    ax3.legend()\n    ax3.set_yscale('log')\n    ax3.grid(True, alpha=0.3)\n    \n    # Plot 4: Zoomed in first few steps\n    ax4 = fig.add_subplot(224)\n    \n    # Show step-by-step for good learning rate\n    for i in range(min(8, len(hist_good)-1)):\n        p1 = hist_good[i]\n        p2 = hist_good[i+1]\n        grad = grad_f(p1)\n        \n        # Point\n        ax4.scatter([p1[0]], [p1[1]], color='blue', s=60, zorder=5)\n        ax4.annotate(f'{i}', xy=(p1[0], p1[1]), xytext=(p1[0]+0.1, p1[1]+0.1), fontsize=9)\n        \n        # Gradient (scaled for visualization)\n        ax4.arrow(p1[0], p1[1], -grad[0]*0.02, -grad[1]*0.02, \n                 head_width=0.05, head_length=0.02, fc='red', ec='red', alpha=0.5)\n        \n        # Actual step\n        ax4.arrow(p1[0], p1[1], (p2[0]-p1[0])*0.95, (p2[1]-p1[1])*0.95,\n                 head_width=0.05, head_length=0.02, fc='green', ec='green')\n    \n    contour2 = ax4.contour(X, Y, Z, levels=20, cmap=cm.viridis, alpha=0.5)\n    ax4.set_xlabel('Sugar Amount')\n    ax4.set_ylabel('Oven Temp')\n    ax4.set_title('Step-by-Step Recipe Changes (lr=0.05)\\nGreen: actual changes, Red: gradient direction')\n    ax4.set_xlim([-1, 4])\n    ax4.set_ylim([-0.5, 1.5])\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"Key observations (recipe optimization):\")\n    print(f\"- lr={lr_conservative} (conservative): Too cautious — wastes baking attempts, slow to improve\")\n    print(f\"- lr={lr_good} (experienced): Efficient — reaches perfect recipe quickly\")\n    print(f\"- lr={lr_aggressive} (aggressive): Oscillates but finds the neighborhood\")\n    print(f\"- lr={lr_reckless} (reckless): Overshoots wildly — dish gets worse before better!\")\n\nvisualize_gradient_descent_comprehensive()",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "cell-26"
  },
  {
   "cell_type": "markdown",
   "source": "### The Local Minima Problem\n\nReal loss surfaces are rarely simple bowls. They often have:\n- **Local minima**: Points that look like minima locally but aren't the global best\n- **Saddle points**: Points where gradient is zero but it's neither min nor max\n- **Plateaus**: Flat regions where gradient is tiny\n\n**Cooking analogy:** This is the classic \"good enough\" recipe trap. You've been tweaking your chocolate chip cookies for weeks and they taste great — the recipe is balanced, friends love them, and small tweaks make them worse. But there might be a completely different approach (say, browning the butter first, or using bread flour instead of all-purpose) that's actually better overall. You've found a **local minimum** — a recipe that's locally optimal but not globally. Getting out requires a bold, creative change (like momentum in gradient descent) or starting from a completely different base recipe.\n\nNeural networks have extremely complex loss landscapes. Fortunately:\n1. In high dimensions, true local minima are rare (saddle points are more common)\n2. Many local minima have similar loss values\n3. Modern optimizers (Adam, etc.) can escape shallow local minima",
   "metadata": {},
   "id": "cell-27"
  },
  {
   "cell_type": "code",
   "source": "# Visualize local minima, saddle points, and the challenges they pose\n# Cooking: Multiple \"good\" recipes exist — but which is THE best?\n\ndef visualize_local_minima_problem():\n    \"\"\"\n    Visualize a flavor landscape with multiple local minima\n    and show how gradient descent can get stuck in a \"good enough\" recipe.\n    \"\"\"\n    \n    # Create a function with multiple local minima\n    # f(x) = sin(x) + 0.1*x² (creates multiple valleys — multiple \"good\" recipes)\n    def f_1d(x):\n        return np.sin(3*x) + 0.1*x**2\n    \n    def df_1d(x):\n        return 3*np.cos(3*x) + 0.2*x\n    \n    # 1D visualization\n    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n    \n    x = np.linspace(-4, 4, 200)\n    y = f_1d(x)\n    \n    axes[0].plot(x, y, 'b-', linewidth=2)\n    axes[0].set_xlabel('Recipe Parameter')\n    axes[0].set_ylabel('Flavor Error')\n    axes[0].set_title('Flavor Landscape with Multiple \"Good\" Recipes')\n    axes[0].grid(True, alpha=0.3)\n    \n    # Mark local minima (where derivative crosses zero from - to +)\n    for xi in np.linspace(-4, 4, 1000):\n        if abs(df_1d(xi)) < 0.05 and f_1d(xi-0.01) > f_1d(xi) < f_1d(xi+0.01):\n            axes[0].scatter([xi], [f_1d(xi)], color='red', s=100, marker='v', zorder=5)\n    \n    axes[0].annotate('Best\\nrecipe', xy=(-2.1, f_1d(-2.1)), xytext=(-3, 1),\n                    arrowprops=dict(arrowstyle='->', color='green'), fontsize=10, color='green')\n    axes[0].annotate('\"Good enough\"\\nrecipe', xy=(0.0, f_1d(0.0)), xytext=(1, 1.5),\n                    arrowprops=dict(arrowstyle='->', color='red'), fontsize=10, color='red')\n    \n    # Run GD from different starting recipes\n    def gd_1d(x0, lr=0.1, n_steps=50):\n        x = x0\n        history = [x]\n        for _ in range(n_steps):\n            x = x - lr * df_1d(x)\n            history.append(x)\n        return np.array(history)\n    \n    starts = [-3.5, -1.0, 1.5, 3.0]\n    colors = ['green', 'red', 'orange', 'purple']\n    \n    axes[1].plot(x, y, 'b-', linewidth=2, alpha=0.5)\n    for start, color in zip(starts, colors):\n        hist = gd_1d(start, lr=0.05, n_steps=100)\n        y_hist = f_1d(hist)\n        axes[1].plot(hist, y_hist, '.-', color=color, markersize=4, \n                    linewidth=1, label=f'Start={start}')\n        axes[1].scatter([start], [f_1d(start)], color=color, s=100, marker='o', zorder=5)\n    \n    axes[1].set_xlabel('Recipe Parameter')\n    axes[1].set_ylabel('Flavor Error')\n    axes[1].set_title('Optimization from Different Starting Recipes')\n    axes[1].legend(fontsize=9)\n    axes[1].grid(True, alpha=0.3)\n    \n    # Show final flavor errors\n    final_errors = []\n    for start in starts:\n        hist = gd_1d(start, lr=0.05, n_steps=100)\n        final_errors.append(f_1d(hist[-1]))\n    \n    axes[2].bar(range(len(starts)), final_errors, color=colors)\n    axes[2].set_xticks(range(len(starts)))\n    axes[2].set_xticklabels([f'Start={s}' for s in starts])\n    axes[2].set_xlabel('Starting Recipe')\n    axes[2].set_ylabel('Final Flavor Error')\n    axes[2].set_title('Final Error Depends on Starting Recipe!\\n(Different bases = different local optima)')\n    axes[2].grid(True, alpha=0.3, axis='y')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"Key insight: Gradient descent finds LOCAL optima, not necessarily the GLOBAL optimum.\")\n    print(\"The final recipe depends on where you started!\")\n    print(\"\\nCooking strategies to escape local minima:\")\n    print(\"1. Try multiple base recipes (multiple random starts)\")\n    print(\"2. Use momentum to 'carry through' past shallow local minima\")\n    print(\"3. Add randomness (SGD noise = trying random ingredient experiments)\")\n    print(\"4. Use learning rate schedules (big changes early, fine-tuning later)\")\n\nvisualize_local_minima_problem()",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "cell-28"
  },
  {
   "cell_type": "code",
   "source": "# Visualize saddle points in 2D\n# Cooking: A recipe where one ingredient is perfect but another is wrong —\n# e.g., sugar is spot-on but oven temperature is way off\n\ndef visualize_saddle_point():\n    \"\"\"\n    Visualize a saddle point and why it's problematic for gradient descent.\n    Cooking: Like a recipe that's perfect for sweetness but terrible for texture.\n    \"\"\"\n    # Classic saddle: f(x,y) = x² - y²\n    def f_saddle(x, y):\n        return x**2 - y**2\n    \n    def grad_saddle(p):\n        return np.array([2*p[0], -2*p[1]])\n    \n    fig = plt.figure(figsize=(16, 5))\n    \n    # Create grid\n    x = np.linspace(-2, 2, 100)\n    y = np.linspace(-2, 2, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = f_saddle(X, Y)\n    \n    # 3D surface\n    ax1 = fig.add_subplot(131, projection='3d')\n    ax1.plot_surface(X, Y, Z, cmap=cm.coolwarm, alpha=0.8)\n    ax1.scatter([0], [0], [0], color='black', s=200, marker='o')\n    ax1.set_xlabel('Sugar Amount')\n    ax1.set_ylabel('Oven Temp')\n    ax1.set_zlabel('Flavor Error')\n    ax1.set_title('Saddle Point: f = sugar² - temp²\\nOptimal in one dimension, not the other')\n    \n    # Contour plot\n    ax2 = fig.add_subplot(132)\n    contour = ax2.contour(X, Y, Z, levels=20, cmap=cm.coolwarm)\n    ax2.clabel(contour, inline=True, fontsize=8)\n    ax2.scatter([0], [0], color='black', s=200, marker='o', label='Saddle point')\n    \n    # Draw gradient arrows around saddle point\n    for px, py in [(0.5, 0), (-0.5, 0), (0, 0.5), (0, -0.5)]:\n        grad = grad_saddle([px, py])\n        ax2.arrow(px, py, -grad[0]*0.15, -grad[1]*0.15, head_width=0.05, \n                 head_length=0.02, fc='green', ec='green')\n    \n    ax2.set_xlabel('Sugar Amount')\n    ax2.set_ylabel('Oven Temp')\n    ax2.set_title('Recipe Space Contours\\nGreen arrows: negative gradient direction')\n    ax2.legend()\n    ax2.set_aspect('equal')\n    \n    # 1D slices through saddle point\n    ax3 = fig.add_subplot(133)\n    x_slice = np.linspace(-2, 2, 100)\n    ax3.plot(x_slice, x_slice**2, 'b-', linewidth=2, label='Vary sugar only: minimum')\n    ax3.plot(x_slice, -x_slice**2, 'r-', linewidth=2, label='Vary temp only: maximum')\n    ax3.axhline(y=0, color='k', linewidth=0.5)\n    ax3.axvline(x=0, color='k', linewidth=0.5)\n    ax3.scatter([0], [0], color='black', s=100, zorder=5)\n    ax3.set_xlabel('Parameter Value')\n    ax3.set_ylabel('Flavor Error')\n    ax3.set_title('1D Slices Through Saddle\\nSame point is min AND max!')\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"At a saddle point:\")\n    print(\"- Gradient is zero (looks like an optimum)\")\n    print(\"- But it's a minimum in some directions, maximum in others\")\n    print(\"- GD can get stuck here if approaching from certain directions\")\n    print(\"\\nCooking analogy: A recipe where sugar is perfect but oven temperature\")\n    print(\"is all wrong. The baker who only tastes for sweetness thinks\")\n    print(\"the recipe is optimized — but changing temperature would unlock better texture!\")\n    print(\"\\nIn high-dimensional neural network loss landscapes:\")\n    print(\"- Saddle points are MUCH more common than local minima\")\n    print(\"- Momentum helps escape saddle points by building up velocity\")\n\nvisualize_saddle_point()",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "cell-29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Chain Rule in Neural Networks\n\nConsider a simple network:\n\n$$\\text{Input } x \\rightarrow z = wx + b \\rightarrow a = \\sigma(z) \\rightarrow L = (a - y)^2$$\n\nTo find $\\frac{\\partial L}{\\partial w}$, we apply the chain rule:\n\n$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}$$\n\n**Cooking analogy:** This is like tracing: \"How does changing the butter amount (w) affect the batter consistency (z), which affects the cake rise (a), which affects the final quality score (L)?\" Each link has its own sensitivity, and we multiply them all together to get the total effect of butter on the final dish.",
   "id": "cell-30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Computational graph example\n# Forward pass: x -> z = wx + b -> a = sigmoid(z) -> L = (a - y)²\n# Cooking: sugar_amount -> sweetness = w*sugar + baseline -> perceived_quality = sigmoid(sweetness) -> error\n\ndef forward_and_backward(x, y_true, w, b):\n    \"\"\"Compute forward pass and gradients using chain rule.\n    \n    Cooking parallel: Trace how an ingredient amount flows through a flavor\n    model and back again to find sensitivities.\n    \"\"\"\n    \n    # Forward pass\n    z = w * x + b\n    a = sigmoid(z)\n    L = (a - y_true)**2\n    \n    print(\"=== Forward Pass (Ingredient -> Flavor -> Quality Error) ===\")\n    print(f\"x (sugar amount) = {x}\")\n    print(f\"z = w*x + b = {w}*{x} + {b} = {z}\")\n    print(f\"a = sigmoid(z) = {a:.6f}\")\n    print(f\"L = (a - y_target)² = ({a:.6f} - {y_true})² = {L:.6f}\")\n    \n    # Backward pass (chain rule)\n    print(\"\\n=== Backward Pass (Chain Rule: Tracing Sensitivities Back) ===\")\n    \n    # dL/da\n    dL_da = 2 * (a - y_true)\n    print(f\"dL/da = 2(a - y) = {dL_da:.6f}\")\n    \n    # da/dz (sigmoid derivative)\n    da_dz = a * (1 - a)\n    print(f\"da/dz = sigmoid(z)(1 - sigmoid(z)) = {da_dz:.6f}\")\n    \n    # dz/dw\n    dz_dw = x\n    print(f\"dz/dw = x = {dz_dw}\")\n    \n    # dz/db\n    dz_db = 1\n    print(f\"dz/db = 1\")\n    \n    # Chain rule\n    dL_dz = dL_da * da_dz\n    dL_dw = dL_dz * dz_dw\n    dL_db = dL_dz * dz_db\n    \n    print(f\"\\ndL/dw = dL/da * da/dz * dz/dw = {dL_dw:.6f}\")\n    print(f\"dL/db = dL/da * da/dz * dz/db = {dL_db:.6f}\")\n    \n    return L, dL_dw, dL_db\n\n# Example\nx = 2.0\ny_true = 1.0\nw = 0.5\nb = 0.1\n\nL, dL_dw, dL_db = forward_and_backward(x, y_true, w, b)",
   "id": "cell-31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify with numerical gradient — the \"sanity check\" every chef should do\nh = 1e-5\n\ndef loss(w, b, x=2.0, y=1.0):\n    z = w * x + b\n    a = sigmoid(z)\n    return (a - y)**2\n\n# Numerical gradients\ndL_dw_numerical = (loss(w + h, b) - loss(w - h, b)) / (2 * h)\ndL_db_numerical = (loss(w, b + h) - loss(w, b - h)) / (2 * h)\n\nprint(\"Verification with numerical gradients (the chef's double-check):\")\nprint(f\"dL/dw: analytical = {dL_dw:.6f}, numerical = {dL_dw_numerical:.6f}\")\nprint(f\"dL/db: analytical = {dL_db:.6f}, numerical = {dL_db_numerical:.6f}\")",
   "id": "cell-32"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Multivariate Chain Rule\n\nWhen a variable affects the output through multiple paths:\n\n$$\\frac{\\partial L}{\\partial x} = \\sum_{i} \\frac{\\partial L}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial x}$$\n\nThis is why we **sum** gradients when a variable is used multiple times.\n\n**Cooking analogy:** Butter affects the final dish through multiple paths simultaneously — it changes both the richness of flavor AND the texture of the crumb AND the browning of the crust. The total effect of butter on dish quality is the **sum** of its effects through each path. This is the multivariate chain rule in action.",
   "id": "cell-33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example: f = x*y + x*z where y and z both depend on x\n# Cooking: Butter (x) affects richness (x*y path) AND texture (x*z path)\n# Actually, let's do: f(x) = x² + x (x is used twice)\n\n# Computational graph:\n# x --> a = x  --\\\n#                 +--> c = a * b --> f = c + d\n# x --> b = x  --/                      |\n#                                       |\n# x --> d = x  -------------------------/\n\n# This is: f = x*x + x = x² + x\n# df/dx = 2x + 1 (by calculus)\n\n# But through the graph:\n# df/dx = df/dc * dc/da * da/dx + df/dc * dc/db * db/dx + df/dd * dd/dx\n#       = 1 * b * 1 + 1 * a * 1 + 1 * 1\n#       = x + x + 1 = 2x + 1\n\nbutter_amount = 3.0\nprint(f\"f(x) = x² + x at x = {butter_amount}\")\nprint(f\"Cooking: Total dish quality effect when butter appears in multiple paths\")\nprint(f\"f({butter_amount}) = {butter_amount**2 + butter_amount}\")\nprint(f\"df/dx (analytical) = 2x + 1 = {2*butter_amount + 1}\")\n\n# Through computational graph — summing all paths\na = butter_amount\nb = butter_amount  \nc = a * b  # = x² (richness effect)\nd = butter_amount  # (texture effect)\nf = c + d  # = x² + x (total)\n\n# Backward — sum gradients from all paths\ndf_dc = 1\ndf_dd = 1\ndc_da = b  # = x\ndc_db = a  # = x\nda_dx = 1\ndb_dx = 1\ndd_dx = 1\n\n# Sum all paths from f to x\ndf_dx = df_dc * dc_da * da_dx + df_dc * dc_db * db_dx + df_dd * dd_dx\nprint(f\"df/dx (computational graph, summing paths) = {df_dx}\")\nprint(f\"\\nKey: We SUM the contributions from each path butter takes through the recipe\")",
   "id": "cell-34"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 5. Gradient Descent\n\n**Gradient descent** is the optimization algorithm that powers deep learning:\n\n$$\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\alpha \\nabla L(\\theta)$$\n\nWhere:\n- $\\theta$: Parameters (weights)\n- $\\alpha$: Learning rate (step size)\n- $\\nabla L$: Gradient of loss with respect to parameters\n\n**Cooking analogy:** This is the mathematical version of what every dedicated home cook does when perfecting a recipe. $\\theta$ is the recipe (ingredient amounts, temperatures, times). $L(\\theta)$ is how far the dish is from perfection. $\\nabla L$ tells you which ingredients to change and by how much. $\\alpha$ controls how aggressive those changes are. Each baking attempt is an iteration of gradient descent, moving the recipe toward the most delicious possible result.",
   "id": "cell-35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def gradient_descent_1d(f, df, x0, learning_rate=0.1, n_steps=50):\n    \"\"\"Gradient descent for 1D function.\n    \n    Cooking: Iteratively adjust a single recipe parameter to minimize flavor error.\n    \"\"\"\n    x = x0\n    history = [(x, f(x))]\n    \n    for i in range(n_steps):\n        grad = df(x)\n        x = x - learning_rate * grad\n        history.append((x, f(x)))\n        \n    return x, history\n\n# Minimize f(x) = (x - 3)² — optimal baking temperature is at x=3\n# Cooking: \"Find the optimal oven temperature\"\nf = lambda x: (x - 3)**2\ndf = lambda x: 2 * (x - 3)\n\noven_temp_final, history = gradient_descent_1d(f, df, x0=10.0, learning_rate=0.1, n_steps=30)\n\nprint(f\"Oven temperature optimization:\")\nprint(f\"  Starting value: 10.0\")\nprint(f\"  Optimal found:  {oven_temp_final:.6f}\")\nprint(f\"  Error at optimum: {f(oven_temp_final):.6f}\")\nprint(f\"  True optimum: x = 3\")\n\n# Visualize\nx_range = np.linspace(-2, 12, 100)\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(x_range, f(x_range), 'b-', linewidth=2, label='Error = (oven_temp - 3)²')\nxs, ys = zip(*history)\nplt.scatter(xs, ys, c=range(len(xs)), cmap='Reds', s=50, zorder=5)\nplt.plot(xs, ys, 'r--', alpha=0.5)\nplt.xlabel('Oven Temperature (scaled)')\nplt.ylabel('Baking Error')\nplt.title('Gradient Descent: Finding Optimal Oven Temperature')\nplt.legend()\nplt.colorbar(label='Iteration (Baking Attempt)')\n\nplt.subplot(1, 2, 2)\nplt.plot([h[1] for h in history], 'b-o')\nplt.xlabel('Baking Attempt (Iteration)')\nplt.ylabel('Baking Error')\nplt.title('Error Improvement Over Attempts')\n\nplt.tight_layout()\nplt.show()",
   "id": "cell-36"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2D Gradient Descent\n\nNow let's optimize two recipe parameters simultaneously — this is where the gradient (not just the derivative) comes in. The gradient tells us the optimal combined direction to adjust **both** parameters at once.",
   "id": "cell-37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def gradient_descent_2d(f, grad_f, start, learning_rate=0.1, n_steps=50):\n    \"\"\"Gradient descent for 2D function.\n    \n    Cooking: Simultaneously optimize two recipe parameters (e.g., sugar amount and oven temp).\n    \"\"\"\n    point = np.array(start, dtype=float)\n    history = [point.copy()]\n    \n    for i in range(n_steps):\n        grad = grad_f(point)\n        point = point - learning_rate * grad\n        history.append(point.copy())\n        \n    return point, np.array(history)\n\n# Minimize flavor_error(sugar, temp) = sugar² + temp²\n# Perfect recipe at (0, 0)\ndef f(p):\n    return p[0]**2 + p[1]**2\n\ndef grad_f(p):\n    return np.array([2*p[0], 2*p[1]])\n\nstart = [4.0, 3.0]  # First attempt recipe\nfinal, history = gradient_descent_2d(f, grad_f, start, learning_rate=0.1, n_steps=30)\n\nprint(f\"First attempt recipe: sugar={start[0]}, temp={start[1]}\")\nprint(f\"Optimal recipe found: sugar={final[0]:.6f}, temp={final[1]:.6f}\")\nprint(f\"Final flavor error: {f(final):.10f}\")\n\n# Visualize the recipe optimization path\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\nZ = X**2 + Y**2\n\nplt.figure(figsize=(10, 8))\nplt.contour(X, Y, Z, levels=20, cmap=cm.viridis)\nplt.plot(history[:, 0], history[:, 1], 'r.-', markersize=10, linewidth=2)\nplt.scatter([start[0]], [start[1]], color='green', s=200, marker='o', label='First Attempt', zorder=5)\nplt.scatter([final[0]], [final[1]], color='red', s=200, marker='*', label='Perfect Recipe', zorder=5)\nplt.xlabel('Sugar Amount')\nplt.ylabel('Oven Temperature')\nplt.title('Gradient Descent: Finding the Perfect Recipe\\nflavor_error(sugar, temp) = sugar² + temp²')\nplt.legend()\nplt.colorbar(label='Flavor Error')\nplt.axis('equal')\nplt.show()",
   "id": "cell-38"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Effect of Learning Rate\n\n**Cooking analogy:** The learning rate is the cook's **aggressiveness dial**. How big a recipe change do you make between attempts? Too small and you waste dozens of baking sessions making imperceptible changes. Too large and the recipe oscillates between too sweet and too bland, never settling on the optimum.",
   "id": "cell-39"
  },
  {
   "cell_type": "markdown",
   "source": "### Calculus Concepts and Their ML Applications\n\n| Calculus Concept | What it Means | ML Application | Cooking Parallel |\n|------------------|---------------|----------------|------------------|\n| **Derivative** | Rate of change of output w.r.t. input | How loss changes when we change one weight | How flavor changes when we adjust sugar amount |\n| **Partial Derivative** | Rate of change w.r.t. one variable (others fixed) | Gradient component for one parameter | Effect of changing ONLY oven temperature on crust quality |\n| **Gradient** | Vector of all partial derivatives | Direction to update ALL weights at once | Complete sensitivity report for all recipe ingredients |\n| **Chain Rule** | Derivative of composed functions = product of derivatives | Backpropagation through network layers | How oven temp flows through crust, moisture, texture to affect quality |\n| **Gradient Descent** | Iteratively move opposite to gradient | Core training algorithm for neural networks | Iterative recipe perfection across baking attempts |\n| **Learning Rate** | Step size in gradient descent | Hyperparameter controlling training speed | How aggressively recipe changes are between attempts |\n| **Local Minimum** | Point where gradient = 0 and function curves up | Where training might get stuck | A \"good enough\" recipe that isn't actually the best |\n| **Saddle Point** | Point where gradient = 0 but not min or max | Common in high-dim; momentum helps escape | Recipe optimal for sweetness but not for texture |\n\n### The Full Picture: How a Neural Network Learns\n\n1. **Forward pass**: Input flows through network, computing activations layer by layer\n2. **Loss computation**: Compare output to target, get a single number (the loss)\n3. **Backward pass**: Use chain rule to compute gradient of loss w.r.t. every weight\n4. **Parameter update**: Use gradient descent to update all weights\n5. **Repeat**: Until loss is small enough\n\n**Cooking parallel:** This is exactly the recipe development workflow:\n1. **Bake the dish** (forward pass through the cooking process)\n2. **Taste and evaluate** (compute the loss — how far from perfection?)\n3. **Analyze what to change** to improve (backward pass / gradient computation)\n4. **Adjust the recipe** (parameter update via gradient descent)\n5. **Bake again** (repeat until delicious)",
   "metadata": {},
   "id": "cell-40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare different learning rates — the cook's aggressiveness dial\nlearning_rates = [0.01, 0.1, 0.5, 0.95]\ncolors = ['blue', 'green', 'orange', 'red']\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Contour plot with paths\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\nZ = X**2 + Y**2\n\naxes[0].contour(X, Y, Z, levels=20, cmap=cm.viridis, alpha=0.5)\n\nfor lr, color in zip(learning_rates, colors):\n    final, history = gradient_descent_2d(f, grad_f, [4.0, 3.0], learning_rate=lr, n_steps=20)\n    axes[0].plot(history[:, 0], history[:, 1], '.-', color=color, markersize=8, \n                 linewidth=2, label=f'lr={lr}')\n    \n    # Flavor error curve\n    losses = [f(p) for p in history]\n    axes[1].plot(losses, color=color, linewidth=2, label=f'lr={lr}')\n\naxes[0].set_xlabel('Sugar Amount')\naxes[0].set_ylabel('Oven Temperature')\naxes[0].set_title('Recipe Optimization Paths\\n(Different Cook Aggressiveness)')\naxes[0].legend()\naxes[0].axis('equal')\n\naxes[1].set_xlabel('Baking Attempt (Iteration)')\naxes[1].set_ylabel('Flavor Error')\naxes[1].set_title('Flavor Error Convergence')\naxes[1].legend()\naxes[1].set_yscale('log')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Cooking observations on learning rate (recipe adjustment aggressiveness):\")\nprint(\"- Too small (0.01): Conservative — wastes baking attempts, slow to find perfection\")\nprint(\"- Good (0.1): Experienced — steady progress toward the perfect recipe\")\nprint(\"- Larger (0.5): Bold — finds neighborhood fast but oscillates around it\")\nprint(\"- Too large (0.95): Reckless — recipe swings wildly between extremes\")",
   "id": "cell-41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### A More Challenging Function: Rosenbrock\n\nThe Rosenbrock function is a classic optimization test:\n\n$$f(x, y) = (1 - x)^2 + 100(y - x^2)^2$$\n\nMinimum at $(1, 1)$. Famous for its narrow, curved valley.\n\n**Cooking analogy:** This is like a recipe landscape with a narrow \"sweet spot\" — a long, winding valley of decent results but only one truly optimal point. Think of it like perfecting a soufflé: there's a narrow corridor of ingredient ratios and temperatures that work, and the optimal point requires precise tuning of correlated parameters (egg whites and oven temperature that depend on each other). This is why simple gradient descent struggles and why adaptive optimizers (like Adam in ML, or experienced pastry chefs in the kitchen) are so valuable.",
   "id": "cell-42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def rosenbrock(p):\n    x, y = p\n    return (1 - x)**2 + 100 * (y - x**2)**2\n\ndef rosenbrock_grad(p):\n    x, y = p\n    dx = -2*(1 - x) - 400*x*(y - x**2)\n    dy = 200*(y - x**2)\n    return np.array([dx, dy])\n\n# Visualize the Rosenbrock \"soufflé\" recipe landscape\nx = np.linspace(-2, 2, 200)\ny = np.linspace(-1, 3, 200)\nX, Y = np.meshgrid(x, y)\nZ = (1 - X)**2 + 100 * (Y - X**2)**2\n\nplt.figure(figsize=(10, 8))\nplt.contour(X, Y, Z, levels=np.logspace(0, 3, 30), cmap=cm.viridis)\nplt.scatter([1], [1], color='red', s=200, marker='*', label='Perfect Recipe (1,1)', zorder=5)\nplt.xlabel('Egg White Ratio')\nplt.ylabel('Oven Temperature')\nplt.title('The Rosenbrock \"Soufflé\" Recipe Landscape\\nNotice the narrow curved valley — hard to optimize!')\nplt.colorbar(label='Flavor Error')\nplt.legend()\nplt.show()",
   "id": "cell-43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Gradient descent on Rosenbrock (challenging! — like perfecting a soufflé)\nstart = [-1.0, 1.0]\nfinal, history = gradient_descent_2d(rosenbrock, rosenbrock_grad, start, \n                                      learning_rate=0.001, n_steps=5000)\n\nprint(f\"Starting recipe: {start}\")\nprint(f\"Final recipe:    [{final[0]:.4f}, {final[1]:.4f}]\")\nprint(f\"Error at final: {rosenbrock(final):.6f}\")\nprint(f\"True perfect recipe: (1, 1), error = 0\")\n\n# Visualize\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.contour(X, Y, Z, levels=np.logspace(0, 3, 30), cmap=cm.viridis, alpha=0.5)\nplt.plot(history[::50, 0], history[::50, 1], 'r.-', markersize=5, linewidth=1)  # Every 50th point\nplt.scatter([start[0]], [start[1]], color='green', s=100, marker='o', label='First Attempt', zorder=5)\nplt.scatter([final[0]], [final[1]], color='red', s=100, marker='*', label='Final Recipe', zorder=5)\nplt.xlabel('Egg White Ratio')\nplt.ylabel('Oven Temperature')\nplt.title('Recipe Optimization on Rosenbrock (Soufflé)')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nlosses = [rosenbrock(p) for p in history[::10]]\nplt.plot(losses)\nplt.xlabel('Iteration (x10)')\nplt.ylabel('Flavor Error')\nplt.title('Error Over Optimization Steps')\nplt.yscale('log')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nNote: Simple gradient descent struggles with narrow valleys!\")\nprint(\"Cooking: This is why experienced pastry chefs and advanced techniques are needed\")\nprint(\"for delicate recipes like soufflés with tight ingredient windows.\")\nprint(\"More advanced optimizers (Adam, etc.) handle this better.\")",
   "id": "cell-44"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 6. Putting It Together: Training a Linear Model\n\nLet's train a simple linear regression model using gradient descent.\n\n**Cooking scenario:** We're building a simple model to predict baking quality from oven temperature. The relationship is roughly linear: higher temperature means faster browning (up to a point). We'll use gradient descent to find the best-fit line — exactly how a baker might calibrate their temperature-to-browning model from practice bakes.",
   "id": "cell-45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate synthetic cooking data: browning score vs oven temperature\nnp.random.seed(42)\nn_bakes = 100\n\n# True parameters: each degree of oven temp adds ~0.035 to browning score, baseline browning is 1.0\n# We'll use scaled values for numerical convenience\ntemp_effect_true = 2.5    # scaled temperature-to-browning sensitivity\nbaseline_browning_true = 1.0  # scaled baseline browning score\n\n# Generate data: browning = temp_effect * oven_temp + baseline + noise\noven_temp = np.random.uniform(-3, 3, n_bakes)  # centered temperature values\nbrowning_score = temp_effect_true * oven_temp + baseline_browning_true + np.random.normal(0, 0.5, n_bakes)\n\nplt.figure(figsize=(10, 6))\nplt.scatter(oven_temp, browning_score, alpha=0.6, label='Practice bake data')\nplt.plot(oven_temp, temp_effect_true * oven_temp + baseline_browning_true, 'r-', linewidth=2, \n         label=f'True model: browning = {temp_effect_true}*temp + {baseline_browning_true}')\nplt.xlabel('Oven Temperature (centered)')\nplt.ylabel('Browning Score (scaled)')\nplt.title('Baking Data: Browning Score vs Oven Temperature\\n(Each dot = one practice bake)')\nplt.legend()\nplt.show()",
   "id": "cell-46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_browning_model(X, y, learning_rate=0.01, n_epochs=100):\n    \"\"\"\n    Train a linear temperature-to-browning model using gradient descent.\n    \n    Model: predicted_browning = w * oven_temp + b\n    Loss: MSE = (1/n) * sum((predicted_browning - actual_browning)^2)\n    \n    Cooking context: The baker wants to learn the temperature effect coefficient\n    and baseline browning from practice bake data.\n    \"\"\"\n    n = len(X)\n    \n    # Initialize parameters (start with no knowledge)\n    w = 0.0  # temperature effect coefficient\n    b = 0.0  # baseline browning\n    \n    history = {'loss': [], 'w': [], 'b': []}\n    \n    for epoch in range(n_epochs):\n        # Forward pass: predict browning scores\n        y_pred = w * X + b\n        \n        # Compute loss (MSE — how wrong are our predictions?)\n        loss = np.mean((y_pred - y)**2)\n        \n        # Compute gradients (which direction improves the model?)\n        # d(loss)/dw = (2/n) * sum((y_pred - y) * x)\n        # d(loss)/db = (2/n) * sum(y_pred - y)\n        dw = (2/n) * np.sum((y_pred - y) * X)\n        db = (2/n) * np.sum(y_pred - y)\n        \n        # Update parameters (gradient descent step)\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n        \n        # Record history\n        history['loss'].append(loss)\n        history['w'].append(w)\n        history['b'].append(b)\n        \n        if epoch % 20 == 0:\n            print(f\"Epoch {epoch:3d}: loss = {loss:.4f}, temp_effect = {w:.4f}, baseline = {b:.4f}\")\n    \n    return w, b, history\n\n# Train the browning model\nw_learned, b_learned, history = train_browning_model(oven_temp, browning_score, learning_rate=0.1, n_epochs=100)\n\nprint(f\"\\nLearned: temp_effect = {w_learned:.4f}, baseline = {b_learned:.4f}\")\nprint(f\"True:    temp_effect = {temp_effect_true:.4f}, baseline = {baseline_browning_true:.4f}\")\nprint(f\"\\nThe model successfully learned the temperature-browning relationship from bake data!\")",
   "id": "cell-47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize the training process\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Loss curve\naxes[0].plot(history['loss'])\naxes[0].set_xlabel('Training Epoch')\naxes[0].set_ylabel('MSE Loss')\naxes[0].set_title('Prediction Error Over Training')\n\n# Parameter trajectory\naxes[1].plot(history['w'], label='temp_effect (w)')\naxes[1].plot(history['b'], label='baseline (b)')\naxes[1].axhline(y=temp_effect_true, color='blue', linestyle='--', alpha=0.5, \n                label=f'true temp_effect={temp_effect_true}')\naxes[1].axhline(y=baseline_browning_true, color='orange', linestyle='--', alpha=0.5, \n                label=f'true baseline={baseline_browning_true}')\naxes[1].set_xlabel('Training Epoch')\naxes[1].set_ylabel('Parameter Value')\naxes[1].set_title('Parameter Convergence\\n(Model learns the true values!)')\naxes[1].legend()\n\n# Final fit\naxes[2].scatter(oven_temp, browning_score, alpha=0.6, label='Bake data')\nx_line = np.linspace(-3, 3, 100)\naxes[2].plot(x_line, temp_effect_true * x_line + baseline_browning_true, 'g-', linewidth=2, \n             label=f'True: {temp_effect_true}*temp + {baseline_browning_true}')\naxes[2].plot(x_line, w_learned * x_line + b_learned, 'r--', linewidth=2, \n             label=f'Learned: {w_learned:.2f}*temp + {b_learned:.2f}')\naxes[2].set_xlabel('Oven Temperature')\naxes[2].set_ylabel('Browning Score')\naxes[2].set_title('Final Model Fit\\n(Red dashed = learned, Green = true)')\naxes[2].legend()\n\nplt.tight_layout()\nplt.show()",
   "id": "cell-48"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercises\n\n### Exercise 1: Implement Gradient Checking for a Caramelization Model\n\n**Cooking scenario:** You've built an analytical model of sugar caramelization and computed its gradients by hand. Before trusting those gradients for recipe optimization, you need to **verify** them against numerical gradients. This is gradient checking — the baker's sanity check before making expensive ingredient changes based on model predictions.\n\nGradient checking is crucial for debugging backpropagation. Compare analytical gradients with numerical gradients.",
   "id": "cell-49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def gradient_check(f, grad_f, point, h=1e-5, threshold=1e-5):\n    \"\"\"\n    Compare analytical gradient with numerical gradient.\n    Returns True if they match within threshold.\n    \n    Cooking context: Verify that your hand-derived caramelization model gradients \n    match numerical estimates before using them to make recipe decisions.\n    \"\"\"\n    point = np.array(point, dtype=float)\n    analytical_grad = grad_f(point)\n    numerical_grad = compute_gradient(f, point, h)\n    \n    # Compute relative error\n    diff = np.abs(analytical_grad - numerical_grad)\n    denom = np.maximum(np.abs(analytical_grad) + np.abs(numerical_grad), 1e-10)\n    relative_error = diff / denom\n    \n    print(f\"Recipe point: {point}\")\n    print(f\"Analytical gradient:  {analytical_grad}\")\n    print(f\"Numerical gradient:   {numerical_grad}\")\n    print(f\"Relative error: {relative_error}\")\n    print(f\"Max relative error: {np.max(relative_error):.2e}\")\n    \n    return np.all(relative_error < threshold)\n\n# Test on a caramelization model: f(temp, sugar_conc) = temp³ + 2*temp*sugar_conc + sugar_conc²\n# Cooking: How caramelization depends on temperature and sugar concentration\ndef caramelization_model(p):\n    temp, sugar_conc = p\n    return temp**3 + 2*temp*sugar_conc + sugar_conc**2\n\ndef caramelization_grad(p):\n    temp, sugar_conc = p\n    return np.array([3*temp**2 + 2*sugar_conc, 2*temp + 2*sugar_conc])\n\npassed = gradient_check(caramelization_model, caramelization_grad, [2.0, 3.0])\nprint(f\"\\nGradient check passed: {passed}\")\nprint(\"The analytical gradients are trustworthy for recipe optimization!\")",
   "id": "cell-50"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2: Implement Softmax and Its Gradient\n\n**Cooking scenario:** Your recipe development team needs to convert raw \"fitness scores\" for different cooking methods (bake, broil, sauté) into **probabilities** — \"What's the probability that each method is the optimal choice for this dish?\" Softmax is the standard way to do this conversion, and it's critical for classification in ML.\n\nSoftmax is critical for classification. Implement it and its gradient.",
   "id": "cell-51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def softmax(x):\n    \"\"\"\n    Compute softmax: softmax(x)_i = exp(x_i) / sum(exp(x_j))\n    Subtract max for numerical stability.\n    \n    Cooking context: Convert cooking method fitness scores into selection probabilities.\n    \"\"\"\n    # TODO: Implement softmax\n    x_shifted = x - np.max(x)  # For numerical stability\n    exp_x = np.exp(x_shifted)\n    return exp_x / np.sum(exp_x)\n\ndef softmax_jacobian(x):\n    \"\"\"\n    Compute Jacobian of softmax.\n    J[i,j] = d(softmax_i)/d(x_j)\n    \n    Formula: J[i,j] = softmax_i * (delta_ij - softmax_j)\n    where delta_ij = 1 if i==j, else 0\n    \n    Cooking context: How does changing the fitness score of one cooking method \n    affect the selection probability of every method?\n    \"\"\"\n    s = softmax(x)\n    n = len(s)\n    jacobian = np.zeros((n, n))\n    \n    # TODO: Implement Jacobian\n    for i in range(n):\n        for j in range(n):\n            if i == j:\n                jacobian[i, j] = s[i] * (1 - s[j])\n            else:\n                jacobian[i, j] = -s[i] * s[j]\n    \n    return jacobian\n\n# Test — cooking method fitness scores: [bake, broil, sauté]\nmethod_scores = np.array([2.0, 1.0, 0.1])\nprint(f\"Cooking method fitness scores: {method_scores}\")\nprint(f\"  (Bake=2.0, Broil=1.0, Sauté=0.1)\")\nprint(f\"\\nSelection probabilities: {softmax(method_scores)}\")\nprint(f\"Sum (should be 1): {softmax(method_scores).sum():.6f}\")\nprint(f\"\\nJacobian (how each score affects each probability):\")\nprint(f\"{softmax_jacobian(method_scores)}\")\nprint(f\"\\nCooking interpretation: Baking has highest probability ({softmax(method_scores)[0]:.1%})\")\nprint(f\"because it has the highest fitness score.\")",
   "id": "cell-52"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 3: Gradient Descent with Momentum\n\n**Cooking scenario:** Standard gradient descent can get stuck in local minima or oscillate in narrow valleys. Momentum is like giving your recipe optimizer \"inertia\" — it builds up speed in consistent directions and carries through small bumps. In cooking terms, instead of reacting purely to the last bake's results, momentum lets you carry the \"trend\" from multiple attempts. If the dish has been getting better with more butter for 3 attempts straight, momentum says \"keep going in that direction even if this one attempt was noisy.\"\n\nMomentum helps accelerate gradient descent. Implement it!",
   "id": "cell-53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def gradient_descent_momentum(f, grad_f, start, learning_rate=0.01, momentum=0.9, n_steps=100):\n    \"\"\"\n    Gradient descent with momentum.\n    \n    v = momentum * v - learning_rate * gradient\n    x = x + v\n    \n    Cooking context: The velocity term carries the \"trend\" from previous attempts,\n    helping the optimizer build speed in consistent directions and carry past\n    noisy bumps in the recipe landscape.\n    \"\"\"\n    point = np.array(start, dtype=float)\n    velocity = np.zeros_like(point)\n    history = [point.copy()]\n    \n    for i in range(n_steps):\n        grad = grad_f(point)\n        velocity = momentum * velocity - learning_rate * grad\n        point = point + velocity\n        history.append(point.copy())\n        \n    return point, np.array(history)\n\n# Compare regular GD vs GD with momentum on Rosenbrock (the \"soufflé\" landscape)\nstart = [-1.0, 1.0]\nn_steps = 1000\n\nfinal_gd, history_gd = gradient_descent_2d(rosenbrock, rosenbrock_grad, start, \n                                            learning_rate=0.001, n_steps=n_steps)\nfinal_mom, history_mom = gradient_descent_momentum(rosenbrock, rosenbrock_grad, start,\n                                                    learning_rate=0.001, momentum=0.9, n_steps=n_steps)\n\nprint(f\"Regular GD final error:  {rosenbrock(final_gd):.6f}\")\nprint(f\"Momentum GD final error: {rosenbrock(final_mom):.6f}\")\nprint(f\"\\nMomentum finds a better recipe in the same number of iterations!\")\n\n# Visualize\nplt.figure(figsize=(14, 5))\n\nplt.subplot(1, 2, 1)\nplt.contour(X, Y, Z, levels=np.logspace(0, 3, 30), cmap=cm.viridis, alpha=0.5)\nplt.plot(history_gd[::20, 0], history_gd[::20, 1], 'b.-', markersize=3, label='Standard GD')\nplt.plot(history_mom[::20, 0], history_mom[::20, 1], 'r.-', markersize=3, label='With Momentum')\nplt.xlabel('Egg White Ratio')\nplt.ylabel('Oven Temperature')\nplt.title('Recipe Optimization Paths\\n(Momentum carries through the narrow valley)')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nlosses_gd = [rosenbrock(p) for p in history_gd]\nlosses_mom = [rosenbrock(p) for p in history_mom]\nplt.plot(losses_gd, 'b-', label='Standard GD')\nplt.plot(losses_mom, 'r-', label='With Momentum')\nplt.xlabel('Iteration')\nplt.ylabel('Flavor Error')\nplt.title('Error: Standard GD vs Momentum\\n(Momentum converges faster)')\nplt.yscale('log')\nplt.legend()\n\nplt.tight_layout()\nplt.show()",
   "id": "cell-54"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Summary\n\n### Key Concepts\n\n| Concept | Mathematical Meaning | Cooking Parallel |\n|---------|---------------------|------------------|\n| **Derivatives** | Measure rate of change — essential for optimization | How browning changes when you adjust oven temperature (rising speed is the derivative of dough volume) |\n| **Partial derivatives** | Handle functions of multiple variables | Effect of changing ONE ingredient (e.g., sugar amount) while holding everything else fixed |\n| **The gradient** | Points in the direction of steepest ascent | The chef's \"sensitivity report\" — which recipe ingredient to change to improve flavor fastest |\n| **Chain rule** | Compute gradients through composed functions (backprop!) | How oven temp flows through crust formation, moisture retention, texture to affect quality (sequential dependencies) |\n| **Gradient descent** | Minimize loss by following the negative gradient | Iteratively tweaking a recipe across baking attempts to minimize the gap from perfection |\n| **Learning rate** | How big each optimization step is | How aggressively the cook adjusts the recipe (too big = overshoot, too small = slow progress) |\n| **Local minima** | A point that's locally optimal but not globally | A \"good enough\" recipe that's not actually the best — getting stuck |\n| **Momentum** | Build up \"velocity\" in consistent directions | Carrying the trend from multiple baking attempts to power through noise and shallow local minima |\n\n### Connection to Deep Learning\n\n- **Forward pass**: Compute function values through the network\n- **Loss**: Scalar measuring prediction quality\n- **Backward pass**: Apply chain rule to compute gradients\n- **Update**: Move parameters in negative gradient direction\n\n### Checklist\n- [ ] I can compute derivatives of common functions\n- [ ] I understand partial derivatives and gradients\n- [ ] I can apply the chain rule to composite functions\n- [ ] I can implement gradient descent from scratch\n- [ ] I understand the effect of learning rate",
   "id": "cell-55"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Next Steps\n\nContinue to **Part 1.3: Probability & Statistics** where we'll cover:\n- Probability distributions\n- Bayes' theorem\n- Maximum likelihood estimation\n- Information theory (entropy, KL divergence)\n\n**Cooking preview:** Probability and statistics are how cooks make decisions under uncertainty — \"What's the probability this bread will rise properly at this altitude?\", \"Given the current rate of caramelization, when should I pull it from the oven?\", \"How confident are we that this recipe change actually improved the dish vs. random variation?\" These are all probability and statistics questions.",
   "id": "cell-56"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}