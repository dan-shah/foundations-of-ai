
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Part 7.1: Reinforcement Learning Fundamentals &#8212; Foundations of AI</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=9c3e77be" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=1ae7504c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/22_rl_fundamentals';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Part 7.2: Q-Learning and Deep Q-Networks" href="23_q_learning_dqn.html" />
    <link rel="prev" title="Part 6.5: Fine-tuning &amp; PEFT" href="21_finetuning_and_peft.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Foundations of AI</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 1: Mathematical Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_linear_algebra.html">Part 1.1: Linear Algebra for Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_calculus.html">Part 1.2: Calculus for Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_probability_statistics.html">Part 1.3: Probability &amp; Statistics for Deep Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 2: Programming Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_python_oop.html">Part 2.1: Python OOP for Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_numpy_deep_dive.html">Part 2.2: NumPy Deep Dive</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 3: Classical ML &amp; Optimization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="06_classical_ml.html">Part 3.1: Classical Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_optimization_linear_programming.html">Part 3.2: Optimization &amp; Linear Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_optimization_theory.html">Part 3.3: Optimization Theory for Machine Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 4: Neural Network Fundamentals</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="09_perceptrons_basic_networks.html">Part 4.1: Perceptrons &amp; Basic Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_backpropagation.html">Part 4.2: Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_pytorch_fundamentals.html">Part 4.3: PyTorch Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_training_deep_networks.html">Part 4.4: Training Deep Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 5: Neural Network Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="13_convolutional_neural_networks.html">Part 5.1: Convolutional Neural Networks (CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_computer_vision_depth.html">Part 5.2: Computer Vision — Beyond Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_recurrent_neural_networks.html">Part 5.3: Recurrent Neural Networks (RNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_attention_mechanisms.html">Part 5.4: Attention Mechanisms</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 6: Transformers &amp; LLMs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="17_transformer_architecture.html">Part 6.1: Transformer Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_embeddings.html">Part 6.2: Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="19_tokenization_lm_training.html">Part 6.3: Tokenization &amp; Language Model Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="20_language_models.html">Part 6.4: Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="21_finetuning_and_peft.html">Part 6.5: Fine-tuning &amp; PEFT</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 7: Reinforcement Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Part 7.1: Reinforcement Learning Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="23_q_learning_dqn.html">Part 7.2: Q-Learning and Deep Q-Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="24_policy_gradients.html">Part 7.3: Policy Gradient Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="25_ppo_modern_rl.html">Part 7.4: PPO and Modern RL</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 8: Applied AI Systems</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="26_rag.html">Part 8.1: Retrieval-Augmented Generation (RAG)</a></li>
<li class="toctree-l1"><a class="reference internal" href="27_ai_agents.html">Part 8.2: AI Agents and Tool Use</a></li>
<li class="toctree-l1"><a class="reference internal" href="28_ai_evals.html">Part 8.3: Evaluating AI Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="29_production_monitoring.html">Part 8.4: Production AI Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 9: Advanced Topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="30_inference_optimization.html">Part 9.1: LLM Inference Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="31_ml_systems.html">Part 9.2: ML Systems &amp; Experiment Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="32_multimodal_ai.html">Part 9.3: Multimodal AI</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/dan-shah/foundations-of-ai/blob/main/notebooks/22_rl_fundamentals.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/dan-shah/foundations-of-ai" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/dan-shah/foundations-of-ai/edit/main/notebooks/22_rl_fundamentals.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/dan-shah/foundations-of-ai/issues/new?title=Issue%20on%20page%20%2Fnotebooks/22_rl_fundamentals.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notebooks/22_rl_fundamentals.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Part 7.1: Reinforcement Learning Fundamentals</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-reinforcement-learning-paradigm">1. The Reinforcement Learning Paradigm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-agent-environment-loop">The Agent-Environment Loop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-the-rl-loop">Visualization: The RL Loop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-rl-terminology">Key RL Terminology</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-decision-processes-mdps">2. Markov Decision Processes (MDPs)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-markov-property">The Markov Property</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuitive-explanation">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-f1-race-as-an-mdp">The F1 Race as an MDP</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-gridworld-mdp">Building a Gridworld MDP</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-the-gridworld">Visualization: The Gridworld</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#returns-and-discounting">3. Returns and Discounting</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-discount">Why discount?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-effect-of-discount-factor">Visualization: Effect of Discount Factor</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#value-functions-and-the-bellman-equation">4. Value Functions and the Bellman Equation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#state-value-function-v-s">State-Value Function V(s)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#action-value-function-q-s-a">Action-Value Function Q(s, a)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bellman-equation">The Bellman Equation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-why-the-bellman-equation-matters">Deep Dive: Why the Bellman Equation Matters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-bellman-equation-backup-diagram">Visualization: Bellman Equation Backup Diagram</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-evaluation">5. Policy Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-convergence-of-policy-evaluation">Visualization: Convergence of Policy Evaluation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-improvement-and-policy-iteration">6. Policy Improvement and Policy Iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#value-iteration">7. Value Iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exploration-vs-exploitation">8. Exploration vs. Exploitation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-exploration-strategies">Common Exploration Strategies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-exploration-vs-exploitation">Visualization: Exploration vs. Exploitation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#temporal-difference-learning">9. Temporal Difference Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#td-0-update-rule">TD(0) Update Rule</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#value-based-vs-policy-based-methods">10. Value-Based vs. Policy-Based Methods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#value-based-methods">Value-Based Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-based-methods">Policy-Based Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#actor-critic-methods">Actor-Critic Methods</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#connecting-rl-to-llm-alignment">11. Connecting RL to LLM Alignment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-custom-gridworld-the-monaco-grand-prix">Exercise 1: Custom Gridworld (The Monaco Grand Prix)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-monte-carlo-vs-td-0-post-race-review-vs-lap-by-lap-learning">Exercise 2: Monte Carlo vs. TD(0) — Post-Race Review vs. Lap-by-Lap Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-ucb-exploration-the-untested-tire-compound">Exercise 3: UCB Exploration — The Untested Tire Compound</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts">Key Concepts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamental-insight">Fundamental Insight</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">Next Steps</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="part-7-1-reinforcement-learning-fundamentals">
<h1>Part 7.1: Reinforcement Learning Fundamentals<a class="headerlink" href="#part-7-1-reinforcement-learning-fundamentals" title="Link to this heading">#</a></h1>
<p>We’ve spent 16 notebooks learning how to build models that learn from <strong>labeled data</strong> (supervised) or <strong>unlabeled data</strong> (self-supervised). But there’s a third paradigm — one that learns from <strong>experience and rewards</strong>, just like a child learning to walk by falling down and getting back up.</p>
<p><strong>Reinforcement Learning (RL)</strong> is about an agent interacting with an environment, taking actions, receiving rewards, and learning a strategy to maximize long-term success. It’s the foundation of game-playing AI (AlphaGo), robotics, and — critically — <strong>RLHF</strong>, the technique that makes language models like ChatGPT helpful and safe.</p>
<p><strong>The F1 Connection:</strong> Every Formula 1 race is a reinforcement learning problem in disguise. The race engineer and driver together form an <em>agent</em> that must make real-time decisions — when to pit, how hard to push, when to conserve tires — in a stochastic <em>environment</em> (weather changes, safety cars, tire degradation). The <em>reward</em> is championship points. A race strategy is literally a <em>policy</em>: a mapping from the car’s current situation to the optimal action. In this notebook, we’ll build the mathematical framework behind these decisions.</p>
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>[ ] Understand the agent-environment loop and how RL differs from supervised learning</p></li>
<li><p>[ ] Define Markov Decision Processes (MDPs) and their components</p></li>
<li><p>[ ] Derive and implement the Bellman equations for value functions</p></li>
<li><p>[ ] Distinguish between state-value functions V(s) and action-value functions Q(s,a)</p></li>
<li><p>[ ] Implement policy evaluation and policy iteration from scratch</p></li>
<li><p>[ ] Understand the exploration vs. exploitation tradeoff</p></li>
<li><p>[ ] Implement value iteration to solve a gridworld environment</p></li>
<li><p>[ ] Compare policy-based vs. value-based methods at a high level</p></li>
<li><p>[ ] Build intuition for temporal difference learning</p></li>
<li><p>[ ] Connect RL concepts to the RLHF pipeline introduced in Notebook 21</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.patches</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">mpatches</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib.colors</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearSegmentedColormap</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>

<span class="c1"># For reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Part 7.1: Reinforcement Learning Fundamentals&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="the-reinforcement-learning-paradigm">
<h2>1. The Reinforcement Learning Paradigm<a class="headerlink" href="#the-reinforcement-learning-paradigm" title="Link to this heading">#</a></h2>
<p>In supervised learning, we have input-output pairs and minimize a loss. In RL, there are no labels — the agent must <strong>discover</strong> which actions lead to rewards through trial and error.</p>
<section id="the-agent-environment-loop">
<h3>The Agent-Environment Loop<a class="headerlink" href="#the-agent-environment-loop" title="Link to this heading">#</a></h3>
<p>The core RL cycle works like this:</p>
<ol class="arabic simple">
<li><p>The <strong>agent</strong> observes the current <strong>state</strong> <span class="math notranslate nohighlight">\(s_t\)</span></p></li>
<li><p>The agent selects an <strong>action</strong> <span class="math notranslate nohighlight">\(a_t\)</span> based on its <strong>policy</strong> <span class="math notranslate nohighlight">\(\pi\)</span></p></li>
<li><p>The <strong>environment</strong> transitions to a new state <span class="math notranslate nohighlight">\(s_{t+1}\)</span></p></li>
<li><p>The environment returns a <strong>reward</strong> <span class="math notranslate nohighlight">\(r_t\)</span></p></li>
<li><p>Repeat</p></li>
</ol>
<p>The goal: find a policy <span class="math notranslate nohighlight">\(\pi^*\)</span> that maximizes the <strong>expected cumulative reward</strong> over time.</p>
<p><strong>F1 analogy:</strong> The agent is the race strategist. The state is the car’s current situation — track position P3, tire age 15 laps, gap to leader 4.2 seconds, medium compound, fuel load 60%. The actions are: pit now, push hard, conserve tires, defend position, use DRS. The reward is positions gained (or championship points at race end). The policy is the strategy: “If tires are older than 20 laps AND gap to car ahead is under 1 second, pit for fresh softs.”</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Concept</p></th>
<th class="head"><p>Supervised Learning</p></th>
<th class="head"><p>Reinforcement Learning</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Feedback</strong></p></td>
<td><p>Correct labels provided</p></td>
<td><p>Scalar reward signal</p></td>
<td><p>Points scored at end of race</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Timing</strong></p></td>
<td><p>Immediate</p></td>
<td><p>Can be delayed</p></td>
<td><p>Pit stop pain now, position gain later</p></td>
</tr>
<tr class="row-even"><td><p><strong>Data</strong></p></td>
<td><p>Fixed dataset</p></td>
<td><p>Generated by agent’s actions</p></td>
<td><p>Each race is unique — new data from new decisions</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Goal</strong></p></td>
<td><p>Minimize loss</p></td>
<td><p>Maximize cumulative reward</p></td>
<td><p>Maximize championship points</p></td>
</tr>
<tr class="row-even"><td><p><strong>Exploration</strong></p></td>
<td><p>Not needed</p></td>
<td><p>Critical for learning</p></td>
<td><p>Trying an aggressive undercut vs. known overcut</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="visualization-the-rl-loop">
<h3>Visualization: The RL Loop<a class="headerlink" href="#visualization-the-rl-loop" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;The Reinforcement Learning Loop&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># Agent box</span>
<span class="n">agent_box</span> <span class="o">=</span> <span class="n">mpatches</span><span class="o">.</span><span class="n">FancyBboxPatch</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">boxstyle</span><span class="o">=</span><span class="s2">&quot;round,pad=0.3&quot;</span><span class="p">,</span>
                                     <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;#3498db&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">agent_box</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">2.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;AGENT&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span>
        <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">5.4</span><span class="p">,</span> <span class="s1">&#39;Policy π(a|s)&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>

<span class="c1"># Environment box</span>
<span class="n">env_box</span> <span class="o">=</span> <span class="n">mpatches</span><span class="o">.</span><span class="n">FancyBboxPatch</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">boxstyle</span><span class="o">=</span><span class="s2">&quot;round,pad=0.3&quot;</span><span class="p">,</span>
                                   <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;#2ecc71&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">env_box</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">7.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;ENVIRONMENT&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span>
        <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">7.5</span><span class="p">,</span> <span class="mf">5.4</span><span class="p">,</span> <span class="s1">&#39;P(s</span><span class="se">\&#39;</span><span class="s1">|s,a), R(s,a)&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>

<span class="c1"># Action arrow (agent -&gt; environment)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mf">6.5</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mf">6.5</span><span class="p">),</span>
            <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#e74c3c&#39;</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">7.1</span><span class="p">,</span> <span class="s1">&#39;Action $a_t$&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#e74c3c&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># State arrow (environment -&gt; agent, bottom)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mf">5.3</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mf">5.3</span><span class="p">),</span>
            <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#9b59b6&#39;</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">4.6</span><span class="p">,</span> <span class="s1">&#39;State $s_{t+1}$&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#9b59b6&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># Reward arrow (environment -&gt; agent, further below)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">7.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">),</span>
            <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#f39c12&#39;</span><span class="p">,</span>
                           <span class="n">connectionstyle</span><span class="o">=</span><span class="s1">&#39;arc3,rad=0.3&#39;</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="s1">&#39;Reward $r_t$&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#f39c12&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># Time step indicator</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="s1">&#39;At each timestep t = 0, 1, 2, ...&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
        <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;italic&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="key-rl-terminology">
<h3>Key RL Terminology<a class="headerlink" href="#key-rl-terminology" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Term</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Definition</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>State</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span></p></td>
<td><p>Current situation of the agent</p></td>
<td><p>Position, tire condition, gap to rivals, weather, laps remaining</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Action</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(a \in \mathcal{A}\)</span></p></td>
<td><p>Decision the agent can make</p></td>
<td><p>Pit stop, push hard, conserve tires, defend position, use DRS</p></td>
</tr>
<tr class="row-even"><td><p><strong>Policy</strong></p></td>
<td><p>$\pi(a</p></td>
<td><p>s)$</p></td>
<td><p>Strategy mapping states to actions</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Reward</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(r_t\)</span></p></td>
<td><p>Immediate feedback signal</p></td>
<td><p>Positions gained, time advantage, championship points</p></td>
</tr>
<tr class="row-even"><td><p><strong>Return</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(G_t\)</span></p></td>
<td><p>Cumulative discounted future reward</p></td>
<td><p>Total value of remaining race outcome</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Discount factor</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(\gamma \in [0,1]\)</span></p></td>
<td><p>How much we value future vs. present rewards</p></td>
<td><p>How much a position gain on lap 50 matters vs. lap 1</p></td>
</tr>
<tr class="row-even"><td><p><strong>Episode</strong></p></td>
<td><p>—</p></td>
<td><p>One complete sequence from start to terminal state</p></td>
<td><p>One full race, lights out to checkered flag</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Trajectory</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(\tau\)</span></p></td>
<td><p>Sequence of (state, action, reward) tuples</p></td>
<td><p>Full race log: every decision and its outcome</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<hr class="docutils" />
<section id="markov-decision-processes-mdps">
<h2>2. Markov Decision Processes (MDPs)<a class="headerlink" href="#markov-decision-processes-mdps" title="Link to this heading">#</a></h2>
<p>An MDP formalizes the RL problem mathematically. It’s defined by the tuple <span class="math notranslate nohighlight">\((\mathcal{S}, \mathcal{A}, P, R, \gamma)\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{S}\)</span>: Set of states</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{A}\)</span>: Set of actions</p></li>
<li><p><span class="math notranslate nohighlight">\(P(s'|s,a)\)</span>: <strong>Transition probability</strong> — probability of reaching state <span class="math notranslate nohighlight">\(s'\)</span> from state <span class="math notranslate nohighlight">\(s\)</span> after taking action <span class="math notranslate nohighlight">\(a\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(R(s,a)\)</span>: <strong>Reward function</strong> — expected reward for taking action <span class="math notranslate nohighlight">\(a\)</span> in state <span class="math notranslate nohighlight">\(s\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma\)</span>: <strong>Discount factor</strong> — balances immediate vs. future rewards</p></li>
</ul>
<section id="the-markov-property">
<h3>The Markov Property<a class="headerlink" href="#the-markov-property" title="Link to this heading">#</a></h3>
<p>The key assumption: the future depends only on the <strong>current state</strong>, not the history:</p>
<div class="math notranslate nohighlight">
\[P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, \ldots) = P(s_{t+1} | s_t, a_t)\]</div>
<p>This is powerful because it means we can make optimal decisions using only the current state — no need to remember the entire history.</p>
</section>
<section id="intuitive-explanation">
<h3>Intuitive Explanation<a class="headerlink" href="#intuitive-explanation" title="Link to this heading">#</a></h3>
<p>Think of chess: the current board position contains everything you need to make your next move. It doesn’t matter <em>how</em> you got to that position — the optimal strategy depends only on where the pieces are <em>right now</em>.</p>
<p><strong>F1 analogy:</strong> A race is a Markov decision process. The state is your car’s <em>current</em> situation: P4, lap 32 of 57, medium tires at 60% life, 2.1 seconds behind P3, fuel load nominal. Your optimal strategy decision (pit now? push? conserve?) depends only on THIS snapshot, not on whether you gained two positions on lap 1 or started P4. The transition probabilities are stochastic — you choose “push hard,” but there’s a probability your tires degrade faster, or a safety car changes everything. The Markov property isn’t perfect in F1 (tire history matters somewhat), but it’s a powerful approximation — modern F1 strategy tools use exactly this framework.</p>
</section>
<section id="the-f1-race-as-an-mdp">
<h3>The F1 Race as an MDP<a class="headerlink" href="#the-f1-race-as-an-mdp" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>MDP Component</p></th>
<th class="head"><p>F1 Mapping</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathcal{S}\)</span> (states)</p></td>
<td><p>{(position, tire_age, tire_compound, gap_ahead, gap_behind, laps_remaining, weather)}</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\mathcal{A}\)</span> (actions)</p></td>
<td><p>{pit_for_softs, pit_for_mediums, pit_for_hards, push, conserve, defend}</p></td>
</tr>
<tr class="row-even"><td><p>$P(s’</p></td>
<td><p>s,a)$</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(R(s,a)\)</span></p></td>
<td><p>Positions gained, time advantage, championship points at race end</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\gamma\)</span></p></td>
<td><p>How much future laps matter vs. this lap (close to 1 in F1 — every lap counts)</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="building-a-gridworld-mdp">
<h3>Building a Gridworld MDP<a class="headerlink" href="#building-a-gridworld-mdp" title="Link to this heading">#</a></h3>
<p>Let’s build a simple gridworld — the “hello world” of RL. Our agent navigates a 4×4 grid trying to reach a goal while avoiding traps.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">GridWorld</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A simple gridworld MDP environment.&quot;&quot;&quot;</span>
    
    <span class="c1"># Cell types</span>
    <span class="n">EMPTY</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">WALL</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">GOAL</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">TRAP</span> <span class="o">=</span> <span class="mi">3</span>
    
    <span class="c1"># Actions: up, down, left, right</span>
    <span class="n">ACTIONS</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;up&#39;</span><span class="p">,</span> <span class="s1">&#39;down&#39;</span><span class="p">,</span> <span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">]</span>
    <span class="n">ACTION_DELTAS</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;up&#39;</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="s1">&#39;down&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="s1">&#39;left&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
        <span class="s1">&#39;right&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">}</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grid_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">slip_prob</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grid_size</span> <span class="o">=</span> <span class="n">grid_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">slip_prob</span> <span class="o">=</span> <span class="n">slip_prob</span>  <span class="c1"># Probability of slipping to a random adjacent cell</span>
        
        <span class="c1"># Define the grid</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">grid_size</span><span class="p">,</span> <span class="n">grid_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grid</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">GOAL</span>   <span class="c1"># Goal at top-right</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grid</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">WALL</span>   <span class="c1"># Wall</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grid</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">TRAP</span>   <span class="c1"># Trap</span>
        
        <span class="c1"># State and action spaces</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="o">=</span> <span class="p">[(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">grid_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">grid_size</span><span class="p">)</span>
                       <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grid</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">WALL</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">terminal_states</span> <span class="o">=</span> <span class="p">[(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">grid_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">grid_size</span><span class="p">)</span>
                                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grid</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">GOAL</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">TRAP</span><span class="p">]]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_states</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ACTIONS</span><span class="p">)</span>
        
        <span class="c1"># Starting position</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">agent_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">start</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset to starting position.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">agent_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">start</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_pos</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_is_valid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pos</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if a position is valid (in bounds and not a wall).&quot;&quot;&quot;</span>
        <span class="n">r</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">pos</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">grid_size</span> <span class="ow">and</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">grid_size</span> 
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">grid</span><span class="p">[</span><span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">WALL</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">get_transitions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return list of (probability, next_state, reward) for a state-action pair.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">state</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">terminal_states</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)]</span>  <span class="c1"># Terminal states loop with zero reward</span>
        
        <span class="n">transitions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">intended_delta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ACTION_DELTAS</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
        <span class="n">intended_next</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">intended_delta</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">intended_delta</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        
        <span class="c1"># Intended action succeeds with probability (1 - slip_prob)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_valid</span><span class="p">(</span><span class="n">intended_next</span><span class="p">):</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">intended_next</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">state</span>  <span class="c1"># Bounce off wall/boundary</span>
        
        <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_reward</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>
        <span class="n">transitions</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">slip_prob</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">))</span>
        
        <span class="c1"># With slip_prob, agent moves in a random perpendicular direction</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">slip_prob</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">perpendicular</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="n">action</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;up&#39;</span><span class="p">,</span> <span class="s1">&#39;down&#39;</span><span class="p">]:</span>
                <span class="n">perpendicular</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">perpendicular</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;up&#39;</span><span class="p">,</span> <span class="s1">&#39;down&#39;</span><span class="p">]</span>
            
            <span class="k">for</span> <span class="n">perp_action</span> <span class="ow">in</span> <span class="n">perpendicular</span><span class="p">:</span>
                <span class="n">perp_delta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ACTION_DELTAS</span><span class="p">[</span><span class="n">perp_action</span><span class="p">]</span>
                <span class="n">perp_next</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">perp_delta</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">perp_delta</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_valid</span><span class="p">(</span><span class="n">perp_next</span><span class="p">):</span>
                    <span class="n">perp_state</span> <span class="o">=</span> <span class="n">perp_next</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">perp_state</span> <span class="o">=</span> <span class="n">state</span>
                <span class="n">perp_reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_reward</span><span class="p">(</span><span class="n">perp_state</span><span class="p">)</span>
                <span class="n">transitions</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">slip_prob</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">perp_state</span><span class="p">,</span> <span class="n">perp_reward</span><span class="p">))</span>
        
        <span class="k">return</span> <span class="n">transitions</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_get_reward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reward function.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grid</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">GOAL</span><span class="p">:</span>
            <span class="k">return</span> <span class="o">+</span><span class="mf">1.0</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">grid</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">TRAP</span><span class="p">:</span>
            <span class="k">return</span> <span class="o">-</span><span class="mf">1.0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="o">-</span><span class="mf">0.04</span>  <span class="c1"># Small step penalty to encourage efficiency</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Take an action, return (next_state, reward, done).&quot;&quot;&quot;</span>
        <span class="n">transitions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_transitions</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">agent_pos</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">transitions</span><span class="p">]</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">transitions</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="n">probs</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">transitions</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">agent_pos</span> <span class="o">=</span> <span class="n">next_state</span>
        <span class="n">done</span> <span class="o">=</span> <span class="n">next_state</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">terminal_states</span>
        <span class="k">return</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span>


<span class="c1"># Create and display the gridworld</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">GridWorld</span><span class="p">(</span><span class="n">grid_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">slip_prob</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GridWorld MDP&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;States: </span><span class="si">{</span><span class="n">env</span><span class="o">.</span><span class="n">n_states</span><span class="si">}</span><span class="s2"> (excluding walls)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Actions: </span><span class="si">{</span><span class="n">env</span><span class="o">.</span><span class="n">n_actions</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">ACTIONS</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Terminal states: </span><span class="si">{</span><span class="n">env</span><span class="o">.</span><span class="n">terminal_states</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Slip probability: </span><span class="si">{</span><span class="n">env</span><span class="o">.</span><span class="n">slip_prob</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Start: </span><span class="si">{</span><span class="n">env</span><span class="o">.</span><span class="n">start</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualization-the-gridworld">
<h3>Visualization: The Gridworld<a class="headerlink" href="#visualization-the-gridworld" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">visualize_gridworld</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;GridWorld&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize the gridworld with optional value function and policy overlays.&quot;&quot;&quot;</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">grid_size</span>
    
    <span class="c1"># Color map for cell types</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">GridWorld</span><span class="o">.</span><span class="n">EMPTY</span><span class="p">:</span> <span class="s1">&#39;#f0f0f0&#39;</span><span class="p">,</span>
        <span class="n">GridWorld</span><span class="o">.</span><span class="n">WALL</span><span class="p">:</span> <span class="s1">&#39;#2c3e50&#39;</span><span class="p">,</span>
        <span class="n">GridWorld</span><span class="o">.</span><span class="n">GOAL</span><span class="p">:</span> <span class="s1">&#39;#2ecc71&#39;</span><span class="p">,</span>
        <span class="n">GridWorld</span><span class="o">.</span><span class="n">TRAP</span><span class="p">:</span> <span class="s1">&#39;#e74c3c&#39;</span>
    <span class="p">}</span>
    
    <span class="c1"># Draw cells</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">cell_type</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">grid</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
            <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="n">cell_type</span><span class="p">]</span>
            
            <span class="c1"># If we have values, shade empty cells by value</span>
            <span class="k">if</span> <span class="n">values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">cell_type</span> <span class="o">==</span> <span class="n">GridWorld</span><span class="o">.</span><span class="n">EMPTY</span><span class="p">:</span>
                <span class="n">v</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
                <span class="c1"># Normalize to [-1, 1] for coloring</span>
                <span class="n">intensity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">intensity</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">color</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">RdYlGn</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">+</span> <span class="n">intensity</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">color</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">RdYlGn</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">+</span> <span class="n">intensity</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">)</span>
            
            <span class="n">rect</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="n">j</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">i</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="n">color</span><span class="p">,</span>
                                  <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">rect</span><span class="p">)</span>
            
            <span class="c1"># Labels</span>
            <span class="k">if</span> <span class="n">cell_type</span> <span class="o">==</span> <span class="n">GridWorld</span><span class="o">.</span><span class="n">GOAL</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">i</span> <span class="o">+</span> <span class="mf">0.7</span><span class="p">,</span> <span class="s1">&#39;GOAL&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
                        <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">i</span> <span class="o">+</span> <span class="mf">0.4</span><span class="p">,</span> <span class="s1">&#39;+1.0&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
                        <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">cell_type</span> <span class="o">==</span> <span class="n">GridWorld</span><span class="o">.</span><span class="n">TRAP</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">i</span> <span class="o">+</span> <span class="mf">0.7</span><span class="p">,</span> <span class="s1">&#39;TRAP&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
                        <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">i</span> <span class="o">+</span> <span class="mf">0.4</span><span class="p">,</span> <span class="s1">&#39;-1.0&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
                        <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">cell_type</span> <span class="o">==</span> <span class="n">GridWorld</span><span class="o">.</span><span class="n">WALL</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">i</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s1">&#39;WALL&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
                        <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>
            
            <span class="c1"># Show values</span>
            <span class="k">if</span> <span class="n">values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="ow">in</span> <span class="n">values</span> <span class="ow">and</span> <span class="n">cell_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">GridWorld</span><span class="o">.</span><span class="n">WALL</span><span class="p">]:</span>
                <span class="n">v</span> <span class="o">=</span> <span class="n">values</span><span class="p">[(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)]</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">i</span> <span class="o">+</span> <span class="mf">0.15</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">v</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
                        <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;italic&#39;</span><span class="p">)</span>
            
            <span class="c1"># Show policy arrows</span>
            <span class="k">if</span> <span class="n">policy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="ow">in</span> <span class="n">policy</span> <span class="ow">and</span> <span class="n">cell_type</span> <span class="o">==</span> <span class="n">GridWorld</span><span class="o">.</span><span class="n">EMPTY</span><span class="p">:</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">policy</span><span class="p">[(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)]</span>
                <span class="n">arrow_map</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s1">&#39;up&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">),</span>
                    <span class="s1">&#39;down&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.25</span><span class="p">),</span>
                    <span class="s1">&#39;left&#39;</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                    <span class="s1">&#39;right&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                <span class="p">}</span>
                <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span> <span class="o">=</span> <span class="n">arrow_map</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">+</span> <span class="n">dx</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">i</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">+</span> <span class="n">dy</span><span class="p">),</span>
                           <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">i</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">),</span>
                           <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#2c3e50&#39;</span><span class="p">))</span>
    
    <span class="c1"># Mark start</span>
    <span class="n">si</span><span class="p">,</span> <span class="n">sj</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">start</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">sj</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">si</span> <span class="o">+</span> <span class="mf">0.85</span><span class="p">,</span> <span class="s1">&#39;START&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
            <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#3498db&#39;</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Column&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Row&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="n">visualize_gridworld</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;4×4 GridWorld Environment&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The agent starts at the bottom-left and must navigate to the GOAL (+1.0) while avoiding the TRAP (-1.0). Each non-terminal step costs -0.04 to encourage the agent to find the goal quickly. There’s a 10% chance of slipping perpendicular to the intended direction — this stochasticity is what makes the problem interesting.</p>
<p><strong>F1 analogy:</strong> Think of the gridworld as a simplified race. The GOAL is the podium finish, the TRAP is a DNF (retirement), and the small step penalty is tire degradation — every lap costs you something, so you need to reach the finish efficiently. The 10% slip probability mirrors the unpredictability of racing: you plan to push through Turn 3, but you might get understeer and lose time. The wall is like a track limit violation that nullifies your move.</p>
</section>
</section>
<hr class="docutils" />
<section id="returns-and-discounting">
<h2>3. Returns and Discounting<a class="headerlink" href="#returns-and-discounting" title="Link to this heading">#</a></h2>
<p>The agent doesn’t just want the next reward — it wants to maximize the <strong>total reward over time</strong>. We call this the <strong>return</strong>:</p>
<div class="math notranslate nohighlight">
\[G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots = \sum_{k=0}^{\infty} \gamma^k r_{t+k}\]</div>
<p>The <strong>discount factor</strong> <span class="math notranslate nohighlight">\(\gamma\)</span> determines how much we value future rewards:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\gamma = 0\)</span>: Only care about immediate reward (greedy)</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma = 1\)</span>: Value all future rewards equally (far-sighted)</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma = 0.9\)</span>: A reward 10 steps away is worth <span class="math notranslate nohighlight">\(0.9^{10} \approx 0.35\)</span> of an immediate reward</p></li>
</ul>
<section id="why-discount">
<h3>Why discount?<a class="headerlink" href="#why-discount" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Mathematical convenience</strong>: Makes infinite sums converge</p></li>
<li><p><strong>Uncertainty</strong>: The further into the future, the less certain we are</p></li>
<li><p><strong>Human-like behavior</strong>: We prefer rewards sooner rather than later</p></li>
</ol>
<p><strong>F1 analogy:</strong> The discount factor captures how much future laps matter compared to this one. With <span class="math notranslate nohighlight">\(\gamma\)</span> close to 1 (say 0.99), a position gain on lap 50 is almost as valuable as one on lap 1 — which is how F1 strategists actually think. But with <span class="math notranslate nohighlight">\(\gamma = 0.5\)</span>, you’d heavily prioritize immediate gains, like a driver who burns through tires to lead lap 1 but fades by mid-race. A safety car on lap 40 is uncertain — discounting reflects that future rewards are less predictable. Teams running Monte Carlo race simulations with thousands of scenarios are implicitly reasoning about discounted returns across different possible futures.</p>
</section>
<section id="visualization-effect-of-discount-factor">
<h3>Visualization: Effect of Discount Factor<a class="headerlink" href="#visualization-effect-of-discount-factor" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Left: Discount curves</span>
<span class="n">steps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">gammas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">viridis</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">gammas</span><span class="p">)))</span>

<span class="k">for</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">gammas</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">gamma</span><span class="o">**</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">steps</span><span class="p">]</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">steps</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;γ = </span><span class="si">{</span><span class="n">gamma</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Steps into the future (k)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Discount weight (γᵏ)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;How Much We Value Future Rewards&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Right: Cumulative return example</span>
<span class="n">rewards</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>  <span class="c1"># Path to goal</span>
<span class="n">gamma_vals</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">]</span>
<span class="n">bar_width</span> <span class="o">=</span> <span class="mf">0.25</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">gamma</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">gamma_vals</span><span class="p">):</span>
    <span class="n">discounted</span> <span class="o">=</span> <span class="p">[</span><span class="n">rewards</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">gamma</span><span class="o">**</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">))]</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">bar_width</span><span class="p">,</span> <span class="n">discounted</span><span class="p">,</span> <span class="n">bar_width</span><span class="p">,</span>
               <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;γ = </span><span class="si">{</span><span class="n">gamma</span><span class="si">}</span><span class="s1"> (Return = </span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">discounted</span><span class="p">)</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">,</span>
               <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Time step&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Discounted reward&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Same Path, Different Discount Factors&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">bar_width</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39;t=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">))])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Numeric example</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Example: Agent takes 5 steps then reaches goal&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Rewards: </span><span class="si">{</span><span class="n">rewards</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">gamma</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">]:</span>
    <span class="n">G</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">r</span> <span class="o">*</span> <span class="n">gamma</span><span class="o">**</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  γ = </span><span class="si">{</span><span class="n">gamma</span><span class="si">}</span><span class="s2">: Return G₀ = </span><span class="si">{</span><span class="n">G</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="value-functions-and-the-bellman-equation">
<h2>4. Value Functions and the Bellman Equation<a class="headerlink" href="#value-functions-and-the-bellman-equation" title="Link to this heading">#</a></h2>
<p>Value functions answer the question: <strong>“How good is it to be in a particular state (or to take a particular action in a state)?”</strong></p>
<section id="state-value-function-v-s">
<h3>State-Value Function V(s)<a class="headerlink" href="#state-value-function-v-s" title="Link to this heading">#</a></h3>
<p>The <strong>state-value function</strong> <span class="math notranslate nohighlight">\(V^\pi(s)\)</span> is the expected return starting from state <span class="math notranslate nohighlight">\(s\)</span> and following policy <span class="math notranslate nohighlight">\(\pi\)</span>:</p>
<div class="math notranslate nohighlight">
\[V^\pi(s) = \mathbb{E}_\pi[G_t | s_t = s] = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k r_{t+k} \mid s_t = s\right]\]</div>
<p><strong>F1 analogy:</strong> V(s) answers: “How valuable is this race position given current conditions?” If you’re P3 with fresh mediums and 20 laps to go, V(s) is high — you have a great shot at the podium. If you’re P15 with worn hards and 5 laps left, V(s) is low.</p>
</section>
<section id="action-value-function-q-s-a">
<h3>Action-Value Function Q(s, a)<a class="headerlink" href="#action-value-function-q-s-a" title="Link to this heading">#</a></h3>
<p>The <strong>action-value function</strong> <span class="math notranslate nohighlight">\(Q^\pi(s, a)\)</span> is the expected return starting from state <span class="math notranslate nohighlight">\(s\)</span>, taking action <span class="math notranslate nohighlight">\(a\)</span>, then following policy <span class="math notranslate nohighlight">\(\pi\)</span>:</p>
<div class="math notranslate nohighlight">
\[Q^\pi(s, a) = \mathbb{E}_\pi[G_t | s_t = s, a_t = a]\]</div>
<p><strong>F1 analogy:</strong> Q(s, a) answers: “How good is pitting NOW vs. next lap, given we’re P3 with worn tires?” Q(P3_worn_tires, pit_now) might be 0.7 (likely P4 finish after losing track position). Q(P3_worn_tires, push_one_more_lap) might be 0.6 (risk of tire failure, but could maintain position if tires hold). The strategist compares Q-values to make the call.</p>
</section>
<section id="the-bellman-equation">
<h3>The Bellman Equation<a class="headerlink" href="#the-bellman-equation" title="Link to this heading">#</a></h3>
<p>The key insight: we can express the value of a state <strong>recursively</strong> in terms of the values of successor states:</p>
<div class="math notranslate nohighlight">
\[V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) \left[R(s,a) + \gamma V^\pi(s')\right]\]</div>
<p>This is the <strong>Bellman expectation equation</strong> — the foundation of almost every RL algorithm.</p>
<p><strong>In words</strong>: The value of a state equals the expected immediate reward plus the discounted value of the next state, averaged over all actions and transitions.</p>
<p><strong>F1 analogy:</strong> The Bellman equation says: “The value of being P3 on lap 30 = the immediate reward from this lap’s action + the discounted value of wherever we end up on lap 31.” Today’s position value = immediate reward + future race value. This is exactly how a strategy engineer thinks: “If we pit now, we lose 2 seconds (immediate cost) but gain tire advantage for the remaining laps (future value).”</p>
</section>
<section id="deep-dive-why-the-bellman-equation-matters">
<h3>Deep Dive: Why the Bellman Equation Matters<a class="headerlink" href="#deep-dive-why-the-bellman-equation-matters" title="Link to this heading">#</a></h3>
<p>The Bellman equation transforms an intractable problem (compute expected infinite sums) into a system of linear equations that can be solved iteratively. It’s the RL equivalent of dynamic programming — breaking a hard problem into overlapping subproblems.</p>
</section>
<section id="visualization-bellman-equation-backup-diagram">
<h3>Visualization: Bellman Equation Backup Diagram<a class="headerlink" href="#visualization-bellman-equation-backup-diagram" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Left: V(s) backup</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;V(s) Bellman Backup&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># Root state</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#3498db&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>

<span class="c1"># Action nodes</span>
<span class="n">action_x</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">]</span>
<span class="n">action_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a₁&#39;</span><span class="p">,</span> <span class="s1">&#39;a₂&#39;</span><span class="p">,</span> <span class="s1">&#39;a₃&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">action_x</span><span class="p">,</span> <span class="n">action_labels</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#e74c3c&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.2</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">],</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">((</span><span class="mi">0</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span> <span class="o">-</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">2.7</span><span class="p">,</span> <span class="s1">&#39;π(a|s)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

<span class="c1"># Next states from action a2</span>
<span class="n">next_x</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">next_x</span><span class="p">:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#2ecc71&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s2">&quot;s&#39;&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.8</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="s1">&#39;P(s</span><span class="se">\&#39;</span><span class="s1">|s,a)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="s1">&#39;r + γV(s</span><span class="se">\&#39;</span><span class="s1">)&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;italic&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#2c3e50&#39;</span><span class="p">)</span>

<span class="c1"># Right: Q(s,a) backup</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Q(s,a) Bellman Backup&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># Root action</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#e74c3c&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>

<span class="c1"># Next states</span>
<span class="n">next_x</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">next_x</span><span class="p">:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">22</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#2ecc71&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;s&#39;&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.2</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">],</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">2.7</span><span class="p">,</span> <span class="s1">&#39;P(s</span><span class="se">\&#39;</span><span class="s1">|s,a)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

<span class="c1"># Next actions from s&#39;</span>
<span class="k">for</span> <span class="n">base_x</span> <span class="ow">in</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]:</span>
    <span class="n">offsets</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">off</span> <span class="ow">in</span> <span class="n">offsets</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">base_x</span> <span class="o">+</span> <span class="n">off</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#e74c3c&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s2">&quot;a&#39;&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">base_x</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.8</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="s1">&#39;π(a</span><span class="se">\&#39;</span><span class="s1">|s</span><span class="se">\&#39;</span><span class="s1">)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="s1">&#39;r + γ Σ π(a</span><span class="se">\&#39;</span><span class="s1">|s</span><span class="se">\&#39;</span><span class="s1">)Q(s</span><span class="se">\&#39;</span><span class="s1">,a</span><span class="se">\&#39;</span><span class="s1">)&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;italic&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#2c3e50&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="policy-evaluation">
<h2>5. Policy Evaluation<a class="headerlink" href="#policy-evaluation" title="Link to this heading">#</a></h2>
<p>Given a policy <span class="math notranslate nohighlight">\(\pi\)</span>, <strong>policy evaluation</strong> computes <span class="math notranslate nohighlight">\(V^\pi(s)\)</span> for every state. We do this by repeatedly applying the Bellman equation until convergence:</p>
<div class="math notranslate nohighlight">
\[V_{k+1}(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) \left[R(s,a) + \gamma V_k(s')\right]\]</div>
<p>Starting from <span class="math notranslate nohighlight">\(V_0(s) = 0\)</span> for all states, this iterative process is guaranteed to converge to <span class="math notranslate nohighlight">\(V^\pi\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">policy_evaluation</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">max_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Evaluate a policy by iteratively applying the Bellman expectation equation.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        env: GridWorld environment</span>
<span class="sd">        policy: dict mapping state -&gt; action (deterministic policy)</span>
<span class="sd">        gamma: discount factor</span>
<span class="sd">        theta: convergence threshold</span>
<span class="sd">        max_iterations: safety limit</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        V: dict mapping state -&gt; value</span>
<span class="sd">        history: list of value dicts at each iteration (for visualization)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Initialize values to zero</span>
    <span class="n">V</span> <span class="o">=</span> <span class="p">{</span><span class="n">s</span><span class="p">:</span> <span class="mf">0.0</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">states</span><span class="p">}</span>
    <span class="n">history</span> <span class="o">=</span> <span class="p">[</span><span class="n">V</span><span class="o">.</span><span class="n">copy</span><span class="p">()]</span>
    
    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iterations</span><span class="p">):</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">V_new</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">states</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">terminal_states</span><span class="p">:</span>
                <span class="c1"># Terminal states have fixed values based on reward</span>
                <span class="k">if</span> <span class="n">env</span><span class="o">.</span><span class="n">grid</span><span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">==</span> <span class="n">GridWorld</span><span class="o">.</span><span class="n">GOAL</span><span class="p">:</span>
                    <span class="n">V_new</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
                <span class="k">elif</span> <span class="n">env</span><span class="o">.</span><span class="n">grid</span><span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">==</span> <span class="n">GridWorld</span><span class="o">.</span><span class="n">TRAP</span><span class="p">:</span>
                    <span class="n">V_new</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span>
                <span class="k">continue</span>
            
            <span class="c1"># Bellman expectation equation for deterministic policy</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">)</span>  <span class="c1"># Default action</span>
            <span class="n">transitions</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">get_transitions</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
            
            <span class="n">v</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">s_next</span><span class="p">])</span>
                    <span class="k">for</span> <span class="n">prob</span><span class="p">,</span> <span class="n">s_next</span><span class="p">,</span> <span class="n">reward</span> <span class="ow">in</span> <span class="n">transitions</span><span class="p">)</span>
            
            <span class="n">delta</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="nb">abs</span><span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]))</span>
            <span class="n">V_new</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
        
        <span class="n">V</span> <span class="o">=</span> <span class="n">V_new</span>
        <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">V</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        
        <span class="k">if</span> <span class="n">delta</span> <span class="o">&lt;</span> <span class="n">theta</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Policy evaluation converged after </span><span class="si">{</span><span class="n">iteration</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> iterations (Δ &lt; </span><span class="si">{</span><span class="n">theta</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
            <span class="k">break</span>
    
    <span class="k">return</span> <span class="n">V</span><span class="p">,</span> <span class="n">history</span>


<span class="c1"># Evaluate a simple policy: always go right</span>
<span class="n">simple_policy</span> <span class="o">=</span> <span class="p">{</span><span class="n">s</span><span class="p">:</span> <span class="s1">&#39;right&#39;</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">states</span><span class="p">}</span>
<span class="n">V_simple</span><span class="p">,</span> <span class="n">history_simple</span> <span class="o">=</span> <span class="n">policy_evaluation</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">simple_policy</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Values under &#39;always go right&#39; policy:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">grid_size</span><span class="p">):</span>
    <span class="n">row_vals</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">grid_size</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="ow">in</span> <span class="n">V_simple</span><span class="p">:</span>
            <span class="n">row_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">V_simple</span><span class="p">[(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)]</span><span class="si">:</span><span class="s2">7.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">row_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;  WALL &quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot; | &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">row_vals</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize the values</span>
<span class="n">visualize_gridworld</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">V_simple</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">simple_policy</span><span class="p">,</span>
                    <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Policy Evaluation: &quot;Always Go Right&quot;&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="visualization-convergence-of-policy-evaluation">
<h3>Visualization: Convergence of Policy Evaluation<a class="headerlink" href="#visualization-convergence-of-policy-evaluation" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Show how values converge over iterations</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">iterations_to_show</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">history_simple</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">grid_size</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">it</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">iterations_to_show</span><span class="p">):</span>
    <span class="n">V_it</span> <span class="o">=</span> <span class="n">history_simple</span><span class="p">[</span><span class="n">it</span><span class="p">]</span>
    <span class="n">grid_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="ow">in</span> <span class="n">V_it</span><span class="p">:</span>
                <span class="n">grid_vals</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">V_it</span><span class="p">[(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">grid_vals</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
    
    <span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">grid_vals</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdYlGn&#39;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Iteration </span><span class="si">{</span><span class="n">it</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="ow">in</span> <span class="n">V_it</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">V_it</span><span class="p">[(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)]</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
                       <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">env</span><span class="o">.</span><span class="n">grid</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="n">GridWorld</span><span class="o">.</span><span class="n">WALL</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Policy Evaluation Convergence&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.02</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Notice how the values propagate backward from the goal and trap states, iteration by iteration. This is the Bellman equation at work — each iteration, information about future rewards flows one more step backward through the state space.</p>
<p><strong>F1 analogy:</strong> This is exactly how race strategy propagates backward from the finish. On the last lap, the only thing that matters is crossing the line. On the second-to-last lap, value depends on whether you’re positioned to gain or lose a place at the flag. Each earlier lap’s value builds on the laps that follow — just like the Bellman backup propagating from the GOAL cell outward through the grid.</p>
</section>
</section>
<hr class="docutils" />
<section id="policy-improvement-and-policy-iteration">
<h2>6. Policy Improvement and Policy Iteration<a class="headerlink" href="#policy-improvement-and-policy-iteration" title="Link to this heading">#</a></h2>
<p>Policy evaluation tells us <em>how good</em> a policy is. But we want the <em>best</em> policy. <strong>Policy improvement</strong> uses the value function to greedily select better actions:</p>
<div class="math notranslate nohighlight">
\[\pi'(s) = \arg\max_a \sum_{s'} P(s'|s,a) \left[R(s,a) + \gamma V^\pi(s')\right]\]</div>
<p><strong>Policy iteration</strong> alternates between:</p>
<ol class="arabic simple">
<li><p><strong>Evaluate</strong>: Compute <span class="math notranslate nohighlight">\(V^\pi\)</span> for the current policy</p></li>
<li><p><strong>Improve</strong>: Update the policy greedily with respect to <span class="math notranslate nohighlight">\(V^\pi\)</span></p></li>
</ol>
<p>This is guaranteed to converge to the optimal policy <span class="math notranslate nohighlight">\(\pi^*\)</span>.</p>
<p><strong>F1 analogy:</strong> Policy iteration is how teams improve their race strategy over a season. First, they <em>evaluate</em> a strategy — “our one-stop medium-hard plan at Silverstone scored 12 points” (policy evaluation). Then they <em>improve</em> — “given what we know about tire degradation, switching to a two-stop soft-medium plan would have scored 18 points” (policy improvement). They test the new strategy at the next race, evaluate it again, improve again. Over a season, the strategy converges toward the optimal approach for each circuit. This evaluate-improve cycle is exactly policy iteration.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">policy_improvement</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Improve policy greedily based on value function.&quot;&quot;&quot;</span>
    <span class="n">policy</span> <span class="o">=</span> <span class="p">{}</span>
    
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">states</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">terminal_states</span><span class="p">:</span>
            <span class="k">continue</span>
        
        <span class="n">best_action</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">best_value</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">ACTIONS</span><span class="p">:</span>
            <span class="n">transitions</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">get_transitions</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
            <span class="n">q_sa</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">s_next</span><span class="p">])</span>
                       <span class="k">for</span> <span class="n">prob</span><span class="p">,</span> <span class="n">s_next</span><span class="p">,</span> <span class="n">reward</span> <span class="ow">in</span> <span class="n">transitions</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">q_sa</span> <span class="o">&gt;</span> <span class="n">best_value</span><span class="p">:</span>
                <span class="n">best_value</span> <span class="o">=</span> <span class="n">q_sa</span>
                <span class="n">best_action</span> <span class="o">=</span> <span class="n">action</span>
        
        <span class="n">policy</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_action</span>
    
    <span class="k">return</span> <span class="n">policy</span>


<span class="k">def</span><span class="w"> </span><span class="nf">policy_iteration</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Full policy iteration algorithm.&quot;&quot;&quot;</span>
    <span class="c1"># Start with random policy</span>
    <span class="n">policy</span> <span class="o">=</span> <span class="p">{</span><span class="n">s</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">ACTIONS</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">states</span>
              <span class="k">if</span> <span class="n">s</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">terminal_states</span><span class="p">}</span>
    
    <span class="n">iteration</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="c1"># Policy evaluation</span>
        <span class="n">V</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">policy_evaluation</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
        
        <span class="c1"># Policy improvement</span>
        <span class="n">new_policy</span> <span class="o">=</span> <span class="n">policy_improvement</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
        
        <span class="c1"># Check for convergence</span>
        <span class="k">if</span> <span class="n">new_policy</span> <span class="o">==</span> <span class="n">policy</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Policy iteration converged after </span><span class="si">{</span><span class="n">iteration</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> improvement steps!&quot;</span><span class="p">)</span>
            <span class="k">break</span>
        
        <span class="n">policy</span> <span class="o">=</span> <span class="n">new_policy</span>
        <span class="n">iteration</span> <span class="o">+=</span> <span class="mi">1</span>
    
    <span class="k">return</span> <span class="n">policy</span><span class="p">,</span> <span class="n">V</span>


<span class="n">optimal_policy</span><span class="p">,</span> <span class="n">optimal_V</span> <span class="o">=</span> <span class="n">policy_iteration</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize the optimal policy</span>
<span class="n">visualize_gridworld</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">optimal_V</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">optimal_policy</span><span class="p">,</span>
                    <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Optimal Policy (Policy Iteration, γ=0.9)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The arrows show the optimal action in each state. Notice how the agent learns to navigate around the wall, move toward the goal, and stay away from the trap. The values decrease as we move further from the goal, reflecting the discounted future reward.</p>
<p><strong>F1 analogy:</strong> The optimal policy arrows are like the “strategy map” a team builds for every possible race situation. From P8 with fresh tires (high value), the arrow points toward “push” — close the gap and overtake. From a position near the trap (running on worn tires near a rival), the arrow says “conserve” — avoid the risk. A great strategist has an arrow for every situation before the race even starts.</p>
</section>
<hr class="docutils" />
<section id="value-iteration">
<h2>7. Value Iteration<a class="headerlink" href="#value-iteration" title="Link to this heading">#</a></h2>
<p><strong>Value iteration</strong> combines evaluation and improvement into a single step, updating values directly with the Bellman <strong>optimality</strong> equation:</p>
<div class="math notranslate nohighlight">
\[V_{k+1}(s) = \max_a \sum_{s'} P(s'|s,a) \left[R(s,a) + \gamma V_k(s')\right]\]</div>
<p>Instead of fully evaluating a policy before improving it, value iteration takes the max over actions at every step — essentially doing greedy improvement as part of the evaluation.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Steps per Iteration</p></th>
<th class="head"><p>Convergence</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Policy Iteration</strong></p></td>
<td><p>Full evaluation + improvement</p></td>
<td><p>Fewer outer iterations</p></td>
<td><p>Full season review then strategy overhaul</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Value Iteration</strong></p></td>
<td><p>Single Bellman optimality update</p></td>
<td><p>More iterations but simpler</p></td>
<td><p>Lap-by-lap real-time strategy adjustments</p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">value_iteration</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">max_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Value iteration: combine evaluation and improvement in one step.&quot;&quot;&quot;</span>
    <span class="n">V</span> <span class="o">=</span> <span class="p">{</span><span class="n">s</span><span class="p">:</span> <span class="mf">0.0</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">states</span><span class="p">}</span>
    <span class="n">history</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iterations</span><span class="p">):</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">V_new</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">states</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">terminal_states</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">env</span><span class="o">.</span><span class="n">grid</span><span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">==</span> <span class="n">GridWorld</span><span class="o">.</span><span class="n">GOAL</span><span class="p">:</span>
                    <span class="n">V_new</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
                <span class="k">elif</span> <span class="n">env</span><span class="o">.</span><span class="n">grid</span><span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">==</span> <span class="n">GridWorld</span><span class="o">.</span><span class="n">TRAP</span><span class="p">:</span>
                    <span class="n">V_new</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span>
                <span class="k">continue</span>
            
            <span class="c1"># Bellman optimality equation: take the MAX over actions</span>
            <span class="n">action_values</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">ACTIONS</span><span class="p">:</span>
                <span class="n">transitions</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">get_transitions</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
                <span class="n">q</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">s_next</span><span class="p">])</span>
                        <span class="k">for</span> <span class="n">prob</span><span class="p">,</span> <span class="n">s_next</span><span class="p">,</span> <span class="n">reward</span> <span class="ow">in</span> <span class="n">transitions</span><span class="p">)</span>
                <span class="n">action_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
            
            <span class="n">best_value</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">action_values</span><span class="p">)</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="nb">abs</span><span class="p">(</span><span class="n">best_value</span> <span class="o">-</span> <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]))</span>
            <span class="n">V_new</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_value</span>
        
        <span class="n">V</span> <span class="o">=</span> <span class="n">V_new</span>
        <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;iteration&#39;</span><span class="p">:</span> <span class="n">iteration</span><span class="p">,</span> <span class="s1">&#39;delta&#39;</span><span class="p">:</span> <span class="n">delta</span><span class="p">,</span> <span class="s1">&#39;V&#39;</span><span class="p">:</span> <span class="n">V</span><span class="o">.</span><span class="n">copy</span><span class="p">()})</span>
        
        <span class="k">if</span> <span class="n">delta</span> <span class="o">&lt;</span> <span class="n">theta</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Value iteration converged after </span><span class="si">{</span><span class="n">iteration</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> iterations&quot;</span><span class="p">)</span>
            <span class="k">break</span>
    
    <span class="c1"># Extract policy from final values</span>
    <span class="n">policy</span> <span class="o">=</span> <span class="n">policy_improvement</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">V</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">history</span>


<span class="n">V_vi</span><span class="p">,</span> <span class="n">policy_vi</span><span class="p">,</span> <span class="n">history_vi</span> <span class="o">=</span> <span class="n">value_iteration</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="c1"># Compare with policy iteration</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Value Iteration vs Policy Iteration values match:&quot;</span><span class="p">,</span>
      <span class="nb">all</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">V_vi</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">-</span> <span class="n">optimal_V</span><span class="p">[</span><span class="n">s</span><span class="p">])</span> <span class="o">&lt;</span> <span class="mf">1e-4</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">states</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Policies match:&quot;</span><span class="p">,</span>
      <span class="nb">all</span><span class="p">(</span><span class="n">policy_vi</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">==</span> <span class="n">optimal_policy</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">states</span>
          <span class="k">if</span> <span class="n">s</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">terminal_states</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize convergence speed</span>
<span class="n">deltas</span> <span class="o">=</span> <span class="p">[</span><span class="n">h</span><span class="p">[</span><span class="s1">&#39;delta&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">history_vi</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">deltas</span><span class="p">)),</span> <span class="n">deltas</span><span class="p">,</span> <span class="s1">&#39;b-o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Convergence threshold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Max value change (Δ)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Value Iteration Convergence&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="exploration-vs-exploitation">
<h2>8. Exploration vs. Exploitation<a class="headerlink" href="#exploration-vs-exploitation" title="Link to this heading">#</a></h2>
<p>One of the fundamental challenges in RL: should the agent <strong>exploit</strong> what it already knows works, or <strong>explore</strong> new actions that might lead to better outcomes?</p>
<ul class="simple">
<li><p><strong>Exploitation</strong>: Choose the action with the highest estimated value</p></li>
<li><p><strong>Exploration</strong>: Try less-visited or uncertain actions</p></li>
</ul>
<p>Too much exploitation → stuck in local optima (never discovers the best path)<br />
Too much exploration → wastes time on suboptimal actions</p>
<p><strong>F1 analogy:</strong> This is the tension every race strategist lives with. <em>Exploitation</em> is sticking with the proven one-stop strategy that has worked all season. <em>Exploration</em> is trying an aggressive undercut, an untested tire compound, or a radically different pit window. Red Bull in 2021 often explored novel strategies against Mercedes — sometimes they found gold (Abu Dhabi), sometimes they lost out. A team that never explores gets predictable and loses; a team that always explores never capitalizes on what works. The sweet spot is exploring early in the season (high epsilon) and exploiting your best strategies for the championship-deciding races (low epsilon).</p>
<section id="common-exploration-strategies">
<h3>Common Exploration Strategies<a class="headerlink" href="#common-exploration-strategies" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Strategy</p></th>
<th class="head"><p>How it Works</p></th>
<th class="head"><p>Tradeoff</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>epsilon-greedy</strong></p></td>
<td><p>With prob epsilon, random action; otherwise, best action</p></td>
<td><p>Simple, widely used</p></td>
<td><p>10% of the time, try something unconventional</p></td>
</tr>
<tr class="row-odd"><td><p><strong>epsilon-decay</strong></p></td>
<td><p>Start with high epsilon, decrease over time</p></td>
<td><p>Explores early, exploits later</p></td>
<td><p>Experiment in practice/early races, lock strategy for title fight</p></td>
</tr>
<tr class="row-even"><td><p><strong>Softmax/Boltzmann</strong></p></td>
<td><p>Sample actions proportional to estimated values</p></td>
<td><p>Smooth exploration</p></td>
<td><p>Weight new strategies by estimated value, not purely random</p></td>
</tr>
<tr class="row-odd"><td><p><strong>UCB</strong></p></td>
<td><p>Bonus for under-explored actions</p></td>
<td><p>Principled, optimistic</p></td>
<td><p>“We haven’t tried the hard compound at this track — give it a bonus”</p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">epsilon_greedy</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">actions</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Select action using epsilon-greedy strategy.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>  <span class="c1"># Explore</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">q_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">Q</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">a</span><span class="p">),</span> <span class="mf">0.0</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">actions</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">actions</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q_values</span><span class="p">)]</span>  <span class="c1"># Exploit</span>


<span class="k">def</span><span class="w"> </span><span class="nf">softmax_action</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">temperature</span><span class="p">,</span> <span class="n">actions</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Select action using softmax (Boltzmann) exploration.&quot;&quot;&quot;</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">Q</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">a</span><span class="p">),</span> <span class="mf">0.0</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">actions</span><span class="p">])</span>
    <span class="c1"># Numerical stability</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="n">q_values</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">q_values</span><span class="p">)</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">q_values</span> <span class="o">/</span> <span class="n">temperature</span><span class="p">)</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">probs</span> <span class="o">/</span> <span class="n">probs</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">probs</span><span class="p">)</span>


<span class="c1"># Demonstrate the multi-armed bandit problem — the simplest explore/exploit scenario</span>
<span class="k">class</span><span class="w"> </span><span class="nc">MultiArmedBandit</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A simple multi-armed bandit with Gaussian rewards.&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_arms</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_arms</span> <span class="o">=</span> <span class="n">n_arms</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">true_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>  <span class="c1"># True reward means</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">pull</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arm</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Pull an arm, get noisy reward.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">true_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.5</span>


<span class="k">def</span><span class="w"> </span><span class="nf">run_bandit_experiment</span><span class="p">(</span><span class="n">n_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_runs</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare exploration strategies on a bandit problem.&quot;&quot;&quot;</span>
    <span class="n">strategies</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;ε=0 (pure greedy)&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s1">&#39;ε=0.01&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="s1">&#39;ε=0.1&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="s1">&#39;ε=0.5&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="p">}</span>
    
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_steps</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">strategies</span><span class="p">}</span>
    
    <span class="k">for</span> <span class="n">run</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_runs</span><span class="p">):</span>
        <span class="n">bandit</span> <span class="o">=</span> <span class="n">MultiArmedBandit</span><span class="p">(</span><span class="n">n_arms</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
        <span class="n">best_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">bandit</span><span class="o">.</span><span class="n">true_means</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">epsilon</span> <span class="ow">in</span> <span class="n">strategies</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>  <span class="c1"># Estimated values</span>
            <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>  <span class="c1"># Action counts</span>
            
            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
                <span class="c1"># Epsilon-greedy selection</span>
                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
                    <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
                
                <span class="n">reward</span> <span class="o">=</span> <span class="n">bandit</span><span class="o">.</span><span class="n">pull</span><span class="p">(</span><span class="n">arm</span><span class="p">)</span>
                <span class="n">N</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">Q</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">Q</span><span class="p">[</span><span class="n">arm</span><span class="p">])</span> <span class="o">/</span> <span class="n">N</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>  <span class="c1"># Running average</span>
                
                <span class="n">results</span><span class="p">[</span><span class="n">name</span><span class="p">][</span><span class="n">t</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
    
    <span class="c1"># Average over runs</span>
    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
        <span class="n">results</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">/=</span> <span class="n">n_runs</span>
    
    <span class="k">return</span> <span class="n">results</span>


<span class="n">bandit_results</span> <span class="o">=</span> <span class="n">run_bandit_experiment</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Bandit experiment complete!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualization-exploration-vs-exploitation">
<h3>Visualization: Exploration vs. Exploitation<a class="headerlink" href="#visualization-exploration-vs-exploitation" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Left: Average reward over time</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;#e74c3c&#39;</span><span class="p">,</span> <span class="s1">&#39;#f39c12&#39;</span><span class="p">,</span> <span class="s1">&#39;#2ecc71&#39;</span><span class="p">,</span> <span class="s1">&#39;#3498db&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">rewards</span><span class="p">),</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">bandit_results</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">colors</span><span class="p">):</span>
    <span class="c1"># Smooth with running average</span>
    <span class="n">window</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="n">smoothed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">window</span><span class="p">)</span><span class="o">/</span><span class="n">window</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">smoothed</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Step&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Average Reward&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Multi-Armed Bandit: Exploration Strategies&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Right: Epsilon decay schedule</span>
<span class="n">steps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">decay_schedules</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;Constant ε=0.1&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="s1">&#39;Linear decay&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">steps</span> <span class="o">/</span> <span class="mi">500</span><span class="p">),</span>
    <span class="s1">&#39;Exponential decay&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">steps</span> <span class="o">/</span> <span class="mi">200</span><span class="p">)),</span>
<span class="p">}</span>

<span class="k">for</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">schedule</span><span class="p">),</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">decay_schedules</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;#e74c3c&#39;</span><span class="p">,</span> <span class="s1">&#39;#2ecc71&#39;</span><span class="p">,</span> <span class="s1">&#39;#3498db&#39;</span><span class="p">]):</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">steps</span><span class="p">,</span> <span class="n">schedule</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Step&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Epsilon (exploration rate)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Common ε-Decay Schedules&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Key insight: ε=0.1 finds a good balance — enough exploration to find&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;the best arm, but not so much that it wastes pulls on bad arms.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Pure greedy (ε=0) often gets stuck on a suboptimal arm early on.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="temporal-difference-learning">
<h2>9. Temporal Difference Learning<a class="headerlink" href="#temporal-difference-learning" title="Link to this heading">#</a></h2>
<p>So far, our methods require knowing the environment’s transition dynamics <span class="math notranslate nohighlight">\(P(s'|s,a)\)</span>. In practice, the agent often doesn’t have this information — it must learn from experience.</p>
<p><strong>Temporal Difference (TD) learning</strong> bridges the gap between dynamic programming (which requires a model) and Monte Carlo methods (which require complete episodes).</p>
<section id="td-0-update-rule">
<h3>TD(0) Update Rule<a class="headerlink" href="#td-0-update-rule" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[V(s_t) \leftarrow V(s_t) + \alpha \left[r_t + \gamma V(s_{t+1}) - V(s_t)\right]\]</div>
<p>The term in brackets is the <strong>TD error</strong> <span class="math notranslate nohighlight">\(\delta_t\)</span>:</p>
<div class="math notranslate nohighlight">
\[\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)\]</div>
<p><strong>Intuition</strong>: The TD error measures how “surprised” the agent is. If the actual reward plus estimated future value is higher than expected, the TD error is positive, and we increase the value estimate.</p>
<p><strong>F1 analogy:</strong> The TD error is the gap between what the strategist <em>expected</em> and what <em>actually happened</em>. Before a pit stop, they predicted: “We’ll lose 22 seconds, come out P5, and the value of P5 at this tire age is X.” After the stop, they got an undercut and came out P4 — the TD error is positive. The strategist updates their model: “Pitting at that tire age is better than we thought.” Over many races, these lap-by-lap surprises refine the strategy model. This is learning from experience, not from a simulator.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Updates</p></th>
<th class="head"><p>Requires</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Dynamic Programming</strong></p></td>
<td><p>After full sweep of all states</p></td>
<td><p>Model of environment</p></td>
<td><p>Pre-race simulator with full track model</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Monte Carlo</strong></p></td>
<td><p>After complete episode</p></td>
<td><p>Complete episodes</p></td>
<td><p>Post-race review: “How did the whole race go?”</p></td>
</tr>
<tr class="row-even"><td><p><strong>TD Learning</strong></p></td>
<td><p>After each step</p></td>
<td><p>Only current transition</p></td>
<td><p>Lap-by-lap strategy updates during the race</p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">td_zero</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">n_episodes</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;TD(0) prediction: learn V(s) from experience using an ε-greedy policy.&quot;&quot;&quot;</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">visit_counts</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">td_errors</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_episodes</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">episode_errors</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>  <span class="c1"># Max steps per episode</span>
            <span class="c1"># ε-greedy action selection (using current V to estimate Q)</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">ACTIONS</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Greedy: pick action that leads to highest-value next state</span>
                <span class="n">action_values</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">ACTIONS</span><span class="p">:</span>
                    <span class="n">transitions</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">get_transitions</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
                    <span class="n">q</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">s_</span><span class="p">])</span> <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">s_</span><span class="p">,</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">transitions</span><span class="p">)</span>
                    <span class="n">action_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">ACTIONS</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">action_values</span><span class="p">)]</span>
            
            <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">visit_counts</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            
            <span class="c1"># TD(0) update</span>
            <span class="n">td_target</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">0</span> <span class="k">if</span> <span class="n">done</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">td_error</span> <span class="o">=</span> <span class="n">td_target</span> <span class="o">-</span> <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
            <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">td_error</span>
            
            <span class="n">episode_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">td_error</span><span class="p">))</span>
            
            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                <span class="c1"># Update terminal state values</span>
                <span class="k">if</span> <span class="n">env</span><span class="o">.</span><span class="n">grid</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">==</span> <span class="n">GridWorld</span><span class="o">.</span><span class="n">GOAL</span><span class="p">:</span>
                    <span class="n">V</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
                <span class="k">elif</span> <span class="n">env</span><span class="o">.</span><span class="n">grid</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">==</span> <span class="n">GridWorld</span><span class="o">.</span><span class="n">TRAP</span><span class="p">:</span>
                    <span class="n">V</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span>
                <span class="k">break</span>
            
            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
        
        <span class="k">if</span> <span class="n">episode_errors</span><span class="p">:</span>
            <span class="n">td_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">episode_errors</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="n">V</span><span class="p">),</span> <span class="n">td_errors</span>


<span class="n">V_td</span><span class="p">,</span> <span class="n">td_errors</span> <span class="o">=</span> <span class="n">td_zero</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">n_episodes</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="c1"># Compare TD-learned values with exact values</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TD(0) learned values vs. exact (value iteration):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;State&#39;</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;TD(0)&#39;</span><span class="si">:</span><span class="s2">&gt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Exact&#39;</span><span class="si">:</span><span class="s2">&gt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Diff&#39;</span><span class="si">:</span><span class="s2">&gt;8</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">36</span><span class="p">)</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">states</span><span class="p">):</span>
    <span class="n">td_val</span> <span class="o">=</span> <span class="n">V_td</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">exact_val</span> <span class="o">=</span> <span class="n">V_vi</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">td_val</span><span class="si">:</span><span class="s2">8.3f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">exact_val</span><span class="si">:</span><span class="s2">8.3f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">td_val</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">exact_val</span><span class="p">)</span><span class="si">:</span><span class="s2">8.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize TD learning convergence</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Left: TD error over episodes</span>
<span class="n">window</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">smoothed_errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">td_errors</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">window</span><span class="p">)</span><span class="o">/</span><span class="n">window</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">smoothed_errors</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#3498db&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Episode&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Average |TD Error|&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;TD(0) Learning: Error Convergence&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Right: Comparison scatter plot</span>
<span class="n">td_vals</span> <span class="o">=</span> <span class="p">[</span><span class="n">V_td</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">states</span><span class="p">]</span>
<span class="n">exact_vals</span> <span class="o">=</span> <span class="p">[</span><span class="n">V_vi</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">states</span><span class="p">]</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">exact_vals</span><span class="p">,</span> <span class="n">td_vals</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#2ecc71&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Perfect agreement&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Exact V(s) (Value Iteration)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Learned V(s) (TD(0))&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;TD(0) vs. Exact Values&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TD(0) learns values close to the exact solution, but from experience only!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;No knowledge of transition probabilities was needed.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="value-based-vs-policy-based-methods">
<h2>10. Value-Based vs. Policy-Based Methods<a class="headerlink" href="#value-based-vs-policy-based-methods" title="Link to this heading">#</a></h2>
<p>RL methods fall into two broad categories:</p>
<section id="value-based-methods">
<h3>Value-Based Methods<a class="headerlink" href="#value-based-methods" title="Link to this heading">#</a></h3>
<p>Learn a value function <span class="math notranslate nohighlight">\(V(s)\)</span> or <span class="math notranslate nohighlight">\(Q(s,a)\)</span>, then derive a policy from it.</p>
<ul class="simple">
<li><p>Examples: Q-learning, DQN, SARSA</p></li>
<li><p><strong>Pros</strong>: Sample efficient, stable convergence</p></li>
<li><p><strong>Cons</strong>: Can only handle discrete actions (without extensions)</p></li>
</ul>
<p><strong>F1 analogy:</strong> Like building a massive lookup table — “for every possible race situation, here’s how valuable each action is.” Then the strategist just picks the highest-value action. Works great when the situations are enumerable, but F1 has effectively infinite states.</p>
</section>
<section id="policy-based-methods">
<h3>Policy-Based Methods<a class="headerlink" href="#policy-based-methods" title="Link to this heading">#</a></h3>
<p>Learn the policy <span class="math notranslate nohighlight">\(\pi(a|s)\)</span> directly, without a value function.</p>
<ul class="simple">
<li><p>Examples: REINFORCE, PPO, A2C</p></li>
<li><p><strong>Pros</strong>: Handle continuous actions, can learn stochastic policies</p></li>
<li><p><strong>Cons</strong>: Higher variance, less sample efficient</p></li>
</ul>
<p><strong>F1 analogy:</strong> Like training a driver’s instincts directly — “in this situation, do this.” The driver doesn’t compute values; they’ve internalized the optimal response. This works for continuous decisions like steering angle and throttle modulation.</p>
</section>
<section id="actor-critic-methods">
<h3>Actor-Critic Methods<a class="headerlink" href="#actor-critic-methods" title="Link to this heading">#</a></h3>
<p>Combine both: an <strong>actor</strong> (policy) and a <strong>critic</strong> (value function).</p>
<ul class="simple">
<li><p>Examples: A2C, PPO, SAC</p></li>
<li><p><strong>Pros</strong>: Lower variance than pure policy methods, more flexible than pure value methods</p></li>
</ul>
<p><strong>F1 analogy:</strong> The driver (actor) makes real-time decisions, while the strategist on the pit wall (critic) evaluates how those decisions affect the race outcome. The driver learns from the strategist’s feedback, and the strategist refines their model from the driver’s results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualization: taxonomy of RL methods</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Taxonomy of RL Methods&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># RL root</span>
<span class="n">root</span> <span class="o">=</span> <span class="n">mpatches</span><span class="o">.</span><span class="n">FancyBboxPatch</span><span class="p">((</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">6.5</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">boxstyle</span><span class="o">=</span><span class="s2">&quot;round,pad=0.2&quot;</span><span class="p">,</span>
                                <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;#2c3e50&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">root</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="s1">&#39;Reinforcement Learning&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
        <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>

<span class="c1"># Three branches</span>
<span class="n">branches</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;Value-Based&#39;</span><span class="p">,</span> <span class="s1">&#39;#3498db&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;Q-Learning&#39;</span><span class="p">,</span> <span class="s1">&#39;DQN&#39;</span><span class="p">,</span> <span class="s1">&#39;SARSA&#39;</span><span class="p">]),</span>
    <span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;Actor-Critic&#39;</span><span class="p">,</span> <span class="s1">&#39;#9b59b6&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;A2C/A3C&#39;</span><span class="p">,</span> <span class="s1">&#39;PPO&#39;</span><span class="p">,</span> <span class="s1">&#39;SAC&#39;</span><span class="p">]),</span>
    <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;Policy-Based&#39;</span><span class="p">,</span> <span class="s1">&#39;#e74c3c&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;REINFORCE&#39;</span><span class="p">,</span> <span class="s1">&#39;TRPO&#39;</span><span class="p">,</span> <span class="s1">&#39;ES&#39;</span><span class="p">]),</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">color</span><span class="p">,</span> <span class="n">methods</span> <span class="ow">in</span> <span class="n">branches</span><span class="p">:</span>
    <span class="n">box</span> <span class="o">=</span> <span class="n">mpatches</span><span class="o">.</span><span class="n">FancyBboxPatch</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">boxstyle</span><span class="o">=</span><span class="s2">&quot;round,pad=0.2&quot;</span><span class="p">,</span>
                                   <span class="n">facecolor</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">box</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">y</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
            <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>
    
    <span class="c1"># Connect to root</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="n">x</span> <span class="o">+</span> <span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.5</span><span class="p">,</span> <span class="n">y</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
    
    <span class="c1"># Method labels</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">method</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">methods</span><span class="p">):</span>
        <span class="n">my</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="mf">0.7</span> <span class="o">-</span> <span class="n">i</span> <span class="o">*</span> <span class="mf">0.6</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">my</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;• </span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
                <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>

<span class="c1"># Annotations</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;Learn Q(s,a)</span><span class="se">\n</span><span class="s1">Derive policy&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
        <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;italic&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span>
        <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s1">&#39;round&#39;</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;#ecf0f1&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;Learn both V(s)</span><span class="se">\n</span><span class="s1">and π(a|s)&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
        <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;italic&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span>
        <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s1">&#39;round&#39;</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;#ecf0f1&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">9.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;Learn π(a|s)</span><span class="se">\n</span><span class="s1">directly&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
        <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;italic&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span>
        <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s1">&#39;round&#39;</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;#ecf0f1&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;In the next notebooks:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  NB18: Q-Learning &amp; DQN (value-based)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  NB19: REINFORCE &amp; Actor-Critic (policy-based)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  NB20: PPO &amp; Modern RL (actor-critic, RLHF)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="connecting-rl-to-llm-alignment">
<h2>11. Connecting RL to LLM Alignment<a class="headerlink" href="#connecting-rl-to-llm-alignment" title="Link to this heading">#</a></h2>
<p>In Notebook 21, we introduced <strong>RLHF</strong> (Reinforcement Learning from Human Feedback). Now you can see how it fits the RL framework:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>RL Concept</p></th>
<th class="head"><p>RLHF for LLMs</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Agent</strong></p></td>
<td><p>The language model</p></td>
<td><p>Driver + strategist</p></td>
</tr>
<tr class="row-odd"><td><p><strong>State</strong></p></td>
<td><p>The prompt + tokens generated so far</p></td>
<td><p>Current race situation</p></td>
</tr>
<tr class="row-even"><td><p><strong>Action</strong></p></td>
<td><p>Choosing the next token</p></td>
<td><p>Pit now, push, conserve</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Policy</strong></p></td>
<td><p>The model’s probability distribution over tokens</p></td>
<td><p>Race strategy mapping</p></td>
</tr>
<tr class="row-even"><td><p><strong>Reward</strong></p></td>
<td><p>Score from a trained reward model (human preferences)</p></td>
<td><p>Championship points, positions gained</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Environment</strong></p></td>
<td><p>The token generation process</p></td>
<td><p>The race: track, rivals, weather</p></td>
</tr>
</tbody>
</table>
</div>
<p>The <strong>PPO</strong> algorithm (Notebook 25) is the standard method for this optimization — it updates the LLM’s policy to maximize the reward model’s scores while staying close to the original model (to prevent degradation).</p>
<p>This is the bridge between everything we’ve learned about language models and the RL techniques we’ll explore in this part of the curriculum.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Quick simulation: how RL improves a &quot;language model&quot;</span>
<span class="c1"># Simplified example with discrete token choices</span>

<span class="k">def</span><span class="w"> </span><span class="nf">simulate_rlhf_analogy</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Simulate how RL can steer a policy toward higher-reward outputs.&quot;&quot;&quot;</span>
    <span class="c1"># Pretend we have 5 possible response &quot;styles&quot; with different reward scores</span>
    <span class="n">styles</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Verbose &amp; Vague&#39;</span><span class="p">,</span> <span class="s1">&#39;Concise &amp; Clear&#39;</span><span class="p">,</span> <span class="s1">&#39;Rude &amp; Brief&#39;</span><span class="p">,</span> 
              <span class="s1">&#39;Helpful &amp; Detailed&#39;</span><span class="p">,</span> <span class="s1">&#39;Off-topic&#39;</span><span class="p">]</span>
    <span class="n">true_rewards</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">]</span>  <span class="c1"># Human preference scores</span>
    
    <span class="c1"># Initial policy: uniform over styles</span>
    <span class="n">policy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="o">/</span> <span class="mi">5</span>
    <span class="n">policy_history</span> <span class="o">=</span> <span class="p">[</span><span class="n">policy</span><span class="o">.</span><span class="n">copy</span><span class="p">()]</span>
    
    <span class="c1"># Simple policy gradient update (simplified)</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.3</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
        <span class="c1"># Sample an action from policy</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">policy</span><span class="p">)</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">true_rewards</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.1</span>
        
        <span class="c1"># Update: increase probability of rewarded actions</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">gradient</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward</span>
        
        <span class="c1"># Softmax update</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">policy</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span> <span class="o">+</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradient</span>
        <span class="n">policy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">policy_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
    
    <span class="k">return</span> <span class="n">styles</span><span class="p">,</span> <span class="n">true_rewards</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">policy_history</span><span class="p">)</span>


<span class="n">styles</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">policy_hist</span> <span class="o">=</span> <span class="n">simulate_rlhf_analogy</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;#e74c3c&#39;</span><span class="p">,</span> <span class="s1">&#39;#2ecc71&#39;</span><span class="p">,</span> <span class="s1">&#39;#e67e22&#39;</span><span class="p">,</span> <span class="s1">&#39;#3498db&#39;</span><span class="p">,</span> <span class="s1">&#39;#95a5a6&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">style</span><span class="p">,</span> <span class="n">color</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">styles</span><span class="p">,</span> <span class="n">colors</span><span class="p">)):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">policy_hist</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">style</span><span class="si">}</span><span class="s1"> (r=</span><span class="si">{</span><span class="n">rewards</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">,</span> 
            <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;RL Update Step&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Policy Probability&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;How RL Steers a Model Toward Better Outputs&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;center left&#39;</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The policy gradually shifts probability toward &#39;Helpful &amp; Detailed&#39;&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;and &#39;Concise &amp; Clear&#39; — the responses humans prefer.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;This is the core idea behind RLHF!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<section id="exercise-1-custom-gridworld-the-monaco-grand-prix">
<h3>Exercise 1: Custom Gridworld (The Monaco Grand Prix)<a class="headerlink" href="#exercise-1-custom-gridworld-the-monaco-grand-prix" title="Link to this heading">#</a></h3>
<p>Create a 5x5 gridworld that represents a simplified Monaco street circuit — multiple goals (podium positions) and traps (barriers/DNF zones). Run value iteration and visualize the optimal policy. Experiment with different discount factors (gamma = 0.5, 0.9, 0.99) and observe how the policy changes. How does a short-sighted agent (low gamma) differ from a far-sighted one (high gamma) — does the far-sighted agent take longer detours to avoid traps?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exercise 1: Your code here</span>
<span class="c1"># Hint: Modify the GridWorld class to accept a custom grid layout</span>
<span class="c1"># Then run value_iteration with different gamma values and compare</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-2-monte-carlo-vs-td-0-post-race-review-vs-lap-by-lap-learning">
<h3>Exercise 2: Monte Carlo vs. TD(0) — Post-Race Review vs. Lap-by-Lap Learning<a class="headerlink" href="#exercise-2-monte-carlo-vs-td-0-post-race-review-vs-lap-by-lap-learning" title="Link to this heading">#</a></h3>
<p>Implement first-visit Monte Carlo prediction alongside TD(0) for the same gridworld. Compare their convergence rates and final value estimates. Monte Carlo is like doing a full post-race debrief — you wait until the checkered flag, then review the whole race. TD(0) is like the strategist updating their model after every single lap. Which converges faster? Which has lower variance?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exercise 2: Your code here</span>
<span class="c1"># Hint: Monte Carlo waits until the end of an episode to update V(s)</span>
<span class="c1"># using the actual return G_t, while TD(0) updates after each step</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-3-ucb-exploration-the-untested-tire-compound">
<h3>Exercise 3: UCB Exploration — The Untested Tire Compound<a class="headerlink" href="#exercise-3-ucb-exploration-the-untested-tire-compound" title="Link to this heading">#</a></h3>
<p>Implement Upper Confidence Bound (UCB) exploration for the multi-armed bandit problem and compare it against epsilon-greedy. In F1 terms, UCB adds a “curiosity bonus” for strategies that haven’t been tried much — like giving extra optimism to a tire compound you’ve never raced with at this circuit. The less data you have, the bigger the bonus.</p>
<p>UCB selects:</p>
<div class="math notranslate nohighlight">
\[a_t = \arg\max_a \left[Q(a) + c\sqrt{\frac{\ln t}{N(a)}}\right]\]</div>
<p>where <span class="math notranslate nohighlight">\(c\)</span> controls exploration strength and <span class="math notranslate nohighlight">\(N(a)\)</span> is the number of times action <span class="math notranslate nohighlight">\(a\)</span> has been selected.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exercise 3: Your code here</span>
<span class="c1"># Hint: The UCB bonus term goes to infinity for unvisited actions,</span>
<span class="c1"># ensuring every action is tried at least once</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<section id="key-concepts">
<h3>Key Concepts<a class="headerlink" href="#key-concepts" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Concept</p></th>
<th class="head"><p>What It Means</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Reinforcement Learning</strong></p></td>
<td><p>Agent learns to maximize cumulative reward through interaction</p></td>
<td><p>Driver/strategist learns optimal decisions across races</p></td>
</tr>
<tr class="row-odd"><td><p><strong>MDPs</strong></p></td>
<td><p>Formalize RL with states, actions, transitions, rewards, discount</p></td>
<td><p>Race as a Markov process: position, tires, gaps, weather</p></td>
</tr>
<tr class="row-even"><td><p><strong>Bellman equation</strong></p></td>
<td><p>Value = immediate reward + discounted future value</p></td>
<td><p>Today’s position value = this lap’s gain + remaining race value</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Policy evaluation</strong></p></td>
<td><p>Compute how good a policy is</p></td>
<td><p>Assess: “How many points does a one-stop strategy average?”</p></td>
</tr>
<tr class="row-even"><td><p><strong>Policy improvement</strong></p></td>
<td><p>Greedily make the policy better</p></td>
<td><p>Switch to two-stop when evaluation shows it scores higher</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Value iteration</strong></p></td>
<td><p>Combine evaluation and improvement in one step</p></td>
<td><p>Lap-by-lap strategy optimization</p></td>
</tr>
<tr class="row-even"><td><p><strong>Exploration vs. exploitation</strong></p></td>
<td><p>Try new things or stick with what works</p></td>
<td><p>New undercut strategy vs. proven conservative approach</p></td>
</tr>
<tr class="row-odd"><td><p><strong>TD learning</strong></p></td>
<td><p>Learn from experience without a model</p></td>
<td><p>Update strategy beliefs after each lap, not just post-race</p></td>
</tr>
<tr class="row-even"><td><p><strong>Value-based vs. policy-based</strong></p></td>
<td><p>Learn Q-values vs. learn policy directly</p></td>
<td><p>Lookup table vs. driver instinct</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="fundamental-insight">
<h3>Fundamental Insight<a class="headerlink" href="#fundamental-insight" title="Link to this heading">#</a></h3>
<p>The Bellman equation transforms the RL problem from “predict the infinite future” into “look one step ahead and use your current estimate.” This simple recursive trick — combined with sufficient exploration — is powerful enough to learn optimal behavior in complex environments. In F1 terms, the Bellman equation says you don’t need to simulate the entire remaining race — just evaluate the next lap and trust your value estimates for the rest.</p>
</section>
</section>
<hr class="docutils" />
<section id="next-steps">
<h2>Next Steps<a class="headerlink" href="#next-steps" title="Link to this heading">#</a></h2>
<p>Now that we understand the RL framework, value functions, and the Bellman equation, we’re ready to build <strong>practical RL agents</strong>. In <strong>Notebook 23: Q-Learning &amp; Deep Q-Networks</strong>, we’ll:</p>
<ul class="simple">
<li><p>Implement tabular Q-learning (model-free control) — learning optimal pit stop timing from experience alone</p></li>
<li><p>Scale to function approximation with neural networks (DQN) — a deep network learning strategy from thousands of simulated races</p></li>
<li><p>Learn key techniques: experience replay and target networks</p></li>
<li><p>Train a DQN agent to solve a control task from raw observations</p></li>
</ul>
<p>The journey from Bellman equations to DQN is one of the most elegant progressions in all of machine learning — and it mirrors how F1 strategy has evolved from gut instinct to simulation-driven decision-making.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="21_finetuning_and_peft.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Part 6.5: Fine-tuning &amp; PEFT</p>
      </div>
    </a>
    <a class="right-next"
       href="23_q_learning_dqn.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Part 7.2: Q-Learning and Deep Q-Networks</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-reinforcement-learning-paradigm">1. The Reinforcement Learning Paradigm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-agent-environment-loop">The Agent-Environment Loop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-the-rl-loop">Visualization: The RL Loop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-rl-terminology">Key RL Terminology</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-decision-processes-mdps">2. Markov Decision Processes (MDPs)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-markov-property">The Markov Property</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuitive-explanation">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-f1-race-as-an-mdp">The F1 Race as an MDP</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-gridworld-mdp">Building a Gridworld MDP</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-the-gridworld">Visualization: The Gridworld</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#returns-and-discounting">3. Returns and Discounting</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-discount">Why discount?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-effect-of-discount-factor">Visualization: Effect of Discount Factor</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#value-functions-and-the-bellman-equation">4. Value Functions and the Bellman Equation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#state-value-function-v-s">State-Value Function V(s)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#action-value-function-q-s-a">Action-Value Function Q(s, a)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bellman-equation">The Bellman Equation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-why-the-bellman-equation-matters">Deep Dive: Why the Bellman Equation Matters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-bellman-equation-backup-diagram">Visualization: Bellman Equation Backup Diagram</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-evaluation">5. Policy Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-convergence-of-policy-evaluation">Visualization: Convergence of Policy Evaluation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-improvement-and-policy-iteration">6. Policy Improvement and Policy Iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#value-iteration">7. Value Iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exploration-vs-exploitation">8. Exploration vs. Exploitation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-exploration-strategies">Common Exploration Strategies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-exploration-vs-exploitation">Visualization: Exploration vs. Exploitation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#temporal-difference-learning">9. Temporal Difference Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#td-0-update-rule">TD(0) Update Rule</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#value-based-vs-policy-based-methods">10. Value-Based vs. Policy-Based Methods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#value-based-methods">Value-Based Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-based-methods">Policy-Based Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#actor-critic-methods">Actor-Critic Methods</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#connecting-rl-to-llm-alignment">11. Connecting RL to LLM Alignment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-custom-gridworld-the-monaco-grand-prix">Exercise 1: Custom Gridworld (The Monaco Grand Prix)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-monte-carlo-vs-td-0-post-race-review-vs-lap-by-lap-learning">Exercise 2: Monte Carlo vs. TD(0) — Post-Race Review vs. Lap-by-Lap Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-ucb-exploration-the-untested-tire-compound">Exercise 3: UCB Exploration — The Untested Tire Compound</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts">Key Concepts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamental-insight">Fundamental Insight</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">Next Steps</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dan Shah
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>