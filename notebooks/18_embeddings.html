
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Part 6.2: Embeddings &#8212; Foundations of AI</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=9c3e77be" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=1ae7504c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/18_embeddings';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Part 6.3: Tokenization &amp; Language Model Training" href="19_tokenization_lm_training.html" />
    <link rel="prev" title="Part 6.1: Transformer Architecture" href="17_transformer_architecture.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Foundations of AI</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 1: Mathematical Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_linear_algebra.html">Part 1.1: Linear Algebra for Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_calculus.html">Part 1.2: Calculus for Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_probability_statistics.html">Part 1.3: Probability &amp; Statistics for Deep Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 2: Programming Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_python_oop.html">Part 2.1: Python OOP for Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_numpy_deep_dive.html">Part 2.2: NumPy Deep Dive</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 3: Classical ML &amp; Optimization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="06_classical_ml.html">Part 3.1: Classical Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_optimization_linear_programming.html">Part 3.2: Optimization &amp; Linear Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_optimization_theory.html">Part 3.3: Optimization Theory for Machine Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 4: Neural Network Fundamentals</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="09_perceptrons_basic_networks.html">Part 4.1: Perceptrons &amp; Basic Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_backpropagation.html">Part 4.2: Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_pytorch_fundamentals.html">Part 4.3: PyTorch Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_training_deep_networks.html">Part 4.4: Training Deep Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 5: Neural Network Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="13_convolutional_neural_networks.html">Part 5.1: Convolutional Neural Networks (CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_computer_vision_depth.html">Part 5.2: Computer Vision — Beyond Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_recurrent_neural_networks.html">Part 5.3: Recurrent Neural Networks (RNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_attention_mechanisms.html">Part 5.4: Attention Mechanisms</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 6: Transformers &amp; LLMs</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="17_transformer_architecture.html">Part 6.1: Transformer Architecture</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Part 6.2: Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="19_tokenization_lm_training.html">Part 6.3: Tokenization &amp; Language Model Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="20_language_models.html">Part 6.4: Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="21_finetuning_and_peft.html">Part 6.5: Fine-tuning &amp; PEFT</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 7: Reinforcement Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="22_rl_fundamentals.html">Part 7.1: Reinforcement Learning Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="23_q_learning_dqn.html">Part 7.2: Q-Learning and Deep Q-Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="24_policy_gradients.html">Part 7.3: Policy Gradient Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="25_ppo_modern_rl.html">Part 7.4: PPO and Modern RL</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 8: Applied AI Systems</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="26_rag.html">Part 8.1: Retrieval-Augmented Generation (RAG)</a></li>
<li class="toctree-l1"><a class="reference internal" href="27_ai_agents.html">Part 8.2: AI Agents and Tool Use</a></li>
<li class="toctree-l1"><a class="reference internal" href="28_ai_evals.html">Part 8.3: Evaluating AI Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="29_production_monitoring.html">Part 8.4: Production AI Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 9: Advanced Topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="30_inference_optimization.html">Part 9.1: LLM Inference Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="31_ml_systems.html">Part 9.2: ML Systems &amp; Experiment Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="32_multimodal_ai.html">Part 9.3: Multimodal AI</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/dan-shah/foundations-of-ai/blob/main/notebooks/18_embeddings.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/dan-shah/foundations-of-ai" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/dan-shah/foundations-of-ai/edit/main/notebooks/18_embeddings.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/dan-shah/foundations-of-ai/issues/new?title=Issue%20on%20page%20%2Fnotebooks/18_embeddings.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notebooks/18_embeddings.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Part 6.2: Embeddings</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-embeddings">1. Why Embeddings?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem-with-one-hot-encoding">The Problem with One-Hot Encoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-one-hot-vs-dense-embeddings">Visualization: One-Hot vs Dense Embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-distributional-hypothesis">The Distributional Hypothesis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-why-embeddings-matter">Deep Dive: Why Embeddings Matter</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#key-insight">Key Insight</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#common-misconceptions">Common Misconceptions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word2vec">2. Word2Vec</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuitive-explanation">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-skip-gram-vs-cbow">Visualization: Skip-gram vs CBOW</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-word2vec-skip-gram-from-scratch">Implementing Word2Vec (Skip-gram) from Scratch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-learned-word-embeddings-in-2d">Visualization: Learned Word Embeddings in 2D</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-magic-vector-arithmetic">The Magic: Vector Arithmetic</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-vector-arithmetic-in-embedding-space">Visualization: Vector Arithmetic in Embedding Space</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-why-does-word2vec-capture-semantics">Deep Dive: Why Does Word2Vec Capture Semantics?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Key Insight</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-negative-sampling-trick">The Negative Sampling Trick</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#glove-global-vectors-for-word-representation">3. GloVe (Global Vectors for Word Representation)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#glove-building-the-co-occurrence-matrix">GloVe: Building the Co-occurrence Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-word2vec-vs-glove">Comparison: Word2Vec vs GloVe</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modern-embeddings-from-static-to-contextual">4. Modern Embeddings: From Static to Contextual</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-contextual-embeddings-work">How Contextual Embeddings Work</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-static-vs-contextual-embeddings">Visualization: Static vs Contextual Embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sentence-embeddings-from-words-to-sentences">Sentence Embeddings: From Words to Sentences</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-the-evolution-of-embeddings">Deep Dive: The Evolution of Embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Key Insight</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#similarity-and-distance">5. Similarity and Distance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-comparing-similarity-measures">Visualization: Comparing Similarity Measures</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use-which">When to Use Which?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interactive-exploration-similarity-in-embedding-space">Interactive Exploration: Similarity in Embedding Space</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-databases-and-retrieval">6. Vector Databases and Retrieval</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-not-just-use-a-regular-database">Why Not Just Use a Regular Database?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approximate-nearest-neighbors-ann">Approximate Nearest Neighbors (ANN)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-simple-semantic-search-engine-from-scratch">Building a Simple Semantic Search Engine from Scratch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-embedding-space-with-query-and-retrieved-results">Visualization: Embedding Space with Query and Retrieved Results</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-vector-database-ecosystem">The Vector Database Ecosystem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-retrieval-augmented-generation">RAG: Retrieval-Augmented Generation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-applications">7. Practical Applications</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-this-matters-in-machine-learning">Why This Matters in Machine Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-clustering-in-embedding-space">Visualization: Clustering in Embedding Space</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-implement-cosine-similarity-from-scratch">Exercise 1: Implement Cosine Similarity from Scratch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-implement-cbow-word2vec">Exercise 2: Implement CBOW Word2Vec</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-build-an-anomaly-detector-using-embeddings">Exercise 3: Build an Anomaly Detector Using Embeddings</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts">Key Concepts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-deep-learning">Connection to Deep Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checklist">Checklist</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">Next Steps</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="part-6-2-embeddings">
<h1>Part 6.2: Embeddings<a class="headerlink" href="#part-6-2-embeddings" title="Link to this heading">#</a></h1>
<p>Embeddings are the <strong>bridge between human concepts and machine learning</strong>. Every word you type into ChatGPT, every product recommendation you receive, every song Spotify suggests – all of these rely on embeddings to represent meaning as numbers. The core idea is deceptively simple: represent things as dense vectors in a space where <strong>similar things are close together</strong>. This single idea has transformed NLP, recommendation systems, search engines, and much of modern AI.</p>
<p><strong>F1 analogy:</strong> Imagine representing every F1 driver as a vector of numbers – <code class="docutils literal notranslate"><span class="pre">[race_craft,</span> <span class="pre">tire_management,</span> <span class="pre">wet_weather,</span> <span class="pre">qualifying_pace,</span> <span class="pre">consistency,</span> <span class="pre">overtaking]</span></code>. Drivers with similar styles would cluster together in this space. Verstappen and Schumacher might be nearby (dominant champions), while Hamilton and Prost might cluster (smooth, strategic drivers). Embeddings do this automatically, learning the right dimensions from data rather than having an engineer define them.</p>
<p>In this notebook, you’ll build embeddings from scratch, see how vector arithmetic can capture analogies like “king - man + woman = queen,” implement semantic search, and understand why embeddings are the foundation of virtually every modern AI system.</p>
<hr class="docutils" />
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<p>By the end of this notebook, you should be able to:</p>
<ul class="simple">
<li><p>[ ] Explain why one-hot encoding fails and why dense embeddings are needed</p></li>
<li><p>[ ] Describe the distributional hypothesis and how it motivates embedding methods</p></li>
<li><p>[ ] Implement Word2Vec (skip-gram) from scratch in PyTorch</p></li>
<li><p>[ ] Perform and visualize vector arithmetic on word embeddings</p></li>
<li><p>[ ] Compare Word2Vec and GloVe approaches</p></li>
<li><p>[ ] Explain the difference between static and contextual embeddings</p></li>
<li><p>[ ] Compute cosine similarity, Euclidean distance, and dot product similarity</p></li>
<li><p>[ ] Build a simple semantic search engine from scratch</p></li>
<li><p>[ ] Explain the concept of RAG (Retrieval-Augmented Generation)</p></li>
<li><p>[ ] Describe practical applications of embeddings across domains</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">Counter</span><span class="p">,</span> <span class="n">defaultdict</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn-v0_8-whitegrid&#39;</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="why-embeddings">
<h2>1. Why Embeddings?<a class="headerlink" href="#why-embeddings" title="Link to this heading">#</a></h2>
<section id="the-problem-with-one-hot-encoding">
<h3>The Problem with One-Hot Encoding<a class="headerlink" href="#the-problem-with-one-hot-encoding" title="Link to this heading">#</a></h3>
<p>Imagine you have a vocabulary of 50,000 words. The simplest way to represent each word as a number is <strong>one-hot encoding</strong>: give each word a vector of length 50,000 where exactly one position is 1 and the rest are 0.</p>
<p>This seems fine at first, but it has three devastating problems:</p>
<ol class="arabic simple">
<li><p><strong>Sparse and high-dimensional:</strong> Each vector has 49,999 zeros and a single 1. This wastes enormous memory and computation.</p></li>
<li><p><strong>No notion of similarity:</strong> The one-hot vectors for “cat” and “kitten” are just as different as “cat” and “refrigerator.” Every word is equally distant from every other word.</p></li>
<li><p><strong>No generalization:</strong> If a model learns something about “cat,” that knowledge tells it absolutely nothing about “kitten.”</p></li>
</ol>
<p><strong>The key insight:</strong> What if we could represent each word as a short, dense vector (say, 300 numbers) where words with similar meanings are close together in space? That is exactly what embeddings do.</p>
<p><strong>F1 analogy:</strong> One-hot encoding drivers would mean Verstappen = <code class="docutils literal notranslate"><span class="pre">[1,0,0,...,0]</span></code>, Hamilton = <code class="docutils literal notranslate"><span class="pre">[0,1,0,...,0]</span></code>, Norris = <code class="docutils literal notranslate"><span class="pre">[0,0,1,...,0]</span></code>. Every driver is equally “far” from every other – Verstappen is as different from Hamilton as from a Formula E driver. But a dense “driver embedding” like <code class="docutils literal notranslate"><span class="pre">[speed:</span> <span class="pre">0.95,</span> <span class="pre">racecraft:</span> <span class="pre">0.92,</span> <span class="pre">consistency:</span> <span class="pre">0.97,</span> <span class="pre">wet_skill:</span> <span class="pre">0.88]</span></code> captures meaningful relationships. Similar drivers are nearby, and knowledge about one transfers to similar others.</p>
</section>
<section id="visualization-one-hot-vs-dense-embeddings">
<h3>Visualization: One-Hot vs Dense Embeddings<a class="headerlink" href="#visualization-one-hot-vs-dense-embeddings" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compare one-hot encoding vs dense embeddings</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># === Left: One-hot encoding ===</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;kitten&#39;</span><span class="p">,</span> <span class="s1">&#39;dog&#39;</span><span class="p">,</span> <span class="s1">&#39;puppy&#39;</span><span class="p">,</span> <span class="s1">&#39;car&#39;</span><span class="p">,</span> <span class="s1">&#39;truck&#39;</span><span class="p">]</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

<span class="c1"># Create one-hot matrix</span>
<span class="n">one_hot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">one_hot</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39;dim </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;One-Hot Encoding</span><span class="se">\n</span><span class="s1">(Sparse, No Similarity)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Dimensions&#39;</span><span class="p">)</span>

<span class="c1"># Annotate values</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">):</span>
        <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;white&#39;</span> <span class="k">if</span> <span class="n">one_hot</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="k">else</span> <span class="s1">&#39;black&#39;</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">one_hot</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="si">:</span><span class="s1">.0f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> 
                <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># === Right: Dense embeddings ===</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="c1"># Simulate meaningful dense embeddings where similar words are close</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">],</span>   <span class="c1"># cat</span>
    <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">],</span>   <span class="c1"># kitten (close to cat)</span>
    <span class="p">[</span><span class="mf">0.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6</span><span class="p">],</span>  <span class="c1"># dog</span>
    <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">],</span>  <span class="c1"># puppy (close to dog)</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span>   <span class="c1"># car</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>  <span class="c1"># truck (close to car)</span>
<span class="p">])</span>

<span class="n">im2</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdBu_r&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;dim 0&#39;</span><span class="p">,</span> <span class="s1">&#39;dim 1&#39;</span><span class="p">,</span> <span class="s1">&#39;dim 2&#39;</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Dense Embeddings</span><span class="se">\n</span><span class="s1">(Compact, Similar = Close)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Dimensions&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

<span class="c1"># Annotate values</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">embeddings</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> 
                <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Show similarity comparison</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Cosine Similarities ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">One-hot encoding:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">)]:</span>
    <span class="n">sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">one_hot</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">one_hot</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">one_hot</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">one_hot</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">words</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">8s</span><span class="si">}</span><span class="s2"> &lt;-&gt; </span><span class="si">{</span><span class="n">words</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="si">:</span><span class="s2">8s</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">sim</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Dense embeddings:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">)]:</span>
    <span class="n">sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">words</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">8s</span><span class="si">}</span><span class="s2"> &lt;-&gt; </span><span class="si">{</span><span class="n">words</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="si">:</span><span class="s2">8s</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">sim</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-distributional-hypothesis">
<h3>The Distributional Hypothesis<a class="headerlink" href="#the-distributional-hypothesis" title="Link to this heading">#</a></h3>
<blockquote>
<div><p><em>“You shall know a word by the company it keeps.”</em> – J.R. Firth (1957)</p>
</div></blockquote>
<p>This is the foundational insight behind all embedding methods. Words that appear in similar contexts tend to have similar meanings:</p>
<ul class="simple">
<li><p>“The <strong>cat</strong> sat on the mat” / “The <strong>kitten</strong> sat on the mat”</p></li>
<li><p>“I drove my <strong>car</strong> to work” / “I drove my <strong>truck</strong> to work”</p></li>
</ul>
<p>If two words frequently appear in the same contexts (surrounded by the same neighboring words), they probably mean similar things. This is what embedding algorithms learn to capture: they convert co-occurrence patterns into geometric relationships.</p>
<p><strong>F1 analogy:</strong> The distributional hypothesis in F1 terms: “You shall know a driver by the results they produce in similar conditions.” Drivers who consistently produce similar results at similar circuits, in similar weather, against similar competition, will end up with similar embeddings – even if we never explicitly told the model they are similar. Verstappen and peak-Hamilton both dominate wet races and street circuits, so their embeddings converge, learned purely from performance data.</p>
</section>
<section id="deep-dive-why-embeddings-matter">
<h3>Deep Dive: Why Embeddings Matter<a class="headerlink" href="#deep-dive-why-embeddings-matter" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Problem with One-Hot</p></th>
<th class="head"><p>How Embeddings Fix It</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Vectors are huge (vocab_size dimensions)</p></td>
<td><p>Vectors are small (50-1000 dimensions)</p></td>
<td><p>20-driver one-hot vs. compact performance profile</p></td>
</tr>
<tr class="row-odd"><td><p>All words are equidistant</p></td>
<td><p>Similar words are nearby</p></td>
<td><p>All drivers equally different vs. similar styles cluster</p></td>
</tr>
<tr class="row-even"><td><p>No generalization between words</p></td>
<td><p>Knowledge transfers between similar words</p></td>
<td><p>Learning about Verstappen helps predict Leclerc’s behavior</p></td>
</tr>
<tr class="row-odd"><td><p>Memory scales as O(V^2) for pairwise ops</p></td>
<td><p>Memory scales as O(V * d) where d &lt;&lt; V</p></td>
<td><p>Efficient comparison of any two drivers</p></td>
</tr>
<tr class="row-even"><td><p>Cannot capture relationships</p></td>
<td><p>Vector arithmetic captures analogies</p></td>
<td><p>“Leclerc - Ferrari + McLaren = Norris”</p></td>
</tr>
</tbody>
</table>
</div>
<section id="key-insight">
<h4>Key Insight<a class="headerlink" href="#key-insight" title="Link to this heading">#</a></h4>
<p>Embeddings transform <strong>discrete symbols</strong> (words, products, users) into <strong>continuous vectors</strong> in a learned space. This is what makes gradient-based optimization possible – you cannot take the gradient of a one-hot vector, but you can take the gradient of a dense embedding and update it during training.</p>
</section>
<section id="common-misconceptions">
<h4>Common Misconceptions<a class="headerlink" href="#common-misconceptions" title="Link to this heading">#</a></h4>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Misconception</p></th>
<th class="head"><p>Reality</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Embeddings are hand-designed features</p></td>
<td><p>They are <strong>learned</strong> from data</p></td>
</tr>
<tr class="row-odd"><td><p>Each dimension has a clear meaning</p></td>
<td><p>Dimensions are usually not individually interpretable</p></td>
</tr>
<tr class="row-even"><td><p>Embeddings are only for words</p></td>
<td><p>They work for anything: users, products, genes, molecules</p></td>
</tr>
<tr class="row-odd"><td><p>Bigger embeddings are always better</p></td>
<td><p>There is an optimal size; too large overfits, too small underfits</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
</section>
<hr class="docutils" />
<section id="word2vec">
<h2>2. Word2Vec<a class="headerlink" href="#word2vec" title="Link to this heading">#</a></h2>
<section id="intuitive-explanation">
<h3>Intuitive Explanation<a class="headerlink" href="#intuitive-explanation" title="Link to this heading">#</a></h3>
<p>Word2Vec (Mikolov et al., 2013) was a breakthrough: a simple neural network that learns word embeddings by predicting context. The core idea has two flavors:</p>
<p><strong>Skip-gram:</strong> Given a center word, predict the surrounding context words.</p>
<ul class="simple">
<li><p>Input: “cat” -&gt; Predict: “the”, “sat”, “on”, “mat”</p></li>
</ul>
<p><strong>CBOW (Continuous Bag of Words):</strong> Given surrounding context words, predict the center word.</p>
<ul class="simple">
<li><p>Input: “the”, “sat”, “on”, “mat” -&gt; Predict: “cat”</p></li>
</ul>
<p><strong>Why does this work?</strong> If “cat” and “kitten” both predict similar context words (“sat,” “purring,” “fur”), then the model is forced to give them similar embeddings. The training objective implicitly encodes semantic similarity.</p>
<p><strong>The surprising result:</strong> The learned embeddings capture not just similarity, but <strong>relational structure</strong>. The vector difference between “king” and “man” is approximately the same as between “queen” and “woman.” This means:</p>
<div class="math notranslate nohighlight">
\[\text{king} - \text{man} + \text{woman} \approx \text{queen}\]</div>
<p>This was one of the most shocking results in NLP history.</p>
<p><strong>F1 analogy:</strong> Word2Vec applied to F1 would learn <strong>driver embeddings</strong> where similar drivers cluster together. If you trained on race reports and commentary, drivers who appear in similar contexts (“dominated the race,” “set the fastest lap,” “led from pole”) would get similar embeddings. The model would learn, without being told, that Verstappen, Hamilton, and Schumacher belong in a “dominant champion” cluster, while Norris, Leclerc, and Russell form a “rising star” cluster. And the arithmetic works too:</p>
<div class="math notranslate nohighlight">
\[\vec{\text{Leclerc}} - \vec{\text{Ferrari}} + \vec{\text{McLaren}} \approx \vec{\text{Norris}}\]</div>
<p>(the “star young driver” concept transfers between teams)</p>
</section>
<section id="visualization-skip-gram-vs-cbow">
<h3>Visualization: Skip-gram vs CBOW<a class="headerlink" href="#visualization-skip-gram-vs-cbow" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize Skip-gram vs CBOW architectures</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;the&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;sat&quot;</span><span class="p">,</span> <span class="s2">&quot;on&quot;</span><span class="p">,</span> <span class="s2">&quot;the&quot;</span><span class="p">]</span>
<span class="n">center_idx</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># &quot;sat&quot;</span>
<span class="n">window</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># === Left: Skip-gram ===</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Skip-gram</span><span class="se">\n</span><span class="s1">&quot;Predict context from center&quot;&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># Draw words</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>
    <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;steelblue&#39;</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">center_idx</span> <span class="k">else</span> <span class="s1">&#39;lightcoral&#39;</span>
    <span class="n">edge</span> <span class="o">=</span> <span class="s1">&#39;navy&#39;</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">center_idx</span> <span class="k">else</span> <span class="s1">&#39;darkred&#39;</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">center_idx</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">window</span> <span class="k">else</span> <span class="mf">0.3</span>
    
    <span class="n">rect</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="n">i</span> <span class="o">-</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">),</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> 
                          <span class="n">edgecolor</span><span class="o">=</span><span class="n">edge</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">rect</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mf">3.05</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> 
            <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>

<span class="c1"># Draw arrows from center to context</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">center_idx</span> <span class="ow">and</span> <span class="nb">abs</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">center_idx</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">window</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">center_idx</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">),</span>
                    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkred&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>

<span class="c1"># Labels</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">center_idx</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="s1">&#39;INPUT</span><span class="se">\n</span><span class="s1">(center word)&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
        <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;navy&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="s1">&#39;Neural</span><span class="se">\n</span><span class="s1">Network&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
        <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s1">&#39;round,pad=0.5&#39;</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;lightyellow&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;&quot;sat&quot; -&gt; predict &quot;the&quot;, &quot;cat&quot;, &quot;on&quot;, &quot;the&quot;&#39;</span><span class="p">,</span> 
        <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;italic&#39;</span><span class="p">)</span>

<span class="c1"># === Right: CBOW ===</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;CBOW</span><span class="se">\n</span><span class="s1">&quot;Predict center from context&quot;&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># Draw words</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>
    <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;lightcoral&#39;</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">center_idx</span> <span class="k">else</span> <span class="s1">&#39;steelblue&#39;</span>
    <span class="n">edge</span> <span class="o">=</span> <span class="s1">&#39;darkred&#39;</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">center_idx</span> <span class="k">else</span> <span class="s1">&#39;navy&#39;</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">center_idx</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">window</span> <span class="k">else</span> <span class="mf">0.3</span>
    
    <span class="n">rect</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="n">i</span> <span class="o">-</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">),</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> 
                          <span class="n">edgecolor</span><span class="o">=</span><span class="n">edge</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">rect</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mf">3.05</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> 
            <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>

<span class="c1"># Draw arrows from context to center</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">center_idx</span> <span class="ow">and</span> <span class="nb">abs</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">center_idx</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">window</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">center_idx</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">),</span>
                    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;navy&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>

<span class="c1"># Labels</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">center_idx</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="s1">&#39;OUTPUT</span><span class="se">\n</span><span class="s1">(center word)&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
        <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;navy&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="s1">&#39;Neural</span><span class="se">\n</span><span class="s1">Network&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
        <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s1">&#39;round,pad=0.5&#39;</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;lightyellow&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;&quot;the&quot;, &quot;cat&quot;, &quot;on&quot;, &quot;the&quot; -&gt; predict &quot;sat&quot;&#39;</span><span class="p">,</span> 
        <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;italic&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="implementing-word2vec-skip-gram-from-scratch">
<h3>Implementing Word2Vec (Skip-gram) from Scratch<a class="headerlink" href="#implementing-word2vec-skip-gram-from-scratch" title="Link to this heading">#</a></h3>
<p>Let’s build a skip-gram model step by step. The architecture is surprisingly simple:</p>
<ol class="arabic simple">
<li><p><strong>Input:</strong> One-hot encoded center word</p></li>
<li><p><strong>Hidden layer:</strong> Embedding lookup (this IS the embedding we want to learn)</p></li>
<li><p><strong>Output:</strong> Probability distribution over vocabulary (which words are likely context words)</p></li>
</ol>
<p>With negative sampling, we simplify this further: instead of predicting over the entire vocabulary, we just need to distinguish true context words from random “negative” words.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Step 1: Prepare a toy corpus with enough structure to learn from</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;the king rules the kingdom with wisdom&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the queen rules the kingdom with grace&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the prince will become king one day&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the princess will become queen one day&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the man works in the village&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the woman works in the village&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the boy plays in the village&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the girl plays in the village&quot;</span><span class="p">,</span>
    <span class="s2">&quot;a king and queen rule together&quot;</span><span class="p">,</span>
    <span class="s2">&quot;a man and woman live together&quot;</span><span class="p">,</span>
    <span class="s2">&quot;a boy and girl play together&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the king sits on the royal throne&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the queen sits on the royal throne&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the prince is the son of the king&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the princess is the daughter of the queen&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the man is the father of the boy&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the woman is the mother of the girl&quot;</span><span class="p">,</span>
    <span class="s2">&quot;a brave king protects the kingdom&quot;</span><span class="p">,</span>
    <span class="s2">&quot;a wise queen protects the kingdom&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the young prince trains with the knight&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the young princess studies with the scholar&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the strong man builds the house&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the kind woman tends the garden&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the king wears the golden crown&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the queen wears the silver crown&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="c1"># Tokenize</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">]</span>
<span class="n">all_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">]</span>

<span class="c1"># Build vocabulary</span>
<span class="n">word_counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">all_words</span><span class="p">)</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">word_counts</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="n">word2idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">w</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
<span class="n">idx2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">w</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">word2idx</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Vocabulary size: </span><span class="si">{</span><span class="n">vocab_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total tokens: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">all_words</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample words: </span><span class="si">{</span><span class="n">vocab</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Word counts (top 10): </span><span class="si">{</span><span class="n">word_counts</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Step 2: Generate skip-gram training pairs</span>
<span class="k">def</span><span class="w"> </span><span class="nf">generate_skipgram_pairs</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate (center_word, context_word) pairs for skip-gram training.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        sentences: List of tokenized sentences</span>
<span class="sd">        word2idx: Word to index mapping</span>
<span class="sd">        window_size: Number of words on each side to consider as context</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        List of (center_idx, context_idx) tuples</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">word2idx</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">center</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">indices</span><span class="p">):</span>
            <span class="c1"># Look at words within the window</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="n">window_size</span><span class="p">),</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">),</span> <span class="n">i</span> <span class="o">+</span> <span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)):</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">j</span><span class="p">:</span>
                    <span class="n">pairs</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">center</span><span class="p">,</span> <span class="n">indices</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">pairs</span>

<span class="n">pairs</span> <span class="o">=</span> <span class="n">generate_skipgram_pairs</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generated </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span><span class="si">}</span><span class="s2"> training pairs&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Sample pairs (center -&gt; context):&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">center</span><span class="p">,</span> <span class="n">context</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">[:</span><span class="mi">8</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">idx2word</span><span class="p">[</span><span class="n">center</span><span class="p">]</span><span class="si">:</span><span class="s2">12s</span><span class="si">}</span><span class="s2"> -&gt; </span><span class="si">{</span><span class="n">idx2word</span><span class="p">[</span><span class="n">context</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Step 3: Define the Skip-gram model with Negative Sampling</span>
<span class="k">class</span><span class="w"> </span><span class="nc">SkipGramNegSampling</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Skip-gram Word2Vec with negative sampling.</span>
<span class="sd">    </span>
<span class="sd">    Instead of computing softmax over the entire vocabulary (expensive!),</span>
<span class="sd">    we train a binary classifier: is this a real (center, context) pair</span>
<span class="sd">    or a fake one?</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Two embedding matrices:</span>
        <span class="c1"># - center_embeddings: for center words (this is what we keep as our word vectors)</span>
        <span class="c1"># - context_embeddings: for context words (used during training only)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">center_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
        
        <span class="c1"># Initialize with small random values</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">center_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="o">/</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="mf">0.5</span><span class="o">/</span><span class="n">embedding_dim</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">context_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="o">/</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="mf">0.5</span><span class="o">/</span><span class="n">embedding_dim</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">center_words</span><span class="p">,</span> <span class="n">context_words</span><span class="p">,</span> <span class="n">negative_words</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            center_words: (batch_size,) center word indices</span>
<span class="sd">            context_words: (batch_size,) true context word indices</span>
<span class="sd">            negative_words: (batch_size, num_neg) negative sample indices</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">            loss: negative sampling loss</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Get embeddings</span>
        <span class="n">center_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">center_embeddings</span><span class="p">(</span><span class="n">center_words</span><span class="p">)</span>      <span class="c1"># (batch, emb_dim)</span>
        <span class="n">context_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">context_embeddings</span><span class="p">(</span><span class="n">context_words</span><span class="p">)</span>    <span class="c1"># (batch, emb_dim)</span>
        <span class="n">neg_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">context_embeddings</span><span class="p">(</span><span class="n">negative_words</span><span class="p">)</span>       <span class="c1"># (batch, num_neg, emb_dim)</span>
        
        <span class="c1"># Positive score: dot product of center and true context</span>
        <span class="n">pos_score</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">center_emb</span> <span class="o">*</span> <span class="n">context_emb</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch,)</span>
        <span class="n">pos_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">logsigmoid</span><span class="p">(</span><span class="n">pos_score</span><span class="p">)</span>                       <span class="c1"># log(sigmoid(score))</span>
        
        <span class="c1"># Negative scores: dot product of center with each negative sample</span>
        <span class="c1"># center_emb: (batch, emb_dim) -&gt; (batch, emb_dim, 1)</span>
        <span class="n">neg_score</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">neg_emb</span><span class="p">,</span> <span class="n">center_emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, num_neg)</span>
        <span class="n">neg_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">logsigmoid</span><span class="p">(</span><span class="o">-</span><span class="n">neg_score</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># log(sigmoid(-score))</span>
        
        <span class="c1"># Total loss: maximize positive score, minimize negative scores</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">pos_loss</span> <span class="o">+</span> <span class="n">neg_loss</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># Create model</span>
<span class="n">EMBEDDING_DIM</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># Small for our toy corpus</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SkipGramNegSampling</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">EMBEDDING_DIM</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model parameters: </span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Center embeddings shape: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">center_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Context embeddings shape: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">context_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Step 4: Training loop</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_negative_samples</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_neg</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">word_counts</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sample negative words proportional to frequency^(3/4).</span>
<span class="sd">    The 3/4 power smooths the distribution, giving rare words more chance.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Build sampling distribution: freq^(3/4)</span>
    <span class="n">freqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">word_counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">idx2word</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="n">freqs</span> <span class="o">=</span> <span class="n">freqs</span> <span class="o">**</span> <span class="mf">0.75</span>
    <span class="n">freqs</span> <span class="o">/=</span> <span class="n">freqs</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    
    <span class="n">neg_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_neg</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="n">freqs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">neg_samples</span><span class="p">)</span>

<span class="c1"># Training</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">NUM_NEG</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># Convert pairs to tensors</span>
<span class="n">center_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">])</span>
<span class="n">context_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">])</span>

<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
    <span class="c1"># Shuffle</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">))</span>
    <span class="n">center_shuffled</span> <span class="o">=</span> <span class="n">center_indices</span><span class="p">[</span><span class="n">perm</span><span class="p">]</span>
    <span class="n">context_shuffled</span> <span class="o">=</span> <span class="n">context_indices</span><span class="p">[</span><span class="n">perm</span><span class="p">]</span>
    
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">num_batches</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">),</span> <span class="n">BATCH_SIZE</span><span class="p">):</span>
        <span class="n">batch_center</span> <span class="o">=</span> <span class="n">center_shuffled</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">BATCH_SIZE</span><span class="p">]</span>
        <span class="n">batch_context</span> <span class="o">=</span> <span class="n">context_shuffled</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">BATCH_SIZE</span><span class="p">]</span>
        <span class="n">batch_neg</span> <span class="o">=</span> <span class="n">get_negative_samples</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">batch_center</span><span class="p">),</span> <span class="n">NUM_NEG</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">word_counts</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">)</span>
        
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch_center</span><span class="p">,</span> <span class="n">batch_context</span><span class="p">,</span> <span class="n">batch_neg</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">num_batches</span> <span class="o">+=</span> <span class="mi">1</span>
    
    <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">epoch_loss</span> <span class="o">/</span> <span class="n">num_batches</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_loss</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="s2">3d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">EPOCHS</span><span class="si">}</span><span class="s2">: Loss = </span><span class="si">{</span><span class="n">avg_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Plot training loss</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Skip-gram Training Loss&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualization-learned-word-embeddings-in-2d">
<h3>Visualization: Learned Word Embeddings in 2D<a class="headerlink" href="#visualization-learned-word-embeddings-in-2d" title="Link to this heading">#</a></h3>
<p>Let’s project our learned embeddings down to 2D using PCA and see if similar words cluster together.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Extract learned embeddings</span>
<span class="n">embeddings_matrix</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">center_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="c1"># PCA for 2D projection</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pca_2d</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Project data to 2D using PCA.&quot;&quot;&quot;</span>
    <span class="n">X_centered</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_centered</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>
    <span class="c1"># Take top 2 eigenvectors (largest eigenvalues)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">top_vectors</span> <span class="o">=</span> <span class="n">eigenvectors</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">X_centered</span> <span class="o">@</span> <span class="n">top_vectors</span>

<span class="c1"># Project to 2D</span>
<span class="n">embeddings_2d</span> <span class="o">=</span> <span class="n">pca_2d</span><span class="p">(</span><span class="n">embeddings_matrix</span><span class="p">)</span>

<span class="c1"># Define word groups for coloring</span>
<span class="n">groups</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;royalty_male&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;king&#39;</span><span class="p">,</span> <span class="s1">&#39;prince&#39;</span><span class="p">],</span>
    <span class="s1">&#39;royalty_female&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;queen&#39;</span><span class="p">,</span> <span class="s1">&#39;princess&#39;</span><span class="p">],</span>
    <span class="s1">&#39;people_male&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;man&#39;</span><span class="p">,</span> <span class="s1">&#39;boy&#39;</span><span class="p">,</span> <span class="s1">&#39;father&#39;</span><span class="p">,</span> <span class="s1">&#39;son&#39;</span><span class="p">],</span>
    <span class="s1">&#39;people_female&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;woman&#39;</span><span class="p">,</span> <span class="s1">&#39;girl&#39;</span><span class="p">,</span> <span class="s1">&#39;mother&#39;</span><span class="p">,</span> <span class="s1">&#39;daughter&#39;</span><span class="p">],</span>
<span class="p">}</span>

<span class="n">group_colors</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;royalty_male&#39;</span><span class="p">:</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span>
    <span class="s1">&#39;royalty_female&#39;</span><span class="p">:</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span>
    <span class="s1">&#39;people_male&#39;</span><span class="p">:</span> <span class="s1">&#39;steelblue&#39;</span><span class="p">,</span>
    <span class="s1">&#39;people_female&#39;</span><span class="p">:</span> <span class="s1">&#39;lightcoral&#39;</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">group_labels</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;royalty_male&#39;</span><span class="p">:</span> <span class="s1">&#39;Male Royalty&#39;</span><span class="p">,</span>
    <span class="s1">&#39;royalty_female&#39;</span><span class="p">:</span> <span class="s1">&#39;Female Royalty&#39;</span><span class="p">,</span>
    <span class="s1">&#39;people_male&#39;</span><span class="p">:</span> <span class="s1">&#39;Male Common&#39;</span><span class="p">,</span>
    <span class="s1">&#39;people_female&#39;</span><span class="p">:</span> <span class="s1">&#39;Female Common&#39;</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="c1"># Plot all words in gray</span>
<span class="n">all_group_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">w</span> <span class="k">for</span> <span class="n">words</span> <span class="ow">in</span> <span class="n">groups</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">all_group_words</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">embeddings_2d</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">embeddings_2d</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;lightgray&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="p">(</span><span class="n">embeddings_2d</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">embeddings_2d</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>

<span class="c1"># Plot grouped words with colors</span>
<span class="k">for</span> <span class="n">group_name</span><span class="p">,</span> <span class="n">words</span> <span class="ow">in</span> <span class="n">groups</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word2idx</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">word2idx</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">embeddings_2d</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">embeddings_2d</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> 
                      <span class="n">c</span><span class="o">=</span><span class="n">group_colors</span><span class="p">[</span><span class="n">group_name</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
                      <span class="n">label</span><span class="o">=</span><span class="n">group_labels</span><span class="p">[</span><span class="n">group_name</span><span class="p">]</span> <span class="k">if</span> <span class="n">word</span> <span class="o">==</span> <span class="n">words</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="p">(</span><span class="n">embeddings_2d</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">embeddings_2d</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
                       <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> 
                       <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Learned Word2Vec Embeddings (PCA Projection)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;PC 1&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;PC 2&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-magic-vector-arithmetic">
<h3>The Magic: Vector Arithmetic<a class="headerlink" href="#the-magic-vector-arithmetic" title="Link to this heading">#</a></h3>
<p>The most surprising property of word embeddings is that <strong>vector arithmetic captures analogies</strong>. The direction from “man” to “woman” encodes the concept of gender. Adding that direction to “king” should land near “queen.”</p>
<div class="math notranslate nohighlight">
\[\vec{\text{king}} - \vec{\text{man}} + \vec{\text{woman}} \approx \vec{\text{queen}}\]</div>
<p><strong>F1 analogy:</strong> Vector arithmetic in F1 embedding space would capture team-driver relationships. The direction from Ferrari to Leclerc encodes “lead driver of this team.” Applying that same direction to other teams:</p>
<div class="math notranslate nohighlight">
\[\vec{\text{Leclerc}} - \vec{\text{Ferrari}} + \vec{\text{McLaren}} \approx \vec{\text{Norris}}\]</div>
<div class="math notranslate nohighlight">
\[\vec{\text{Verstappen}} - \vec{\text{Red Bull}} + \vec{\text{Mercedes}} \approx \vec{\text{Hamilton}}\]</div>
<p>Or for circuit characteristics: the direction from “Monaco” to “Singapore” encodes “street circuit in a different continent.” The direction from “Monza” to “Spa” encodes “classic European power circuit.”</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Vector arithmetic on our learned embeddings</span>
<span class="k">def</span><span class="w"> </span><span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute cosine similarity between two vectors.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">find_nearest</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="n">embeddings_matrix</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">exclude</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Find the top_k nearest words to a query vector.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        query_vec: Query embedding vector</span>
<span class="sd">        embeddings_matrix: All word embeddings (vocab_size, emb_dim)</span>
<span class="sd">        idx2word: Index to word mapping</span>
<span class="sd">        top_k: Number of results</span>
<span class="sd">        exclude: Set of words to exclude from results</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        List of (word, similarity) tuples</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">exclude</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">exclude</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    
    <span class="n">similarities</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">idx2word</span><span class="p">)):</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">idx2word</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">exclude</span><span class="p">:</span>
            <span class="n">sim</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="n">embeddings_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">similarities</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">word</span><span class="p">,</span> <span class="n">sim</span><span class="p">))</span>
    
    <span class="n">similarities</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">similarities</span><span class="p">[:</span><span class="n">top_k</span><span class="p">]</span>

<span class="k">def</span><span class="w"> </span><span class="nf">analogy</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">embeddings_matrix</span><span class="p">,</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Solve: a is to b as c is to ?</span>
<span class="sd">    Computes: b - a + c and finds nearest word.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">vec_a</span> <span class="o">=</span> <span class="n">embeddings_matrix</span><span class="p">[</span><span class="n">word2idx</span><span class="p">[</span><span class="n">a</span><span class="p">]]</span>
    <span class="n">vec_b</span> <span class="o">=</span> <span class="n">embeddings_matrix</span><span class="p">[</span><span class="n">word2idx</span><span class="p">[</span><span class="n">b</span><span class="p">]]</span>
    <span class="n">vec_c</span> <span class="o">=</span> <span class="n">embeddings_matrix</span><span class="p">[</span><span class="n">word2idx</span><span class="p">[</span><span class="n">c</span><span class="p">]]</span>
    
    <span class="n">query</span> <span class="o">=</span> <span class="n">vec_b</span> <span class="o">-</span> <span class="n">vec_a</span> <span class="o">+</span> <span class="n">vec_c</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">find_nearest</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">embeddings_matrix</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">exclude</span><span class="o">=</span><span class="p">{</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">})</span>
    <span class="k">return</span> <span class="n">results</span>

<span class="c1"># Test analogies</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Vector Arithmetic Analogies ===</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">analogy_tests</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s2">&quot;man&quot;</span><span class="p">,</span> <span class="s2">&quot;king&quot;</span><span class="p">,</span> <span class="s2">&quot;woman&quot;</span><span class="p">,</span> <span class="s2">&quot;queen&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;man&quot;</span><span class="p">,</span> <span class="s2">&quot;boy&quot;</span><span class="p">,</span> <span class="s2">&quot;woman&quot;</span><span class="p">,</span> <span class="s2">&quot;girl&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;king&quot;</span><span class="p">,</span> <span class="s2">&quot;prince&quot;</span><span class="p">,</span> <span class="s2">&quot;queen&quot;</span><span class="p">,</span> <span class="s2">&quot;princess&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;king&quot;</span><span class="p">,</span> <span class="s2">&quot;kingdom&quot;</span><span class="p">,</span> <span class="s2">&quot;queen&quot;</span><span class="p">,</span> <span class="s2">&quot;kingdom&quot;</span><span class="p">),</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">expected</span> <span class="ow">in</span> <span class="n">analogy_tests</span><span class="p">:</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">analogy</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">embeddings_matrix</span><span class="p">,</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">)</span>
    <span class="n">top_word</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">marker</span> <span class="o">=</span> <span class="s2">&quot;  &lt;&lt;&lt; correct!&quot;</span> <span class="k">if</span> <span class="n">top_word</span> <span class="o">==</span> <span class="n">expected</span> <span class="k">else</span> <span class="sa">f</span><span class="s2">&quot;  (expected: </span><span class="si">{</span><span class="n">expected</span><span class="si">}</span><span class="s2">)&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">a</span><span class="si">:</span><span class="s2">10s</span><span class="si">}</span><span class="s2"> -&gt; </span><span class="si">{</span><span class="n">b</span><span class="si">:</span><span class="s2">10s</span><span class="si">}</span><span class="s2"> :: </span><span class="si">{</span><span class="n">c</span><span class="si">:</span><span class="s2">10s</span><span class="si">}</span><span class="s2"> -&gt; </span><span class="si">{</span><span class="n">top_word</span><span class="si">:</span><span class="s2">10s</span><span class="si">}</span><span class="s2"> (sim=</span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)</span><span class="si">{</span><span class="n">marker</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">sim</span> <span class="ow">in</span> <span class="n">results</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;&#39;</span><span class="si">:</span><span class="s2">42s</span><span class="si">}</span><span class="s2">   </span><span class="si">{</span><span class="n">word</span><span class="si">:</span><span class="s2">10s</span><span class="si">}</span><span class="s2"> (sim=</span><span class="si">{</span><span class="n">sim</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualization-vector-arithmetic-in-embedding-space">
<h3>Visualization: Vector Arithmetic in Embedding Space<a class="headerlink" href="#visualization-vector-arithmetic-in-embedding-space" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize the king - man + woman = queen analogy in 2D</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># === Left: The analogy in embedding space ===</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">analogy_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;king&#39;</span><span class="p">,</span> <span class="s1">&#39;queen&#39;</span><span class="p">,</span> <span class="s1">&#39;man&#39;</span><span class="p">,</span> <span class="s1">&#39;woman&#39;</span><span class="p">]</span>
<span class="n">analogy_colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;steelblue&#39;</span><span class="p">,</span> <span class="s1">&#39;lightcoral&#39;</span><span class="p">]</span>
<span class="n">analogy_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">word2idx</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">analogy_words</span><span class="p">]</span>

<span class="c1"># Plot the four words</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">color</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">analogy_words</span><span class="p">,</span> <span class="n">analogy_colors</span><span class="p">)):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">word2idx</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">embeddings_2d</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">embeddings_2d</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> 
              <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="p">(</span><span class="n">embeddings_2d</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">embeddings_2d</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
               <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> 
               <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">)</span>

<span class="c1"># Draw &quot;gender&quot; arrows: man-&gt;woman, king-&gt;queen</span>
<span class="k">for</span> <span class="n">start_word</span><span class="p">,</span> <span class="n">end_word</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="p">[(</span><span class="s1">&#39;man&#39;</span><span class="p">,</span> <span class="s1">&#39;woman&#39;</span><span class="p">,</span> <span class="s1">&#39;gender</span><span class="se">\n</span><span class="s1">direction&#39;</span><span class="p">),</span> 
                                      <span class="p">(</span><span class="s1">&#39;king&#39;</span><span class="p">,</span> <span class="s1">&#39;queen&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)]:</span>
    <span class="n">si</span> <span class="o">=</span> <span class="n">word2idx</span><span class="p">[</span><span class="n">start_word</span><span class="p">]</span>
    <span class="n">ei</span> <span class="o">=</span> <span class="n">word2idx</span><span class="p">[</span><span class="n">end_word</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">embeddings_2d</span><span class="p">[</span><span class="n">ei</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">embeddings_2d</span><span class="p">[</span><span class="n">ei</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
               <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">embeddings_2d</span><span class="p">[</span><span class="n">si</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">embeddings_2d</span><span class="p">[</span><span class="n">si</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
               <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">label</span><span class="p">:</span>
        <span class="n">mid_x</span> <span class="o">=</span> <span class="p">(</span><span class="n">embeddings_2d</span><span class="p">[</span><span class="n">si</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">embeddings_2d</span><span class="p">[</span><span class="n">ei</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="n">mid_y</span> <span class="o">=</span> <span class="p">(</span><span class="n">embeddings_2d</span><span class="p">[</span><span class="n">si</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">embeddings_2d</span><span class="p">[</span><span class="n">ei</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="p">(</span><span class="n">mid_x</span><span class="p">,</span> <span class="n">mid_y</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> 
                   <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
                   <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">30</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">)</span>

<span class="c1"># Draw &quot;royalty&quot; arrows: man-&gt;king, woman-&gt;queen</span>
<span class="k">for</span> <span class="n">start_word</span><span class="p">,</span> <span class="n">end_word</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="p">[(</span><span class="s1">&#39;man&#39;</span><span class="p">,</span> <span class="s1">&#39;king&#39;</span><span class="p">,</span> <span class="s1">&#39;royalty</span><span class="se">\n</span><span class="s1">direction&#39;</span><span class="p">),</span> 
                                      <span class="p">(</span><span class="s1">&#39;woman&#39;</span><span class="p">,</span> <span class="s1">&#39;queen&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)]:</span>
    <span class="n">si</span> <span class="o">=</span> <span class="n">word2idx</span><span class="p">[</span><span class="n">start_word</span><span class="p">]</span>
    <span class="n">ei</span> <span class="o">=</span> <span class="n">word2idx</span><span class="p">[</span><span class="n">end_word</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">embeddings_2d</span><span class="p">[</span><span class="n">ei</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">embeddings_2d</span><span class="p">[</span><span class="n">ei</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
               <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">embeddings_2d</span><span class="p">[</span><span class="n">si</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">embeddings_2d</span><span class="p">[</span><span class="n">si</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
               <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">label</span><span class="p">:</span>
        <span class="n">mid_x</span> <span class="o">=</span> <span class="p">(</span><span class="n">embeddings_2d</span><span class="p">[</span><span class="n">si</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">embeddings_2d</span><span class="p">[</span><span class="n">ei</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="n">mid_y</span> <span class="o">=</span> <span class="p">(</span><span class="n">embeddings_2d</span><span class="p">[</span><span class="n">si</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">embeddings_2d</span><span class="p">[</span><span class="n">ei</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="p">(</span><span class="n">mid_x</span><span class="p">,</span> <span class="n">mid_y</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">,</span> 
                   <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span>
                   <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;king - man + woman = queen</span><span class="se">\n</span><span class="s1">(Parallel Relationship Structure)&#39;</span><span class="p">,</span> 
             <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;PC 1&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;PC 2&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># === Right: Similarity heatmap ===</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">focus_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;king&#39;</span><span class="p">,</span> <span class="s1">&#39;queen&#39;</span><span class="p">,</span> <span class="s1">&#39;prince&#39;</span><span class="p">,</span> <span class="s1">&#39;princess&#39;</span><span class="p">,</span> <span class="s1">&#39;man&#39;</span><span class="p">,</span> <span class="s1">&#39;woman&#39;</span><span class="p">,</span> <span class="s1">&#39;boy&#39;</span><span class="p">,</span> <span class="s1">&#39;girl&#39;</span><span class="p">]</span>
<span class="n">focus_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">word2idx</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">focus_words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">word2idx</span><span class="p">]</span>
<span class="n">focus_words_filtered</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">focus_words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">word2idx</span><span class="p">]</span>

<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">focus_words_filtered</span><span class="p">)</span>
<span class="n">sim_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">sim_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span>
            <span class="n">embeddings_matrix</span><span class="p">[</span><span class="n">word2idx</span><span class="p">[</span><span class="n">focus_words_filtered</span><span class="p">[</span><span class="n">i</span><span class="p">]]],</span>
            <span class="n">embeddings_matrix</span><span class="p">[</span><span class="n">word2idx</span><span class="p">[</span><span class="n">focus_words_filtered</span><span class="p">[</span><span class="n">j</span><span class="p">]]]</span>
        <span class="p">)</span>

<span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">sim_matrix</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdBu_r&#39;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">focus_words_filtered</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">focus_words_filtered</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Cosine Similarity Between Word Embeddings&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

<span class="c1"># Annotate</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">sim_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="deep-dive-why-does-word2vec-capture-semantics">
<h3>Deep Dive: Why Does Word2Vec Capture Semantics?<a class="headerlink" href="#deep-dive-why-does-word2vec-capture-semantics" title="Link to this heading">#</a></h3>
<p>The math behind this is elegant. Consider the skip-gram objective: we want to maximize <span class="math notranslate nohighlight">\(P(\text{context} | \text{center})\)</span>. With negative sampling, we learn embeddings where:</p>
<div class="math notranslate nohighlight">
\[\vec{w}_{\text{center}} \cdot \vec{w}_{\text{context}} \approx \log P(\text{context} | \text{center})\]</div>
<p>Mikolov et al. showed that this implicitly factorizes a <strong>pointwise mutual information (PMI) matrix</strong> – a matrix that captures how much more often two words co-occur than you would expect by chance. Words that co-occur in similar contexts end up with similar vectors because they have similar PMI profiles.</p>
<section id="id1">
<h4>Key Insight<a class="headerlink" href="#id1" title="Link to this heading">#</a></h4>
<p>Word2Vec does not “understand” language. It discovers statistical regularities in word co-occurrence and encodes them as geometric relationships. The fact that this produces semantically meaningful vectors is a profound statement about how meaning relates to usage patterns.</p>
<p><strong>F1 analogy:</strong> A “DriverVec” model would not “understand” racing. It would discover that Verstappen and Hamilton co-occur with words like “pole,” “fastest lap,” and “victory” far more than chance would predict, and encode this as geometric proximity. The model captures the <em>structure</em> of racing discourse without understanding a single corner.</p>
</section>
<section id="the-negative-sampling-trick">
<h4>The Negative Sampling Trick<a class="headerlink" href="#the-negative-sampling-trick" title="Link to this heading">#</a></h4>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Approach</p></th>
<th class="head"><p>What it does</p></th>
<th class="head"><p>Cost per step</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Full softmax</p></td>
<td><p>Normalize over all V words</p></td>
<td><p>O(V) – very expensive</p></td>
</tr>
<tr class="row-odd"><td><p>Negative sampling</p></td>
<td><p>Compare 1 positive + k negatives</p></td>
<td><p>O(k) – fast!</p></td>
</tr>
<tr class="row-even"><td><p>Hierarchical softmax</p></td>
<td><p>Binary tree over vocabulary</p></td>
<td><p>O(log V)</p></td>
</tr>
</tbody>
</table>
</div>
<p>Negative sampling typically uses k=5 for large datasets and k=15 for small ones. The negative words are sampled proportional to <span class="math notranslate nohighlight">\(f(w)^{3/4}\)</span>, where <span class="math notranslate nohighlight">\(f(w)\)</span> is the word frequency. The 3/4 exponent was found empirically to work best – it smooths the distribution so rare words get sampled more than their raw frequency would suggest.</p>
</section>
</section>
</section>
<hr class="docutils" />
<section id="glove-global-vectors-for-word-representation">
<h2>3. GloVe (Global Vectors for Word Representation)<a class="headerlink" href="#glove-global-vectors-for-word-representation" title="Link to this heading">#</a></h2>
<section id="id2">
<h3>Intuitive Explanation<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>While Word2Vec learns from <strong>local</strong> context windows (one word at a time), GloVe (Pennington et al., 2014) takes a different approach: it first builds a global <strong>co-occurrence matrix</strong> and then factorizes it.</p>
<p>Think of it this way:</p>
<ul class="simple">
<li><p><strong>Word2Vec</strong> reads through the text word by word, like a human reading a book</p></li>
<li><p><strong>GloVe</strong> first counts all word pair co-occurrences across the entire corpus, then finds embeddings that best explain those counts</p></li>
</ul>
<p>The GloVe objective is:</p>
<div class="math notranslate nohighlight">
\[J = \sum_{i,j=1}^{V} f(X_{ij}) \left( \vec{w}_i^T \vec{w}_j + b_i + b_j - \log X_{ij} \right)^2\]</div>
<p><strong>What this means:</strong> We want the dot product of two word vectors (plus bias terms) to approximate the log of how often they co-occur. The weighting function <span class="math notranslate nohighlight">\(f(X_{ij})\)</span> prevents very common pairs (like “the, the”) from dominating.</p>
<p><strong>F1 analogy:</strong> If Word2Vec is like a scout watching one race at a time and building driver impressions lap by lap, GloVe is like a data analyst who first compiles the complete season statistics – every driver-circuit combination, every head-to-head result, every qualifying gap – and then finds the driver/circuit embeddings that best explain the full dataset at once. Same destination, different route.</p>
</section>
<section id="glove-building-the-co-occurrence-matrix">
<h3>GloVe: Building the Co-occurrence Matrix<a class="headerlink" href="#glove-building-the-co-occurrence-matrix" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Build co-occurrence matrix from our corpus</span>
<span class="k">def</span><span class="w"> </span><span class="nf">build_cooccurrence_matrix</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Build a word-word co-occurrence matrix.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        sentences: List of tokenized sentences</span>
<span class="sd">        word2idx: Word to index mapping</span>
<span class="sd">        window_size: Context window size</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        Co-occurrence matrix of shape (vocab_size, vocab_size)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">V</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2idx</span><span class="p">)</span>
    <span class="n">cooccur</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">V</span><span class="p">,</span> <span class="n">V</span><span class="p">))</span>
    
    <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">word2idx</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">center</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">indices</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="n">window_size</span><span class="p">),</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">),</span> <span class="n">i</span> <span class="o">+</span> <span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)):</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">j</span><span class="p">:</span>
                    <span class="c1"># Weight by distance: closer words get higher weight</span>
                    <span class="n">distance</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">j</span><span class="p">)</span>
                    <span class="n">cooccur</span><span class="p">[</span><span class="n">center</span><span class="p">,</span> <span class="n">indices</span><span class="p">[</span><span class="n">j</span><span class="p">]]</span> <span class="o">+=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">distance</span>
    
    <span class="k">return</span> <span class="n">cooccur</span>

<span class="n">cooccur_matrix</span> <span class="o">=</span> <span class="n">build_cooccurrence_matrix</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Visualize a portion of the co-occurrence matrix</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">focus_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;king&#39;</span><span class="p">,</span> <span class="s1">&#39;queen&#39;</span><span class="p">,</span> <span class="s1">&#39;prince&#39;</span><span class="p">,</span> <span class="s1">&#39;princess&#39;</span><span class="p">,</span> <span class="s1">&#39;man&#39;</span><span class="p">,</span> <span class="s1">&#39;woman&#39;</span><span class="p">,</span> <span class="s1">&#39;boy&#39;</span><span class="p">,</span> <span class="s1">&#39;girl&#39;</span><span class="p">,</span>
               <span class="s1">&#39;kingdom&#39;</span><span class="p">,</span> <span class="s1">&#39;village&#39;</span><span class="p">,</span> <span class="s1">&#39;throne&#39;</span><span class="p">,</span> <span class="s1">&#39;crown&#39;</span><span class="p">]</span>
<span class="n">focus_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">focus_words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">word2idx</span><span class="p">]</span>
<span class="n">focus_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">word2idx</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">focus_words</span><span class="p">]</span>

<span class="n">sub_matrix</span> <span class="o">=</span> <span class="n">cooccur_matrix</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">focus_indices</span><span class="p">,</span> <span class="n">focus_indices</span><span class="p">)]</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">sub_matrix</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;YlOrRd&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">focus_words</span><span class="p">)))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">focus_words</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">focus_words</span><span class="p">)))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">focus_words</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Co-occurrence Matrix (GloVe Input)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Co-occurrence count&#39;</span><span class="p">)</span>

<span class="c1"># Annotate non-zero values</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">focus_words</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">focus_words</span><span class="p">)):</span>
        <span class="n">val</span> <span class="o">=</span> <span class="n">sub_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">val</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">val</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
                   <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span> <span class="k">if</span> <span class="n">val</span> <span class="o">&gt;</span> <span class="n">sub_matrix</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.6</span> <span class="k">else</span> <span class="s1">&#39;black&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Notice: words that appear in similar contexts (king/queen, man/woman)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;have similar co-occurrence patterns across the columns.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="comparison-word2vec-vs-glove">
<h3>Comparison: Word2Vec vs GloVe<a class="headerlink" href="#comparison-word2vec-vs-glove" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Word2Vec</p></th>
<th class="head"><p>GloVe</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Approach</strong></p></td>
<td><p>Predictive (neural network)</p></td>
<td><p>Count-based (matrix factorization)</p></td>
<td><p>Scout watching races vs. statistician analyzing the spreadsheet</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Training data</strong></p></td>
<td><p>Local context windows</p></td>
<td><p>Global co-occurrence matrix</p></td>
<td><p>One race at a time vs. full season at once</p></td>
</tr>
<tr class="row-even"><td><p><strong>Objective</strong></p></td>
<td><p>Predict context words</p></td>
<td><p>Reconstruct log co-occurrence</p></td>
<td><p>Predict nearby events vs. explain overall patterns</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Strengths</strong></p></td>
<td><p>Good with small data, captures syntax</p></td>
<td><p>Better with large data, captures semantics</p></td>
<td><p>Works with few races vs. works with full history</p></td>
</tr>
<tr class="row-even"><td><p><strong>Weaknesses</strong></p></td>
<td><p>Only sees local context</p></td>
<td><p>Requires building full matrix</p></td>
<td><p>Misses the big picture vs. slow to start</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Training</strong></p></td>
<td><p>Online (stochastic)</p></td>
<td><p>Batch (needs full corpus first)</p></td>
<td><p>Learns during the season vs. analyzes after</p></td>
</tr>
<tr class="row-even"><td><p><strong>Result quality</strong></p></td>
<td><p>Very similar</p></td>
<td><p>Very similar</p></td>
<td><p>Both produce good driver profiles</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Key paper</strong></p></td>
<td><p>Mikolov et al., 2013</p></td>
<td><p>Pennington et al., 2014</p></td>
<td><p>–</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>What this means:</strong> In practice, Word2Vec and GloVe produce similarly good embeddings. The choice often comes down to implementation convenience. Both have been largely superseded by contextual embeddings from models like BERT and GPT.</p>
</section>
</section>
<hr class="docutils" />
<section id="modern-embeddings-from-static-to-contextual">
<h2>4. Modern Embeddings: From Static to Contextual<a class="headerlink" href="#modern-embeddings-from-static-to-contextual" title="Link to this heading">#</a></h2>
<section id="id3">
<h3>Intuitive Explanation<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>Word2Vec and GloVe give each word <strong>one fixed embedding</strong> regardless of context. But consider the word “bank”:</p>
<ul class="simple">
<li><p>“I sat on the river <strong>bank</strong>” (riverbank)</p></li>
<li><p>“I deposited money at the <strong>bank</strong>” (financial institution)</p></li>
</ul>
<p>These are completely different meanings, but Word2Vec assigns the same vector to both! This is a fundamental limitation of <strong>static embeddings</strong>.</p>
<p><strong>Contextual embeddings</strong> (from models like BERT, GPT, and their descendants) solve this by computing a <strong>different embedding for each occurrence</strong> of a word, based on the surrounding context. The same word “bank” gets a different vector depending on whether it appears near “river” or “money.”</p>
<p><strong>F1 analogy:</strong> A static embedding for “Hamilton” would be one fixed vector regardless of context. But Hamilton in 2020 (dominant Mercedes, 7th title) is very different from Hamilton in 2022 (struggling W13, midfield battles). Contextual embeddings capture this: “Hamilton” gets a different vector depending on the surrounding season, team performance, and race context. The same driver name in different contexts produces different representations – just as the same word means different things in different sentences.</p>
</section>
<section id="how-contextual-embeddings-work">
<h3>How Contextual Embeddings Work<a class="headerlink" href="#how-contextual-embeddings-work" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Input:</strong> A full sentence is fed into a Transformer model</p></li>
<li><p><strong>Processing:</strong> Each word attends to all other words via self-attention</p></li>
<li><p><strong>Output:</strong> Each word gets a context-dependent embedding</p></li>
</ol>
<p>The key difference: static embeddings are a <strong>lookup table</strong> (one vector per word), while contextual embeddings are a <strong>function</strong> of the entire input sequence.</p>
</section>
<section id="visualization-static-vs-contextual-embeddings">
<h3>Visualization: Static vs Contextual Embeddings<a class="headerlink" href="#visualization-static-vs-contextual-embeddings" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simulate static vs contextual embeddings for the word &quot;bank&quot;</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># === Left: Static Embeddings ===</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># All instances of &quot;bank&quot; map to the same point</span>
<span class="n">static_words</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;river&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">]),</span>
    <span class="s1">&#39;water&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]),</span>
    <span class="s1">&#39;shore&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">]),</span>
    <span class="s1">&#39;money&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5</span><span class="p">]),</span>
    <span class="s1">&#39;account&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">]),</span>
    <span class="s1">&#39;deposit&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8</span><span class="p">]),</span>
    <span class="s1">&#39;bank&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]),</span>  <span class="c1"># Single point -- ambiguous!</span>
<span class="p">}</span>

<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">pos</span> <span class="ow">in</span> <span class="n">static_words</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;green&#39;</span> <span class="k">if</span> <span class="n">word</span> <span class="o">==</span> <span class="s1">&#39;bank&#39;</span> <span class="k">else</span> <span class="p">(</span><span class="s1">&#39;steelblue&#39;</span> <span class="k">if</span> <span class="n">pos</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="s1">&#39;lightcoral&#39;</span><span class="p">)</span>
    <span class="n">size</span> <span class="o">=</span> <span class="mi">200</span> <span class="k">if</span> <span class="n">word</span> <span class="o">==</span> <span class="s1">&#39;bank&#39;</span> <span class="k">else</span> <span class="mi">100</span>
    <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;*&#39;</span> <span class="k">if</span> <span class="n">word</span> <span class="o">==</span> <span class="s1">&#39;bank&#39;</span> <span class="k">else</span> <span class="s1">&#39;o&#39;</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pos</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pos</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> 
              <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span>
               <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Static Embeddings (Word2Vec/GloVe)</span><span class="se">\n</span><span class="s1">&quot;bank&quot; has ONE vector&#39;</span><span class="p">,</span> 
             <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Dimension 1&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Dimension 2&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Add region labels</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">4.2</span><span class="p">,</span> <span class="s1">&#39;Nature cluster&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">,</span> 
        <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;italic&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5</span><span class="p">,</span> <span class="s1">&#39;Finance cluster&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightcoral&#39;</span><span class="p">,</span> 
        <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;italic&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;Stuck in</span><span class="se">\n</span><span class="s1">the middle!&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">),</span>
           <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span>
           <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>

<span class="c1"># === Right: Contextual Embeddings ===</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="n">contextual_words</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;river&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">]),</span>
    <span class="s1">&#39;water&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]),</span>
    <span class="s1">&#39;shore&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">]),</span>
    <span class="s1">&#39;money&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5</span><span class="p">]),</span>
    <span class="s1">&#39;account&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">]),</span>
    <span class="s1">&#39;deposit&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8</span><span class="p">]),</span>
    <span class="s1">&#39;bank</span><span class="se">\n</span><span class="s1">(river context)&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.8</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">]),</span>     <span class="c1"># Near nature words!</span>
    <span class="s1">&#39;bank</span><span class="se">\n</span><span class="s1">(finance context)&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.2</span><span class="p">]),</span>  <span class="c1"># Near finance words!</span>
<span class="p">}</span>

<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">pos</span> <span class="ow">in</span> <span class="n">contextual_words</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="s1">&#39;bank&#39;</span> <span class="ow">in</span> <span class="n">word</span><span class="p">:</span>
        <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;green&#39;</span>
        <span class="n">size</span> <span class="o">=</span> <span class="mi">200</span>
        <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;*&#39;</span>
    <span class="k">elif</span> <span class="n">pos</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;steelblue&#39;</span>
        <span class="n">size</span> <span class="o">=</span> <span class="mi">100</span>
        <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;lightcoral&#39;</span>
        <span class="n">size</span> <span class="o">=</span> <span class="mi">100</span>
        <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pos</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pos</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> 
              <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">)</span>
    <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">10</span> <span class="k">if</span> <span class="s1">&#39;bank&#39;</span> <span class="ow">in</span> <span class="n">word</span> <span class="k">else</span> <span class="mi">11</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">fontsize</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span>
               <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">)</span>

<span class="c1"># Draw arrow showing same word, different positions</span>
<span class="n">bank1</span> <span class="o">=</span> <span class="n">contextual_words</span><span class="p">[</span><span class="s1">&#39;bank</span><span class="se">\n</span><span class="s1">(river context)&#39;</span><span class="p">]</span>
<span class="n">bank2</span> <span class="o">=</span> <span class="n">contextual_words</span><span class="p">[</span><span class="s1">&#39;bank</span><span class="se">\n</span><span class="s1">(finance context)&#39;</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="n">bank2</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="n">bank1</span><span class="p">,</span>
           <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;&lt;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">((</span><span class="n">bank1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">bank2</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">1.2</span><span class="p">,</span> <span class="p">(</span><span class="n">bank1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">bank2</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> 
        <span class="s1">&#39;Same word,</span><span class="se">\n</span><span class="s1">different vectors!&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> 
        <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Contextual Embeddings (BERT/GPT)</span><span class="se">\n</span><span class="s1">&quot;bank&quot; has DIFFERENT vectors per context&#39;</span><span class="p">,</span> 
             <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Dimension 1&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Dimension 2&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">4.2</span><span class="p">,</span> <span class="s1">&#39;Nature cluster&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">,</span> 
        <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;italic&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5</span><span class="p">,</span> <span class="s1">&#39;Finance cluster&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightcoral&#39;</span><span class="p">,</span> 
        <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;italic&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="sentence-embeddings-from-words-to-sentences">
<h3>Sentence Embeddings: From Words to Sentences<a class="headerlink" href="#sentence-embeddings-from-words-to-sentences" title="Link to this heading">#</a></h3>
<p>How do you get a single embedding for an entire sentence? There are several approaches:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>How It Works</p></th>
<th class="head"><p>Pros</p></th>
<th class="head"><p>Cons</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Mean pooling</strong></p></td>
<td><p>Average all word/token embeddings</p></td>
<td><p>Simple, works well</p></td>
<td><p>Treats all tokens equally</p></td>
<td><p>Average all lap sector times – loses nuance</p></td>
</tr>
<tr class="row-odd"><td><p><strong>[CLS] token</strong></p></td>
<td><p>Use BERT’s special classification token</p></td>
<td><p>Built into BERT</p></td>
<td><p>Not optimized for similarity</p></td>
<td><p>Race summary statistic</p></td>
</tr>
<tr class="row-even"><td><p><strong>Max pooling</strong></p></td>
<td><p>Take element-wise max across tokens</p></td>
<td><p>Captures strongest signals</p></td>
<td><p>Loses ordering information</p></td>
<td><p>Best sector times (peak performance)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Specialized models</strong></p></td>
<td><p>Models trained specifically for sentence similarity (e.g., Sentence-BERT)</p></td>
<td><p>Best quality</p></td>
<td><p>Requires fine-tuned model</p></td>
<td><p>Purpose-built race comparison system</p></td>
</tr>
</tbody>
</table>
</div>
<p>In practice, <strong>specialized sentence embedding models</strong> (like Sentence-BERT or OpenAI’s embedding models) give the best results because they are explicitly trained so that similar sentences have similar embeddings.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Demonstrate different pooling strategies using our Word2Vec embeddings</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_sentence_embedding</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">embeddings_matrix</span><span class="p">,</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a sentence embedding from word embeddings.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        sentence: String sentence</span>
<span class="sd">        embeddings_matrix: Word embedding matrix</span>
<span class="sd">        word2idx: Word to index mapping</span>
<span class="sd">        method: &#39;mean&#39;, &#39;max&#39;, or &#39;sum&#39;</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        Sentence embedding vector</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">sentence</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="n">word_vecs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word2idx</span><span class="p">:</span>
            <span class="n">word_vecs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">embeddings_matrix</span><span class="p">[</span><span class="n">word2idx</span><span class="p">[</span><span class="n">word</span><span class="p">]])</span>
    
    <span class="k">if</span> <span class="ow">not</span> <span class="n">word_vecs</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">embeddings_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="n">word_vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">word_vecs</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">word_vecs</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;max&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">word_vecs</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">word_vecs</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Test with example sentences</span>
<span class="n">test_sentences</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;the king rules the kingdom&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the queen rules the kingdom&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the boy plays in the village&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the girl plays in the village&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the prince will become king&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Sentence Similarity (Mean Pooling) ===</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">sent_embeddings</span> <span class="o">=</span> <span class="p">[</span><span class="n">get_sentence_embedding</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">embeddings_matrix</span><span class="p">,</span> <span class="n">word2idx</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> 
                   <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">test_sentences</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test_sentences</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_sentences</span><span class="p">)):</span>
        <span class="n">sim</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">sent_embeddings</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">sent_embeddings</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">sim</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">  |  &#39;</span><span class="si">{</span><span class="n">test_sentences</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;         |  &#39;</span><span class="si">{</span><span class="n">test_sentences</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="deep-dive-the-evolution-of-embeddings">
<h3>Deep Dive: The Evolution of Embeddings<a class="headerlink" href="#deep-dive-the-evolution-of-embeddings" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Era</p></th>
<th class="head"><p>Method</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Key Innovation</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>2003</p></td>
<td><p>Neural LM (Bengio)</p></td>
<td><p>Static</p></td>
<td><p>First neural word embeddings</p></td>
<td><p>First data-driven driver ratings</p></td>
</tr>
<tr class="row-odd"><td><p>2013</p></td>
<td><p>Word2Vec</p></td>
<td><p>Static</p></td>
<td><p>Efficient training at scale</p></td>
<td><p>Scalable driver/circuit profiling</p></td>
</tr>
<tr class="row-even"><td><p>2014</p></td>
<td><p>GloVe</p></td>
<td><p>Static</p></td>
<td><p>Global co-occurrence + local context</p></td>
<td><p>Full-season statistical analysis</p></td>
</tr>
<tr class="row-odd"><td><p>2017</p></td>
<td><p>ELMo</p></td>
<td><p>Contextual</p></td>
<td><p>Bi-directional LSTM embeddings</p></td>
<td><p>Context-aware driver analysis</p></td>
</tr>
<tr class="row-even"><td><p>2018</p></td>
<td><p>BERT</p></td>
<td><p>Contextual</p></td>
<td><p>Transformer-based, bidirectional</p></td>
<td><p>Full race context representations</p></td>
</tr>
<tr class="row-odd"><td><p>2018-now</p></td>
<td><p>GPT family</p></td>
<td><p>Contextual</p></td>
<td><p>Autoregressive Transformer</p></td>
<td><p>Sequential event understanding</p></td>
</tr>
<tr class="row-even"><td><p>2019+</p></td>
<td><p>Sentence-BERT</p></td>
<td><p>Sentence-level</p></td>
<td><p>Contrastive learning for sentences</p></td>
<td><p>Race situation similarity matching</p></td>
</tr>
<tr class="row-odd"><td><p>2022+</p></td>
<td><p>text-embedding-ada-002</p></td>
<td><p>Sentence-level</p></td>
<td><p>Production-grade API embeddings</p></td>
<td><p>Production race analytics systems</p></td>
</tr>
</tbody>
</table>
</div>
<section id="id4">
<h4>Key Insight<a class="headerlink" href="#id4" title="Link to this heading">#</a></h4>
<p>The trend is clear: embeddings have evolved from one-vector-per-word to context-dependent representations computed by increasingly powerful models. Modern embedding models are trained specifically to produce vectors where semantic similarity corresponds to geometric proximity.</p>
</section>
</section>
</section>
<hr class="docutils" />
<section id="similarity-and-distance">
<h2>5. Similarity and Distance<a class="headerlink" href="#similarity-and-distance" title="Link to this heading">#</a></h2>
<section id="id5">
<h3>Intuitive Explanation<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<p>Now that we have embeddings, how do we measure “closeness” between vectors? This connects directly back to linear algebra (Part 1.1). There are three main distance/similarity measures, each with different properties.</p>
<p><strong>Cosine similarity:</strong> How much do two vectors point in the same direction? Ignores magnitude, only cares about angle. This is the most common choice for embeddings.</p>
<div class="math notranslate nohighlight">
\[\text{cosine}(\vec{a}, \vec{b}) = \frac{\vec{a} \cdot \vec{b}}{\|\vec{a}\| \|\vec{b}\|}\]</div>
<p><strong>Euclidean distance:</strong> Straight-line distance between two points. Sensitive to magnitude.</p>
<div class="math notranslate nohighlight">
\[d(\vec{a}, \vec{b}) = \|\vec{a} - \vec{b}\| = \sqrt{\sum_i (a_i - b_i)^2}\]</div>
<p><strong>Dot product similarity:</strong> Raw alignment. Scales with magnitude, so longer vectors have higher scores.</p>
<div class="math notranslate nohighlight">
\[\text{dot}(\vec{a}, \vec{b}) = \vec{a} \cdot \vec{b} = \sum_i a_i b_i\]</div>
<p><strong>F1 analogy:</strong> Comparing driver embeddings:</p>
<ul class="simple">
<li><p><strong>Cosine similarity:</strong> “Do Verstappen and Hamilton have the <em>same style profile</em>, regardless of overall magnitude?” (ignores whether one is ‘more extreme’ in all dimensions)</p></li>
<li><p><strong>Euclidean distance:</strong> “How far apart are Verstappen and Hamilton in the full driver space?” (sensitive to absolute differences)</p></li>
<li><p><strong>Dot product:</strong> “How aligned are these two drivers, amplified by how extreme both are?” (a dominant driver dotted with another dominant driver scores very high)</p></li>
</ul>
</section>
<section id="visualization-comparing-similarity-measures">
<h3>Visualization: Comparing Similarity Measures<a class="headerlink" href="#visualization-comparing-similarity-measures" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Demonstrate the difference between similarity measures</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Create example vectors</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>  <span class="c1"># Different direction, same magnitude</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>  <span class="c1"># Same direction as a, different magnitude</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">])</span>  <span class="c1"># Opposite direction to a</span>

<span class="n">vectors</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;a&#39;</span><span class="p">:</span> <span class="n">a</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">:</span> <span class="n">b</span><span class="p">,</span> <span class="s1">&#39;c (2*a)&#39;</span><span class="p">:</span> <span class="n">c</span><span class="p">,</span> <span class="s1">&#39;d (-a)&#39;</span><span class="p">:</span> <span class="n">d</span><span class="p">}</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;a&#39;</span><span class="p">:</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">:</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;c (2*a)&#39;</span><span class="p">:</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;d (-a)&#39;</span><span class="p">:</span> <span class="s1">&#39;orange&#39;</span><span class="p">}</span>

<span class="c1"># === Left: Cosine Similarity ===</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">vec</span> <span class="ow">in</span> <span class="n">vectors</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="n">vec</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
               <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">name</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mf">2.5</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">vec</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">vec</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">name</span><span class="p">])</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Cosine Similarity</span><span class="se">\n</span><span class="s1">(angle only, ignores length)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Show cosine similarities</span>
<span class="n">sims</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">vec</span> <span class="ow">in</span> <span class="n">vectors</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">name</span> <span class="o">!=</span> <span class="s1">&#39;a&#39;</span><span class="p">:</span>
        <span class="n">sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">vec</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">vec</span><span class="p">))</span>
        <span class="n">sims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;cos(a,</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">)=</span><span class="si">{</span><span class="n">sim</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.98</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sims</span><span class="p">),</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span>
        <span class="n">verticalalignment</span><span class="o">=</span><span class="s1">&#39;top&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s1">&#39;round&#39;</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;lightyellow&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">))</span>

<span class="c1"># === Middle: Euclidean Distance ===</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">vec</span> <span class="ow">in</span> <span class="n">vectors</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">vec</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">vec</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">name</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">vec</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">vec</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">name</span><span class="p">])</span>

<span class="c1"># Draw distance lines from a to others</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">vec</span> <span class="ow">in</span> <span class="n">vectors</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">name</span> <span class="o">!=</span> <span class="s1">&#39;a&#39;</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">vec</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">vec</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">name</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Euclidean Distance</span><span class="se">\n</span><span class="s1">(straight-line, sensitive to length)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">dists</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">vec</span> <span class="ow">in</span> <span class="n">vectors</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">name</span> <span class="o">!=</span> <span class="s1">&#39;a&#39;</span><span class="p">:</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">vec</span><span class="p">)</span>
        <span class="n">dists</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;d(a,</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">)=</span><span class="si">{</span><span class="n">dist</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.98</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dists</span><span class="p">),</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span>
        <span class="n">verticalalignment</span><span class="o">=</span><span class="s1">&#39;top&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s1">&#39;round&#39;</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;lightyellow&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">))</span>

<span class="c1"># === Right: Dot Product ===</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">vec</span> <span class="ow">in</span> <span class="n">vectors</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="n">vec</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
               <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">name</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mf">2.5</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">vec</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">vec</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">name</span><span class="p">])</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Dot Product</span><span class="se">\n</span><span class="s1">(direction + magnitude)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">dots</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">vec</span> <span class="ow">in</span> <span class="n">vectors</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">name</span> <span class="o">!=</span> <span class="s1">&#39;a&#39;</span><span class="p">:</span>
        <span class="n">dot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">vec</span><span class="p">)</span>
        <span class="n">dots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;a . </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">dot</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.98</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dots</span><span class="p">),</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span>
        <span class="n">verticalalignment</span><span class="o">=</span><span class="s1">&#39;top&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s1">&#39;round&#39;</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;lightyellow&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Key observations:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Cosine: a and c(2*a) have sim=1.00 (same direction, ignores length)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Euclidean: a and c(2*a) are far apart (different magnitudes)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Dot product: a . c(2*a) is large (rewards both alignment AND magnitude)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="when-to-use-which">
<h3>When to Use Which?<a class="headerlink" href="#when-to-use-which" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>Best For</p></th>
<th class="head"><p>Range</p></th>
<th class="head"><p>Properties</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Cosine similarity</strong></p></td>
<td><p>Comparing embeddings regardless of magnitude</p></td>
<td><p>[-1, 1]</p></td>
<td><p>Invariant to vector length; 1 = identical direction</p></td>
<td><p>“Are these drivers the same <em>type</em>?”</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Euclidean distance</strong></p></td>
<td><p>Clustering, when magnitude matters</p></td>
<td><p>[0, inf)</p></td>
<td><p>Sensitive to scale; 0 = identical</p></td>
<td><p>“How different are these drivers overall?”</p></td>
</tr>
<tr class="row-even"><td><p><strong>Dot product</strong></p></td>
<td><p>Attention mechanisms, when magnitude encodes importance</p></td>
<td><p>(-inf, inf)</p></td>
<td><p>Fast to compute; used in Transformers</p></td>
<td><p>“How relevant is this telemetry to this strategy?”</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Rule of thumb:</strong> Use <strong>cosine similarity</strong> for semantic similarity tasks (search, recommendation). Use <strong>dot product</strong> when magnitude matters (attention scores, learned relevance). Use <strong>Euclidean distance</strong> for clustering and when you want a proper distance metric.</p>
</section>
<section id="interactive-exploration-similarity-in-embedding-space">
<h3>Interactive Exploration: Similarity in Embedding Space<a class="headerlink" href="#interactive-exploration-similarity-in-embedding-space" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Interactive: nearest neighbors search with different metrics</span>
<span class="k">def</span><span class="w"> </span><span class="nf">nearest_neighbors</span><span class="p">(</span><span class="n">query_word</span><span class="p">,</span> <span class="n">embeddings_matrix</span><span class="p">,</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Find k nearest neighbors using all three metrics.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">query_word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">word2idx</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&#39;</span><span class="si">{</span><span class="n">query_word</span><span class="si">}</span><span class="s2">&#39; not in vocabulary&quot;</span><span class="p">)</span>
        <span class="k">return</span>
    
    <span class="n">query_vec</span> <span class="o">=</span> <span class="n">embeddings_matrix</span><span class="p">[</span><span class="n">word2idx</span><span class="p">[</span><span class="n">query_word</span><span class="p">]]</span>
    
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;cosine&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;euclidean&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;dot_product&#39;</span><span class="p">:</span> <span class="p">[]}</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">idx2word</span><span class="p">)):</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">idx2word</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">word</span> <span class="o">==</span> <span class="n">query_word</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">vec</span> <span class="o">=</span> <span class="n">embeddings_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        
        <span class="n">cos_sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="n">vec</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">query_vec</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">vec</span><span class="p">))</span>
        <span class="n">euc_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">query_vec</span> <span class="o">-</span> <span class="n">vec</span><span class="p">)</span>
        <span class="n">dot_prod</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="n">vec</span><span class="p">)</span>
        
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;cosine&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">word</span><span class="p">,</span> <span class="n">cos_sim</span><span class="p">))</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;euclidean&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">word</span><span class="p">,</span> <span class="n">euc_dist</span><span class="p">))</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;dot_product&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">word</span><span class="p">,</span> <span class="n">dot_prod</span><span class="p">))</span>
    
    <span class="n">results</span><span class="p">[</span><span class="s1">&#39;cosine&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">results</span><span class="p">[</span><span class="s1">&#39;euclidean&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Lower = closer</span>
    <span class="n">results</span><span class="p">[</span><span class="s1">&#39;dot_product&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">results</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

<span class="c1"># Try different query words</span>
<span class="k">for</span> <span class="n">query</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;king&#39;</span><span class="p">,</span> <span class="s1">&#39;woman&#39;</span><span class="p">,</span> <span class="s1">&#39;village&#39;</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Nearest neighbors of &#39;</span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="n">results</span> <span class="o">=</span> <span class="n">nearest_neighbors</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">embeddings_matrix</span><span class="p">,</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">  </span><span class="si">{</span><span class="s1">&#39;Cosine Similarity&#39;</span><span class="si">:</span><span class="s2">&lt;25s</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Euclidean Distance&#39;</span><span class="si">:</span><span class="s2">&lt;25s</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Dot Product&#39;</span><span class="si">:</span><span class="s2">&lt;25s</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="s1">&#39;-&#39;</span><span class="o">*</span><span class="mi">23</span><span class="si">:</span><span class="s2">&lt;25s</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;-&#39;</span><span class="o">*</span><span class="mi">23</span><span class="si">:</span><span class="s2">&lt;25s</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;-&#39;</span><span class="o">*</span><span class="mi">23</span><span class="si">:</span><span class="s2">&lt;25s</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="n">cos_word</span><span class="p">,</span> <span class="n">cos_val</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;cosine&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
        <span class="n">euc_word</span><span class="p">,</span> <span class="n">euc_val</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;euclidean&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
        <span class="n">dot_word</span><span class="p">,</span> <span class="n">dot_val</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;dot_product&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">cos_word</span><span class="si">:</span><span class="s2">12s</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">cos_val</span><span class="si">:</span><span class="s2">+.4f</span><span class="si">}</span><span class="s2">    </span><span class="si">{</span><span class="n">euc_word</span><span class="si">:</span><span class="s2">12s</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">euc_val</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">    </span><span class="si">{</span><span class="n">dot_word</span><span class="si">:</span><span class="s2">12s</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">dot_val</span><span class="si">:</span><span class="s2">+.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="vector-databases-and-retrieval">
<h2>6. Vector Databases and Retrieval<a class="headerlink" href="#vector-databases-and-retrieval" title="Link to this heading">#</a></h2>
<section id="id6">
<h3>Intuitive Explanation<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<p>Once you have embeddings, a natural question is: given a query, how do I quickly find the most similar items? With a small collection, you can compare the query to every item (brute force). But with millions or billions of items, this becomes impossibly slow.</p>
<p><strong>Vector databases</strong> solve this problem. They store embeddings and provide fast <strong>approximate nearest neighbor (ANN)</strong> search. The key insight is that you do not need the exact nearest neighbor – an approximate answer that is 95% as good but 1000x faster is far more practical.</p>
<p><strong>F1 analogy:</strong> Imagine an F1 strategy database containing embeddings for every race situation in history – thousands of races, millions of individual laps. When a new situation arises (e.g., “rain starting, leader on 20-lap old mediums, safety car likely”), you need to instantly find the most similar historical situations to guide your strategy. Brute-force comparison against every historical moment is too slow. A vector database indexes these embeddings so the query “find me similar situations” returns in milliseconds.</p>
</section>
<section id="why-not-just-use-a-regular-database">
<h3>Why Not Just Use a Regular Database?<a class="headerlink" href="#why-not-just-use-a-regular-database" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Traditional Database</p></th>
<th class="head"><p>Vector Database</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Exact match: “WHERE name = ‘cat’”</p></td>
<td><p>Similarity search: “find things like ‘cat’”</p></td>
<td><p>“Find all races at Monza” vs. “Find races <em>like</em> this one”</p></td>
</tr>
<tr class="row-odd"><td><p>Structured queries (SQL)</p></td>
<td><p>Semantic queries (natural language)</p></td>
<td><p>Filter by lap number vs. search by race situation</p></td>
</tr>
<tr class="row-even"><td><p>Indexes on exact values</p></td>
<td><p>Indexes on vector similarity</p></td>
<td><p>Index by circuit name vs. index by race characteristics</p></td>
</tr>
<tr class="row-odd"><td><p>Returns exact matches</p></td>
<td><p>Returns ranked by similarity</p></td>
<td><p>Exact match vs. closest historical parallels</p></td>
</tr>
<tr class="row-even"><td><p>Fast for equality/range queries</p></td>
<td><p>Fast for nearest-neighbor queries</p></td>
<td><p>Fast for filters vs. fast for similarity</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="approximate-nearest-neighbors-ann">
<h3>Approximate Nearest Neighbors (ANN)<a class="headerlink" href="#approximate-nearest-neighbors-ann" title="Link to this heading">#</a></h3>
<p>The main ANN algorithms trade a small amount of accuracy for huge speed gains:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Algorithm</p></th>
<th class="head"><p>How It Works</p></th>
<th class="head"><p>Used By</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>IVF (Inverted File Index)</strong></p></td>
<td><p>Cluster vectors, search only nearby clusters</p></td>
<td><p>FAISS</p></td>
</tr>
<tr class="row-odd"><td><p><strong>HNSW (Hierarchical NSW)</strong></p></td>
<td><p>Build a graph of neighbors at multiple scales</p></td>
<td><p>Most modern systems</p></td>
</tr>
<tr class="row-even"><td><p><strong>LSH (Locality-Sensitive Hashing)</strong></p></td>
<td><p>Hash similar vectors to same bucket</p></td>
<td><p>Early systems</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Product Quantization</strong></p></td>
<td><p>Compress vectors by splitting into subspaces</p></td>
<td><p>FAISS (with IVF)</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="building-a-simple-semantic-search-engine-from-scratch">
<h3>Building a Simple Semantic Search Engine from Scratch<a class="headerlink" href="#building-a-simple-semantic-search-engine-from-scratch" title="Link to this heading">#</a></h3>
<p>Let’s build a complete semantic search pipeline. We will:</p>
<ol class="arabic simple">
<li><p>Create a document collection</p></li>
<li><p>Embed each document using our Word2Vec model</p></li>
<li><p>Accept a query, embed it, and find the most similar documents</p></li>
</ol>
<p><strong>F1 framing:</strong> Think of this as building a race situation search engine. You embed every historical race situation in your database, and when a new situation arises during a live race, you embed it and find the closest historical matches – “In 2019 at Hockenheim, a similar rain situation led to…” This is exactly how modern strategy tools augment human decision-making.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SimpleVectorStore</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A minimal vector database for semantic search.</span>
<span class="sd">    Uses brute-force cosine similarity (no ANN indexing).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="n">embedding_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vectors</span> <span class="o">=</span> <span class="p">[]</span>       <span class="c1"># List of embedding vectors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">documents</span> <span class="o">=</span> <span class="p">[]</span>     <span class="c1"># List of original documents</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metadata</span> <span class="o">=</span> <span class="p">[]</span>      <span class="c1"># Optional metadata</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">document</span><span class="p">,</span> <span class="n">vector</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add a document and its embedding to the store.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vectors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vector</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">vector</span><span class="p">))</span>  <span class="c1"># Normalize for cosine sim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">documents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">document</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metadata</span> <span class="ow">or</span> <span class="p">{})</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">search</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query_vector</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Find the top_k most similar documents to the query.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            query_vector: Query embedding</span>
<span class="sd">            top_k: Number of results to return</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">            List of (document, similarity, metadata) tuples</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">query_norm</span> <span class="o">=</span> <span class="n">query_vector</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">query_vector</span><span class="p">)</span>
        
        <span class="c1"># Compute cosine similarity with all documents</span>
        <span class="n">similarities</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">query_norm</span><span class="p">,</span> <span class="n">vec</span><span class="p">)</span> <span class="k">for</span> <span class="n">vec</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vectors</span><span class="p">]</span>
        
        <span class="c1"># Get top-k indices</span>
        <span class="n">top_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">similarities</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="n">top_k</span><span class="p">]</span>
        
        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">top_indices</span><span class="p">:</span>
            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">documents</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span>
                <span class="n">similarities</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
            <span class="p">))</span>
        
        <span class="k">return</span> <span class="n">results</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">documents</span><span class="p">)</span>

<span class="c1"># Create document collection</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;the king rules the kingdom with wisdom and power&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the queen leads the kingdom with grace and intelligence&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the prince trains to become a future king&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the princess studies diplomacy and leadership&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the man works hard in the village every day&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the woman tends the garden with great care&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the boy plays with friends in the village square&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the girl reads books under the old oak tree&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the knight protects the kingdom from invaders&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the scholar teaches young students in the academy&quot;</span><span class="p">,</span>
    <span class="s2">&quot;a brave warrior defends the castle walls&quot;</span><span class="p">,</span>
    <span class="s2">&quot;the royal throne sits in the great hall&quot;</span><span class="p">,</span>
    <span class="s2">&quot;golden crown jewels are kept in the vault&quot;</span><span class="p">,</span>
    <span class="s2">&quot;village life is simple and peaceful&quot;</span><span class="p">,</span>
    <span class="s2">&quot;children play games in the meadow&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="c1"># Build the vector store</span>
<span class="n">store</span> <span class="o">=</span> <span class="n">SimpleVectorStore</span><span class="p">(</span><span class="n">EMBEDDING_DIM</span><span class="p">)</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">get_sentence_embedding</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">embeddings_matrix</span><span class="p">,</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
    <span class="n">store</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">vec</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;length&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">split</span><span class="p">())})</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Vector store contains </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">store</span><span class="p">)</span><span class="si">}</span><span class="s2"> documents&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Embedding dimension: </span><span class="si">{</span><span class="n">store</span><span class="o">.</span><span class="n">embedding_dim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Search!</span>
<span class="n">queries</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;king and queen rule together&quot;</span><span class="p">,</span>
    <span class="s2">&quot;boy plays in village&quot;</span><span class="p">,</span>
    <span class="s2">&quot;royal crown and throne&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">query</span> <span class="ow">in</span> <span class="n">queries</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Query: &#39;</span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="n">query_vec</span> <span class="o">=</span> <span class="n">get_sentence_embedding</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">embeddings_matrix</span><span class="p">,</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">sim</span><span class="p">,</span> <span class="n">meta</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  #</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2"> (sim=</span><span class="si">{</span><span class="n">sim</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">doc</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualization-embedding-space-with-query-and-retrieved-results">
<h3>Visualization: Embedding Space with Query and Retrieved Results<a class="headerlink" href="#visualization-embedding-space-with-query-and-retrieved-results" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize retrieval in embedding space</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># Get 2D projections of all document embeddings</span>
<span class="n">doc_embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">get_sentence_embedding</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">embeddings_matrix</span><span class="p">,</span> <span class="n">word2idx</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> 
                           <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">])</span>
<span class="n">doc_2d</span> <span class="o">=</span> <span class="n">pca_2d</span><span class="p">(</span><span class="n">doc_embeddings</span><span class="p">)</span>

<span class="c1"># Plot all documents</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">doc_2d</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">doc_2d</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;lightgray&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>

<span class="c1"># Add short labels for each document</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">documents</span><span class="p">):</span>
    <span class="n">short</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">split</span><span class="p">()[:</span><span class="mi">4</span><span class="p">])</span> <span class="o">+</span> <span class="s1">&#39;...&#39;</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">short</span><span class="p">,</span> <span class="p">(</span><span class="n">doc_2d</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">doc_2d</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
               <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">)</span>

<span class="c1"># Process a query and highlight results</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;king and queen rule together&quot;</span>
<span class="n">query_vec</span> <span class="o">=</span> <span class="n">get_sentence_embedding</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">embeddings_matrix</span><span class="p">,</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Project query into same 2D space</span>
<span class="n">all_vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">doc_embeddings</span><span class="p">,</span> <span class="n">query_vec</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)])</span>
<span class="n">all_2d</span> <span class="o">=</span> <span class="n">pca_2d</span><span class="p">(</span><span class="n">all_vecs</span><span class="p">)</span>
<span class="n">query_2d</span> <span class="o">=</span> <span class="n">all_2d</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">doc_2d_new</span> <span class="o">=</span> <span class="n">all_2d</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Re-plot with query</span>
<span class="n">ax</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">doc_2d_new</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">doc_2d_new</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;lightgray&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">documents</span><span class="p">):</span>
    <span class="n">short</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">split</span><span class="p">()[:</span><span class="mi">4</span><span class="p">])</span> <span class="o">+</span> <span class="s1">&#39;...&#39;</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">short</span><span class="p">,</span> <span class="p">(</span><span class="n">doc_2d_new</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">doc_2d_new</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
               <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">)</span>

<span class="c1"># Highlight retrieved documents</span>
<span class="n">retrieved_docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>
<span class="n">retrieved_sims</span> <span class="o">=</span> <span class="p">[</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>
<span class="n">colors_retrieved</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;limegreen&#39;</span><span class="p">,</span> <span class="s1">&#39;yellowgreen&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">sim</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">results</span><span class="p">):</span>
    <span class="n">doc_idx</span> <span class="o">=</span> <span class="n">documents</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">doc_2d_new</span><span class="p">[</span><span class="n">doc_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">doc_2d_new</span><span class="p">[</span><span class="n">doc_idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> 
              <span class="n">c</span><span class="o">=</span><span class="n">colors_retrieved</span><span class="p">[</span><span class="n">rank</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
              <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;#</span><span class="si">{</span><span class="n">rank</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">: sim=</span><span class="si">{</span><span class="n">sim</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    
    <span class="c1"># Draw line from query to result</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">query_2d</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">doc_2d_new</span><span class="p">[</span><span class="n">doc_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> 
            <span class="p">[</span><span class="n">query_2d</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">doc_2d_new</span><span class="p">[</span><span class="n">doc_idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> 
            <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors_retrieved</span><span class="p">[</span><span class="n">rank</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>

<span class="c1"># Plot query</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">query_2d</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">query_2d</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> 
          <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Query: &quot;</span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s1">&quot;&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Semantic Search: Query and Retrieved Documents in Embedding Space&#39;</span><span class="p">,</span> 
             <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;PC 1&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;PC 2&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-vector-database-ecosystem">
<h3>The Vector Database Ecosystem<a class="headerlink" href="#the-vector-database-ecosystem" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Tool</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Key Feature</p></th>
<th class="head"><p>Best For</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>FAISS</strong> (Meta)</p></td>
<td><p>Library</p></td>
<td><p>Blazing fast, GPU support</p></td>
<td><p>Research, large-scale</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Pinecone</strong></p></td>
<td><p>Managed service</p></td>
<td><p>Fully hosted, easy API</p></td>
<td><p>Production, no ops</p></td>
</tr>
<tr class="row-even"><td><p><strong>Chroma</strong></p></td>
<td><p>Open source</p></td>
<td><p>Lightweight, embedded</p></td>
<td><p>Prototyping, small projects</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Weaviate</strong></p></td>
<td><p>Open source</p></td>
<td><p>Hybrid search (vector + keyword)</p></td>
<td><p>Complex search needs</p></td>
</tr>
<tr class="row-even"><td><p><strong>Qdrant</strong></p></td>
<td><p>Open source</p></td>
<td><p>Filtering + vector search</p></td>
<td><p>Production, self-hosted</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Milvus</strong></p></td>
<td><p>Open source</p></td>
<td><p>Distributed, scalable</p></td>
<td><p>Very large scale</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="rag-retrieval-augmented-generation">
<h3>RAG: Retrieval-Augmented Generation<a class="headerlink" href="#rag-retrieval-augmented-generation" title="Link to this heading">#</a></h3>
<p>RAG is one of the most important patterns in modern AI. The idea is simple:</p>
<ol class="arabic simple">
<li><p><strong>Retrieve:</strong> Use embedding similarity to find relevant documents from a knowledge base</p></li>
<li><p><strong>Augment:</strong> Add those documents to the prompt as context</p></li>
<li><p><strong>Generate:</strong> Have an LLM generate an answer using the retrieved context</p></li>
</ol>
<p><strong>Why RAG matters:</strong></p>
<ul class="simple">
<li><p>LLMs have a knowledge cutoff – RAG gives them access to current information</p></li>
<li><p>LLMs can hallucinate – RAG grounds answers in real documents</p></li>
<li><p>You can update the knowledge base without retraining the model</p></li>
<li><p>The user can verify answers by checking the source documents</p></li>
</ul>
<p><strong>F1 analogy:</strong> RAG for F1 strategy would work like this: (1) <strong>Retrieve</strong> the most similar historical race situations from the embedded database; (2) <strong>Augment</strong> the strategy model’s input with those historical cases – “In 5 similar situations, early pit stops won 4 times”; (3) <strong>Generate</strong> a strategy recommendation grounded in real precedent. This is essentially what experienced strategists do intuitively – they recall similar historical situations to inform current decisions. RAG automates and scales that institutional memory.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simulate a RAG pipeline</span>
<span class="k">def</span><span class="w"> </span><span class="nf">rag_pipeline</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">vector_store</span><span class="p">,</span> <span class="n">embeddings_matrix</span><span class="p">,</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Simulate a RAG pipeline:</span>
<span class="sd">    1. Embed the question</span>
<span class="sd">    2. Retrieve relevant documents</span>
<span class="sd">    3. Show what would be sent to an LLM</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        question: User&#39;s question</span>
<span class="sd">        vector_store: Our SimpleVectorStore</span>
<span class="sd">        embeddings_matrix: Word embeddings</span>
<span class="sd">        word2idx: Word to index mapping</span>
<span class="sd">        top_k: Number of documents to retrieve</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        The constructed prompt (in a real system, this goes to an LLM)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Step 1: Embed the question</span>
    <span class="n">query_vec</span> <span class="o">=</span> <span class="n">get_sentence_embedding</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">embeddings_matrix</span><span class="p">,</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
    
    <span class="c1"># Step 2: Retrieve relevant documents</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">vector_store</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">)</span>
    
    <span class="c1"># Step 3: Construct prompt</span>
    <span class="n">context</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s2">&quot;- </span><span class="si">{</span><span class="n">doc</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">sim</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">results</span><span class="p">])</span>
    
    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;Answer the question based on the following context.</span>

<span class="s2">Context:</span>
<span class="si">{</span><span class="n">context</span><span class="si">}</span>

<span class="s2">Question: </span><span class="si">{</span><span class="n">question</span><span class="si">}</span>
<span class="s2">Answer:&quot;&quot;&quot;</span>
    
    <span class="k">return</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">results</span>

<span class="c1"># Demo the RAG pipeline</span>
<span class="n">question</span> <span class="o">=</span> <span class="s2">&quot;who rules the kingdom&quot;</span>
<span class="n">prompt</span><span class="p">,</span> <span class="n">results</span> <span class="o">=</span> <span class="n">rag_pipeline</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">store</span><span class="p">,</span> <span class="n">embeddings_matrix</span><span class="p">,</span> <span class="n">word2idx</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== RAG Pipeline Demo ===</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Step 1: User asks a question&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Question: &#39;</span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="s2">&#39;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Step 2: Retrieve relevant documents&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">sim</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  #</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2"> (sim=</span><span class="si">{</span><span class="n">sim</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">doc</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Step 3: Construct prompt for LLM&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Step 4: Send to LLM (not implemented here -- would call GPT/Claude API)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The LLM would answer based on the retrieved context!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="practical-applications">
<h2>7. Practical Applications<a class="headerlink" href="#practical-applications" title="Link to this heading">#</a></h2>
<section id="id7">
<h3>Intuitive Explanation<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<p>Embeddings are not just an academic curiosity – they power some of the most impactful applications in technology. The fundamental pattern is always the same: represent items as vectors, then use similarity to find related items.</p>
</section>
<section id="why-this-matters-in-machine-learning">
<h3>Why This Matters in Machine Learning<a class="headerlink" href="#why-this-matters-in-machine-learning" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Application</p></th>
<th class="head"><p>How Embeddings Are Used</p></th>
<th class="head"><p>Example</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Semantic search</strong></p></td>
<td><p>Query and documents are embedded; find documents closest to query</p></td>
<td><p>Google Search, Bing</p></td>
<td><p>“Find races similar to Hungary 2021”</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Recommendation systems</strong></p></td>
<td><p>Users and items are embedded; recommend items close to user</p></td>
<td><p>Netflix, Spotify, Amazon</p></td>
<td><p>“Fans who liked Silverstone also liked Spa”</p></td>
</tr>
<tr class="row-even"><td><p><strong>Clustering / topic modeling</strong></p></td>
<td><p>Embed documents, cluster similar ones</p></td>
<td><p>News categorization</p></td>
<td><p>Cluster race weekends by characteristics</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Anomaly detection</strong></p></td>
<td><p>Items far from all clusters are anomalies</p></td>
<td><p>Fraud detection</p></td>
<td><p>Unusual lap times or telemetry patterns</p></td>
</tr>
<tr class="row-even"><td><p><strong>Duplicate detection</strong></p></td>
<td><p>Similar embeddings = potential duplicates</p></td>
<td><p>Customer deduplication</p></td>
<td><p>Detect duplicate timing entries</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Classification</strong></p></td>
<td><p>Use embeddings as features for classifiers</p></td>
<td><p>Sentiment analysis</p></td>
<td><p>Classify race incidents (racing incident vs. penalty)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Cross-lingual tasks</strong></p></td>
<td><p>Align embeddings across languages</p></td>
<td><p>Translation, multilingual search</p></td>
<td><p>Align driver profiles across different data sources</p></td>
</tr>
<tr class="row-odd"><td><p><strong>RAG</strong></p></td>
<td><p>Retrieve relevant documents to augment LLM prompts</p></td>
<td><p>ChatGPT with plugins</p></td>
<td><p>Strategy recommendations grounded in historical data</p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Demo: Simple recommendation system using embeddings</span>
<span class="c1"># Simulate a &quot;user liked these items, recommend similar ones&quot;</span>

<span class="c1"># Items with hand-crafted embeddings simulating a movie recommendation scenario</span>
<span class="c1"># Dimensions roughly represent: [action, romance, comedy, scifi, drama]</span>
<span class="n">movie_embeddings</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;The Matrix&#39;</span><span class="p">:</span>       <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]),</span>
    <span class="s1">&#39;Inception&#39;</span><span class="p">:</span>        <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]),</span>
    <span class="s1">&#39;Interstellar&#39;</span><span class="p">:</span>     <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]),</span>
    <span class="s1">&#39;Titanic&#39;</span><span class="p">:</span>          <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]),</span>
    <span class="s1">&#39;The Notebook&#39;</span><span class="p">:</span>     <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]),</span>
    <span class="s1">&#39;Pride &amp; Prejudice&#39;</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]),</span>
    <span class="s1">&#39;Superbad&#39;</span><span class="p">:</span>         <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]),</span>
    <span class="s1">&#39;The Hangover&#39;</span><span class="p">:</span>     <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]),</span>
    <span class="s1">&#39;Step Brothers&#39;</span><span class="p">:</span>    <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]),</span>
    <span class="s1">&#39;John Wick&#39;</span><span class="p">:</span>        <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]),</span>
    <span class="s1">&#39;Mad Max&#39;</span><span class="p">:</span>          <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]),</span>
    <span class="s1">&#39;Blade Runner&#39;</span><span class="p">:</span>     <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]),</span>
<span class="p">}</span>

<span class="k">def</span><span class="w"> </span><span class="nf">recommend</span><span class="p">(</span><span class="n">liked_movies</span><span class="p">,</span> <span class="n">movie_embeddings</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Recommend movies based on user&#39;s liked movies.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        liked_movies: List of movie titles the user liked</span>
<span class="sd">        movie_embeddings: Dict of movie -&gt; embedding</span>
<span class="sd">        top_k: Number of recommendations</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        List of (movie, similarity) tuples</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Create user profile: average of liked movie embeddings</span>
    <span class="n">user_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">movie_embeddings</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">liked_movies</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="c1"># Find most similar movies (excluding already liked)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">movie</span><span class="p">,</span> <span class="n">emb</span> <span class="ow">in</span> <span class="n">movie_embeddings</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">movie</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">liked_movies</span><span class="p">:</span>
            <span class="n">sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">user_vec</span><span class="p">,</span> <span class="n">emb</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">user_vec</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">emb</span><span class="p">))</span>
            <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">movie</span><span class="p">,</span> <span class="n">sim</span><span class="p">))</span>
    
    <span class="n">scores</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scores</span><span class="p">[:</span><span class="n">top_k</span><span class="p">]</span>

<span class="c1"># Test recommendations</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Movie Recommendation Demo ===</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">user_profiles</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;Sci-fi fan&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;The Matrix&#39;</span><span class="p">,</span> <span class="s1">&#39;Inception&#39;</span><span class="p">],</span>
    <span class="s1">&#39;Romance fan&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Titanic&#39;</span><span class="p">,</span> <span class="s1">&#39;The Notebook&#39;</span><span class="p">],</span>
    <span class="s1">&#39;Comedy fan&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Superbad&#39;</span><span class="p">,</span> <span class="s1">&#39;The Hangover&#39;</span><span class="p">],</span>
<span class="p">}</span>

<span class="k">for</span> <span class="n">profile_name</span><span class="p">,</span> <span class="n">liked</span> <span class="ow">in</span> <span class="n">user_profiles</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">recs</span> <span class="o">=</span> <span class="n">recommend</span><span class="p">(</span><span class="n">liked</span><span class="p">,</span> <span class="n">movie_embeddings</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">profile_name</span><span class="si">}</span><span class="s2"> (liked: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">liked</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">movie</span><span class="p">,</span> <span class="n">sim</span> <span class="ow">in</span> <span class="n">recs</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  -&gt; </span><span class="si">{</span><span class="n">movie</span><span class="si">:</span><span class="s2">20s</span><span class="si">}</span><span class="s2"> (similarity: </span><span class="si">{</span><span class="n">sim</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualization-clustering-in-embedding-space">
<h3>Visualization: Clustering in Embedding Space<a class="headerlink" href="#visualization-clustering-in-embedding-space" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize movie embeddings in 2D with genre clusters</span>
<span class="n">movie_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">movie_embeddings</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="n">movie_vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">movie_embeddings</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>

<span class="c1"># PCA to 2D</span>
<span class="n">movie_2d</span> <span class="o">=</span> <span class="n">pca_2d</span><span class="p">(</span><span class="n">movie_vecs</span><span class="p">)</span>

<span class="c1"># Assign genres for coloring</span>
<span class="n">genres</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;Sci-fi/Action&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;The Matrix&#39;</span><span class="p">,</span> <span class="s1">&#39;Inception&#39;</span><span class="p">,</span> <span class="s1">&#39;Interstellar&#39;</span><span class="p">,</span> <span class="s1">&#39;Blade Runner&#39;</span><span class="p">,</span> <span class="s1">&#39;Mad Max&#39;</span><span class="p">,</span> <span class="s1">&#39;John Wick&#39;</span><span class="p">],</span>
    <span class="s1">&#39;Romance/Drama&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Titanic&#39;</span><span class="p">,</span> <span class="s1">&#39;The Notebook&#39;</span><span class="p">,</span> <span class="s1">&#39;Pride &amp; Prejudice&#39;</span><span class="p">],</span>
    <span class="s1">&#39;Comedy&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Superbad&#39;</span><span class="p">,</span> <span class="s1">&#39;The Hangover&#39;</span><span class="p">,</span> <span class="s1">&#39;Step Brothers&#39;</span><span class="p">],</span>
<span class="p">}</span>

<span class="n">genre_colors</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Sci-fi/Action&#39;</span><span class="p">:</span> <span class="s1">&#39;steelblue&#39;</span><span class="p">,</span> <span class="s1">&#39;Romance/Drama&#39;</span><span class="p">:</span> <span class="s1">&#39;lightcoral&#39;</span><span class="p">,</span> <span class="s1">&#39;Comedy&#39;</span><span class="p">:</span> <span class="s1">&#39;green&#39;</span><span class="p">}</span>
<span class="n">movie_genre</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">genre</span><span class="p">,</span> <span class="n">movies</span> <span class="ow">in</span> <span class="n">genres</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">movie</span> <span class="ow">in</span> <span class="n">movies</span><span class="p">:</span>
        <span class="n">movie_genre</span><span class="p">[</span><span class="n">movie</span><span class="p">]</span> <span class="o">=</span> <span class="n">genre</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">movie</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">movie_names</span><span class="p">):</span>
    <span class="n">genre</span> <span class="o">=</span> <span class="n">movie_genre</span><span class="p">[</span><span class="n">movie</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">movie_2d</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">movie_2d</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">genre_colors</span><span class="p">[</span><span class="n">genre</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> 
              <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
              <span class="n">label</span><span class="o">=</span><span class="n">genre</span> <span class="k">if</span> <span class="n">movie</span> <span class="o">==</span> <span class="n">genres</span><span class="p">[</span><span class="n">genre</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">movie</span><span class="p">,</span> <span class="p">(</span><span class="n">movie_2d</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">movie_2d</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span>
               <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Movie Embeddings: Genre Clusters Emerge Naturally&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;PC 1&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;PC 2&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Embeddings naturally cluster by genre -- no explicit genre labels were used!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;This is the power of learning representations from data.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<section id="exercise-1-implement-cosine-similarity-from-scratch">
<h3>Exercise 1: Implement Cosine Similarity from Scratch<a class="headerlink" href="#exercise-1-implement-cosine-similarity-from-scratch" title="Link to this heading">#</a></h3>
<p>Implement cosine similarity using only NumPy, without using any library similarity function.</p>
<p><strong>F1 framing:</strong> You are building a driver similarity tool. Given two driver embeddings (vectors of performance stats across multiple dimensions), compute how similar their profiles are using cosine similarity. Two drivers pointing in the same direction in embedding space have similar styles, regardless of whether one is “stronger” overall.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># EXERCISE 1: Implement cosine similarity</span>
<span class="k">def</span><span class="w"> </span><span class="nf">cosine_similarity_manual</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute cosine similarity between two vectors using only NumPy.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        a: First vector (1D numpy array)</span>
<span class="sd">        b: Second vector (1D numpy array)</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        Cosine similarity (scalar between -1 and 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement this!</span>
    <span class="c1"># Step 1: Compute dot product of a and b</span>
    <span class="c1"># Step 2: Compute L2 norm of a</span>
    <span class="c1"># Step 3: Compute L2 norm of b</span>
    <span class="c1"># Step 4: Return dot_product / (norm_a * norm_b)</span>
    <span class="c1"># Hint: Use np.dot(), np.sqrt(), np.sum()</span>
    
    <span class="k">pass</span>  <span class="c1"># Replace with your implementation</span>

<span class="c1"># Test</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>  <span class="c1"># Identical</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0</span><span class="p">])</span>  <span class="c1"># Opposite</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>  <span class="c1"># Partially aligned</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test results:&quot;</span><span class="p">)</span>
<span class="n">result1</span> <span class="o">=</span> <span class="n">cosine_similarity_manual</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Identical vectors: </span><span class="si">{</span><span class="n">result1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Expected: 1.0000&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Correct: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">result1</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0</span><span class="p">)</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">result1</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;Not implemented&#39;</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">result2</span> <span class="o">=</span> <span class="n">cosine_similarity_manual</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">  Opposite vectors: </span><span class="si">{</span><span class="n">result2</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Expected: -1.0000&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Correct: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">result2</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">1.0</span><span class="p">)</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">result2</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;Not implemented&#39;</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">result3</span> <span class="o">=</span> <span class="n">cosine_similarity_manual</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">expected3</span> <span class="o">=</span> <span class="mf">3.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">14</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">  Partial alignment: </span><span class="si">{</span><span class="n">result3</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Expected: </span><span class="si">{</span><span class="n">expected3</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Correct: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">result3</span><span class="p">,</span><span class="w"> </span><span class="n">expected3</span><span class="p">)</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">result3</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;Not implemented&#39;</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-2-implement-cbow-word2vec">
<h3>Exercise 2: Implement CBOW Word2Vec<a class="headerlink" href="#exercise-2-implement-cbow-word2vec" title="Link to this heading">#</a></h3>
<p>Modify the skip-gram model to implement CBOW (Continuous Bag of Words). Instead of predicting context from center, predict the center word from the average of context word embeddings.</p>
<p><strong>F1 framing:</strong> If skip-gram is “given Verstappen, predict what events surround him (pole, victory, fastest lap),” then CBOW is “given a set of events (pole, victory, fastest lap), predict which driver is at the center.” Both learn the same driver embeddings from different directions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># EXERCISE 2: Implement CBOW</span>
<span class="k">class</span><span class="w"> </span><span class="nc">CBOWNegSampling</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    CBOW Word2Vec with negative sampling.</span>
<span class="sd">    </span>
<span class="sd">    Given context words, predict the center word.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
        
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="o">/</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="mf">0.5</span><span class="o">/</span><span class="n">embedding_dim</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="o">/</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="mf">0.5</span><span class="o">/</span><span class="n">embedding_dim</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context_words</span><span class="p">,</span> <span class="n">center_word</span><span class="p">,</span> <span class="n">negative_words</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            context_words: (batch_size, 2*window) context word indices</span>
<span class="sd">            center_word: (batch_size,) center word index</span>
<span class="sd">            negative_words: (batch_size, num_neg) negative sample indices</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">            loss: negative sampling loss</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO: Implement CBOW forward pass!</span>
        <span class="c1"># Step 1: Look up context word embeddings and average them</span>
        <span class="c1">#   context_emb = self.embeddings(context_words)  # (batch, 2*window, emb_dim)</span>
        <span class="c1">#   context_avg = context_emb.mean(dim=1)          # (batch, emb_dim)</span>
        <span class="c1">#</span>
        <span class="c1"># Step 2: Look up center word output embedding</span>
        <span class="c1">#   center_emb = self.output_embeddings(center_word)  # (batch, emb_dim)</span>
        <span class="c1">#</span>
        <span class="c1"># Step 3: Compute positive score (dot product of context_avg and center_emb)</span>
        <span class="c1">#   pos_score = (context_avg * center_emb).sum(dim=1)</span>
        <span class="c1">#   pos_loss = F.logsigmoid(pos_score)</span>
        <span class="c1">#</span>
        <span class="c1"># Step 4: Compute negative scores</span>
        <span class="c1">#   neg_emb = self.output_embeddings(negative_words)</span>
        <span class="c1">#   neg_score = torch.bmm(neg_emb, context_avg.unsqueeze(2)).squeeze(2)</span>
        <span class="c1">#   neg_loss = F.logsigmoid(-neg_score).sum(dim=1)</span>
        <span class="c1">#</span>
        <span class="c1"># Step 5: Return -(pos_loss + neg_loss).mean()</span>
        
        <span class="k">pass</span>  <span class="c1"># Replace with your implementation</span>

<span class="c1"># Test structure (you would need to generate CBOW training data to fully test)</span>
<span class="c1"># cbow_model = CBOWNegSampling(vocab_size, EMBEDDING_DIM)</span>
<span class="c1"># print(f&quot;CBOW parameters: {sum(p.numel() for p in cbow_model.parameters()):,}&quot;)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-3-build-an-anomaly-detector-using-embeddings">
<h3>Exercise 3: Build an Anomaly Detector Using Embeddings<a class="headerlink" href="#exercise-3-build-an-anomaly-detector-using-embeddings" title="Link to this heading">#</a></h3>
<p>Given a collection of “normal” items, detect anomalies by finding items that are far from all cluster centers.</p>
<p><strong>F1 framing:</strong> Build a telemetry anomaly detector. Embed each lap’s telemetry signature and cluster normal laps together. Laps that are far from all clusters in embedding space are anomalies – possibly indicating mechanical issues, unusual driving, or data errors. An engineer reviewing 50+ laps needs this kind of automated flagging.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># EXERCISE 3: Anomaly detection with embeddings</span>
<span class="k">def</span><span class="w"> </span><span class="nf">detect_anomalies</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">item_embeddings</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Detect anomalies by finding items whose average similarity </span>
<span class="sd">    to all other items is below the threshold.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        items: List of item names</span>
<span class="sd">        item_embeddings: Dict of item_name -&gt; embedding vector</span>
<span class="sd">        threshold: Similarity threshold (items below this are anomalies)</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        List of (item_name, avg_similarity) for items below threshold,</span>
<span class="sd">        sorted by similarity (most anomalous first)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement this!</span>
    <span class="c1"># Step 1: For each item, compute its average cosine similarity to all other items</span>
    <span class="c1"># Step 2: Flag items with average similarity below threshold</span>
    <span class="c1"># Step 3: Return sorted list of anomalies</span>
    <span class="c1"># Hint: Use the cosine_similarity function defined earlier</span>
    
    <span class="k">pass</span>  <span class="c1"># Replace with your implementation</span>

<span class="c1"># Test data: mostly animals, with some outliers</span>
<span class="n">test_embeddings</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;cat&#39;</span><span class="p">:</span>        <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]),</span>
    <span class="s1">&#39;dog&#39;</span><span class="p">:</span>        <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]),</span>
    <span class="s1">&#39;hamster&#39;</span><span class="p">:</span>    <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]),</span>
    <span class="s1">&#39;parrot&#39;</span><span class="p">:</span>     <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]),</span>
    <span class="s1">&#39;goldfish&#39;</span><span class="p">:</span>   <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]),</span>
    <span class="s1">&#39;airplane&#39;</span><span class="p">:</span>   <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">]),</span>   <span class="c1"># ANOMALY!</span>
    <span class="s1">&#39;submarine&#39;</span><span class="p">:</span>  <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">]),</span>   <span class="c1"># ANOMALY!</span>
    <span class="s1">&#39;rabbit&#39;</span><span class="p">:</span>     <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]),</span>
<span class="p">}</span>

<span class="c1"># anomalies = detect_anomalies(list(test_embeddings.keys()), test_embeddings, threshold=0.5)</span>
<span class="c1"># if anomalies:</span>
<span class="c1">#     print(&quot;Detected anomalies:&quot;)</span>
<span class="c1">#     for item, sim in anomalies:</span>
<span class="c1">#         print(f&quot;  {item:12s}: avg similarity = {sim:.4f}&quot;)</span>
<span class="c1"># else:</span>
<span class="c1">#     print(&quot;No anomalies detected (check your implementation)&quot;)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Uncomment the test code above after implementing detect_anomalies()&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<section id="key-concepts">
<h3>Key Concepts<a class="headerlink" href="#key-concepts" title="Link to this heading">#</a></h3>
<p><strong>Why Embeddings:</strong></p>
<ul class="simple">
<li><p>One-hot encoding is sparse, high-dimensional, and encodes no similarity</p></li>
<li><p>Dense embeddings represent items as short vectors where similar items are close</p></li>
<li><p>The distributional hypothesis: meaning comes from usage patterns</p></li>
</ul>
<p><strong>Word2Vec:</strong></p>
<ul class="simple">
<li><p>Skip-gram: predict context words from center word</p></li>
<li><p>CBOW: predict center word from context words</p></li>
<li><p>Negative sampling makes training efficient</p></li>
<li><p>Implicitly factorizes a word co-occurrence (PMI) matrix</p></li>
<li><p>Captures analogies via vector arithmetic: king - man + woman = queen</p></li>
</ul>
<p><strong>GloVe:</strong></p>
<ul class="simple">
<li><p>Explicitly builds and factorizes a co-occurrence matrix</p></li>
<li><p>Global context vs Word2Vec’s local context windows</p></li>
<li><p>Produces similarly good embeddings in practice</p></li>
</ul>
<p><strong>Modern Embeddings:</strong></p>
<ul class="simple">
<li><p>Static (Word2Vec, GloVe): one vector per word, regardless of context</p></li>
<li><p>Contextual (BERT, GPT): different vector for each word occurrence based on context</p></li>
<li><p>Sentence embeddings: mean pooling, [CLS] token, or specialized models</p></li>
</ul>
<p><strong>Similarity Measures:</strong></p>
<ul class="simple">
<li><p>Cosine similarity: direction only, range [-1, 1]</p></li>
<li><p>Euclidean distance: straight-line distance, sensitive to magnitude</p></li>
<li><p>Dot product: direction and magnitude, used in attention</p></li>
</ul>
<p><strong>Vector Databases and Retrieval:</strong></p>
<ul class="simple">
<li><p>Store embeddings for fast nearest-neighbor search</p></li>
<li><p>ANN algorithms (IVF, HNSW) trade small accuracy loss for huge speed gains</p></li>
<li><p>RAG: Retrieve relevant documents to augment LLM responses</p></li>
</ul>
</section>
<section id="connection-to-deep-learning">
<h3>Connection to Deep Learning<a class="headerlink" href="#connection-to-deep-learning" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Concept</p></th>
<th class="head"><p>Where It’s Used</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Word embeddings</p></td>
<td><p>First layer of every NLP model</p></td>
<td><p>Driver/circuit/event representation as vectors</p></td>
</tr>
<tr class="row-odd"><td><p>nn.Embedding</p></td>
<td><p>PyTorch lookup table, trained end-to-end</p></td>
<td><p>Converting driver IDs to performance profiles</p></td>
</tr>
<tr class="row-even"><td><p>Cosine similarity</p></td>
<td><p>Contrastive learning, similarity search</p></td>
<td><p>“How similar are these two drivers/situations?”</p></td>
</tr>
<tr class="row-odd"><td><p>Dot product similarity</p></td>
<td><p>Attention mechanism (Q * K)</p></td>
<td><p>Relevance scoring in strategy systems</p></td>
</tr>
<tr class="row-even"><td><p>Contextual embeddings</p></td>
<td><p>BERT, GPT, all modern language models</p></td>
<td><p>Context-dependent driver/event representations</p></td>
</tr>
<tr class="row-odd"><td><p>Sentence embeddings</p></td>
<td><p>Semantic search, RAG, classification</p></td>
<td><p>Full race-situation embeddings</p></td>
</tr>
<tr class="row-even"><td><p>Vector databases</p></td>
<td><p>Production search and retrieval systems</p></td>
<td><p>Historical race situation lookup at speed</p></td>
</tr>
<tr class="row-odd"><td><p>Negative sampling</p></td>
<td><p>Contrastive learning, SimCLR, CLIP</p></td>
<td><p>Efficient training with positive/negative pairs</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="checklist">
<h3>Checklist<a class="headerlink" href="#checklist" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>[ ] I can explain why one-hot encoding fails and why dense embeddings are needed</p></li>
<li><p>[ ] I understand the distributional hypothesis and how it motivates embeddings</p></li>
<li><p>[ ] I can implement skip-gram Word2Vec with negative sampling</p></li>
<li><p>[ ] I can perform vector arithmetic and solve analogies with embeddings</p></li>
<li><p>[ ] I can compare Word2Vec and GloVe approaches</p></li>
<li><p>[ ] I understand the difference between static and contextual embeddings</p></li>
<li><p>[ ] I can compute and compare cosine similarity, Euclidean distance, and dot product</p></li>
<li><p>[ ] I can build a simple semantic search engine</p></li>
<li><p>[ ] I can explain RAG and why it matters for modern AI</p></li>
<li><p>[ ] I can describe practical applications of embeddings across domains</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="next-steps">
<h2>Next Steps<a class="headerlink" href="#next-steps" title="Link to this heading">#</a></h2>
<p>You now understand <strong>embeddings</strong> – the fundamental representation that powers modern AI. Every time you use a search engine, get a recommendation, or chat with an LLM, embeddings are working behind the scenes to represent meaning as geometry. In F1 terms, you have learned how to represent drivers, circuits, and race situations as vectors where similarity means proximity – the foundation for any data-driven strategy system.</p>
<p>The key ideas to carry forward:</p>
<ol class="arabic simple">
<li><p><strong>Representation matters more than algorithms.</strong> A good embedding makes downstream tasks dramatically easier. This is why so much research focuses on learning better representations. In F1: a good driver/circuit representation is more valuable than a clever strategy algorithm operating on bad data.</p></li>
<li><p><strong>Similarity = proximity in embedding space.</strong> This one idea connects search, recommendation, clustering, anomaly detection, and more. If you can embed it, you can compare it. In F1: similar race situations cluster together, and finding the closest historical parallel guides strategy.</p></li>
<li><p><strong>From static to contextual.</strong> The evolution from Word2Vec to BERT/GPT shows how richer context produces better representations. Modern models compute embeddings as a function of the entire input. In F1: a driver’s embedding should depend on the full context – team, season, conditions – not be a fixed profile.</p></li>
<li><p><strong>RAG bridges embeddings and generation.</strong> Retrieval-Augmented Generation is one of the most practical patterns in modern AI, combining the precision of search with the fluency of language models. In F1: grounding strategy recommendations in retrieved historical precedent.</p></li>
</ol>
<p><strong>Practical next steps:</strong></p>
<ul class="simple">
<li><p>Try using pre-trained embeddings (e.g., <code class="docutils literal notranslate"><span class="pre">gensim</span></code> for Word2Vec/GloVe, <code class="docutils literal notranslate"><span class="pre">sentence-transformers</span></code> for sentence embeddings)</p></li>
<li><p>Build a RAG pipeline with a real embedding API and vector database</p></li>
<li><p>Experiment with embedding dimensions: how does quality change with 50 vs 300 vs 768 dimensions?</p></li>
<li><p>Explore multimodal embeddings (CLIP) that embed both images and text in the same space</p></li>
</ul>
<p><strong>In the next notebook,</strong> we will explore fine-tuning and parameter-efficient methods (LoRA, adapters), where you learn to adapt pre-trained models – including their embeddings – to specific tasks with minimal data and computation.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="17_transformer_architecture.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Part 6.1: Transformer Architecture</p>
      </div>
    </a>
    <a class="right-next"
       href="19_tokenization_lm_training.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Part 6.3: Tokenization &amp; Language Model Training</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-embeddings">1. Why Embeddings?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem-with-one-hot-encoding">The Problem with One-Hot Encoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-one-hot-vs-dense-embeddings">Visualization: One-Hot vs Dense Embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-distributional-hypothesis">The Distributional Hypothesis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-why-embeddings-matter">Deep Dive: Why Embeddings Matter</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#key-insight">Key Insight</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#common-misconceptions">Common Misconceptions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word2vec">2. Word2Vec</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuitive-explanation">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-skip-gram-vs-cbow">Visualization: Skip-gram vs CBOW</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-word2vec-skip-gram-from-scratch">Implementing Word2Vec (Skip-gram) from Scratch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-learned-word-embeddings-in-2d">Visualization: Learned Word Embeddings in 2D</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-magic-vector-arithmetic">The Magic: Vector Arithmetic</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-vector-arithmetic-in-embedding-space">Visualization: Vector Arithmetic in Embedding Space</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-why-does-word2vec-capture-semantics">Deep Dive: Why Does Word2Vec Capture Semantics?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Key Insight</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-negative-sampling-trick">The Negative Sampling Trick</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#glove-global-vectors-for-word-representation">3. GloVe (Global Vectors for Word Representation)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#glove-building-the-co-occurrence-matrix">GloVe: Building the Co-occurrence Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-word2vec-vs-glove">Comparison: Word2Vec vs GloVe</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modern-embeddings-from-static-to-contextual">4. Modern Embeddings: From Static to Contextual</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-contextual-embeddings-work">How Contextual Embeddings Work</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-static-vs-contextual-embeddings">Visualization: Static vs Contextual Embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sentence-embeddings-from-words-to-sentences">Sentence Embeddings: From Words to Sentences</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-the-evolution-of-embeddings">Deep Dive: The Evolution of Embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Key Insight</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#similarity-and-distance">5. Similarity and Distance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-comparing-similarity-measures">Visualization: Comparing Similarity Measures</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use-which">When to Use Which?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interactive-exploration-similarity-in-embedding-space">Interactive Exploration: Similarity in Embedding Space</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-databases-and-retrieval">6. Vector Databases and Retrieval</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-not-just-use-a-regular-database">Why Not Just Use a Regular Database?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approximate-nearest-neighbors-ann">Approximate Nearest Neighbors (ANN)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-simple-semantic-search-engine-from-scratch">Building a Simple Semantic Search Engine from Scratch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-embedding-space-with-query-and-retrieved-results">Visualization: Embedding Space with Query and Retrieved Results</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-vector-database-ecosystem">The Vector Database Ecosystem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-retrieval-augmented-generation">RAG: Retrieval-Augmented Generation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-applications">7. Practical Applications</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-this-matters-in-machine-learning">Why This Matters in Machine Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-clustering-in-embedding-space">Visualization: Clustering in Embedding Space</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-implement-cosine-similarity-from-scratch">Exercise 1: Implement Cosine Similarity from Scratch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-implement-cbow-word2vec">Exercise 2: Implement CBOW Word2Vec</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-build-an-anomaly-detector-using-embeddings">Exercise 3: Build an Anomaly Detector Using Embeddings</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts">Key Concepts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-deep-learning">Connection to Deep Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checklist">Checklist</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">Next Steps</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dan Shah
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>