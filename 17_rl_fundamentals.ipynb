{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Part 6.1: Reinforcement Learning Fundamentals — The Formula 1 Edition\n\nWe've spent 16 notebooks learning how to build models that learn from **labeled data** (supervised) or **unlabeled data** (self-supervised). But there's a third paradigm — one that learns from **experience and rewards**, just like a child learning to walk by falling down and getting back up.\n\n**Reinforcement Learning (RL)** is about an agent interacting with an environment, taking actions, receiving rewards, and learning a strategy to maximize long-term success. It's the foundation of game-playing AI (AlphaGo), robotics, and — critically — **RLHF**, the technique that makes language models like ChatGPT helpful and safe.\n\n**The F1 Connection:** Every Formula 1 race is a reinforcement learning problem in disguise. The race engineer and driver together form an *agent* that must make real-time decisions — when to pit, how hard to push, when to conserve tires — in a stochastic *environment* (weather changes, safety cars, tire degradation). The *reward* is championship points. A race strategy is literally a *policy*: a mapping from the car's current situation to the optimal action. In this notebook, we'll build the mathematical framework behind these decisions.\n\n## Learning Objectives\n\n- [ ] Understand the agent-environment loop and how RL differs from supervised learning\n- [ ] Define Markov Decision Processes (MDPs) and their components\n- [ ] Derive and implement the Bellman equations for value functions\n- [ ] Distinguish between state-value functions V(s) and action-value functions Q(s,a)\n- [ ] Implement policy evaluation and policy iteration from scratch\n- [ ] Understand the exploration vs. exploitation tradeoff\n- [ ] Implement value iteration to solve a gridworld environment\n- [ ] Compare policy-based vs. value-based methods at a high level\n- [ ] Build intuition for temporal difference learning\n- [ ] Connect RL concepts to the RLHF pipeline introduced in Notebook 16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Part 6.1: Reinforcement Learning Fundamentals\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 1. The Reinforcement Learning Paradigm\n\nIn supervised learning, we have input-output pairs and minimize a loss. In RL, there are no labels — the agent must **discover** which actions lead to rewards through trial and error.\n\n### The Agent-Environment Loop\n\nThe core RL cycle works like this:\n\n1. The **agent** observes the current **state** $s_t$\n2. The agent selects an **action** $a_t$ based on its **policy** $\\pi$\n3. The **environment** transitions to a new state $s_{t+1}$\n4. The environment returns a **reward** $r_t$\n5. Repeat\n\nThe goal: find a policy $\\pi^*$ that maximizes the **expected cumulative reward** over time.\n\n**F1 analogy:** The agent is the race strategist. The state is the car's current situation — track position P3, tire age 15 laps, gap to leader 4.2 seconds, medium compound, fuel load 60%. The actions are: pit now, push hard, conserve tires, defend position, use DRS. The reward is positions gained (or championship points at race end). The policy is the strategy: \"If tires are older than 20 laps AND gap to car ahead is under 1 second, pit for fresh softs.\"\n\n| Concept | Supervised Learning | Reinforcement Learning | F1 Parallel |\n|---------|-------------------|----------------------|-------------|\n| **Feedback** | Correct labels provided | Scalar reward signal | Points scored at end of race |\n| **Timing** | Immediate | Can be delayed | Pit stop pain now, position gain later |\n| **Data** | Fixed dataset | Generated by agent's actions | Each race is unique — new data from new decisions |\n| **Goal** | Minimize loss | Maximize cumulative reward | Maximize championship points |\n| **Exploration** | Not needed | Critical for learning | Trying an aggressive undercut vs. known overcut |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: The RL Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "ax.set_title('The Reinforcement Learning Loop', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Agent box\n",
    "agent_box = mpatches.FancyBboxPatch((1, 5), 3, 2, boxstyle=\"round,pad=0.3\",\n",
    "                                     facecolor='#3498db', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(agent_box)\n",
    "ax.text(2.5, 6, 'AGENT', ha='center', va='center', fontsize=14,\n",
    "        fontweight='bold', color='white')\n",
    "ax.text(2.5, 5.4, 'Policy π(a|s)', ha='center', va='center', fontsize=10, color='white')\n",
    "\n",
    "# Environment box\n",
    "env_box = mpatches.FancyBboxPatch((6, 5), 3, 2, boxstyle=\"round,pad=0.3\",\n",
    "                                   facecolor='#2ecc71', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(env_box)\n",
    "ax.text(7.5, 6, 'ENVIRONMENT', ha='center', va='center', fontsize=14,\n",
    "        fontweight='bold', color='white')\n",
    "ax.text(7.5, 5.4, 'P(s\\'|s,a), R(s,a)', ha='center', va='center', fontsize=10, color='white')\n",
    "\n",
    "# Action arrow (agent -> environment)\n",
    "ax.annotate('', xy=(6, 6.5), xytext=(4, 6.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2.5, color='#e74c3c'))\n",
    "ax.text(5, 7.1, 'Action $a_t$', ha='center', va='center', fontsize=12,\n",
    "        color='#e74c3c', fontweight='bold')\n",
    "\n",
    "# State arrow (environment -> agent, bottom)\n",
    "ax.annotate('', xy=(4, 5.3), xytext=(6, 5.3),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2.5, color='#9b59b6'))\n",
    "ax.text(5, 4.6, 'State $s_{t+1}$', ha='center', va='center', fontsize=12,\n",
    "        color='#9b59b6', fontweight='bold')\n",
    "\n",
    "# Reward arrow (environment -> agent, further below)\n",
    "ax.annotate('', xy=(2.5, 3.5), xytext=(7.5, 3.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2.5, color='#f39c12',\n",
    "                           connectionstyle='arc3,rad=0.3'))\n",
    "ax.text(5, 2.5, 'Reward $r_t$', ha='center', va='center', fontsize=12,\n",
    "        color='#f39c12', fontweight='bold')\n",
    "\n",
    "# Time step indicator\n",
    "ax.text(5, 1.2, 'At each timestep t = 0, 1, 2, ...', ha='center', va='center',\n",
    "        fontsize=11, style='italic', color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Key RL Terminology\n\n| Term | Symbol | Definition | F1 Parallel |\n|------|--------|------------|-------------|\n| **State** | $s \\in \\mathcal{S}$ | Current situation of the agent | Position, tire condition, gap to rivals, weather, laps remaining |\n| **Action** | $a \\in \\mathcal{A}$ | Decision the agent can make | Pit stop, push hard, conserve tires, defend position, use DRS |\n| **Policy** | $\\pi(a|s)$ | Strategy mapping states to actions | Race strategy: \"given THIS situation, do THIS\" |\n| **Reward** | $r_t$ | Immediate feedback signal | Positions gained, time advantage, championship points |\n| **Return** | $G_t$ | Cumulative discounted future reward | Total value of remaining race outcome |\n| **Discount factor** | $\\gamma \\in [0,1]$ | How much we value future vs. present rewards | How much a position gain on lap 50 matters vs. lap 1 |\n| **Episode** | — | One complete sequence from start to terminal state | One full race, lights out to checkered flag |\n| **Trajectory** | $\\tau$ | Sequence of (state, action, reward) tuples | Full race log: every decision and its outcome |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 2. Markov Decision Processes (MDPs)\n\nAn MDP formalizes the RL problem mathematically. It's defined by the tuple $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$:\n\n- $\\mathcal{S}$: Set of states\n- $\\mathcal{A}$: Set of actions\n- $P(s'|s,a)$: **Transition probability** — probability of reaching state $s'$ from state $s$ after taking action $a$\n- $R(s,a)$: **Reward function** — expected reward for taking action $a$ in state $s$\n- $\\gamma$: **Discount factor** — balances immediate vs. future rewards\n\n### The Markov Property\n\nThe key assumption: the future depends only on the **current state**, not the history:\n\n$$P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, \\ldots) = P(s_{t+1} | s_t, a_t)$$\n\nThis is powerful because it means we can make optimal decisions using only the current state — no need to remember the entire history.\n\n### Intuitive Explanation\n\nThink of chess: the current board position contains everything you need to make your next move. It doesn't matter *how* you got to that position — the optimal strategy depends only on where the pieces are *right now*.\n\n**F1 analogy:** A race is a Markov decision process. The state is your car's *current* situation: P4, lap 32 of 57, medium tires at 60% life, 2.1 seconds behind P3, fuel load nominal. Your optimal strategy decision (pit now? push? conserve?) depends only on THIS snapshot, not on whether you gained two positions on lap 1 or started P4. The transition probabilities are stochastic — you choose \"push hard,\" but there's a probability your tires degrade faster, or a safety car changes everything. The Markov property isn't perfect in F1 (tire history matters somewhat), but it's a powerful approximation — modern F1 strategy tools use exactly this framework.\n\n### The F1 Race as an MDP\n\n| MDP Component | F1 Mapping |\n|---------------|-----------|\n| $\\mathcal{S}$ (states) | {(position, tire_age, tire_compound, gap_ahead, gap_behind, laps_remaining, weather)} |\n| $\\mathcal{A}$ (actions) | {pit_for_softs, pit_for_mediums, pit_for_hards, push, conserve, defend} |\n| $P(s'|s,a)$ | Probability of new gaps/positions given current state and action — stochastic due to rivals, weather, safety cars |\n| $R(s,a)$ | Positions gained, time advantage, championship points at race end |\n| $\\gamma$ | How much future laps matter vs. this lap (close to 1 in F1 — every lap counts) |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Gridworld MDP\n",
    "\n",
    "Let's build a simple gridworld — the \"hello world\" of RL. Our agent navigates a 4×4 grid trying to reach a goal while avoiding traps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"A simple gridworld MDP environment.\"\"\"\n",
    "    \n",
    "    # Cell types\n",
    "    EMPTY = 0\n",
    "    WALL = 1\n",
    "    GOAL = 2\n",
    "    TRAP = 3\n",
    "    \n",
    "    # Actions: up, down, left, right\n",
    "    ACTIONS = ['up', 'down', 'left', 'right']\n",
    "    ACTION_DELTAS = {\n",
    "        'up': (-1, 0),\n",
    "        'down': (1, 0),\n",
    "        'left': (0, -1),\n",
    "        'right': (0, 1)\n",
    "    }\n",
    "    \n",
    "    def __init__(self, grid_size=4, slip_prob=0.1):\n",
    "        self.grid_size = grid_size\n",
    "        self.slip_prob = slip_prob  # Probability of slipping to a random adjacent cell\n",
    "        \n",
    "        # Define the grid\n",
    "        self.grid = np.zeros((grid_size, grid_size), dtype=int)\n",
    "        self.grid[0, 3] = self.GOAL   # Goal at top-right\n",
    "        self.grid[1, 1] = self.WALL   # Wall\n",
    "        self.grid[2, 3] = self.TRAP   # Trap\n",
    "        \n",
    "        # State and action spaces\n",
    "        self.states = [(i, j) for i in range(grid_size) for j in range(grid_size)\n",
    "                       if self.grid[i, j] != self.WALL]\n",
    "        self.terminal_states = [(i, j) for i in range(grid_size) for j in range(grid_size)\n",
    "                                if self.grid[i, j] in [self.GOAL, self.TRAP]]\n",
    "        self.n_states = len(self.states)\n",
    "        self.n_actions = len(self.ACTIONS)\n",
    "        \n",
    "        # Starting position\n",
    "        self.start = (3, 0)\n",
    "        self.agent_pos = self.start\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset to starting position.\"\"\"\n",
    "        self.agent_pos = self.start\n",
    "        return self.agent_pos\n",
    "    \n",
    "    def _is_valid(self, pos):\n",
    "        \"\"\"Check if a position is valid (in bounds and not a wall).\"\"\"\n",
    "        r, c = pos\n",
    "        return (0 <= r < self.grid_size and 0 <= c < self.grid_size \n",
    "                and self.grid[r, c] != self.WALL)\n",
    "    \n",
    "    def get_transitions(self, state, action):\n",
    "        \"\"\"Return list of (probability, next_state, reward) for a state-action pair.\"\"\"\n",
    "        if state in self.terminal_states:\n",
    "            return [(1.0, state, 0.0)]  # Terminal states loop with zero reward\n",
    "        \n",
    "        transitions = []\n",
    "        intended_delta = self.ACTION_DELTAS[action]\n",
    "        intended_next = (state[0] + intended_delta[0], state[1] + intended_delta[1])\n",
    "        \n",
    "        # Intended action succeeds with probability (1 - slip_prob)\n",
    "        if self._is_valid(intended_next):\n",
    "            next_state = intended_next\n",
    "        else:\n",
    "            next_state = state  # Bounce off wall/boundary\n",
    "        \n",
    "        reward = self._get_reward(next_state)\n",
    "        transitions.append((1.0 - self.slip_prob, next_state, reward))\n",
    "        \n",
    "        # With slip_prob, agent moves in a random perpendicular direction\n",
    "        if self.slip_prob > 0:\n",
    "            perpendicular = []\n",
    "            if action in ['up', 'down']:\n",
    "                perpendicular = ['left', 'right']\n",
    "            else:\n",
    "                perpendicular = ['up', 'down']\n",
    "            \n",
    "            for perp_action in perpendicular:\n",
    "                perp_delta = self.ACTION_DELTAS[perp_action]\n",
    "                perp_next = (state[0] + perp_delta[0], state[1] + perp_delta[1])\n",
    "                if self._is_valid(perp_next):\n",
    "                    perp_state = perp_next\n",
    "                else:\n",
    "                    perp_state = state\n",
    "                perp_reward = self._get_reward(perp_state)\n",
    "                transitions.append((self.slip_prob / 2, perp_state, perp_reward))\n",
    "        \n",
    "        return transitions\n",
    "    \n",
    "    def _get_reward(self, state):\n",
    "        \"\"\"Reward function.\"\"\"\n",
    "        if self.grid[state[0], state[1]] == self.GOAL:\n",
    "            return +1.0\n",
    "        elif self.grid[state[0], state[1]] == self.TRAP:\n",
    "            return -1.0\n",
    "        else:\n",
    "            return -0.04  # Small step penalty to encourage efficiency\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action, return (next_state, reward, done).\"\"\"\n",
    "        transitions = self.get_transitions(self.agent_pos, action)\n",
    "        probs = [t[0] for t in transitions]\n",
    "        idx = np.random.choice(len(transitions), p=probs)\n",
    "        _, next_state, reward = transitions[idx]\n",
    "        \n",
    "        self.agent_pos = next_state\n",
    "        done = next_state in self.terminal_states\n",
    "        return next_state, reward, done\n",
    "\n",
    "\n",
    "# Create and display the gridworld\n",
    "env = GridWorld(grid_size=4, slip_prob=0.1)\n",
    "\n",
    "print(\"GridWorld MDP\")\n",
    "print(f\"States: {env.n_states} (excluding walls)\")\n",
    "print(f\"Actions: {env.n_actions} ({', '.join(env.ACTIONS)})\")\n",
    "print(f\"Terminal states: {env.terminal_states}\")\n",
    "print(f\"Slip probability: {env.slip_prob}\")\n",
    "print(f\"Start: {env.start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: The Gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gridworld(env, values=None, policy=None, title='GridWorld'):\n",
    "    \"\"\"Visualize the gridworld with optional value function and policy overlays.\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
    "    n = env.grid_size\n",
    "    \n",
    "    # Color map for cell types\n",
    "    colors = {\n",
    "        GridWorld.EMPTY: '#f0f0f0',\n",
    "        GridWorld.WALL: '#2c3e50',\n",
    "        GridWorld.GOAL: '#2ecc71',\n",
    "        GridWorld.TRAP: '#e74c3c'\n",
    "    }\n",
    "    \n",
    "    # Draw cells\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            cell_type = env.grid[i, j]\n",
    "            color = colors[cell_type]\n",
    "            \n",
    "            # If we have values, shade empty cells by value\n",
    "            if values is not None and cell_type == GridWorld.EMPTY:\n",
    "                v = values.get((i, j), 0)\n",
    "                # Normalize to [-1, 1] for coloring\n",
    "                intensity = np.clip(v, -1, 1)\n",
    "                if intensity >= 0:\n",
    "                    color = plt.cm.RdYlGn(0.5 + intensity * 0.5)\n",
    "                else:\n",
    "                    color = plt.cm.RdYlGn(0.5 + intensity * 0.5)\n",
    "            \n",
    "            rect = plt.Rectangle((j, n - 1 - i), 1, 1, facecolor=color,\n",
    "                                  edgecolor='black', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Labels\n",
    "            if cell_type == GridWorld.GOAL:\n",
    "                ax.text(j + 0.5, n - 1 - i + 0.7, 'GOAL', ha='center', va='center',\n",
    "                        fontsize=11, fontweight='bold', color='white')\n",
    "                ax.text(j + 0.5, n - 1 - i + 0.4, '+1.0', ha='center', va='center',\n",
    "                        fontsize=10, color='white')\n",
    "            elif cell_type == GridWorld.TRAP:\n",
    "                ax.text(j + 0.5, n - 1 - i + 0.7, 'TRAP', ha='center', va='center',\n",
    "                        fontsize=11, fontweight='bold', color='white')\n",
    "                ax.text(j + 0.5, n - 1 - i + 0.4, '-1.0', ha='center', va='center',\n",
    "                        fontsize=10, color='white')\n",
    "            elif cell_type == GridWorld.WALL:\n",
    "                ax.text(j + 0.5, n - 1 - i + 0.5, 'WALL', ha='center', va='center',\n",
    "                        fontsize=11, fontweight='bold', color='white')\n",
    "            \n",
    "            # Show values\n",
    "            if values is not None and (i, j) in values and cell_type not in [GridWorld.WALL]:\n",
    "                v = values[(i, j)]\n",
    "                ax.text(j + 0.5, n - 1 - i + 0.15, f'{v:.3f}', ha='center', va='center',\n",
    "                        fontsize=9, color='black', style='italic')\n",
    "            \n",
    "            # Show policy arrows\n",
    "            if policy is not None and (i, j) in policy and cell_type == GridWorld.EMPTY:\n",
    "                action = policy[(i, j)]\n",
    "                arrow_map = {\n",
    "                    'up': (0, 0.25),\n",
    "                    'down': (0, -0.25),\n",
    "                    'left': (-0.25, 0),\n",
    "                    'right': (0.25, 0)\n",
    "                }\n",
    "                dx, dy = arrow_map[action]\n",
    "                ax.annotate('', xy=(j + 0.5 + dx, n - 1 - i + 0.5 + dy),\n",
    "                           xytext=(j + 0.5, n - 1 - i + 0.5),\n",
    "                           arrowprops=dict(arrowstyle='->', lw=2.5, color='#2c3e50'))\n",
    "    \n",
    "    # Mark start\n",
    "    si, sj = env.start\n",
    "    ax.text(sj + 0.5, n - 1 - si + 0.85, 'START', ha='center', va='center',\n",
    "            fontsize=8, fontweight='bold', color='#3498db')\n",
    "    \n",
    "    ax.set_xlim(0, n)\n",
    "    ax.set_ylim(0, n)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(range(n))\n",
    "    ax.set_yticks(range(n))\n",
    "    ax.set_xticklabels(range(n))\n",
    "    ax.set_yticklabels(range(n-1, -1, -1))\n",
    "    ax.set_xlabel('Column')\n",
    "    ax.set_ylabel('Row')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_gridworld(env, title='4×4 GridWorld Environment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The agent starts at the bottom-left and must navigate to the GOAL (+1.0) while avoiding the TRAP (-1.0). Each non-terminal step costs -0.04 to encourage the agent to find the goal quickly. There's a 10% chance of slipping perpendicular to the intended direction — this stochasticity is what makes the problem interesting.\n\n**F1 analogy:** Think of the gridworld as a simplified race. The GOAL is the podium finish, the TRAP is a DNF (retirement), and the small step penalty is tire degradation — every lap costs you something, so you need to reach the finish efficiently. The 10% slip probability mirrors the unpredictability of racing: you plan to push through Turn 3, but you might get understeer and lose time. The wall is like a track limit violation that nullifies your move."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 3. Returns and Discounting\n\nThe agent doesn't just want the next reward — it wants to maximize the **total reward over time**. We call this the **return**:\n\n$$G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}$$\n\nThe **discount factor** $\\gamma$ determines how much we value future rewards:\n\n- $\\gamma = 0$: Only care about immediate reward (greedy)\n- $\\gamma = 1$: Value all future rewards equally (far-sighted)\n- $\\gamma = 0.9$: A reward 10 steps away is worth $0.9^{10} \\approx 0.35$ of an immediate reward\n\n### Why discount?\n\n1. **Mathematical convenience**: Makes infinite sums converge\n2. **Uncertainty**: The further into the future, the less certain we are\n3. **Human-like behavior**: We prefer rewards sooner rather than later\n\n**F1 analogy:** The discount factor captures how much future laps matter compared to this one. With $\\gamma$ close to 1 (say 0.99), a position gain on lap 50 is almost as valuable as one on lap 1 — which is how F1 strategists actually think. But with $\\gamma = 0.5$, you'd heavily prioritize immediate gains, like a driver who burns through tires to lead lap 1 but fades by mid-race. A safety car on lap 40 is uncertain — discounting reflects that future rewards are less predictable. Teams running Monte Carlo race simulations with thousands of scenarios are implicitly reasoning about discounted returns across different possible futures."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Effect of Discount Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Discount curves\n",
    "steps = np.arange(0, 20)\n",
    "gammas = [0.0, 0.5, 0.9, 0.99, 1.0]\n",
    "colors = plt.cm.viridis(np.linspace(0.1, 0.9, len(gammas)))\n",
    "\n",
    "for gamma, color in zip(gammas, colors):\n",
    "    weights = [gamma**k for k in steps]\n",
    "    axes[0].plot(steps, weights, 'o-', label=f'γ = {gamma}', color=color, markersize=4)\n",
    "\n",
    "axes[0].set_xlabel('Steps into the future (k)', fontsize=12)\n",
    "axes[0].set_ylabel('Discount weight (γᵏ)', fontsize=12)\n",
    "axes[0].set_title('How Much We Value Future Rewards', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Cumulative return example\n",
    "rewards = [0, -0.04, -0.04, -0.04, -0.04, 1.0]  # Path to goal\n",
    "gamma_vals = [0.5, 0.9, 0.99]\n",
    "bar_width = 0.25\n",
    "x = np.arange(len(rewards))\n",
    "\n",
    "for i, gamma in enumerate(gamma_vals):\n",
    "    discounted = [rewards[k] * gamma**k for k in range(len(rewards))]\n",
    "    axes[1].bar(x + i * bar_width, discounted, bar_width,\n",
    "               label=f'γ = {gamma} (Return = {sum(discounted):.3f})',\n",
    "               alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel('Time step', fontsize=12)\n",
    "axes[1].set_ylabel('Discounted reward', fontsize=12)\n",
    "axes[1].set_title('Same Path, Different Discount Factors', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xticks(x + bar_width)\n",
    "axes[1].set_xticklabels([f't={k}' for k in range(len(rewards))])\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Numeric example\n",
    "print(\"Example: Agent takes 5 steps then reaches goal\")\n",
    "print(f\"Rewards: {rewards}\")\n",
    "for gamma in [0.5, 0.9, 0.99]:\n",
    "    G = sum(r * gamma**k for k, r in enumerate(rewards))\n",
    "    print(f\"  γ = {gamma}: Return G₀ = {G:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 4. Value Functions and the Bellman Equation\n\nValue functions answer the question: **\"How good is it to be in a particular state (or to take a particular action in a state)?\"**\n\n### State-Value Function V(s)\n\nThe **state-value function** $V^\\pi(s)$ is the expected return starting from state $s$ and following policy $\\pi$:\n\n$$V^\\pi(s) = \\mathbb{E}_\\pi[G_t | s_t = s] = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k r_{t+k} \\mid s_t = s\\right]$$\n\n**F1 analogy:** V(s) answers: \"How valuable is this race position given current conditions?\" If you're P3 with fresh mediums and 20 laps to go, V(s) is high — you have a great shot at the podium. If you're P15 with worn hards and 5 laps left, V(s) is low.\n\n### Action-Value Function Q(s, a)\n\nThe **action-value function** $Q^\\pi(s, a)$ is the expected return starting from state $s$, taking action $a$, then following policy $\\pi$:\n\n$$Q^\\pi(s, a) = \\mathbb{E}_\\pi[G_t | s_t = s, a_t = a]$$\n\n**F1 analogy:** Q(s, a) answers: \"How good is pitting NOW vs. next lap, given we're P3 with worn tires?\" Q(P3_worn_tires, pit_now) might be 0.7 (likely P4 finish after losing track position). Q(P3_worn_tires, push_one_more_lap) might be 0.6 (risk of tire failure, but could maintain position if tires hold). The strategist compares Q-values to make the call.\n\n### The Bellman Equation\n\nThe key insight: we can express the value of a state **recursively** in terms of the values of successor states:\n\n$$V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) \\left[R(s,a) + \\gamma V^\\pi(s')\\right]$$\n\nThis is the **Bellman expectation equation** — the foundation of almost every RL algorithm.\n\n**In words**: The value of a state equals the expected immediate reward plus the discounted value of the next state, averaged over all actions and transitions.\n\n**F1 analogy:** The Bellman equation says: \"The value of being P3 on lap 30 = the immediate reward from this lap's action + the discounted value of wherever we end up on lap 31.\" Today's position value = immediate reward + future race value. This is exactly how a strategy engineer thinks: \"If we pit now, we lose 2 seconds (immediate cost) but gain tire advantage for the remaining laps (future value).\"\n\n### Deep Dive: Why the Bellman Equation Matters\n\nThe Bellman equation transforms an intractable problem (compute expected infinite sums) into a system of linear equations that can be solved iteratively. It's the RL equivalent of dynamic programming — breaking a hard problem into overlapping subproblems."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Bellman Equation Backup Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: V(s) backup\n",
    "ax = axes[0]\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-1, 4)\n",
    "ax.axis('off')\n",
    "ax.set_title('V(s) Bellman Backup', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Root state\n",
    "ax.plot(0, 3.5, 'o', markersize=25, color='#3498db', zorder=5)\n",
    "ax.text(0, 3.5, 's', ha='center', va='center', fontsize=12, fontweight='bold', color='white')\n",
    "\n",
    "# Action nodes\n",
    "action_x = [-1.2, 0, 1.2]\n",
    "action_labels = ['a₁', 'a₂', 'a₃']\n",
    "for x, label in zip(action_x, action_labels):\n",
    "    ax.plot(x, 2, 's', markersize=15, color='#e74c3c', zorder=5)\n",
    "    ax.text(x, 2, label, ha='center', va='center', fontsize=9, color='white', fontweight='bold')\n",
    "    ax.plot([0, x], [3.2, 2.2], '-', color='gray', lw=1.5)\n",
    "    ax.text((0 + x)/2 - 0.15, 2.7, 'π(a|s)', fontsize=7, color='gray', ha='center')\n",
    "\n",
    "# Next states from action a2\n",
    "next_x = [-0.5, 0.5]\n",
    "for x in next_x:\n",
    "    ax.plot(x, 0.5, 'o', markersize=20, color='#2ecc71', zorder=5)\n",
    "    ax.text(x, 0.5, \"s'\", ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "    ax.plot([0, x], [1.8, 0.7], '-', color='gray', lw=1.5)\n",
    "\n",
    "ax.text(0.5, 1.3, 'P(s\\'|s,a)', fontsize=8, color='gray', ha='center')\n",
    "ax.text(0, -0.3, 'r + γV(s\\')', ha='center', fontsize=10, style='italic', color='#2c3e50')\n",
    "\n",
    "# Right: Q(s,a) backup\n",
    "ax = axes[1]\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-1, 4)\n",
    "ax.axis('off')\n",
    "ax.set_title('Q(s,a) Bellman Backup', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Root action\n",
    "ax.plot(0, 3.5, 's', markersize=20, color='#e74c3c', zorder=5)\n",
    "ax.text(0, 3.5, 'a', ha='center', va='center', fontsize=12, fontweight='bold', color='white')\n",
    "\n",
    "# Next states\n",
    "next_x = [-1, 0, 1]\n",
    "for x in next_x:\n",
    "    ax.plot(x, 2, 'o', markersize=22, color='#2ecc71', zorder=5)\n",
    "    ax.text(x, 2, \"s'\", ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "    ax.plot([0, x], [3.2, 2.2], '-', color='gray', lw=1.5)\n",
    "\n",
    "ax.text(0.7, 2.7, 'P(s\\'|s,a)', fontsize=8, color='gray', ha='center')\n",
    "\n",
    "# Next actions from s'\n",
    "for base_x in [-1, 1]:\n",
    "    offsets = [-0.3, 0.3]\n",
    "    for off in offsets:\n",
    "        x = base_x + off\n",
    "        ax.plot(x, 0.5, 's', markersize=12, color='#e74c3c', zorder=5)\n",
    "        ax.text(x, 0.5, \"a'\", ha='center', va='center', fontsize=7, color='white', fontweight='bold')\n",
    "        ax.plot([base_x, x], [1.8, 0.7], '-', color='gray', lw=1)\n",
    "\n",
    "ax.text(0.7, 1.3, 'π(a\\'|s\\')', fontsize=8, color='gray', ha='center')\n",
    "ax.text(0, -0.3, 'r + γ Σ π(a\\'|s\\')Q(s\\',a\\')', ha='center', fontsize=10, style='italic', color='#2c3e50')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Policy Evaluation\n",
    "\n",
    "Given a policy $\\pi$, **policy evaluation** computes $V^\\pi(s)$ for every state. We do this by repeatedly applying the Bellman equation until convergence:\n",
    "\n",
    "$$V_{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) \\left[R(s,a) + \\gamma V_k(s')\\right]$$\n",
    "\n",
    "Starting from $V_0(s) = 0$ for all states, this iterative process is guaranteed to converge to $V^\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, gamma=0.9, theta=1e-6, max_iterations=1000):\n",
    "    \"\"\"Evaluate a policy by iteratively applying the Bellman expectation equation.\n",
    "    \n",
    "    Args:\n",
    "        env: GridWorld environment\n",
    "        policy: dict mapping state -> action (deterministic policy)\n",
    "        gamma: discount factor\n",
    "        theta: convergence threshold\n",
    "        max_iterations: safety limit\n",
    "    \n",
    "    Returns:\n",
    "        V: dict mapping state -> value\n",
    "        history: list of value dicts at each iteration (for visualization)\n",
    "    \"\"\"\n",
    "    # Initialize values to zero\n",
    "    V = {s: 0.0 for s in env.states}\n",
    "    history = [V.copy()]\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        delta = 0\n",
    "        V_new = V.copy()\n",
    "        \n",
    "        for s in env.states:\n",
    "            if s in env.terminal_states:\n",
    "                # Terminal states have fixed values based on reward\n",
    "                if env.grid[s[0], s[1]] == GridWorld.GOAL:\n",
    "                    V_new[s] = 1.0\n",
    "                elif env.grid[s[0], s[1]] == GridWorld.TRAP:\n",
    "                    V_new[s] = -1.0\n",
    "                continue\n",
    "            \n",
    "            # Bellman expectation equation for deterministic policy\n",
    "            action = policy.get(s, 'right')  # Default action\n",
    "            transitions = env.get_transitions(s, action)\n",
    "            \n",
    "            v = sum(prob * (reward + gamma * V[s_next])\n",
    "                    for prob, s_next, reward in transitions)\n",
    "            \n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "            V_new[s] = v\n",
    "        \n",
    "        V = V_new\n",
    "        history.append(V.copy())\n",
    "        \n",
    "        if delta < theta:\n",
    "            print(f\"Policy evaluation converged after {iteration + 1} iterations (Δ < {theta})\")\n",
    "            break\n",
    "    \n",
    "    return V, history\n",
    "\n",
    "\n",
    "# Evaluate a simple policy: always go right\n",
    "simple_policy = {s: 'right' for s in env.states}\n",
    "V_simple, history_simple = policy_evaluation(env, simple_policy, gamma=0.9)\n",
    "\n",
    "print(\"\\nValues under 'always go right' policy:\")\n",
    "for i in range(env.grid_size):\n",
    "    row_vals = []\n",
    "    for j in range(env.grid_size):\n",
    "        if (i, j) in V_simple:\n",
    "            row_vals.append(f\"{V_simple[(i,j)]:7.3f}\")\n",
    "        else:\n",
    "            row_vals.append(\"  WALL \")\n",
    "    print(\" | \".join(row_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the values\n",
    "visualize_gridworld(env, values=V_simple, policy=simple_policy,\n",
    "                    title='Policy Evaluation: \"Always Go Right\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Convergence of Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how values converge over iterations\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "iterations_to_show = [0, 2, 10, len(history_simple) - 1]\n",
    "n = env.grid_size\n",
    "\n",
    "for ax, it in zip(axes, iterations_to_show):\n",
    "    V_it = history_simple[it]\n",
    "    grid_vals = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if (i, j) in V_it:\n",
    "                grid_vals[i, j] = V_it[(i, j)]\n",
    "            else:\n",
    "                grid_vals[i, j] = np.nan\n",
    "    \n",
    "    im = ax.imshow(grid_vals, cmap='RdYlGn', vmin=-1, vmax=1)\n",
    "    ax.set_title(f'Iteration {it}', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if (i, j) in V_it:\n",
    "                ax.text(j, i, f'{V_it[(i,j)]:.2f}', ha='center', va='center',\n",
    "                       fontsize=9, fontweight='bold')\n",
    "            elif env.grid[i, j] == GridWorld.WALL:\n",
    "                ax.text(j, i, 'W', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.set_xticks(range(n))\n",
    "    ax.set_yticks(range(n))\n",
    "\n",
    "plt.suptitle('Policy Evaluation Convergence', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Notice how the values propagate backward from the goal and trap states, iteration by iteration. This is the Bellman equation at work — each iteration, information about future rewards flows one more step backward through the state space.\n\n**F1 analogy:** This is exactly how race strategy propagates backward from the finish. On the last lap, the only thing that matters is crossing the line. On the second-to-last lap, value depends on whether you're positioned to gain or lose a place at the flag. Each earlier lap's value builds on the laps that follow — just like the Bellman backup propagating from the GOAL cell outward through the grid."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 6. Policy Improvement and Policy Iteration\n\nPolicy evaluation tells us *how good* a policy is. But we want the *best* policy. **Policy improvement** uses the value function to greedily select better actions:\n\n$$\\pi'(s) = \\arg\\max_a \\sum_{s'} P(s'|s,a) \\left[R(s,a) + \\gamma V^\\pi(s')\\right]$$\n\n**Policy iteration** alternates between:\n1. **Evaluate**: Compute $V^\\pi$ for the current policy\n2. **Improve**: Update the policy greedily with respect to $V^\\pi$\n\nThis is guaranteed to converge to the optimal policy $\\pi^*$.\n\n**F1 analogy:** Policy iteration is how teams improve their race strategy over a season. First, they *evaluate* a strategy — \"our one-stop medium-hard plan at Silverstone scored 12 points\" (policy evaluation). Then they *improve* — \"given what we know about tire degradation, switching to a two-stop soft-medium plan would have scored 18 points\" (policy improvement). They test the new strategy at the next race, evaluate it again, improve again. Over a season, the strategy converges toward the optimal approach for each circuit. This evaluate-improve cycle is exactly policy iteration."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, V, gamma=0.9):\n",
    "    \"\"\"Improve policy greedily based on value function.\"\"\"\n",
    "    policy = {}\n",
    "    \n",
    "    for s in env.states:\n",
    "        if s in env.terminal_states:\n",
    "            continue\n",
    "        \n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "        \n",
    "        for action in env.ACTIONS:\n",
    "            transitions = env.get_transitions(s, action)\n",
    "            q_sa = sum(prob * (reward + gamma * V[s_next])\n",
    "                       for prob, s_next, reward in transitions)\n",
    "            \n",
    "            if q_sa > best_value:\n",
    "                best_value = q_sa\n",
    "                best_action = action\n",
    "        \n",
    "        policy[s] = best_action\n",
    "    \n",
    "    return policy\n",
    "\n",
    "\n",
    "def policy_iteration(env, gamma=0.9):\n",
    "    \"\"\"Full policy iteration algorithm.\"\"\"\n",
    "    # Start with random policy\n",
    "    policy = {s: np.random.choice(env.ACTIONS) for s in env.states\n",
    "              if s not in env.terminal_states}\n",
    "    \n",
    "    iteration = 0\n",
    "    while True:\n",
    "        # Policy evaluation\n",
    "        V, _ = policy_evaluation(env, policy, gamma)\n",
    "        \n",
    "        # Policy improvement\n",
    "        new_policy = policy_improvement(env, V, gamma)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if new_policy == policy:\n",
    "            print(f\"\\nPolicy iteration converged after {iteration + 1} improvement steps!\")\n",
    "            break\n",
    "        \n",
    "        policy = new_policy\n",
    "        iteration += 1\n",
    "    \n",
    "    return policy, V\n",
    "\n",
    "\n",
    "optimal_policy, optimal_V = policy_iteration(env, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the optimal policy\n",
    "visualize_gridworld(env, values=optimal_V, policy=optimal_policy,\n",
    "                    title='Optimal Policy (Policy Iteration, γ=0.9)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The arrows show the optimal action in each state. Notice how the agent learns to navigate around the wall, move toward the goal, and stay away from the trap. The values decrease as we move further from the goal, reflecting the discounted future reward.\n\n**F1 analogy:** The optimal policy arrows are like the \"strategy map\" a team builds for every possible race situation. From P8 with fresh tires (high value), the arrow points toward \"push\" — close the gap and overtake. From a position near the trap (running on worn tires near a rival), the arrow says \"conserve\" — avoid the risk. A great strategist has an arrow for every situation before the race even starts."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 7. Value Iteration\n\n**Value iteration** combines evaluation and improvement into a single step, updating values directly with the Bellman **optimality** equation:\n\n$$V_{k+1}(s) = \\max_a \\sum_{s'} P(s'|s,a) \\left[R(s,a) + \\gamma V_k(s')\\right]$$\n\nInstead of fully evaluating a policy before improving it, value iteration takes the max over actions at every step — essentially doing greedy improvement as part of the evaluation.\n\n| Method | Steps per Iteration | Convergence | F1 Parallel |\n|--------|-------------------|-------------|-------------|\n| **Policy Iteration** | Full evaluation + improvement | Fewer outer iterations | Full season review then strategy overhaul |\n| **Value Iteration** | Single Bellman optimality update | More iterations but simpler | Lap-by-lap real-time strategy adjustments |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.9, theta=1e-6, max_iterations=1000):\n",
    "    \"\"\"Value iteration: combine evaluation and improvement in one step.\"\"\"\n",
    "    V = {s: 0.0 for s in env.states}\n",
    "    history = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        delta = 0\n",
    "        V_new = V.copy()\n",
    "        \n",
    "        for s in env.states:\n",
    "            if s in env.terminal_states:\n",
    "                if env.grid[s[0], s[1]] == GridWorld.GOAL:\n",
    "                    V_new[s] = 1.0\n",
    "                elif env.grid[s[0], s[1]] == GridWorld.TRAP:\n",
    "                    V_new[s] = -1.0\n",
    "                continue\n",
    "            \n",
    "            # Bellman optimality equation: take the MAX over actions\n",
    "            action_values = []\n",
    "            for action in env.ACTIONS:\n",
    "                transitions = env.get_transitions(s, action)\n",
    "                q = sum(prob * (reward + gamma * V[s_next])\n",
    "                        for prob, s_next, reward in transitions)\n",
    "                action_values.append(q)\n",
    "            \n",
    "            best_value = max(action_values)\n",
    "            delta = max(delta, abs(best_value - V[s]))\n",
    "            V_new[s] = best_value\n",
    "        \n",
    "        V = V_new\n",
    "        history.append({'iteration': iteration, 'delta': delta, 'V': V.copy()})\n",
    "        \n",
    "        if delta < theta:\n",
    "            print(f\"Value iteration converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "    \n",
    "    # Extract policy from final values\n",
    "    policy = policy_improvement(env, V, gamma)\n",
    "    \n",
    "    return V, policy, history\n",
    "\n",
    "\n",
    "V_vi, policy_vi, history_vi = value_iteration(env, gamma=0.9)\n",
    "\n",
    "# Compare with policy iteration\n",
    "print(\"\\nValue Iteration vs Policy Iteration values match:\",\n",
    "      all(abs(V_vi[s] - optimal_V[s]) < 1e-4 for s in env.states))\n",
    "print(\"Policies match:\",\n",
    "      all(policy_vi.get(s) == optimal_policy.get(s) for s in env.states\n",
    "          if s not in env.terminal_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convergence speed\n",
    "deltas = [h['delta'] for h in history_vi]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.semilogy(range(len(deltas)), deltas, 'b-o', markersize=3)\n",
    "ax.axhline(y=1e-6, color='r', linestyle='--', label='Convergence threshold')\n",
    "ax.set_xlabel('Iteration', fontsize=12)\n",
    "ax.set_ylabel('Max value change (Δ)', fontsize=12)\n",
    "ax.set_title('Value Iteration Convergence', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 8. Exploration vs. Exploitation\n\nOne of the fundamental challenges in RL: should the agent **exploit** what it already knows works, or **explore** new actions that might lead to better outcomes?\n\n- **Exploitation**: Choose the action with the highest estimated value\n- **Exploration**: Try less-visited or uncertain actions\n\nToo much exploitation → stuck in local optima (never discovers the best path)  \nToo much exploration → wastes time on suboptimal actions\n\n**F1 analogy:** This is the tension every race strategist lives with. *Exploitation* is sticking with the proven one-stop strategy that has worked all season. *Exploration* is trying an aggressive undercut, an untested tire compound, or a radically different pit window. Red Bull in 2021 often explored novel strategies against Mercedes — sometimes they found gold (Abu Dhabi), sometimes they lost out. A team that never explores gets predictable and loses; a team that always explores never capitalizes on what works. The sweet spot is exploring early in the season (high epsilon) and exploiting your best strategies for the championship-deciding races (low epsilon).\n\n### Common Exploration Strategies\n\n| Strategy | How it Works | Tradeoff | F1 Parallel |\n|----------|-------------|----------|-------------|\n| **epsilon-greedy** | With prob epsilon, random action; otherwise, best action | Simple, widely used | 10% of the time, try something unconventional |\n| **epsilon-decay** | Start with high epsilon, decrease over time | Explores early, exploits later | Experiment in practice/early races, lock strategy for title fight |\n| **Softmax/Boltzmann** | Sample actions proportional to estimated values | Smooth exploration | Weight new strategies by estimated value, not purely random |\n| **UCB** | Bonus for under-explored actions | Principled, optimistic | \"We haven't tried the hard compound at this track — give it a bonus\" |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q, state, epsilon, actions):\n",
    "    \"\"\"Select action using epsilon-greedy strategy.\"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.choice(actions)  # Explore\n",
    "    else:\n",
    "        q_values = [Q.get((state, a), 0.0) for a in actions]\n",
    "        return actions[np.argmax(q_values)]  # Exploit\n",
    "\n",
    "\n",
    "def softmax_action(Q, state, temperature, actions):\n",
    "    \"\"\"Select action using softmax (Boltzmann) exploration.\"\"\"\n",
    "    q_values = np.array([Q.get((state, a), 0.0) for a in actions])\n",
    "    # Numerical stability\n",
    "    q_values = q_values - np.max(q_values)\n",
    "    probs = np.exp(q_values / temperature)\n",
    "    probs = probs / probs.sum()\n",
    "    return np.random.choice(actions, p=probs)\n",
    "\n",
    "\n",
    "# Demonstrate the multi-armed bandit problem — the simplest explore/exploit scenario\n",
    "class MultiArmedBandit:\n",
    "    \"\"\"A simple multi-armed bandit with Gaussian rewards.\"\"\"\n",
    "    def __init__(self, n_arms=5):\n",
    "        self.n_arms = n_arms\n",
    "        self.true_means = np.random.randn(n_arms)  # True reward means\n",
    "    \n",
    "    def pull(self, arm):\n",
    "        \"\"\"Pull an arm, get noisy reward.\"\"\"\n",
    "        return self.true_means[arm] + np.random.randn() * 0.5\n",
    "\n",
    "\n",
    "def run_bandit_experiment(n_steps=1000, n_runs=200):\n",
    "    \"\"\"Compare exploration strategies on a bandit problem.\"\"\"\n",
    "    strategies = {\n",
    "        'ε=0 (pure greedy)': 0.0,\n",
    "        'ε=0.01': 0.01,\n",
    "        'ε=0.1': 0.1,\n",
    "        'ε=0.5': 0.5,\n",
    "    }\n",
    "    \n",
    "    results = {name: np.zeros(n_steps) for name in strategies}\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        bandit = MultiArmedBandit(n_arms=10)\n",
    "        best_mean = np.max(bandit.true_means)\n",
    "        \n",
    "        for name, epsilon in strategies.items():\n",
    "            Q = np.zeros(10)  # Estimated values\n",
    "            N = np.zeros(10)  # Action counts\n",
    "            \n",
    "            for t in range(n_steps):\n",
    "                # Epsilon-greedy selection\n",
    "                if np.random.random() < epsilon:\n",
    "                    arm = np.random.randint(10)\n",
    "                else:\n",
    "                    arm = np.argmax(Q)\n",
    "                \n",
    "                reward = bandit.pull(arm)\n",
    "                N[arm] += 1\n",
    "                Q[arm] += (reward - Q[arm]) / N[arm]  # Running average\n",
    "                \n",
    "                results[name][t] += reward\n",
    "    \n",
    "    # Average over runs\n",
    "    for name in results:\n",
    "        results[name] /= n_runs\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "bandit_results = run_bandit_experiment()\n",
    "print(\"Bandit experiment complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Exploration vs. Exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Average reward over time\n",
    "colors = ['#e74c3c', '#f39c12', '#2ecc71', '#3498db']\n",
    "for (name, rewards), color in zip(bandit_results.items(), colors):\n",
    "    # Smooth with running average\n",
    "    window = 50\n",
    "    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    axes[0].plot(smoothed, label=name, color=color, linewidth=2)\n",
    "\n",
    "axes[0].set_xlabel('Step', fontsize=12)\n",
    "axes[0].set_ylabel('Average Reward', fontsize=12)\n",
    "axes[0].set_title('Multi-Armed Bandit: Exploration Strategies', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Epsilon decay schedule\n",
    "steps = np.arange(1000)\n",
    "decay_schedules = {\n",
    "    'Constant ε=0.1': np.ones(1000) * 0.1,\n",
    "    'Linear decay': np.maximum(0.01, 1.0 - steps / 500),\n",
    "    'Exponential decay': np.maximum(0.01, np.exp(-steps / 200)),\n",
    "}\n",
    "\n",
    "for (name, schedule), color in zip(decay_schedules.items(), ['#e74c3c', '#2ecc71', '#3498db']):\n",
    "    axes[1].plot(steps, schedule, label=name, color=color, linewidth=2)\n",
    "\n",
    "axes[1].set_xlabel('Step', fontsize=12)\n",
    "axes[1].set_ylabel('Epsilon (exploration rate)', fontsize=12)\n",
    "axes[1].set_title('Common ε-Decay Schedules', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: ε=0.1 finds a good balance — enough exploration to find\")\n",
    "print(\"the best arm, but not so much that it wastes pulls on bad arms.\")\n",
    "print(\"Pure greedy (ε=0) often gets stuck on a suboptimal arm early on.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 9. Temporal Difference Learning\n\nSo far, our methods require knowing the environment's transition dynamics $P(s'|s,a)$. In practice, the agent often doesn't have this information — it must learn from experience.\n\n**Temporal Difference (TD) learning** bridges the gap between dynamic programming (which requires a model) and Monte Carlo methods (which require complete episodes).\n\n### TD(0) Update Rule\n\n$$V(s_t) \\leftarrow V(s_t) + \\alpha \\left[r_t + \\gamma V(s_{t+1}) - V(s_t)\\right]$$\n\nThe term in brackets is the **TD error** $\\delta_t$:\n\n$$\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n\n**Intuition**: The TD error measures how \"surprised\" the agent is. If the actual reward plus estimated future value is higher than expected, the TD error is positive, and we increase the value estimate.\n\n**F1 analogy:** The TD error is the gap between what the strategist *expected* and what *actually happened*. Before a pit stop, they predicted: \"We'll lose 22 seconds, come out P5, and the value of P5 at this tire age is X.\" After the stop, they got an undercut and came out P4 — the TD error is positive. The strategist updates their model: \"Pitting at that tire age is better than we thought.\" Over many races, these lap-by-lap surprises refine the strategy model. This is learning from experience, not from a simulator.\n\n| Method | Updates | Requires | F1 Parallel |\n|--------|---------|----------|-------------|\n| **Dynamic Programming** | After full sweep of all states | Model of environment | Pre-race simulator with full track model |\n| **Monte Carlo** | After complete episode | Complete episodes | Post-race review: \"How did the whole race go?\" |\n| **TD Learning** | After each step | Only current transition | Lap-by-lap strategy updates during the race |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_zero(env, n_episodes=5000, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    \"\"\"TD(0) prediction: learn V(s) from experience using an ε-greedy policy.\"\"\"\n",
    "    V = defaultdict(float)\n",
    "    visit_counts = defaultdict(int)\n",
    "    td_errors = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        episode_errors = []\n",
    "        \n",
    "        for _ in range(100):  # Max steps per episode\n",
    "            # ε-greedy action selection (using current V to estimate Q)\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.choice(env.ACTIONS)\n",
    "            else:\n",
    "                # Greedy: pick action that leads to highest-value next state\n",
    "                action_values = []\n",
    "                for a in env.ACTIONS:\n",
    "                    transitions = env.get_transitions(state, a)\n",
    "                    q = sum(p * (r + gamma * V[s_]) for p, s_, r in transitions)\n",
    "                    action_values.append(q)\n",
    "                action = env.ACTIONS[np.argmax(action_values)]\n",
    "            \n",
    "            next_state, reward, done = env.step(action)\n",
    "            visit_counts[state] += 1\n",
    "            \n",
    "            # TD(0) update\n",
    "            td_target = reward + gamma * V[next_state] * (0 if done else 1)\n",
    "            td_error = td_target - V[state]\n",
    "            V[state] += alpha * td_error\n",
    "            \n",
    "            episode_errors.append(abs(td_error))\n",
    "            \n",
    "            if done:\n",
    "                # Update terminal state values\n",
    "                if env.grid[next_state[0], next_state[1]] == GridWorld.GOAL:\n",
    "                    V[next_state] = 1.0\n",
    "                elif env.grid[next_state[0], next_state[1]] == GridWorld.TRAP:\n",
    "                    V[next_state] = -1.0\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        if episode_errors:\n",
    "            td_errors.append(np.mean(episode_errors))\n",
    "    \n",
    "    return dict(V), td_errors\n",
    "\n",
    "\n",
    "V_td, td_errors = td_zero(env, n_episodes=10000, alpha=0.1, gamma=0.9)\n",
    "\n",
    "# Compare TD-learned values with exact values\n",
    "print(\"TD(0) learned values vs. exact (value iteration):\")\n",
    "print(f\"{'State':<10} {'TD(0)':>8} {'Exact':>8} {'Diff':>8}\")\n",
    "print(\"-\" * 36)\n",
    "for s in sorted(env.states):\n",
    "    td_val = V_td.get(s, 0)\n",
    "    exact_val = V_vi.get(s, 0)\n",
    "    print(f\"{str(s):<10} {td_val:8.3f} {exact_val:8.3f} {abs(td_val - exact_val):8.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize TD learning convergence\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: TD error over episodes\n",
    "window = 100\n",
    "smoothed_errors = np.convolve(td_errors, np.ones(window)/window, mode='valid')\n",
    "axes[0].plot(smoothed_errors, color='#3498db', linewidth=1.5)\n",
    "axes[0].set_xlabel('Episode', fontsize=12)\n",
    "axes[0].set_ylabel('Average |TD Error|', fontsize=12)\n",
    "axes[0].set_title('TD(0) Learning: Error Convergence', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Comparison scatter plot\n",
    "td_vals = [V_td.get(s, 0) for s in env.states]\n",
    "exact_vals = [V_vi.get(s, 0) for s in env.states]\n",
    "axes[1].scatter(exact_vals, td_vals, s=100, color='#2ecc71', edgecolor='black', zorder=5)\n",
    "axes[1].plot([-1, 1], [-1, 1], 'r--', label='Perfect agreement', linewidth=2)\n",
    "axes[1].set_xlabel('Exact V(s) (Value Iteration)', fontsize=12)\n",
    "axes[1].set_ylabel('Learned V(s) (TD(0))', fontsize=12)\n",
    "axes[1].set_title('TD(0) vs. Exact Values', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"TD(0) learns values close to the exact solution, but from experience only!\")\n",
    "print(\"No knowledge of transition probabilities was needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 10. Value-Based vs. Policy-Based Methods\n\nRL methods fall into two broad categories:\n\n### Value-Based Methods\nLearn a value function $V(s)$ or $Q(s,a)$, then derive a policy from it.\n- Examples: Q-learning, DQN, SARSA\n- **Pros**: Sample efficient, stable convergence\n- **Cons**: Can only handle discrete actions (without extensions)\n\n**F1 analogy:** Like building a massive lookup table — \"for every possible race situation, here's how valuable each action is.\" Then the strategist just picks the highest-value action. Works great when the situations are enumerable, but F1 has effectively infinite states.\n\n### Policy-Based Methods\nLearn the policy $\\pi(a|s)$ directly, without a value function.\n- Examples: REINFORCE, PPO, A2C\n- **Pros**: Handle continuous actions, can learn stochastic policies\n- **Cons**: Higher variance, less sample efficient\n\n**F1 analogy:** Like training a driver's instincts directly — \"in this situation, do this.\" The driver doesn't compute values; they've internalized the optimal response. This works for continuous decisions like steering angle and throttle modulation.\n\n### Actor-Critic Methods\nCombine both: an **actor** (policy) and a **critic** (value function).\n- Examples: A2C, PPO, SAC\n- **Pros**: Lower variance than pure policy methods, more flexible than pure value methods\n\n**F1 analogy:** The driver (actor) makes real-time decisions, while the strategist on the pit wall (critic) evaluates how those decisions affect the race outcome. The driver learns from the strategist's feedback, and the strategist refines their model from the driver's results."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: taxonomy of RL methods\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "ax.set_title('Taxonomy of RL Methods', fontsize=16, fontweight='bold')\n",
    "\n",
    "# RL root\n",
    "root = mpatches.FancyBboxPatch((4.5, 6.5), 3, 1, boxstyle=\"round,pad=0.2\",\n",
    "                                facecolor='#2c3e50', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(root)\n",
    "ax.text(6, 7, 'Reinforcement Learning', ha='center', va='center',\n",
    "        fontsize=12, fontweight='bold', color='white')\n",
    "\n",
    "# Three branches\n",
    "branches = [\n",
    "    (1, 4, 'Value-Based', '#3498db', ['Q-Learning', 'DQN', 'SARSA']),\n",
    "    (4.5, 4, 'Actor-Critic', '#9b59b6', ['A2C/A3C', 'PPO', 'SAC']),\n",
    "    (8, 4, 'Policy-Based', '#e74c3c', ['REINFORCE', 'TRPO', 'ES']),\n",
    "]\n",
    "\n",
    "for x, y, label, color, methods in branches:\n",
    "    box = mpatches.FancyBboxPatch((x, y), 3, 1, boxstyle=\"round,pad=0.2\",\n",
    "                                   facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x + 1.5, y + 0.5, label, ha='center', va='center',\n",
    "            fontsize=11, fontweight='bold', color='white')\n",
    "    \n",
    "    # Connect to root\n",
    "    ax.plot([6, x + 1.5], [6.5, y + 1], '-', color='gray', lw=1.5)\n",
    "    \n",
    "    # Method labels\n",
    "    for i, method in enumerate(methods):\n",
    "        my = y - 0.7 - i * 0.6\n",
    "        ax.text(x + 1.5, my, f'• {method}', ha='center', va='center',\n",
    "                fontsize=10, color=color)\n",
    "\n",
    "# Annotations\n",
    "ax.text(2.5, 1.0, 'Learn Q(s,a)\\nDerive policy', ha='center', va='center',\n",
    "        fontsize=9, style='italic', color='gray',\n",
    "        bbox=dict(boxstyle='round', facecolor='#ecf0f1', alpha=0.8))\n",
    "ax.text(6, 1.0, 'Learn both V(s)\\nand π(a|s)', ha='center', va='center',\n",
    "        fontsize=9, style='italic', color='gray',\n",
    "        bbox=dict(boxstyle='round', facecolor='#ecf0f1', alpha=0.8))\n",
    "ax.text(9.5, 1.0, 'Learn π(a|s)\\ndirectly', ha='center', va='center',\n",
    "        fontsize=9, style='italic', color='gray',\n",
    "        bbox=dict(boxstyle='round', facecolor='#ecf0f1', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"In the next notebooks:\")\n",
    "print(\"  NB18: Q-Learning & DQN (value-based)\")\n",
    "print(\"  NB19: REINFORCE & Actor-Critic (policy-based)\")\n",
    "print(\"  NB20: PPO & Modern RL (actor-critic, RLHF)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 11. Connecting RL to LLM Alignment\n\nIn Notebook 16, we introduced **RLHF** (Reinforcement Learning from Human Feedback). Now you can see how it fits the RL framework:\n\n| RL Concept | RLHF for LLMs | F1 Parallel |\n|-----------|----------------|-------------|\n| **Agent** | The language model | Driver + strategist |\n| **State** | The prompt + tokens generated so far | Current race situation |\n| **Action** | Choosing the next token | Pit now, push, conserve |\n| **Policy** | The model's probability distribution over tokens | Race strategy mapping |\n| **Reward** | Score from a trained reward model (human preferences) | Championship points, positions gained |\n| **Environment** | The token generation process | The race: track, rivals, weather |\n\nThe **PPO** algorithm (Notebook 20) is the standard method for this optimization — it updates the LLM's policy to maximize the reward model's scores while staying close to the original model (to prevent degradation).\n\nThis is the bridge between everything we've learned about language models and the RL techniques we'll explore in this part of the curriculum."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick simulation: how RL improves a \"language model\"\n",
    "# Simplified example with discrete token choices\n",
    "\n",
    "def simulate_rlhf_analogy():\n",
    "    \"\"\"Simulate how RL can steer a policy toward higher-reward outputs.\"\"\"\n",
    "    # Pretend we have 5 possible response \"styles\" with different reward scores\n",
    "    styles = ['Verbose & Vague', 'Concise & Clear', 'Rude & Brief', \n",
    "              'Helpful & Detailed', 'Off-topic']\n",
    "    true_rewards = [-0.2, 0.7, -0.8, 0.9, -0.5]  # Human preference scores\n",
    "    \n",
    "    # Initial policy: uniform over styles\n",
    "    policy = np.ones(5) / 5\n",
    "    policy_history = [policy.copy()]\n",
    "    \n",
    "    # Simple policy gradient update (simplified)\n",
    "    learning_rate = 0.3\n",
    "    for step in range(20):\n",
    "        # Sample an action from policy\n",
    "        action = np.random.choice(5, p=policy)\n",
    "        reward = true_rewards[action] + np.random.randn() * 0.1\n",
    "        \n",
    "        # Update: increase probability of rewarded actions\n",
    "        gradient = np.zeros(5)\n",
    "        gradient[action] = reward\n",
    "        \n",
    "        # Softmax update\n",
    "        logits = np.log(policy + 1e-8) + learning_rate * gradient\n",
    "        policy = np.exp(logits) / np.exp(logits).sum()\n",
    "        policy_history.append(policy.copy())\n",
    "    \n",
    "    return styles, true_rewards, np.array(policy_history)\n",
    "\n",
    "\n",
    "styles, rewards, policy_hist = simulate_rlhf_analogy()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "colors = ['#e74c3c', '#2ecc71', '#e67e22', '#3498db', '#95a5a6']\n",
    "for i, (style, color) in enumerate(zip(styles, colors)):\n",
    "    ax.plot(policy_hist[:, i], label=f'{style} (r={rewards[i]})', \n",
    "            color=color, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('RL Update Step', fontsize=12)\n",
    "ax.set_ylabel('Policy Probability', fontsize=12)\n",
    "ax.set_title('How RL Steers a Model Toward Better Outputs', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=9, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The policy gradually shifts probability toward 'Helpful & Detailed'\")\n",
    "print(\"and 'Concise & Clear' — the responses humans prefer.\")\n",
    "print(\"This is the core idea behind RLHF!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercises\n\n### Exercise 1: Custom Gridworld (The Monaco Grand Prix)\n\nCreate a 5x5 gridworld that represents a simplified Monaco street circuit — multiple goals (podium positions) and traps (barriers/DNF zones). Run value iteration and visualize the optimal policy. Experiment with different discount factors (gamma = 0.5, 0.9, 0.99) and observe how the policy changes. How does a short-sighted agent (low gamma) differ from a far-sighted one (high gamma) — does the far-sighted agent take longer detours to avoid traps?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Hint: Modify the GridWorld class to accept a custom grid layout\n",
    "# Then run value_iteration with different gamma values and compare\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2: Monte Carlo vs. TD(0) — Post-Race Review vs. Lap-by-Lap Learning\n\nImplement first-visit Monte Carlo prediction alongside TD(0) for the same gridworld. Compare their convergence rates and final value estimates. Monte Carlo is like doing a full post-race debrief — you wait until the checkered flag, then review the whole race. TD(0) is like the strategist updating their model after every single lap. Which converges faster? Which has lower variance?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Hint: Monte Carlo waits until the end of an episode to update V(s)\n",
    "# using the actual return G_t, while TD(0) updates after each step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 3: UCB Exploration — The Untested Tire Compound\n\nImplement Upper Confidence Bound (UCB) exploration for the multi-armed bandit problem and compare it against epsilon-greedy. In F1 terms, UCB adds a \"curiosity bonus\" for strategies that haven't been tried much — like giving extra optimism to a tire compound you've never raced with at this circuit. The less data you have, the bigger the bonus.\n\nUCB selects:\n\n$$a_t = \\arg\\max_a \\left[Q(a) + c\\sqrt{\\frac{\\ln t}{N(a)}}\\right]$$\n\nwhere $c$ controls exploration strength and $N(a)$ is the number of times action $a$ has been selected."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "# Hint: The UCB bonus term goes to infinity for unvisited actions,\n",
    "# ensuring every action is tried at least once\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Summary\n\n### Key Concepts\n\n| Concept | What It Means | F1 Parallel |\n|---------|--------------|-------------|\n| **Reinforcement Learning** | Agent learns to maximize cumulative reward through interaction | Driver/strategist learns optimal decisions across races |\n| **MDPs** | Formalize RL with states, actions, transitions, rewards, discount | Race as a Markov process: position, tires, gaps, weather |\n| **Bellman equation** | Value = immediate reward + discounted future value | Today's position value = this lap's gain + remaining race value |\n| **Policy evaluation** | Compute how good a policy is | Assess: \"How many points does a one-stop strategy average?\" |\n| **Policy improvement** | Greedily make the policy better | Switch to two-stop when evaluation shows it scores higher |\n| **Value iteration** | Combine evaluation and improvement in one step | Lap-by-lap strategy optimization |\n| **Exploration vs. exploitation** | Try new things or stick with what works | New undercut strategy vs. proven conservative approach |\n| **TD learning** | Learn from experience without a model | Update strategy beliefs after each lap, not just post-race |\n| **Value-based vs. policy-based** | Learn Q-values vs. learn policy directly | Lookup table vs. driver instinct |\n\n### Fundamental Insight\n\nThe Bellman equation transforms the RL problem from \"predict the infinite future\" into \"look one step ahead and use your current estimate.\" This simple recursive trick — combined with sufficient exploration — is powerful enough to learn optimal behavior in complex environments. In F1 terms, the Bellman equation says you don't need to simulate the entire remaining race — just evaluate the next lap and trust your value estimates for the rest."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Next Steps\n\nNow that we understand the RL framework, value functions, and the Bellman equation, we're ready to build **practical RL agents**. In **Notebook 18: Q-Learning & Deep Q-Networks**, we'll:\n\n- Implement tabular Q-learning (model-free control) — learning optimal pit stop timing from experience alone\n- Scale to function approximation with neural networks (DQN) — a deep network learning strategy from thousands of simulated races\n- Learn key techniques: experience replay and target networks\n- Train a DQN agent to solve a control task from raw observations\n\nThe journey from Bellman equations to DQN is one of the most elegant progressions in all of machine learning — and it mirrors how F1 strategy has evolved from gut instinct to simulation-driven decision-making."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}