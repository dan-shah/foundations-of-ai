{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "source": "# Part 6.2: Embeddings\n\nEmbeddings are the **bridge between human concepts and machine learning**. Every word you type into ChatGPT, every product recommendation you receive, every song Spotify suggests -- all of these rely on embeddings to represent meaning as numbers. The core idea is deceptively simple: represent things as dense vectors in a space where **similar things are close together**. This single idea has transformed NLP, recommendation systems, search engines, and much of modern AI.\n\n**F1 analogy:** Imagine representing every F1 driver as a vector of numbers -- `[race_craft, tire_management, wet_weather, qualifying_pace, consistency, overtaking]`. Drivers with similar styles would cluster together in this space. Verstappen and Schumacher might be nearby (dominant champions), while Hamilton and Prost might cluster (smooth, strategic drivers). Embeddings do this automatically, learning the right dimensions from data rather than having an engineer define them.\n\nIn this notebook, you'll build embeddings from scratch, see how vector arithmetic can capture analogies like \"king - man + woman = queen,\" implement semantic search, and understand why embeddings are the foundation of virtually every modern AI system.\n\n---\n\n## Learning Objectives\n\nBy the end of this notebook, you should be able to:\n\n- [ ] Explain why one-hot encoding fails and why dense embeddings are needed\n- [ ] Describe the distributional hypothesis and how it motivates embedding methods\n- [ ] Implement Word2Vec (skip-gram) from scratch in PyTorch\n- [ ] Perform and visualize vector arithmetic on word embeddings\n- [ ] Compare Word2Vec and GloVe approaches\n- [ ] Explain the difference between static and contextual embeddings\n- [ ] Compute cosine similarity, Euclidean distance, and dot product similarity\n- [ ] Build a simple semantic search engine from scratch\n- [ ] Explain the concept of RAG (Retrieval-Augmented Generation)\n- [ ] Describe practical applications of embeddings across domains",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cell-1",
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom collections import Counter, defaultdict\n\n%matplotlib inline\nplt.style.use('seaborn-v0_8-whitegrid')\ntorch.manual_seed(42)\nnp.random.seed(42)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "source": "---\n\n## 1. Why Embeddings?\n\n### The Problem with One-Hot Encoding\n\nImagine you have a vocabulary of 50,000 words. The simplest way to represent each word as a number is **one-hot encoding**: give each word a vector of length 50,000 where exactly one position is 1 and the rest are 0.\n\nThis seems fine at first, but it has three devastating problems:\n\n1. **Sparse and high-dimensional:** Each vector has 49,999 zeros and a single 1. This wastes enormous memory and computation.\n2. **No notion of similarity:** The one-hot vectors for \"cat\" and \"kitten\" are just as different as \"cat\" and \"refrigerator.\" Every word is equally distant from every other word.\n3. **No generalization:** If a model learns something about \"cat,\" that knowledge tells it absolutely nothing about \"kitten.\"\n\n**The key insight:** What if we could represent each word as a short, dense vector (say, 300 numbers) where words with similar meanings are close together in space? That is exactly what embeddings do.\n\n**F1 analogy:** One-hot encoding drivers would mean Verstappen = `[1,0,0,...,0]`, Hamilton = `[0,1,0,...,0]`, Norris = `[0,0,1,...,0]`. Every driver is equally \"far\" from every other -- Verstappen is as different from Hamilton as from a Formula E driver. But a dense \"driver embedding\" like `[speed: 0.95, racecraft: 0.92, consistency: 0.97, wet_skill: 0.88]` captures meaningful relationships. Similar drivers are nearby, and knowledge about one transfers to similar others.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "source": "### Visualization: One-Hot vs Dense Embeddings",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cell-4",
   "source": [
    "# Compare one-hot encoding vs dense embeddings\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# === Left: One-hot encoding ===\nax = axes[0]\nwords = ['cat', 'kitten', 'dog', 'puppy', 'car', 'truck']\nvocab_size = len(words)\n\n# Create one-hot matrix\none_hot = np.eye(vocab_size)\nim = ax.imshow(one_hot, cmap='Blues', aspect='auto', vmin=0, vmax=1)\nax.set_xticks(range(vocab_size))\nax.set_xticklabels([f'dim {i}' for i in range(vocab_size)], fontsize=9)\nax.set_yticks(range(vocab_size))\nax.set_yticklabels(words, fontsize=11, fontweight='bold')\nax.set_title('One-Hot Encoding\\n(Sparse, No Similarity)', fontsize=13, fontweight='bold')\nax.set_xlabel('Dimensions')\n\n# Annotate values\nfor i in range(vocab_size):\n    for j in range(vocab_size):\n        color = 'white' if one_hot[i, j] > 0.5 else 'black'\n        ax.text(j, i, f'{one_hot[i,j]:.0f}', ha='center', va='center', \n                color=color, fontsize=10)\n\n# === Right: Dense embeddings ===\nax = axes[1]\n# Simulate meaningful dense embeddings where similar words are close\nembeddings = np.array([\n    [0.8, 0.2, -0.5],   # cat\n    [0.7, 0.3, -0.4],   # kitten (close to cat)\n    [0.6, -0.3, -0.6],  # dog\n    [0.5, -0.2, -0.5],  # puppy (close to dog)\n    [-0.7, 0.1, 0.8],   # car\n    [-0.6, -0.1, 0.9],  # truck (close to car)\n])\n\nim2 = ax.imshow(embeddings, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\nax.set_xticks(range(3))\nax.set_xticklabels(['dim 0', 'dim 1', 'dim 2'], fontsize=9)\nax.set_yticks(range(vocab_size))\nax.set_yticklabels(words, fontsize=11, fontweight='bold')\nax.set_title('Dense Embeddings\\n(Compact, Similar = Close)', fontsize=13, fontweight='bold')\nax.set_xlabel('Dimensions')\nplt.colorbar(im2, ax=ax, shrink=0.8)\n\n# Annotate values\nfor i in range(vocab_size):\n    for j in range(3):\n        ax.text(j, i, f'{embeddings[i,j]:.1f}', ha='center', va='center', \n                fontsize=10, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# Show similarity comparison\nprint(\"=== Cosine Similarities ===\")\nprint(\"\\nOne-hot encoding:\")\nfor i, j in [(0,1), (0,2), (0,4)]:\n    sim = np.dot(one_hot[i], one_hot[j]) / (np.linalg.norm(one_hot[i]) * np.linalg.norm(one_hot[j]))\n    print(f\"  {words[i]:8s} <-> {words[j]:8s}: {sim:.4f}\")\n\nprint(\"\\nDense embeddings:\")\nfor i, j in [(0,1), (0,2), (0,4)]:\n    sim = np.dot(embeddings[i], embeddings[j]) / (np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j]))\n    print(f\"  {words[i]:8s} <-> {words[j]:8s}: {sim:.4f}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "source": "### The Distributional Hypothesis\n\n> *\"You shall know a word by the company it keeps.\"* -- J.R. Firth (1957)\n\nThis is the foundational insight behind all embedding methods. Words that appear in similar contexts tend to have similar meanings:\n\n- \"The **cat** sat on the mat\" / \"The **kitten** sat on the mat\"\n- \"I drove my **car** to work\" / \"I drove my **truck** to work\"\n\nIf two words frequently appear in the same contexts (surrounded by the same neighboring words), they probably mean similar things. This is what embedding algorithms learn to capture: they convert co-occurrence patterns into geometric relationships.\n\n**F1 analogy:** The distributional hypothesis in F1 terms: \"You shall know a driver by the results they produce in similar conditions.\" Drivers who consistently produce similar results at similar circuits, in similar weather, against similar competition, will end up with similar embeddings -- even if we never explicitly told the model they are similar. Verstappen and peak-Hamilton both dominate wet races and street circuits, so their embeddings converge, learned purely from performance data.\n\n### Deep Dive: Why Embeddings Matter\n\n| Problem with One-Hot | How Embeddings Fix It | F1 Parallel |\n|---|---|---|\n| Vectors are huge (vocab_size dimensions) | Vectors are small (50-1000 dimensions) | 20-driver one-hot vs. compact performance profile |\n| All words are equidistant | Similar words are nearby | All drivers equally different vs. similar styles cluster |\n| No generalization between words | Knowledge transfers between similar words | Learning about Verstappen helps predict Leclerc's behavior |\n| Memory scales as O(V^2) for pairwise ops | Memory scales as O(V * d) where d << V | Efficient comparison of any two drivers |\n| Cannot capture relationships | Vector arithmetic captures analogies | \"Leclerc - Ferrari + McLaren = Norris\" |\n\n#### Key Insight\n\nEmbeddings transform **discrete symbols** (words, products, users) into **continuous vectors** in a learned space. This is what makes gradient-based optimization possible -- you cannot take the gradient of a one-hot vector, but you can take the gradient of a dense embedding and update it during training.\n\n#### Common Misconceptions\n\n| Misconception | Reality |\n|---|---|\n| Embeddings are hand-designed features | They are **learned** from data |\n| Each dimension has a clear meaning | Dimensions are usually not individually interpretable |\n| Embeddings are only for words | They work for anything: users, products, genes, molecules |\n| Bigger embeddings are always better | There is an optimal size; too large overfits, too small underfits |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "source": "---\n\n## 2. Word2Vec\n\n### Intuitive Explanation\n\nWord2Vec (Mikolov et al., 2013) was a breakthrough: a simple neural network that learns word embeddings by predicting context. The core idea has two flavors:\n\n**Skip-gram:** Given a center word, predict the surrounding context words.\n- Input: \"cat\" -> Predict: \"the\", \"sat\", \"on\", \"mat\"\n\n**CBOW (Continuous Bag of Words):** Given surrounding context words, predict the center word.\n- Input: \"the\", \"sat\", \"on\", \"mat\" -> Predict: \"cat\"\n\n**Why does this work?** If \"cat\" and \"kitten\" both predict similar context words (\"sat,\" \"purring,\" \"fur\"), then the model is forced to give them similar embeddings. The training objective implicitly encodes semantic similarity.\n\n**The surprising result:** The learned embeddings capture not just similarity, but **relational structure**. The vector difference between \"king\" and \"man\" is approximately the same as between \"queen\" and \"woman.\" This means:\n\n$$\\text{king} - \\text{man} + \\text{woman} \\approx \\text{queen}$$\n\nThis was one of the most shocking results in NLP history.\n\n**F1 analogy:** Word2Vec applied to F1 would learn **driver embeddings** where similar drivers cluster together. If you trained on race reports and commentary, drivers who appear in similar contexts (\"dominated the race,\" \"set the fastest lap,\" \"led from pole\") would get similar embeddings. The model would learn, without being told, that Verstappen, Hamilton, and Schumacher belong in a \"dominant champion\" cluster, while Norris, Leclerc, and Russell form a \"rising star\" cluster. And the arithmetic works too:\n\n$$\\vec{\\text{Leclerc}} - \\vec{\\text{Ferrari}} + \\vec{\\text{McLaren}} \\approx \\vec{\\text{Norris}}$$\n\n(the \"star young driver\" concept transfers between teams)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "source": "### Visualization: Skip-gram vs CBOW",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cell-8",
   "source": "# Visualize Skip-gram vs CBOW architectures\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\nsentence = [\"the\", \"cat\", \"sat\", \"on\", \"the\"]\ncenter_idx = 2  # \"sat\"\nwindow = 2\n\n# === Left: Skip-gram ===\nax = axes[0]\nax.set_xlim(-1, 5)\nax.set_ylim(-0.5, 3.5)\nax.set_aspect('equal')\nax.axis('off')\nax.set_title('Skip-gram\\n\"Predict context from center\"', fontsize=14, fontweight='bold')\n\n# Draw words\nfor i, word in enumerate(sentence):\n    color = 'steelblue' if i == center_idx else 'lightcoral'\n    edge = 'navy' if i == center_idx else 'darkred'\n    alpha = 1.0 if abs(i - center_idx) <= window else 0.3\n    \n    rect = plt.Rectangle((i - 0.4, 2.8), 0.8, 0.5, facecolor=color, \n                          edgecolor=edge, linewidth=2, alpha=alpha)\n    ax.add_patch(rect)\n    ax.text(i, 3.05, word, ha='center', va='center', fontsize=12, \n            fontweight='bold', alpha=alpha)\n\n# Draw arrows from center to context\nfor i in range(len(sentence)):\n    if i != center_idx and abs(i - center_idx) <= window:\n        ax.annotate('', xy=(i, 2.8), xytext=(center_idx, 2.8),\n                    arrowprops=dict(arrowstyle='->', color='darkred', lw=2))\n\n# Labels\nax.text(center_idx, 2.2, 'INPUT\\n(center word)', ha='center', va='center',\n        fontsize=10, color='navy', fontweight='bold')\nax.text(2, 0.8, 'Neural\\nNetwork', ha='center', va='center', fontsize=12,\n        bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow', edgecolor='orange', linewidth=2))\nax.text(2, 0.0, '\"sat\" -> predict \"the\", \"cat\", \"on\", \"the\"', \n        ha='center', va='center', fontsize=10, style='italic')\n\n# === Right: CBOW ===\nax = axes[1]\nax.set_xlim(-1, 5)\nax.set_ylim(-0.5, 3.5)\nax.set_aspect('equal')\nax.axis('off')\nax.set_title('CBOW\\n\"Predict center from context\"', fontsize=14, fontweight='bold')\n\n# Draw words\nfor i, word in enumerate(sentence):\n    color = 'lightcoral' if i != center_idx else 'steelblue'\n    edge = 'darkred' if i != center_idx else 'navy'\n    alpha = 1.0 if abs(i - center_idx) <= window else 0.3\n    \n    rect = plt.Rectangle((i - 0.4, 2.8), 0.8, 0.5, facecolor=color, \n                          edgecolor=edge, linewidth=2, alpha=alpha)\n    ax.add_patch(rect)\n    ax.text(i, 3.05, word, ha='center', va='center', fontsize=12, \n            fontweight='bold', alpha=alpha)\n\n# Draw arrows from context to center\nfor i in range(len(sentence)):\n    if i != center_idx and abs(i - center_idx) <= window:\n        ax.annotate('', xy=(center_idx, 2.8), xytext=(i, 2.8),\n                    arrowprops=dict(arrowstyle='->', color='navy', lw=2))\n\n# Labels\nax.text(center_idx, 2.2, 'OUTPUT\\n(center word)', ha='center', va='center',\n        fontsize=10, color='navy', fontweight='bold')\nax.text(2, 0.8, 'Neural\\nNetwork', ha='center', va='center', fontsize=12,\n        bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow', edgecolor='orange', linewidth=2))\nax.text(2, 0.0, '\"the\", \"cat\", \"on\", \"the\" -> predict \"sat\"', \n        ha='center', va='center', fontsize=10, style='italic')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "source": "### Implementing Word2Vec (Skip-gram) from Scratch\n\nLet's build a skip-gram model step by step. The architecture is surprisingly simple:\n\n1. **Input:** One-hot encoded center word\n2. **Hidden layer:** Embedding lookup (this IS the embedding we want to learn)\n3. **Output:** Probability distribution over vocabulary (which words are likely context words)\n\nWith negative sampling, we simplify this further: instead of predicting over the entire vocabulary, we just need to distinguish true context words from random \"negative\" words.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cell-10",
   "source": "# Step 1: Prepare a toy corpus with enough structure to learn from\ncorpus = [\n    \"the king rules the kingdom with wisdom\",\n    \"the queen rules the kingdom with grace\",\n    \"the prince will become king one day\",\n    \"the princess will become queen one day\",\n    \"the man works in the village\",\n    \"the woman works in the village\",\n    \"the boy plays in the village\",\n    \"the girl plays in the village\",\n    \"a king and queen rule together\",\n    \"a man and woman live together\",\n    \"a boy and girl play together\",\n    \"the king sits on the royal throne\",\n    \"the queen sits on the royal throne\",\n    \"the prince is the son of the king\",\n    \"the princess is the daughter of the queen\",\n    \"the man is the father of the boy\",\n    \"the woman is the mother of the girl\",\n    \"a brave king protects the kingdom\",\n    \"a wise queen protects the kingdom\",\n    \"the young prince trains with the knight\",\n    \"the young princess studies with the scholar\",\n    \"the strong man builds the house\",\n    \"the kind woman tends the garden\",\n    \"the king wears the golden crown\",\n    \"the queen wears the silver crown\",\n]\n\n# Tokenize\nsentences = [s.lower().split() for s in corpus]\nall_words = [word for sentence in sentences for word in sentence]\n\n# Build vocabulary\nword_counts = Counter(all_words)\nvocab = sorted(word_counts.keys())\nword2idx = {w: i for i, w in enumerate(vocab)}\nidx2word = {i: w for w, i in word2idx.items()}\nvocab_size = len(vocab)\n\nprint(f\"Vocabulary size: {vocab_size}\")\nprint(f\"Total tokens: {len(all_words)}\")\nprint(f\"Sample words: {vocab[:10]}\")\nprint(f\"Word counts (top 10): {word_counts.most_common(10)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cell-11",
   "source": "# Step 2: Generate skip-gram training pairs\ndef generate_skipgram_pairs(sentences, word2idx, window_size=2):\n    \"\"\"\n    Generate (center_word, context_word) pairs for skip-gram training.\n    \n    Args:\n        sentences: List of tokenized sentences\n        word2idx: Word to index mapping\n        window_size: Number of words on each side to consider as context\n    \n    Returns:\n        List of (center_idx, context_idx) tuples\n    \"\"\"\n    pairs = []\n    for sentence in sentences:\n        indices = [word2idx[w] for w in sentence]\n        for i, center in enumerate(indices):\n            # Look at words within the window\n            for j in range(max(0, i - window_size), min(len(indices), i + window_size + 1)):\n                if i != j:\n                    pairs.append((center, indices[j]))\n    return pairs\n\npairs = generate_skipgram_pairs(sentences, word2idx, window_size=2)\nprint(f\"Generated {len(pairs)} training pairs\")\nprint(f\"\\nSample pairs (center -> context):\")\nfor center, context in pairs[:8]:\n    print(f\"  {idx2word[center]:12s} -> {idx2word[context]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cell-12",
   "source": "# Step 3: Define the Skip-gram model with Negative Sampling\nclass SkipGramNegSampling(nn.Module):\n    \"\"\"\n    Skip-gram Word2Vec with negative sampling.\n    \n    Instead of computing softmax over the entire vocabulary (expensive!),\n    we train a binary classifier: is this a real (center, context) pair\n    or a fake one?\n    \"\"\"\n    def __init__(self, vocab_size, embedding_dim):\n        super().__init__()\n        # Two embedding matrices:\n        # - center_embeddings: for center words (this is what we keep as our word vectors)\n        # - context_embeddings: for context words (used during training only)\n        self.center_embeddings = nn.Embedding(vocab_size, embedding_dim)\n        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n        \n        # Initialize with small random values\n        nn.init.uniform_(self.center_embeddings.weight, -0.5/embedding_dim, 0.5/embedding_dim)\n        nn.init.uniform_(self.context_embeddings.weight, -0.5/embedding_dim, 0.5/embedding_dim)\n    \n    def forward(self, center_words, context_words, negative_words):\n        \"\"\"\n        Args:\n            center_words: (batch_size,) center word indices\n            context_words: (batch_size,) true context word indices\n            negative_words: (batch_size, num_neg) negative sample indices\n        \n        Returns:\n            loss: negative sampling loss\n        \"\"\"\n        # Get embeddings\n        center_emb = self.center_embeddings(center_words)      # (batch, emb_dim)\n        context_emb = self.context_embeddings(context_words)    # (batch, emb_dim)\n        neg_emb = self.context_embeddings(negative_words)       # (batch, num_neg, emb_dim)\n        \n        # Positive score: dot product of center and true context\n        pos_score = torch.sum(center_emb * context_emb, dim=1)  # (batch,)\n        pos_loss = F.logsigmoid(pos_score)                       # log(sigmoid(score))\n        \n        # Negative scores: dot product of center with each negative sample\n        # center_emb: (batch, emb_dim) -> (batch, emb_dim, 1)\n        neg_score = torch.bmm(neg_emb, center_emb.unsqueeze(2)).squeeze(2)  # (batch, num_neg)\n        neg_loss = F.logsigmoid(-neg_score).sum(dim=1)  # log(sigmoid(-score))\n        \n        # Total loss: maximize positive score, minimize negative scores\n        loss = -(pos_loss + neg_loss).mean()\n        return loss\n\n# Create model\nEMBEDDING_DIM = 20  # Small for our toy corpus\nmodel = SkipGramNegSampling(vocab_size, EMBEDDING_DIM)\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"Center embeddings shape: {model.center_embeddings.weight.shape}\")\nprint(f\"Context embeddings shape: {model.context_embeddings.weight.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cell-13",
   "source": "# Step 4: Training loop\ndef get_negative_samples(batch_size, num_neg, vocab_size, word_counts, idx2word):\n    \"\"\"\n    Sample negative words proportional to frequency^(3/4).\n    The 3/4 power smooths the distribution, giving rare words more chance.\n    \"\"\"\n    # Build sampling distribution: freq^(3/4)\n    freqs = np.array([word_counts.get(idx2word[i], 1) for i in range(vocab_size)], dtype=np.float64)\n    freqs = freqs ** 0.75\n    freqs /= freqs.sum()\n    \n    neg_samples = np.random.choice(vocab_size, size=(batch_size, num_neg), p=freqs)\n    return torch.LongTensor(neg_samples)\n\n# Training\noptimizer = optim.Adam(model.parameters(), lr=0.01)\nNUM_NEG = 5\nEPOCHS = 200\nBATCH_SIZE = 64\n\n# Convert pairs to tensors\ncenter_indices = torch.LongTensor([p[0] for p in pairs])\ncontext_indices = torch.LongTensor([p[1] for p in pairs])\n\nlosses = []\nfor epoch in range(EPOCHS):\n    # Shuffle\n    perm = torch.randperm(len(pairs))\n    center_shuffled = center_indices[perm]\n    context_shuffled = context_indices[perm]\n    \n    epoch_loss = 0\n    num_batches = 0\n    \n    for i in range(0, len(pairs), BATCH_SIZE):\n        batch_center = center_shuffled[i:i+BATCH_SIZE]\n        batch_context = context_shuffled[i:i+BATCH_SIZE]\n        batch_neg = get_negative_samples(len(batch_center), NUM_NEG, vocab_size, word_counts, idx2word)\n        \n        optimizer.zero_grad()\n        loss = model(batch_center, batch_context, batch_neg)\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        num_batches += 1\n    \n    avg_loss = epoch_loss / num_batches\n    losses.append(avg_loss)\n    \n    if (epoch + 1) % 50 == 0:\n        print(f\"Epoch {epoch+1:3d}/{EPOCHS}: Loss = {avg_loss:.4f}\")\n\n# Plot training loss\nfig, ax = plt.subplots(figsize=(10, 4))\nax.plot(losses, color='steelblue', linewidth=2)\nax.set_xlabel('Epoch', fontsize=12)\nax.set_ylabel('Loss', fontsize=12)\nax.set_title('Skip-gram Training Loss', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "source": "### Visualization: Learned Word Embeddings in 2D\n\nLet's project our learned embeddings down to 2D using PCA and see if similar words cluster together.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cell-15",
   "source": "# Extract learned embeddings\nembeddings_matrix = model.center_embeddings.weight.detach().numpy()\n\n# PCA for 2D projection\ndef pca_2d(X):\n    \"\"\"Project data to 2D using PCA.\"\"\"\n    X_centered = X - X.mean(axis=0)\n    cov = np.cov(X_centered.T)\n    eigenvalues, eigenvectors = np.linalg.eigh(cov)\n    # Take top 2 eigenvectors (largest eigenvalues)\n    idx = np.argsort(eigenvalues)[::-1][:2]\n    top_vectors = eigenvectors[:, idx]\n    return X_centered @ top_vectors\n\n# Project to 2D\nembeddings_2d = pca_2d(embeddings_matrix)\n\n# Define word groups for coloring\ngroups = {\n    'royalty_male': ['king', 'prince'],\n    'royalty_female': ['queen', 'princess'],\n    'people_male': ['man', 'boy', 'father', 'son'],\n    'people_female': ['woman', 'girl', 'mother', 'daughter'],\n}\n\ngroup_colors = {\n    'royalty_male': 'blue',\n    'royalty_female': 'red',\n    'people_male': 'steelblue',\n    'people_female': 'lightcoral',\n}\n\ngroup_labels = {\n    'royalty_male': 'Male Royalty',\n    'royalty_female': 'Female Royalty',\n    'people_male': 'Male Common',\n    'people_female': 'Female Common',\n}\n\nfig, ax = plt.subplots(figsize=(12, 10))\n\n# Plot all words in gray\nall_group_words = set(w for words in groups.values() for w in words)\nfor i, word in enumerate(vocab):\n    if word not in all_group_words:\n        ax.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], c='lightgray', s=30, alpha=0.5)\n        ax.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=7, alpha=0.4)\n\n# Plot grouped words with colors\nfor group_name, words in groups.items():\n    for word in words:\n        if word in word2idx:\n            idx = word2idx[word]\n            ax.scatter(embeddings_2d[idx, 0], embeddings_2d[idx, 1], \n                      c=group_colors[group_name], s=150, zorder=5, edgecolors='black', linewidth=1.5,\n                      label=group_labels[group_name] if word == words[0] else \"\")\n            ax.annotate(word, (embeddings_2d[idx, 0], embeddings_2d[idx, 1]),\n                       fontsize=12, fontweight='bold', \n                       xytext=(8, 8), textcoords='offset points')\n\nax.set_title('Learned Word2Vec Embeddings (PCA Projection)', fontsize=14, fontweight='bold')\nax.set_xlabel('PC 1', fontsize=12)\nax.set_ylabel('PC 2', fontsize=12)\nax.legend(fontsize=10, loc='best')\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "source": "### The Magic: Vector Arithmetic\n\nThe most surprising property of word embeddings is that **vector arithmetic captures analogies**. The direction from \"man\" to \"woman\" encodes the concept of gender. Adding that direction to \"king\" should land near \"queen.\"\n\n$$\\vec{\\text{king}} - \\vec{\\text{man}} + \\vec{\\text{woman}} \\approx \\vec{\\text{queen}}$$\n\n**F1 analogy:** Vector arithmetic in F1 embedding space would capture team-driver relationships. The direction from Ferrari to Leclerc encodes \"lead driver of this team.\" Applying that same direction to other teams:\n\n$$\\vec{\\text{Leclerc}} - \\vec{\\text{Ferrari}} + \\vec{\\text{McLaren}} \\approx \\vec{\\text{Norris}}$$\n$$\\vec{\\text{Verstappen}} - \\vec{\\text{Red Bull}} + \\vec{\\text{Mercedes}} \\approx \\vec{\\text{Hamilton}}$$\n\nOr for circuit characteristics: the direction from \"Monaco\" to \"Singapore\" encodes \"street circuit in a different continent.\" The direction from \"Monza\" to \"Spa\" encodes \"classic European power circuit.\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cell-17",
   "source": "# Vector arithmetic on our learned embeddings\ndef cosine_similarity(a, b):\n    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\ndef find_nearest(query_vec, embeddings_matrix, idx2word, top_k=5, exclude=None):\n    \"\"\"\n    Find the top_k nearest words to a query vector.\n    \n    Args:\n        query_vec: Query embedding vector\n        embeddings_matrix: All word embeddings (vocab_size, emb_dim)\n        idx2word: Index to word mapping\n        top_k: Number of results\n        exclude: Set of words to exclude from results\n    \n    Returns:\n        List of (word, similarity) tuples\n    \"\"\"\n    if exclude is None:\n        exclude = set()\n    \n    similarities = []\n    for i in range(len(idx2word)):\n        word = idx2word[i]\n        if word not in exclude:\n            sim = cosine_similarity(query_vec, embeddings_matrix[i])\n            similarities.append((word, sim))\n    \n    similarities.sort(key=lambda x: x[1], reverse=True)\n    return similarities[:top_k]\n\ndef analogy(a, b, c, embeddings_matrix, word2idx, idx2word):\n    \"\"\"\n    Solve: a is to b as c is to ?\n    Computes: b - a + c and finds nearest word.\n    \"\"\"\n    vec_a = embeddings_matrix[word2idx[a]]\n    vec_b = embeddings_matrix[word2idx[b]]\n    vec_c = embeddings_matrix[word2idx[c]]\n    \n    query = vec_b - vec_a + vec_c\n    results = find_nearest(query, embeddings_matrix, idx2word, top_k=5, exclude={a, b, c})\n    return results\n\n# Test analogies\nprint(\"=== Vector Arithmetic Analogies ===\\n\")\n\nanalogy_tests = [\n    (\"man\", \"king\", \"woman\", \"queen\"),\n    (\"man\", \"boy\", \"woman\", \"girl\"),\n    (\"king\", \"prince\", \"queen\", \"princess\"),\n    (\"king\", \"kingdom\", \"queen\", \"kingdom\"),\n]\n\nfor a, b, c, expected in analogy_tests:\n    results = analogy(a, b, c, embeddings_matrix, word2idx, idx2word)\n    top_word = results[0][0]\n    marker = \"  <<< correct!\" if top_word == expected else f\"  (expected: {expected})\"\n    print(f\"  {a:10s} -> {b:10s} :: {c:10s} -> {top_word:10s} (sim={results[0][1]:.4f}){marker}\")\n    for word, sim in results[1:3]:\n        print(f\"{'':42s}   {word:10s} (sim={sim:.4f})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "source": "### Visualization: Vector Arithmetic in Embedding Space",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cell-19",
   "source": "# Visualize the king - man + woman = queen analogy in 2D\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# === Left: The analogy in embedding space ===\nax = axes[0]\n\nanalogy_words = ['king', 'queen', 'man', 'woman']\nanalogy_colors = ['blue', 'red', 'steelblue', 'lightcoral']\nanalogy_indices = [word2idx[w] for w in analogy_words]\n\n# Plot the four words\nfor i, (word, color) in enumerate(zip(analogy_words, analogy_colors)):\n    idx = word2idx[word]\n    ax.scatter(embeddings_2d[idx, 0], embeddings_2d[idx, 1], \n              c=color, s=200, zorder=5, edgecolors='black', linewidth=2)\n    ax.annotate(word, (embeddings_2d[idx, 0], embeddings_2d[idx, 1]),\n               fontsize=14, fontweight='bold', \n               xytext=(10, 10), textcoords='offset points')\n\n# Draw \"gender\" arrows: man->woman, king->queen\nfor start_word, end_word, label in [('man', 'woman', 'gender\\ndirection'), \n                                      ('king', 'queen', '')]:\n    si = word2idx[start_word]\n    ei = word2idx[end_word]\n    ax.annotate('', xy=(embeddings_2d[ei, 0], embeddings_2d[ei, 1]),\n               xytext=(embeddings_2d[si, 0], embeddings_2d[si, 1]),\n               arrowprops=dict(arrowstyle='->', color='green', lw=2.5, ls='--'))\n    if label:\n        mid_x = (embeddings_2d[si, 0] + embeddings_2d[ei, 0]) / 2\n        mid_y = (embeddings_2d[si, 1] + embeddings_2d[ei, 1]) / 2\n        ax.annotate(label, (mid_x, mid_y), fontsize=10, color='green', \n                   fontweight='bold', ha='center',\n                   xytext=(-30, 0), textcoords='offset points')\n\n# Draw \"royalty\" arrows: man->king, woman->queen\nfor start_word, end_word, label in [('man', 'king', 'royalty\\ndirection'), \n                                      ('woman', 'queen', '')]:\n    si = word2idx[start_word]\n    ei = word2idx[end_word]\n    ax.annotate('', xy=(embeddings_2d[ei, 0], embeddings_2d[ei, 1]),\n               xytext=(embeddings_2d[si, 0], embeddings_2d[si, 1]),\n               arrowprops=dict(arrowstyle='->', color='purple', lw=2.5, ls='--'))\n    if label:\n        mid_x = (embeddings_2d[si, 0] + embeddings_2d[ei, 0]) / 2\n        mid_y = (embeddings_2d[si, 1] + embeddings_2d[ei, 1]) / 2\n        ax.annotate(label, (mid_x, mid_y), fontsize=10, color='purple', \n                   fontweight='bold', ha='center',\n                   xytext=(30, 0), textcoords='offset points')\n\nax.set_title('king - man + woman = queen\\n(Parallel Relationship Structure)', \n             fontsize=13, fontweight='bold')\nax.set_xlabel('PC 1', fontsize=11)\nax.set_ylabel('PC 2', fontsize=11)\nax.grid(True, alpha=0.3)\n\n# === Right: Similarity heatmap ===\nax = axes[1]\nfocus_words = ['king', 'queen', 'prince', 'princess', 'man', 'woman', 'boy', 'girl']\nfocus_indices = [word2idx[w] for w in focus_words if w in word2idx]\nfocus_words_filtered = [w for w in focus_words if w in word2idx]\n\nn = len(focus_words_filtered)\nsim_matrix = np.zeros((n, n))\nfor i in range(n):\n    for j in range(n):\n        sim_matrix[i, j] = cosine_similarity(\n            embeddings_matrix[word2idx[focus_words_filtered[i]]],\n            embeddings_matrix[word2idx[focus_words_filtered[j]]]\n        )\n\nim = ax.imshow(sim_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\nax.set_xticks(range(n))\nax.set_xticklabels(focus_words_filtered, rotation=45, ha='right', fontsize=11)\nax.set_yticks(range(n))\nax.set_yticklabels(focus_words_filtered, fontsize=11)\nax.set_title('Cosine Similarity Between Word Embeddings', fontsize=13, fontweight='bold')\nplt.colorbar(im, ax=ax, shrink=0.8)\n\n# Annotate\nfor i in range(n):\n    for j in range(n):\n        ax.text(j, i, f'{sim_matrix[i,j]:.2f}', ha='center', va='center', fontsize=8)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "source": "### Deep Dive: Why Does Word2Vec Capture Semantics?\n\nThe math behind this is elegant. Consider the skip-gram objective: we want to maximize $P(\\text{context} | \\text{center})$. With negative sampling, we learn embeddings where:\n\n$$\\vec{w}_{\\text{center}} \\cdot \\vec{w}_{\\text{context}} \\approx \\log P(\\text{context} | \\text{center})$$\n\nMikolov et al. showed that this implicitly factorizes a **pointwise mutual information (PMI) matrix** -- a matrix that captures how much more often two words co-occur than you would expect by chance. Words that co-occur in similar contexts end up with similar vectors because they have similar PMI profiles.\n\n#### Key Insight\n\nWord2Vec does not \"understand\" language. It discovers statistical regularities in word co-occurrence and encodes them as geometric relationships. The fact that this produces semantically meaningful vectors is a profound statement about how meaning relates to usage patterns.\n\n**F1 analogy:** A \"DriverVec\" model would not \"understand\" racing. It would discover that Verstappen and Hamilton co-occur with words like \"pole,\" \"fastest lap,\" and \"victory\" far more than chance would predict, and encode this as geometric proximity. The model captures the *structure* of racing discourse without understanding a single corner.\n\n#### The Negative Sampling Trick\n\n| Approach | What it does | Cost per step |\n|---|---|---|\n| Full softmax | Normalize over all V words | O(V) -- very expensive |\n| Negative sampling | Compare 1 positive + k negatives | O(k) -- fast! |\n| Hierarchical softmax | Binary tree over vocabulary | O(log V) |\n\nNegative sampling typically uses k=5 for large datasets and k=15 for small ones. The negative words are sampled proportional to $f(w)^{3/4}$, where $f(w)$ is the word frequency. The 3/4 exponent was found empirically to work best -- it smooths the distribution so rare words get sampled more than their raw frequency would suggest.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "source": "---\n\n## 3. GloVe (Global Vectors for Word Representation)\n\n### Intuitive Explanation\n\nWhile Word2Vec learns from **local** context windows (one word at a time), GloVe (Pennington et al., 2014) takes a different approach: it first builds a global **co-occurrence matrix** and then factorizes it.\n\nThink of it this way:\n- **Word2Vec** reads through the text word by word, like a human reading a book\n- **GloVe** first counts all word pair co-occurrences across the entire corpus, then finds embeddings that best explain those counts\n\nThe GloVe objective is:\n\n$$J = \\sum_{i,j=1}^{V} f(X_{ij}) \\left( \\vec{w}_i^T \\vec{w}_j + b_i + b_j - \\log X_{ij} \\right)^2$$\n\n**What this means:** We want the dot product of two word vectors (plus bias terms) to approximate the log of how often they co-occur. The weighting function $f(X_{ij})$ prevents very common pairs (like \"the, the\") from dominating.\n\n**F1 analogy:** If Word2Vec is like a scout watching one race at a time and building driver impressions lap by lap, GloVe is like a data analyst who first compiles the complete season statistics -- every driver-circuit combination, every head-to-head result, every qualifying gap -- and then finds the driver/circuit embeddings that best explain the full dataset at once. Same destination, different route.\n\n### GloVe: Building the Co-occurrence Matrix",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cell-22",
   "source": "# Build co-occurrence matrix from our corpus\ndef build_cooccurrence_matrix(sentences, word2idx, window_size=2):\n    \"\"\"\n    Build a word-word co-occurrence matrix.\n    \n    Args:\n        sentences: List of tokenized sentences\n        word2idx: Word to index mapping\n        window_size: Context window size\n    \n    Returns:\n        Co-occurrence matrix of shape (vocab_size, vocab_size)\n    \"\"\"\n    V = len(word2idx)\n    cooccur = np.zeros((V, V))\n    \n    for sentence in sentences:\n        indices = [word2idx[w] for w in sentence]\n        for i, center in enumerate(indices):\n            for j in range(max(0, i - window_size), min(len(indices), i + window_size + 1)):\n                if i != j:\n                    # Weight by distance: closer words get higher weight\n                    distance = abs(i - j)\n                    cooccur[center, indices[j]] += 1.0 / distance\n    \n    return cooccur\n\ncooccur_matrix = build_cooccurrence_matrix(sentences, word2idx, window_size=2)\n\n# Visualize a portion of the co-occurrence matrix\nfig, ax = plt.subplots(figsize=(10, 8))\n\nfocus_words = ['king', 'queen', 'prince', 'princess', 'man', 'woman', 'boy', 'girl',\n               'kingdom', 'village', 'throne', 'crown']\nfocus_words = [w for w in focus_words if w in word2idx]\nfocus_indices = [word2idx[w] for w in focus_words]\n\nsub_matrix = cooccur_matrix[np.ix_(focus_indices, focus_indices)]\nim = ax.imshow(sub_matrix, cmap='YlOrRd', aspect='auto')\nax.set_xticks(range(len(focus_words)))\nax.set_xticklabels(focus_words, rotation=45, ha='right', fontsize=10)\nax.set_yticks(range(len(focus_words)))\nax.set_yticklabels(focus_words, fontsize=10)\nax.set_title('Co-occurrence Matrix (GloVe Input)', fontsize=14, fontweight='bold')\nplt.colorbar(im, ax=ax, shrink=0.8, label='Co-occurrence count')\n\n# Annotate non-zero values\nfor i in range(len(focus_words)):\n    for j in range(len(focus_words)):\n        val = sub_matrix[i, j]\n        if val > 0:\n            ax.text(j, i, f'{val:.1f}', ha='center', va='center', fontsize=7,\n                   color='white' if val > sub_matrix.max() * 0.6 else 'black')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Notice: words that appear in similar contexts (king/queen, man/woman)\")\nprint(\"have similar co-occurrence patterns across the columns.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "source": "### Comparison: Word2Vec vs GloVe\n\n| Feature | Word2Vec | GloVe | F1 Parallel |\n|---|---|---|---|\n| **Approach** | Predictive (neural network) | Count-based (matrix factorization) | Scout watching races vs. statistician analyzing the spreadsheet |\n| **Training data** | Local context windows | Global co-occurrence matrix | One race at a time vs. full season at once |\n| **Objective** | Predict context words | Reconstruct log co-occurrence | Predict nearby events vs. explain overall patterns |\n| **Strengths** | Good with small data, captures syntax | Better with large data, captures semantics | Works with few races vs. works with full history |\n| **Weaknesses** | Only sees local context | Requires building full matrix | Misses the big picture vs. slow to start |\n| **Training** | Online (stochastic) | Batch (needs full corpus first) | Learns during the season vs. analyzes after |\n| **Result quality** | Very similar | Very similar | Both produce good driver profiles |\n| **Key paper** | Mikolov et al., 2013 | Pennington et al., 2014 | -- |\n\n**What this means:** In practice, Word2Vec and GloVe produce similarly good embeddings. The choice often comes down to implementation convenience. Both have been largely superseded by contextual embeddings from models like BERT and GPT.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "source": "---\n\n## 4. Modern Embeddings: From Static to Contextual\n\n### Intuitive Explanation\n\nWord2Vec and GloVe give each word **one fixed embedding** regardless of context. But consider the word \"bank\":\n\n- \"I sat on the river **bank**\" (riverbank)\n- \"I deposited money at the **bank**\" (financial institution)\n\nThese are completely different meanings, but Word2Vec assigns the same vector to both! This is a fundamental limitation of **static embeddings**.\n\n**Contextual embeddings** (from models like BERT, GPT, and their descendants) solve this by computing a **different embedding for each occurrence** of a word, based on the surrounding context. The same word \"bank\" gets a different vector depending on whether it appears near \"river\" or \"money.\"\n\n**F1 analogy:** A static embedding for \"Hamilton\" would be one fixed vector regardless of context. But Hamilton in 2020 (dominant Mercedes, 7th title) is very different from Hamilton in 2022 (struggling W13, midfield battles). Contextual embeddings capture this: \"Hamilton\" gets a different vector depending on the surrounding season, team performance, and race context. The same driver name in different contexts produces different representations -- just as the same word means different things in different sentences.\n\n### How Contextual Embeddings Work\n\n1. **Input:** A full sentence is fed into a Transformer model\n2. **Processing:** Each word attends to all other words via self-attention\n3. **Output:** Each word gets a context-dependent embedding\n\nThe key difference: static embeddings are a **lookup table** (one vector per word), while contextual embeddings are a **function** of the entire input sequence.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "source": "### Visualization: Static vs Contextual Embeddings",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cell-26",
   "source": "# Simulate static vs contextual embeddings for the word \"bank\"\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\nnp.random.seed(42)\n\n# === Left: Static Embeddings ===\nax = axes[0]\n\n# All instances of \"bank\" map to the same point\nstatic_words = {\n    'river': np.array([2.0, 3.5]),\n    'water': np.array([2.5, 3.0]),\n    'shore': np.array([1.5, 3.2]),\n    'money': np.array([-2.0, -2.5]),\n    'account': np.array([-2.5, -2.0]),\n    'deposit': np.array([-1.5, -2.8]),\n    'bank': np.array([0.0, 0.5]),  # Single point -- ambiguous!\n}\n\nfor word, pos in static_words.items():\n    color = 'green' if word == 'bank' else ('steelblue' if pos[1] > 1 else 'lightcoral')\n    size = 200 if word == 'bank' else 100\n    marker = '*' if word == 'bank' else 'o'\n    ax.scatter(pos[0], pos[1], c=color, s=size, zorder=5, edgecolors='black', \n              linewidth=1.5, marker=marker)\n    ax.annotate(word, pos, fontsize=11, fontweight='bold',\n               xytext=(8, 8), textcoords='offset points')\n\nax.set_title('Static Embeddings (Word2Vec/GloVe)\\n\"bank\" has ONE vector', \n             fontsize=13, fontweight='bold')\nax.set_xlabel('Dimension 1', fontsize=11)\nax.set_ylabel('Dimension 2', fontsize=11)\nax.grid(True, alpha=0.3)\n\n# Add region labels\nax.text(2.0, 4.2, 'Nature cluster', fontsize=10, ha='center', color='steelblue', \n        fontweight='bold', style='italic')\nax.text(-2.0, -3.5, 'Finance cluster', fontsize=10, ha='center', color='lightcoral', \n        fontweight='bold', style='italic')\nax.annotate('Stuck in\\nthe middle!', xy=(0, 0.5), xytext=(1.5, -1.5),\n           fontsize=10, color='green', fontweight='bold',\n           arrowprops=dict(arrowstyle='->', color='green', lw=2))\n\n# === Right: Contextual Embeddings ===\nax = axes[1]\n\ncontextual_words = {\n    'river': np.array([2.0, 3.5]),\n    'water': np.array([2.5, 3.0]),\n    'shore': np.array([1.5, 3.2]),\n    'money': np.array([-2.0, -2.5]),\n    'account': np.array([-2.5, -2.0]),\n    'deposit': np.array([-1.5, -2.8]),\n    'bank\\n(river context)': np.array([1.8, 2.8]),     # Near nature words!\n    'bank\\n(finance context)': np.array([-1.8, -2.2]),  # Near finance words!\n}\n\nfor word, pos in contextual_words.items():\n    if 'bank' in word:\n        color = 'green'\n        size = 200\n        marker = '*'\n    elif pos[1] > 1:\n        color = 'steelblue'\n        size = 100\n        marker = 'o'\n    else:\n        color = 'lightcoral'\n        size = 100\n        marker = 'o'\n    \n    ax.scatter(pos[0], pos[1], c=color, s=size, zorder=5, edgecolors='black', \n              linewidth=1.5, marker=marker)\n    fontsize = 10 if 'bank' in word else 11\n    ax.annotate(word, pos, fontsize=fontsize, fontweight='bold',\n               xytext=(8, 8), textcoords='offset points')\n\n# Draw arrow showing same word, different positions\nbank1 = contextual_words['bank\\n(river context)']\nbank2 = contextual_words['bank\\n(finance context)']\nax.annotate('', xy=bank2, xytext=bank1,\n           arrowprops=dict(arrowstyle='<->', color='green', lw=2, ls='--'))\nax.text((bank1[0]+bank2[0])/2 + 1.2, (bank1[1]+bank2[1])/2, \n        'Same word,\\ndifferent vectors!', fontsize=10, color='green', \n        fontweight='bold', ha='center')\n\nax.set_title('Contextual Embeddings (BERT/GPT)\\n\"bank\" has DIFFERENT vectors per context', \n             fontsize=13, fontweight='bold')\nax.set_xlabel('Dimension 1', fontsize=11)\nax.set_ylabel('Dimension 2', fontsize=11)\nax.grid(True, alpha=0.3)\n\nax.text(2.0, 4.2, 'Nature cluster', fontsize=10, ha='center', color='steelblue', \n        fontweight='bold', style='italic')\nax.text(-2.0, -3.5, 'Finance cluster', fontsize=10, ha='center', color='lightcoral', \n        fontweight='bold', style='italic')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "source": "### Sentence Embeddings: From Words to Sentences\n\nHow do you get a single embedding for an entire sentence? There are several approaches:\n\n| Method | How It Works | Pros | Cons | F1 Parallel |\n|---|---|---|---|---|\n| **Mean pooling** | Average all word/token embeddings | Simple, works well | Treats all tokens equally | Average all lap sector times -- loses nuance |\n| **[CLS] token** | Use BERT's special classification token | Built into BERT | Not optimized for similarity | Race summary statistic |\n| **Max pooling** | Take element-wise max across tokens | Captures strongest signals | Loses ordering information | Best sector times (peak performance) |\n| **Specialized models** | Models trained specifically for sentence similarity (e.g., Sentence-BERT) | Best quality | Requires fine-tuned model | Purpose-built race comparison system |\n\nIn practice, **specialized sentence embedding models** (like Sentence-BERT or OpenAI's embedding models) give the best results because they are explicitly trained so that similar sentences have similar embeddings.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cell-28",
   "source": "# Demonstrate different pooling strategies using our Word2Vec embeddings\ndef get_sentence_embedding(sentence, embeddings_matrix, word2idx, method='mean'):\n    \"\"\"\n    Create a sentence embedding from word embeddings.\n    \n    Args:\n        sentence: String sentence\n        embeddings_matrix: Word embedding matrix\n        word2idx: Word to index mapping\n        method: 'mean', 'max', or 'sum'\n    \n    Returns:\n        Sentence embedding vector\n    \"\"\"\n    words = sentence.lower().split()\n    word_vecs = []\n    for word in words:\n        if word in word2idx:\n            word_vecs.append(embeddings_matrix[word2idx[word]])\n    \n    if not word_vecs:\n        return np.zeros(embeddings_matrix.shape[1])\n    \n    word_vecs = np.array(word_vecs)\n    \n    if method == 'mean':\n        return word_vecs.mean(axis=0)\n    elif method == 'max':\n        return word_vecs.max(axis=0)\n    elif method == 'sum':\n        return word_vecs.sum(axis=0)\n\n# Test with example sentences\ntest_sentences = [\n    \"the king rules the kingdom\",\n    \"the queen rules the kingdom\",\n    \"the boy plays in the village\",\n    \"the girl plays in the village\",\n    \"the prince will become king\",\n]\n\nprint(\"=== Sentence Similarity (Mean Pooling) ===\\n\")\nsent_embeddings = [get_sentence_embedding(s, embeddings_matrix, word2idx, 'mean') \n                   for s in test_sentences]\n\nfor i in range(len(test_sentences)):\n    for j in range(i+1, len(test_sentences)):\n        sim = cosine_similarity(sent_embeddings[i], sent_embeddings[j])\n        print(f\"  {sim:.4f}  |  '{test_sentences[i]}'\")\n        print(f\"         |  '{test_sentences[j]}'\")\n        print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "source": "### Deep Dive: The Evolution of Embeddings\n\n| Era | Method | Type | Key Innovation | F1 Parallel |\n|---|---|---|---|---|\n| 2003 | Neural LM (Bengio) | Static | First neural word embeddings | First data-driven driver ratings |\n| 2013 | Word2Vec | Static | Efficient training at scale | Scalable driver/circuit profiling |\n| 2014 | GloVe | Static | Global co-occurrence + local context | Full-season statistical analysis |\n| 2017 | ELMo | Contextual | Bi-directional LSTM embeddings | Context-aware driver analysis |\n| 2018 | BERT | Contextual | Transformer-based, bidirectional | Full race context representations |\n| 2018-now | GPT family | Contextual | Autoregressive Transformer | Sequential event understanding |\n| 2019+ | Sentence-BERT | Sentence-level | Contrastive learning for sentences | Race situation similarity matching |\n| 2022+ | text-embedding-ada-002 | Sentence-level | Production-grade API embeddings | Production race analytics systems |\n\n#### Key Insight\n\nThe trend is clear: embeddings have evolved from one-vector-per-word to context-dependent representations computed by increasingly powerful models. Modern embedding models are trained specifically to produce vectors where semantic similarity corresponds to geometric proximity.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "source": "---\n\n## 5. Similarity and Distance\n\n### Intuitive Explanation\n\nNow that we have embeddings, how do we measure \"closeness\" between vectors? This connects directly back to linear algebra (Part 1.1). There are three main distance/similarity measures, each with different properties.\n\n**Cosine similarity:** How much do two vectors point in the same direction? Ignores magnitude, only cares about angle. This is the most common choice for embeddings.\n\n$$\\text{cosine}(\\vec{a}, \\vec{b}) = \\frac{\\vec{a} \\cdot \\vec{b}}{\\|\\vec{a}\\| \\|\\vec{b}\\|}$$\n\n**Euclidean distance:** Straight-line distance between two points. Sensitive to magnitude.\n\n$$d(\\vec{a}, \\vec{b}) = \\|\\vec{a} - \\vec{b}\\| = \\sqrt{\\sum_i (a_i - b_i)^2}$$\n\n**Dot product similarity:** Raw alignment. Scales with magnitude, so longer vectors have higher scores.\n\n$$\\text{dot}(\\vec{a}, \\vec{b}) = \\vec{a} \\cdot \\vec{b} = \\sum_i a_i b_i$$\n\n**F1 analogy:** Comparing driver embeddings:\n- **Cosine similarity:** \"Do Verstappen and Hamilton have the *same style profile*, regardless of overall magnitude?\" (ignores whether one is 'more extreme' in all dimensions)\n- **Euclidean distance:** \"How far apart are Verstappen and Hamilton in the full driver space?\" (sensitive to absolute differences)\n- **Dot product:** \"How aligned are these two drivers, amplified by how extreme both are?\" (a dominant driver dotted with another dominant driver scores very high)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "source": "### Visualization: Comparing Similarity Measures",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cell-32",
   "source": "# Demonstrate the difference between similarity measures\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Create example vectors\nnp.random.seed(42)\na = np.array([3.0, 1.0])\nb = np.array([1.0, 3.0])  # Different direction, same magnitude\nc = np.array([6.0, 2.0])  # Same direction as a, different magnitude\nd = np.array([-2.0, -1.0])  # Opposite direction to a\n\nvectors = {'a': a, 'b': b, 'c (2*a)': c, 'd (-a)': d}\ncolors = {'a': 'blue', 'b': 'red', 'c (2*a)': 'green', 'd (-a)': 'orange'}\n\n# === Left: Cosine Similarity ===\nax = axes[0]\nfor name, vec in vectors.items():\n    ax.annotate('', xy=vec, xytext=(0, 0),\n               arrowprops=dict(arrowstyle='->', color=colors[name], lw=2.5))\n    ax.text(vec[0]*1.1, vec[1]*1.1, name, fontsize=11, fontweight='bold', color=colors[name])\n\nax.set_xlim(-7, 7)\nax.set_ylim(-3, 7)\nax.set_aspect('equal')\nax.axhline(y=0, color='k', linewidth=0.5)\nax.axvline(x=0, color='k', linewidth=0.5)\nax.set_title('Cosine Similarity\\n(angle only, ignores length)', fontsize=12, fontweight='bold')\nax.grid(True, alpha=0.3)\n\n# Show cosine similarities\nsims = []\nfor name, vec in vectors.items():\n    if name != 'a':\n        sim = np.dot(a, vec) / (np.linalg.norm(a) * np.linalg.norm(vec))\n        sims.append(f\"cos(a,{name})={sim:.2f}\")\nax.text(0.02, 0.98, '\\n'.join(sims), transform=ax.transAxes, fontsize=9,\n        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n\n# === Middle: Euclidean Distance ===\nax = axes[1]\nfor name, vec in vectors.items():\n    ax.scatter(vec[0], vec[1], c=colors[name], s=100, zorder=5, edgecolors='black', linewidth=1.5)\n    ax.text(vec[0]+0.3, vec[1]+0.3, name, fontsize=11, fontweight='bold', color=colors[name])\n\n# Draw distance lines from a to others\nfor name, vec in vectors.items():\n    if name != 'a':\n        ax.plot([a[0], vec[0]], [a[1], vec[1]], '--', color=colors[name], alpha=0.5, linewidth=1.5)\n\nax.set_xlim(-3, 7)\nax.set_ylim(-2, 4)\nax.set_aspect('equal')\nax.axhline(y=0, color='k', linewidth=0.5)\nax.axvline(x=0, color='k', linewidth=0.5)\nax.set_title('Euclidean Distance\\n(straight-line, sensitive to length)', fontsize=12, fontweight='bold')\nax.grid(True, alpha=0.3)\n\ndists = []\nfor name, vec in vectors.items():\n    if name != 'a':\n        dist = np.linalg.norm(a - vec)\n        dists.append(f\"d(a,{name})={dist:.2f}\")\nax.text(0.02, 0.98, '\\n'.join(dists), transform=ax.transAxes, fontsize=9,\n        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n\n# === Right: Dot Product ===\nax = axes[2]\nfor name, vec in vectors.items():\n    ax.annotate('', xy=vec, xytext=(0, 0),\n               arrowprops=dict(arrowstyle='->', color=colors[name], lw=2.5))\n    ax.text(vec[0]*1.1, vec[1]*1.1, name, fontsize=11, fontweight='bold', color=colors[name])\n\nax.set_xlim(-7, 7)\nax.set_ylim(-3, 7)\nax.set_aspect('equal')\nax.axhline(y=0, color='k', linewidth=0.5)\nax.axvline(x=0, color='k', linewidth=0.5)\nax.set_title('Dot Product\\n(direction + magnitude)', fontsize=12, fontweight='bold')\nax.grid(True, alpha=0.3)\n\ndots = []\nfor name, vec in vectors.items():\n    if name != 'a':\n        dot = np.dot(a, vec)\n        dots.append(f\"a . {name}={dot:.2f}\")\nax.text(0.02, 0.98, '\\n'.join(dots), transform=ax.transAxes, fontsize=9,\n        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey observations:\")\nprint(\"  - Cosine: a and c(2*a) have sim=1.00 (same direction, ignores length)\")\nprint(\"  - Euclidean: a and c(2*a) are far apart (different magnitudes)\")\nprint(\"  - Dot product: a . c(2*a) is large (rewards both alignment AND magnitude)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "source": "### When to Use Which?\n\n| Metric | Best For | Range | Properties | F1 Parallel |\n|---|---|---|---|---|\n| **Cosine similarity** | Comparing embeddings regardless of magnitude | [-1, 1] | Invariant to vector length; 1 = identical direction | \"Are these drivers the same *type*?\" |\n| **Euclidean distance** | Clustering, when magnitude matters | [0, inf) | Sensitive to scale; 0 = identical | \"How different are these drivers overall?\" |\n| **Dot product** | Attention mechanisms, when magnitude encodes importance | (-inf, inf) | Fast to compute; used in Transformers | \"How relevant is this telemetry to this strategy?\" |\n\n**Rule of thumb:** Use **cosine similarity** for semantic similarity tasks (search, recommendation). Use **dot product** when magnitude matters (attention scores, learned relevance). Use **Euclidean distance** for clustering and when you want a proper distance metric.\n\n### Interactive Exploration: Similarity in Embedding Space",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cell-34",
   "source": "# Interactive: nearest neighbors search with different metrics\ndef nearest_neighbors(query_word, embeddings_matrix, word2idx, idx2word, k=5):\n    \"\"\"\n    Find k nearest neighbors using all three metrics.\n    \"\"\"\n    if query_word not in word2idx:\n        print(f\"'{query_word}' not in vocabulary\")\n        return\n    \n    query_vec = embeddings_matrix[word2idx[query_word]]\n    \n    results = {'cosine': [], 'euclidean': [], 'dot_product': []}\n    \n    for i in range(len(idx2word)):\n        word = idx2word[i]\n        if word == query_word:\n            continue\n        vec = embeddings_matrix[i]\n        \n        cos_sim = np.dot(query_vec, vec) / (np.linalg.norm(query_vec) * np.linalg.norm(vec))\n        euc_dist = np.linalg.norm(query_vec - vec)\n        dot_prod = np.dot(query_vec, vec)\n        \n        results['cosine'].append((word, cos_sim))\n        results['euclidean'].append((word, euc_dist))\n        results['dot_product'].append((word, dot_prod))\n    \n    results['cosine'].sort(key=lambda x: x[1], reverse=True)\n    results['euclidean'].sort(key=lambda x: x[1])  # Lower = closer\n    results['dot_product'].sort(key=lambda x: x[1], reverse=True)\n    \n    return {k: v[:5] for k, v in results.items()}\n\n# Try different query words\nfor query in ['king', 'woman', 'village']:\n    print(f\"\\n{'='*60}\")\n    print(f\"Nearest neighbors of '{query}'\")\n    print(f\"{'='*60}\")\n    \n    results = nearest_neighbors(query, embeddings_matrix, word2idx, idx2word)\n    \n    print(f\"\\n  {'Cosine Similarity':<25s} {'Euclidean Distance':<25s} {'Dot Product':<25s}\")\n    print(f\"  {'-'*23:<25s} {'-'*23:<25s} {'-'*23:<25s}\")\n    \n    for i in range(5):\n        cos_word, cos_val = results['cosine'][i]\n        euc_word, euc_val = results['euclidean'][i]\n        dot_word, dot_val = results['dot_product'][i]\n        print(f\"  {cos_word:12s} {cos_val:+.4f}    {euc_word:12s} {euc_val:.4f}    {dot_word:12s} {dot_val:+.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "source": "---\n\n## 6. Vector Databases and Retrieval\n\n### Intuitive Explanation\n\nOnce you have embeddings, a natural question is: given a query, how do I quickly find the most similar items? With a small collection, you can compare the query to every item (brute force). But with millions or billions of items, this becomes impossibly slow.\n\n**Vector databases** solve this problem. They store embeddings and provide fast **approximate nearest neighbor (ANN)** search. The key insight is that you do not need the exact nearest neighbor -- an approximate answer that is 95% as good but 1000x faster is far more practical.\n\n**F1 analogy:** Imagine an F1 strategy database containing embeddings for every race situation in history -- thousands of races, millions of individual laps. When a new situation arises (e.g., \"rain starting, leader on 20-lap old mediums, safety car likely\"), you need to instantly find the most similar historical situations to guide your strategy. Brute-force comparison against every historical moment is too slow. A vector database indexes these embeddings so the query \"find me similar situations\" returns in milliseconds.\n\n### Why Not Just Use a Regular Database?\n\n| Traditional Database | Vector Database | F1 Parallel |\n|---|---|---|\n| Exact match: \"WHERE name = 'cat'\" | Similarity search: \"find things like 'cat'\" | \"Find all races at Monza\" vs. \"Find races *like* this one\" |\n| Structured queries (SQL) | Semantic queries (natural language) | Filter by lap number vs. search by race situation |\n| Indexes on exact values | Indexes on vector similarity | Index by circuit name vs. index by race characteristics |\n| Returns exact matches | Returns ranked by similarity | Exact match vs. closest historical parallels |\n| Fast for equality/range queries | Fast for nearest-neighbor queries | Fast for filters vs. fast for similarity |\n\n### Approximate Nearest Neighbors (ANN)\n\nThe main ANN algorithms trade a small amount of accuracy for huge speed gains:\n\n| Algorithm | How It Works | Used By |\n|---|---|---|\n| **IVF (Inverted File Index)** | Cluster vectors, search only nearby clusters | FAISS |\n| **HNSW (Hierarchical NSW)** | Build a graph of neighbors at multiple scales | Most modern systems |\n| **LSH (Locality-Sensitive Hashing)** | Hash similar vectors to same bucket | Early systems |\n| **Product Quantization** | Compress vectors by splitting into subspaces | FAISS (with IVF) |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "source": "### Building a Simple Semantic Search Engine from Scratch\n\nLet's build a complete semantic search pipeline. We will:\n1. Create a document collection\n2. Embed each document using our Word2Vec model\n3. Accept a query, embed it, and find the most similar documents\n\n**F1 framing:** Think of this as building a race situation search engine. You embed every historical race situation in your database, and when a new situation arises during a live race, you embed it and find the closest historical matches -- \"In 2019 at Hockenheim, a similar rain situation led to...\" This is exactly how modern strategy tools augment human decision-making.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cell-37",
   "source": "class SimpleVectorStore:\n    \"\"\"\n    A minimal vector database for semantic search.\n    Uses brute-force cosine similarity (no ANN indexing).\n    \"\"\"\n    def __init__(self, embedding_dim):\n        self.embedding_dim = embedding_dim\n        self.vectors = []       # List of embedding vectors\n        self.documents = []     # List of original documents\n        self.metadata = []      # Optional metadata\n    \n    def add(self, document, vector, metadata=None):\n        \"\"\"Add a document and its embedding to the store.\"\"\"\n        self.vectors.append(vector / np.linalg.norm(vector))  # Normalize for cosine sim\n        self.documents.append(document)\n        self.metadata.append(metadata or {})\n    \n    def search(self, query_vector, top_k=5):\n        \"\"\"\n        Find the top_k most similar documents to the query.\n        \n        Args:\n            query_vector: Query embedding\n            top_k: Number of results to return\n        \n        Returns:\n            List of (document, similarity, metadata) tuples\n        \"\"\"\n        query_norm = query_vector / np.linalg.norm(query_vector)\n        \n        # Compute cosine similarity with all documents\n        similarities = [np.dot(query_norm, vec) for vec in self.vectors]\n        \n        # Get top-k indices\n        top_indices = np.argsort(similarities)[::-1][:top_k]\n        \n        results = []\n        for idx in top_indices:\n            results.append((\n                self.documents[idx],\n                similarities[idx],\n                self.metadata[idx]\n            ))\n        \n        return results\n    \n    def __len__(self):\n        return len(self.documents)\n\n# Create document collection\ndocuments = [\n    \"the king rules the kingdom with wisdom and power\",\n    \"the queen leads the kingdom with grace and intelligence\",\n    \"the prince trains to become a future king\",\n    \"the princess studies diplomacy and leadership\",\n    \"the man works hard in the village every day\",\n    \"the woman tends the garden with great care\",\n    \"the boy plays with friends in the village square\",\n    \"the girl reads books under the old oak tree\",\n    \"the knight protects the kingdom from invaders\",\n    \"the scholar teaches young students in the academy\",\n    \"a brave warrior defends the castle walls\",\n    \"the royal throne sits in the great hall\",\n    \"golden crown jewels are kept in the vault\",\n    \"village life is simple and peaceful\",\n    \"children play games in the meadow\",\n]\n\n# Build the vector store\nstore = SimpleVectorStore(EMBEDDING_DIM)\nfor doc in documents:\n    vec = get_sentence_embedding(doc, embeddings_matrix, word2idx, method='mean')\n    store.add(doc, vec, metadata={'length': len(doc.split())})\n\nprint(f\"Vector store contains {len(store)} documents\")\nprint(f\"Embedding dimension: {store.embedding_dim}\")\n\n# Search!\nqueries = [\n    \"king and queen rule together\",\n    \"boy plays in village\",\n    \"royal crown and throne\",\n]\n\nfor query in queries:\n    print(f\"\\n{'='*60}\")\n    print(f\"Query: '{query}'\")\n    print(f\"{'='*60}\")\n    \n    query_vec = get_sentence_embedding(query, embeddings_matrix, word2idx, method='mean')\n    results = store.search(query_vec, top_k=3)\n    \n    for rank, (doc, sim, meta) in enumerate(results, 1):\n        print(f\"  #{rank} (sim={sim:.4f}): {doc}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "source": "### Visualization: Embedding Space with Query and Retrieved Results",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cell-39",
   "source": "# Visualize retrieval in embedding space\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Get 2D projections of all document embeddings\ndoc_embeddings = np.array([get_sentence_embedding(doc, embeddings_matrix, word2idx, 'mean') \n                           for doc in documents])\ndoc_2d = pca_2d(doc_embeddings)\n\n# Plot all documents\nax.scatter(doc_2d[:, 0], doc_2d[:, 1], c='lightgray', s=60, alpha=0.6, edgecolors='gray')\n\n# Add short labels for each document\nfor i, doc in enumerate(documents):\n    short = ' '.join(doc.split()[:4]) + '...'\n    ax.annotate(short, (doc_2d[i, 0], doc_2d[i, 1]), fontsize=7, alpha=0.5,\n               xytext=(5, 5), textcoords='offset points')\n\n# Process a query and highlight results\nquery = \"king and queen rule together\"\nquery_vec = get_sentence_embedding(query, embeddings_matrix, word2idx, method='mean')\nresults = store.search(query_vec, top_k=3)\n\n# Project query into same 2D space\nall_vecs = np.vstack([doc_embeddings, query_vec.reshape(1, -1)])\nall_2d = pca_2d(all_vecs)\nquery_2d = all_2d[-1]\ndoc_2d_new = all_2d[:-1]\n\n# Re-plot with query\nax.clear()\nax.scatter(doc_2d_new[:, 0], doc_2d_new[:, 1], c='lightgray', s=60, alpha=0.6, edgecolors='gray')\n\nfor i, doc in enumerate(documents):\n    short = ' '.join(doc.split()[:4]) + '...'\n    ax.annotate(short, (doc_2d_new[i, 0], doc_2d_new[i, 1]), fontsize=7, alpha=0.5,\n               xytext=(5, 5), textcoords='offset points')\n\n# Highlight retrieved documents\nretrieved_docs = [r[0] for r in results]\nretrieved_sims = [r[1] for r in results]\ncolors_retrieved = ['green', 'limegreen', 'yellowgreen']\n\nfor rank, (doc, sim, _) in enumerate(results):\n    doc_idx = documents.index(doc)\n    ax.scatter(doc_2d_new[doc_idx, 0], doc_2d_new[doc_idx, 1], \n              c=colors_retrieved[rank], s=200, zorder=5, edgecolors='black', linewidth=2,\n              label=f'#{rank+1}: sim={sim:.3f}')\n    \n    # Draw line from query to result\n    ax.plot([query_2d[0], doc_2d_new[doc_idx, 0]], \n            [query_2d[1], doc_2d_new[doc_idx, 1]], \n            '--', color=colors_retrieved[rank], alpha=0.5, linewidth=1.5)\n\n# Plot query\nax.scatter(query_2d[0], query_2d[1], c='red', s=300, zorder=6, \n          edgecolors='black', linewidth=2, marker='*', label=f'Query: \"{query}\"')\n\nax.set_title('Semantic Search: Query and Retrieved Documents in Embedding Space', \n             fontsize=13, fontweight='bold')\nax.set_xlabel('PC 1', fontsize=11)\nax.set_ylabel('PC 2', fontsize=11)\nax.legend(fontsize=9, loc='best')\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "source": "### The Vector Database Ecosystem\n\n| Tool | Type | Key Feature | Best For |\n|---|---|---|---|\n| **FAISS** (Meta) | Library | Blazing fast, GPU support | Research, large-scale |\n| **Pinecone** | Managed service | Fully hosted, easy API | Production, no ops |\n| **Chroma** | Open source | Lightweight, embedded | Prototyping, small projects |\n| **Weaviate** | Open source | Hybrid search (vector + keyword) | Complex search needs |\n| **Qdrant** | Open source | Filtering + vector search | Production, self-hosted |\n| **Milvus** | Open source | Distributed, scalable | Very large scale |\n\n### RAG: Retrieval-Augmented Generation\n\nRAG is one of the most important patterns in modern AI. The idea is simple:\n\n1. **Retrieve:** Use embedding similarity to find relevant documents from a knowledge base\n2. **Augment:** Add those documents to the prompt as context\n3. **Generate:** Have an LLM generate an answer using the retrieved context\n\n**Why RAG matters:**\n- LLMs have a knowledge cutoff -- RAG gives them access to current information\n- LLMs can hallucinate -- RAG grounds answers in real documents\n- You can update the knowledge base without retraining the model\n- The user can verify answers by checking the source documents\n\n**F1 analogy:** RAG for F1 strategy would work like this: (1) **Retrieve** the most similar historical race situations from the embedded database; (2) **Augment** the strategy model's input with those historical cases -- \"In 5 similar situations, early pit stops won 4 times\"; (3) **Generate** a strategy recommendation grounded in real precedent. This is essentially what experienced strategists do intuitively -- they recall similar historical situations to inform current decisions. RAG automates and scales that institutional memory.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cell-41",
   "source": "# Simulate a RAG pipeline\ndef rag_pipeline(question, vector_store, embeddings_matrix, word2idx, top_k=3):\n    \"\"\"\n    Simulate a RAG pipeline:\n    1. Embed the question\n    2. Retrieve relevant documents\n    3. Show what would be sent to an LLM\n    \n    Args:\n        question: User's question\n        vector_store: Our SimpleVectorStore\n        embeddings_matrix: Word embeddings\n        word2idx: Word to index mapping\n        top_k: Number of documents to retrieve\n    \n    Returns:\n        The constructed prompt (in a real system, this goes to an LLM)\n    \"\"\"\n    # Step 1: Embed the question\n    query_vec = get_sentence_embedding(question, embeddings_matrix, word2idx, method='mean')\n    \n    # Step 2: Retrieve relevant documents\n    results = vector_store.search(query_vec, top_k=top_k)\n    \n    # Step 3: Construct prompt\n    context = \"\\n\".join([f\"- {doc}\" for doc, sim, _ in results])\n    \n    prompt = f\"\"\"Answer the question based on the following context.\n\nContext:\n{context}\n\nQuestion: {question}\nAnswer:\"\"\"\n    \n    return prompt, results\n\n# Demo the RAG pipeline\nquestion = \"who rules the kingdom\"\nprompt, results = rag_pipeline(question, store, embeddings_matrix, word2idx)\n\nprint(\"=== RAG Pipeline Demo ===\\n\")\nprint(\"Step 1: User asks a question\")\nprint(f\"  Question: '{question}'\\n\")\nprint(\"Step 2: Retrieve relevant documents\")\nfor rank, (doc, sim, _) in enumerate(results, 1):\n    print(f\"  #{rank} (sim={sim:.4f}): {doc}\")\nprint(f\"\\nStep 3: Construct prompt for LLM\")\nprint(\"-\" * 50)\nprint(prompt)\nprint(\"-\" * 50)\nprint(\"\\nStep 4: Send to LLM (not implemented here -- would call GPT/Claude API)\")\nprint(\"The LLM would answer based on the retrieved context!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "source": "---\n\n## 7. Practical Applications\n\n### Intuitive Explanation\n\nEmbeddings are not just an academic curiosity -- they power some of the most impactful applications in technology. The fundamental pattern is always the same: represent items as vectors, then use similarity to find related items.\n\n### Why This Matters in Machine Learning\n\n| Application | How Embeddings Are Used | Example | F1 Parallel |\n|---|---|---|---|\n| **Semantic search** | Query and documents are embedded; find documents closest to query | Google Search, Bing | \"Find races similar to Hungary 2021\" |\n| **Recommendation systems** | Users and items are embedded; recommend items close to user | Netflix, Spotify, Amazon | \"Fans who liked Silverstone also liked Spa\" |\n| **Clustering / topic modeling** | Embed documents, cluster similar ones | News categorization | Cluster race weekends by characteristics |\n| **Anomaly detection** | Items far from all clusters are anomalies | Fraud detection | Unusual lap times or telemetry patterns |\n| **Duplicate detection** | Similar embeddings = potential duplicates | Customer deduplication | Detect duplicate timing entries |\n| **Classification** | Use embeddings as features for classifiers | Sentiment analysis | Classify race incidents (racing incident vs. penalty) |\n| **Cross-lingual tasks** | Align embeddings across languages | Translation, multilingual search | Align driver profiles across different data sources |\n| **RAG** | Retrieve relevant documents to augment LLM prompts | ChatGPT with plugins | Strategy recommendations grounded in historical data |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cell-43",
   "source": "# Demo: Simple recommendation system using embeddings\n# Simulate a \"user liked these items, recommend similar ones\"\n\n# Items with hand-crafted embeddings simulating a movie recommendation scenario\n# Dimensions roughly represent: [action, romance, comedy, scifi, drama]\nmovie_embeddings = {\n    'The Matrix':       np.array([0.8, 0.1, 0.0, 0.9, 0.3]),\n    'Inception':        np.array([0.7, 0.2, 0.1, 0.8, 0.5]),\n    'Interstellar':     np.array([0.3, 0.3, 0.0, 0.9, 0.7]),\n    'Titanic':          np.array([0.2, 0.9, 0.0, 0.0, 0.8]),\n    'The Notebook':     np.array([0.0, 0.9, 0.1, 0.0, 0.7]),\n    'Pride & Prejudice':np.array([0.0, 0.8, 0.2, 0.0, 0.6]),\n    'Superbad':         np.array([0.1, 0.2, 0.9, 0.0, 0.3]),\n    'The Hangover':     np.array([0.2, 0.1, 0.9, 0.0, 0.2]),\n    'Step Brothers':    np.array([0.1, 0.0, 0.9, 0.0, 0.1]),\n    'John Wick':        np.array([0.9, 0.0, 0.1, 0.2, 0.3]),\n    'Mad Max':          np.array([0.9, 0.1, 0.1, 0.5, 0.3]),\n    'Blade Runner':     np.array([0.5, 0.2, 0.0, 0.9, 0.6]),\n}\n\ndef recommend(liked_movies, movie_embeddings, top_k=3):\n    \"\"\"\n    Recommend movies based on user's liked movies.\n    \n    Args:\n        liked_movies: List of movie titles the user liked\n        movie_embeddings: Dict of movie -> embedding\n        top_k: Number of recommendations\n    \n    Returns:\n        List of (movie, similarity) tuples\n    \"\"\"\n    # Create user profile: average of liked movie embeddings\n    user_vec = np.mean([movie_embeddings[m] for m in liked_movies], axis=0)\n    \n    # Find most similar movies (excluding already liked)\n    scores = []\n    for movie, emb in movie_embeddings.items():\n        if movie not in liked_movies:\n            sim = np.dot(user_vec, emb) / (np.linalg.norm(user_vec) * np.linalg.norm(emb))\n            scores.append((movie, sim))\n    \n    scores.sort(key=lambda x: x[1], reverse=True)\n    return scores[:top_k]\n\n# Test recommendations\nprint(\"=== Movie Recommendation Demo ===\\n\")\n\nuser_profiles = {\n    'Sci-fi fan': ['The Matrix', 'Inception'],\n    'Romance fan': ['Titanic', 'The Notebook'],\n    'Comedy fan': ['Superbad', 'The Hangover'],\n}\n\nfor profile_name, liked in user_profiles.items():\n    recs = recommend(liked, movie_embeddings, top_k=3)\n    print(f\"{profile_name} (liked: {', '.join(liked)})\")\n    for movie, sim in recs:\n        print(f\"  -> {movie:20s} (similarity: {sim:.4f})\")\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "source": "### Visualization: Clustering in Embedding Space",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cell-45",
   "source": "# Visualize movie embeddings in 2D with genre clusters\nmovie_names = list(movie_embeddings.keys())\nmovie_vecs = np.array(list(movie_embeddings.values()))\n\n# PCA to 2D\nmovie_2d = pca_2d(movie_vecs)\n\n# Assign genres for coloring\ngenres = {\n    'Sci-fi/Action': ['The Matrix', 'Inception', 'Interstellar', 'Blade Runner', 'Mad Max', 'John Wick'],\n    'Romance/Drama': ['Titanic', 'The Notebook', 'Pride & Prejudice'],\n    'Comedy': ['Superbad', 'The Hangover', 'Step Brothers'],\n}\n\ngenre_colors = {'Sci-fi/Action': 'steelblue', 'Romance/Drama': 'lightcoral', 'Comedy': 'green'}\nmovie_genre = {}\nfor genre, movies in genres.items():\n    for movie in movies:\n        movie_genre[movie] = genre\n\nfig, ax = plt.subplots(figsize=(10, 8))\n\nfor i, movie in enumerate(movie_names):\n    genre = movie_genre[movie]\n    ax.scatter(movie_2d[i, 0], movie_2d[i, 1], c=genre_colors[genre], s=150, \n              edgecolors='black', linewidth=1.5, zorder=5,\n              label=genre if movie == genres[genre][0] else \"\")\n    ax.annotate(movie, (movie_2d[i, 0], movie_2d[i, 1]), fontsize=9, fontweight='bold',\n               xytext=(8, 8), textcoords='offset points')\n\nax.set_title('Movie Embeddings: Genre Clusters Emerge Naturally', fontsize=14, fontweight='bold')\nax.set_xlabel('PC 1', fontsize=11)\nax.set_ylabel('PC 2', fontsize=11)\nax.legend(fontsize=10, loc='best')\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"Embeddings naturally cluster by genre -- no explicit genre labels were used!\")\nprint(\"This is the power of learning representations from data.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-46",
   "source": "---\n\n## Exercises\n\n### Exercise 1: Implement Cosine Similarity from Scratch\n\nImplement cosine similarity using only NumPy, without using any library similarity function.\n\n**F1 framing:** You are building a driver similarity tool. Given two driver embeddings (vectors of performance stats across multiple dimensions), compute how similar their profiles are using cosine similarity. Two drivers pointing in the same direction in embedding space have similar styles, regardless of whether one is \"stronger\" overall.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cell-47",
   "source": "# EXERCISE 1: Implement cosine similarity\ndef cosine_similarity_manual(a, b):\n    \"\"\"\n    Compute cosine similarity between two vectors using only NumPy.\n    \n    Args:\n        a: First vector (1D numpy array)\n        b: Second vector (1D numpy array)\n    \n    Returns:\n        Cosine similarity (scalar between -1 and 1)\n    \"\"\"\n    # TODO: Implement this!\n    # Step 1: Compute dot product of a and b\n    # Step 2: Compute L2 norm of a\n    # Step 3: Compute L2 norm of b\n    # Step 4: Return dot_product / (norm_a * norm_b)\n    # Hint: Use np.dot(), np.sqrt(), np.sum()\n    \n    pass  # Replace with your implementation\n\n# Test\na = np.array([1.0, 2.0, 3.0])\nb = np.array([1.0, 2.0, 3.0])  # Identical\nc = np.array([-1.0, -2.0, -3.0])  # Opposite\nd = np.array([0.0, 0.0, 1.0])  # Partially aligned\n\nprint(\"Test results:\")\nresult1 = cosine_similarity_manual(a, b)\nprint(f\"  Identical vectors: {result1}\")\nprint(f\"  Expected: 1.0000\")\nprint(f\"  Correct: {np.isclose(result1, 1.0) if result1 is not None else 'Not implemented'}\")\n\nresult2 = cosine_similarity_manual(a, c)\nprint(f\"\\n  Opposite vectors: {result2}\")\nprint(f\"  Expected: -1.0000\")\nprint(f\"  Correct: {np.isclose(result2, -1.0) if result2 is not None else 'Not implemented'}\")\n\nresult3 = cosine_similarity_manual(a, d)\nexpected3 = 3.0 / (np.sqrt(14) * 1.0)\nprint(f\"\\n  Partial alignment: {result3}\")\nprint(f\"  Expected: {expected3:.4f}\")\nprint(f\"  Correct: {np.isclose(result3, expected3) if result3 is not None else 'Not implemented'}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-48",
   "source": "### Exercise 2: Implement CBOW Word2Vec\n\nModify the skip-gram model to implement CBOW (Continuous Bag of Words). Instead of predicting context from center, predict the center word from the average of context word embeddings.\n\n**F1 framing:** If skip-gram is \"given Verstappen, predict what events surround him (pole, victory, fastest lap),\" then CBOW is \"given a set of events (pole, victory, fastest lap), predict which driver is at the center.\" Both learn the same driver embeddings from different directions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cell-49",
   "source": "# EXERCISE 2: Implement CBOW\nclass CBOWNegSampling(nn.Module):\n    \"\"\"\n    CBOW Word2Vec with negative sampling.\n    \n    Given context words, predict the center word.\n    \"\"\"\n    def __init__(self, vocab_size, embedding_dim):\n        super().__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n        self.output_embeddings = nn.Embedding(vocab_size, embedding_dim)\n        \n        nn.init.uniform_(self.embeddings.weight, -0.5/embedding_dim, 0.5/embedding_dim)\n        nn.init.uniform_(self.output_embeddings.weight, -0.5/embedding_dim, 0.5/embedding_dim)\n    \n    def forward(self, context_words, center_word, negative_words):\n        \"\"\"\n        Args:\n            context_words: (batch_size, 2*window) context word indices\n            center_word: (batch_size,) center word index\n            negative_words: (batch_size, num_neg) negative sample indices\n        \n        Returns:\n            loss: negative sampling loss\n        \"\"\"\n        # TODO: Implement CBOW forward pass!\n        # Step 1: Look up context word embeddings and average them\n        #   context_emb = self.embeddings(context_words)  # (batch, 2*window, emb_dim)\n        #   context_avg = context_emb.mean(dim=1)          # (batch, emb_dim)\n        #\n        # Step 2: Look up center word output embedding\n        #   center_emb = self.output_embeddings(center_word)  # (batch, emb_dim)\n        #\n        # Step 3: Compute positive score (dot product of context_avg and center_emb)\n        #   pos_score = (context_avg * center_emb).sum(dim=1)\n        #   pos_loss = F.logsigmoid(pos_score)\n        #\n        # Step 4: Compute negative scores\n        #   neg_emb = self.output_embeddings(negative_words)\n        #   neg_score = torch.bmm(neg_emb, context_avg.unsqueeze(2)).squeeze(2)\n        #   neg_loss = F.logsigmoid(-neg_score).sum(dim=1)\n        #\n        # Step 5: Return -(pos_loss + neg_loss).mean()\n        \n        pass  # Replace with your implementation\n\n# Test structure (you would need to generate CBOW training data to fully test)\n# cbow_model = CBOWNegSampling(vocab_size, EMBEDDING_DIM)\n# print(f\"CBOW parameters: {sum(p.numel() for p in cbow_model.parameters()):,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-50",
   "source": "### Exercise 3: Build an Anomaly Detector Using Embeddings\n\nGiven a collection of \"normal\" items, detect anomalies by finding items that are far from all cluster centers.\n\n**F1 framing:** Build a telemetry anomaly detector. Embed each lap's telemetry signature and cluster normal laps together. Laps that are far from all clusters in embedding space are anomalies -- possibly indicating mechanical issues, unusual driving, or data errors. An engineer reviewing 50+ laps needs this kind of automated flagging.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cell-51",
   "source": "# EXERCISE 3: Anomaly detection with embeddings\ndef detect_anomalies(items, item_embeddings, threshold=0.5):\n    \"\"\"\n    Detect anomalies by finding items whose average similarity \n    to all other items is below the threshold.\n    \n    Args:\n        items: List of item names\n        item_embeddings: Dict of item_name -> embedding vector\n        threshold: Similarity threshold (items below this are anomalies)\n    \n    Returns:\n        List of (item_name, avg_similarity) for items below threshold,\n        sorted by similarity (most anomalous first)\n    \"\"\"\n    # TODO: Implement this!\n    # Step 1: For each item, compute its average cosine similarity to all other items\n    # Step 2: Flag items with average similarity below threshold\n    # Step 3: Return sorted list of anomalies\n    # Hint: Use the cosine_similarity function defined earlier\n    \n    pass  # Replace with your implementation\n\n# Test data: mostly animals, with some outliers\ntest_embeddings = {\n    'cat':        np.array([0.8, 0.2, 0.1, -0.3, 0.5]),\n    'dog':        np.array([0.7, 0.3, 0.2, -0.2, 0.4]),\n    'hamster':    np.array([0.6, 0.1, 0.0, -0.4, 0.6]),\n    'parrot':     np.array([0.5, 0.4, 0.3, -0.1, 0.3]),\n    'goldfish':   np.array([0.4, 0.2, 0.1, -0.3, 0.5]),\n    'airplane':   np.array([-0.7, -0.5, 0.8, 0.6, -0.2]),   # ANOMALY!\n    'submarine':  np.array([-0.8, -0.6, 0.7, 0.5, -0.3]),   # ANOMALY!\n    'rabbit':     np.array([0.7, 0.1, 0.0, -0.4, 0.5]),\n}\n\n# anomalies = detect_anomalies(list(test_embeddings.keys()), test_embeddings, threshold=0.5)\n# if anomalies:\n#     print(\"Detected anomalies:\")\n#     for item, sim in anomalies:\n#         print(f\"  {item:12s}: avg similarity = {sim:.4f}\")\n# else:\n#     print(\"No anomalies detected (check your implementation)\")\nprint(\"Uncomment the test code above after implementing detect_anomalies()\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-52",
   "source": "---\n\n## Summary\n\n### Key Concepts\n\n**Why Embeddings:**\n- One-hot encoding is sparse, high-dimensional, and encodes no similarity\n- Dense embeddings represent items as short vectors where similar items are close\n- The distributional hypothesis: meaning comes from usage patterns\n\n**Word2Vec:**\n- Skip-gram: predict context words from center word\n- CBOW: predict center word from context words\n- Negative sampling makes training efficient\n- Implicitly factorizes a word co-occurrence (PMI) matrix\n- Captures analogies via vector arithmetic: king - man + woman = queen\n\n**GloVe:**\n- Explicitly builds and factorizes a co-occurrence matrix\n- Global context vs Word2Vec's local context windows\n- Produces similarly good embeddings in practice\n\n**Modern Embeddings:**\n- Static (Word2Vec, GloVe): one vector per word, regardless of context\n- Contextual (BERT, GPT): different vector for each word occurrence based on context\n- Sentence embeddings: mean pooling, [CLS] token, or specialized models\n\n**Similarity Measures:**\n- Cosine similarity: direction only, range [-1, 1]\n- Euclidean distance: straight-line distance, sensitive to magnitude\n- Dot product: direction and magnitude, used in attention\n\n**Vector Databases and Retrieval:**\n- Store embeddings for fast nearest-neighbor search\n- ANN algorithms (IVF, HNSW) trade small accuracy loss for huge speed gains\n- RAG: Retrieve relevant documents to augment LLM responses\n\n### Connection to Deep Learning\n\n| Concept | Where It's Used | F1 Parallel |\n|---|---|---|\n| Word embeddings | First layer of every NLP model | Driver/circuit/event representation as vectors |\n| nn.Embedding | PyTorch lookup table, trained end-to-end | Converting driver IDs to performance profiles |\n| Cosine similarity | Contrastive learning, similarity search | \"How similar are these two drivers/situations?\" |\n| Dot product similarity | Attention mechanism (Q * K) | Relevance scoring in strategy systems |\n| Contextual embeddings | BERT, GPT, all modern language models | Context-dependent driver/event representations |\n| Sentence embeddings | Semantic search, RAG, classification | Full race-situation embeddings |\n| Vector databases | Production search and retrieval systems | Historical race situation lookup at speed |\n| Negative sampling | Contrastive learning, SimCLR, CLIP | Efficient training with positive/negative pairs |\n\n### Checklist\n\n- [ ] I can explain why one-hot encoding fails and why dense embeddings are needed\n- [ ] I understand the distributional hypothesis and how it motivates embeddings\n- [ ] I can implement skip-gram Word2Vec with negative sampling\n- [ ] I can perform vector arithmetic and solve analogies with embeddings\n- [ ] I can compare Word2Vec and GloVe approaches\n- [ ] I understand the difference between static and contextual embeddings\n- [ ] I can compute and compare cosine similarity, Euclidean distance, and dot product\n- [ ] I can build a simple semantic search engine\n- [ ] I can explain RAG and why it matters for modern AI\n- [ ] I can describe practical applications of embeddings across domains",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-53",
   "source": "---\n\n## Next Steps\n\nYou now understand **embeddings** -- the fundamental representation that powers modern AI. Every time you use a search engine, get a recommendation, or chat with an LLM, embeddings are working behind the scenes to represent meaning as geometry. In F1 terms, you have learned how to represent drivers, circuits, and race situations as vectors where similarity means proximity -- the foundation for any data-driven strategy system.\n\nThe key ideas to carry forward:\n\n1. **Representation matters more than algorithms.** A good embedding makes downstream tasks dramatically easier. This is why so much research focuses on learning better representations. In F1: a good driver/circuit representation is more valuable than a clever strategy algorithm operating on bad data.\n\n2. **Similarity = proximity in embedding space.** This one idea connects search, recommendation, clustering, anomaly detection, and more. If you can embed it, you can compare it. In F1: similar race situations cluster together, and finding the closest historical parallel guides strategy.\n\n3. **From static to contextual.** The evolution from Word2Vec to BERT/GPT shows how richer context produces better representations. Modern models compute embeddings as a function of the entire input. In F1: a driver's embedding should depend on the full context -- team, season, conditions -- not be a fixed profile.\n\n4. **RAG bridges embeddings and generation.** Retrieval-Augmented Generation is one of the most practical patterns in modern AI, combining the precision of search with the fluency of language models. In F1: grounding strategy recommendations in retrieved historical precedent.\n\n**Practical next steps:**\n- Try using pre-trained embeddings (e.g., `gensim` for Word2Vec/GloVe, `sentence-transformers` for sentence embeddings)\n- Build a RAG pipeline with a real embedding API and vector database\n- Experiment with embedding dimensions: how does quality change with 50 vs 300 vs 768 dimensions?\n- Explore multimodal embeddings (CLIP) that embed both images and text in the same space\n\n**In the next notebook,** we will explore fine-tuning and parameter-efficient methods (LoRA, adapters), where you learn to adapt pre-trained models -- including their embeddings -- to specific tasks with minimal data and computation.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
