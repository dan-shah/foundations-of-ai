{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Part 1.1: Linear Algebra for Deep Learning — The Tennis Edition\n\nLinear algebra is the foundation of deep learning. Neural networks are essentially compositions of linear transformations (matrix multiplications) and nonlinear activation functions.\n\nBut let's make this concrete: **Tennis is a data sport**. Every match generates rich statistics — serve speeds, first-serve percentages, winners, unforced errors, break points, rally lengths. All of it lives in vectors and matrices. Understanding linear algebra isn't just academic — it's how analysts, coaches, and broadcasters like IBM (with their Match Insights) extract actionable patterns from the game.\n\nThroughout this notebook, we'll learn the math of deep learning through the lens of tennis.\n\n## Learning Objectives\n- [ ] Understand vector spaces and linear transformations\n- [ ] Perform matrix operations fluently with NumPy\n- [ ] Explain the geometric intuition behind eigendecomposition\n- [ ] Apply SVD to dimensionality reduction\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "# For nice inline plots\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Vectors\n\nA **vector** is an ordered list of numbers. In machine learning:\n- A single data point (features) is a vector\n- Model parameters (weights) are vectors\n- Gradients are vectors\n\n### The Tennis Connection\n\nThink of a vector as a **player's match stats snapshot**. At any point during a match, a player's performance can be described as a vector of measurements:\n\n$$\\text{player\\_stats} = [\\text{serve\\_speed\\_mph}, \\text{first\\_serve\\_pct}, \\text{winners}, \\text{unforced\\_errors}, \\text{aces}, \\ldots]$$\n\nEach dimension captures a different aspect of performance — just like in ML, where each dimension of a feature vector captures a different attribute of a data point.\n\n### Geometric Interpretation\nA vector can be thought of as:\n1. A point in space (a player's position on the stat leaderboard)\n2. An arrow from the origin to that point (direction + magnitude — like a serve velocity showing the direction of the ball and how fast it's traveling)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Creating vectors in NumPy\n# A player's serve velocity vector: 120 mph flat, slight kick to the side\nserve_velocity = np.array([120, 5])  # 2D velocity vector (mph)\n\n# A player's match stats snapshot: [serve_speed_mph, first_serve_pct, winners]\nmatch_stats = np.array([125, 68.5, 32])  # 3D stats vector\n\nprint(f\"Serve velocity vector: {serve_velocity}\")\nprint(f\"Shape of serve velocity: {serve_velocity.shape}\")\nprint(f\"Dimension (number of measurements): {serve_velocity.shape[0]}\")\nprint(f\"\\nMatch stats vector: {match_stats}\")\nprint(f\"Match stats dimensions: {match_stats.shape[0]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize a 2D vector — serve velocity on court\ndef plot_vectors(vectors, colors, labels=None):\n    \"\"\"Plot 2D vectors from origin.\"\"\"\n    fig, ax = plt.subplots(figsize=(8, 8))\n    \n    for i, (vec, color) in enumerate(zip(vectors, colors)):\n        label = labels[i] if labels else None\n        ax.quiver(0, 0, vec[0], vec[1], angles='xy', scale_units='xy', scale=1, \n                  color=color, label=label, width=0.015)\n    \n    # Set axis limits\n    all_coords = np.array(vectors)\n    max_val = np.abs(all_coords).max() + 1\n    ax.set_xlim(-max_val, max_val)\n    ax.set_ylim(-max_val, max_val)\n    ax.set_aspect('equal')\n    ax.axhline(y=0, color='k', linewidth=0.5)\n    ax.axvline(x=0, color='k', linewidth=0.5)\n    ax.grid(True, alpha=0.3)\n    if labels:\n        ax.legend()\n    return ax\n\n# Plot a serve velocity vector\nv = np.array([3, 4])  # heading cross-court with topspin\nplot_vectors([v], ['blue'], ['serve velocity = [3, 4]'])\nplt.title(\"A Serve Velocity Vector on Court\")\nplt.xlabel('Across court (m/s)')\nplt.ylabel('Up court / depth (m/s)')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Vector Operations\n\n#### 1. Vector Addition\nVectors add element-wise. Geometrically, place the tail of the second vector at the head of the first.\n\n**Tennis analogy**: A player's performance across a match combines as vectors. Set 1 stats plus Set 2 stats gives cumulative match stats. Or think of forces on a tennis ball mid-flight: gravity pulls down, spin creates a Magnus force sideways, and the initial hit sends it forward. The **net force** on the ball is the vector sum — and that determines its actual trajectory."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Vector addition: forces on a tennis ball mid-rally\ntopspin_force = np.array([2, 0])    # topspin drives the ball forward (x-direction)\nsidespin_force = np.array([0, 3])   # sidespin curves the ball laterally (y-direction)\nnet_force = topspin_force + sidespin_force  # resultant force on ball\n\nprint(f\"Topspin force    = {topspin_force}\")\nprint(f\"Sidespin force   = {sidespin_force}\")\nprint(f\"Net force        = {net_force}\")\n\n# Visualize force addition on a tennis ball\nfig, ax = plt.subplots(figsize=(8, 8))\nax.quiver(0, 0, topspin_force[0], topspin_force[1], angles='xy', scale_units='xy', scale=1,\n          color='red', label='Topspin (forward)', width=0.015)\nax.quiver(0, 0, sidespin_force[0], sidespin_force[1], angles='xy', scale_units='xy', scale=1,\n          color='blue', label='Sidespin (lateral curve)', width=0.015)\nax.quiver(0, 0, net_force[0], net_force[1], angles='xy', scale_units='xy', scale=1,\n          color='green', label='Net force on ball', width=0.015)\n# Show sidespin starting from tip of topspin (parallelogram rule)\nax.quiver(topspin_force[0], topspin_force[1], sidespin_force[0], sidespin_force[1],\n          angles='xy', scale_units='xy', scale=1, color='blue', alpha=0.3, width=0.015)\nax.set_xlim(-1, 5)\nax.set_ylim(-1, 5)\nax.set_aspect('equal')\nax.axhline(y=0, color='k', linewidth=0.5)\nax.axvline(x=0, color='k', linewidth=0.5)\nax.legend()\nax.set_title('Force Vectors on a Tennis Ball Mid-Rally')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### 2. Scalar Multiplication\nMultiplying a vector by a scalar scales its magnitude (and flips direction if negative).\n\n**Tennis analogy**: Imagine a player's groundstroke velocity vector. Playing on grass (surface speed factor ~1.2x) effectively scales that velocity — same direction, more pace through the court. Clay (factor ~0.8x) slows it down. And a mishit that sends the ball back toward the net? That's multiplying by -1 — same line, opposite direction."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Scalar multiplication: grass boost, clay slowdown, and mishit\nshot_velocity = np.array([2, 1])     # groundstroke heading cross-court with depth\ngrass_boost = 1.2 * shot_velocity    # grass court — ball skids through faster!\nmishit = -1 * shot_velocity          # ball goes backward — frame shot\n\nprint(f\"Shot velocity       = {shot_velocity}\")\nprint(f\"Grass boost (1.2x)  = {grass_boost}\")\nprint(f\"Mishit (-1x)        = {mishit}\")\n\nplot_vectors([shot_velocity, grass_boost, mishit], ['blue', 'green', 'red'],\n             ['Normal pace', 'Grass boost (1.2x)', 'Mishit (-1x)'])\nplt.title('Scalar Multiplication: Surface Speed and Mishits')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### 3. Dot Product\n\nThe **dot product** (inner product) of two vectors is fundamental:\n\n$$\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{n} a_i b_i = |\\mathbf{a}| |\\mathbf{b}| \\cos\\theta$$\n\nWhere $\\theta$ is the angle between the vectors.\n\n**Tennis analogy**: Think of the dot product as measuring **alignment between two things**:\n- **Player style vs. surface demands**: A player's style is a vector of strengths [serve_power, baseline_consistency, net_play, ...]. A surface's demands are another vector. The dot product tells you *\"how well does this player's game match what this surface rewards?\"*\n- **Shot placement**: When your shot direction perfectly aligns with the open court (vectors aligned), the winner probability is maximized. At 90 degrees (hitting right at the opponent), there's no placement advantage.\n\n**Key insights:**\n- If dot product = 0, vectors are **orthogonal** (perpendicular) — like a player's serve speed and their return game being statistically independent\n- If positive, vectors point in similar directions — a player's style fits the surface\n- If negative, vectors point in opposite directions — a big-serving grass-court specialist forced onto slow clay\n- Used everywhere in neural networks: weighted sums!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Dot product: How well does a player's style match the surface?\n# Imagine simplified style vectors: [serve_power, baseline_consistency]\n\ngrass_demands = np.array([1, 0])       # Grass = all about serve power and net play\nclay_demands = np.array([0, 1])        # Clay = all about baseline consistency and endurance\nhardcourt_demands = np.array([1, 1])   # Hard court = needs both serve and baseline\n\n# How well does a baseline grinder's style match each surface?\nbaseline_style = np.array([1, 1])\n\nprint(f\"Baseline style · Grass demands = {np.dot(baseline_style, grass_demands)}\")\nprint(f\"Baseline style · Clay demands = {np.dot(baseline_style, clay_demands)}\")\nprint(f\"Baseline style · Hard court demands = {np.dot(baseline_style, hardcourt_demands)}\")\n\n# Using @ operator (preferred in modern NumPy)\nprint(f\"\\nUsing @ operator: style @ hardcourt = {baseline_style @ hardcourt_demands}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Computing angle between vectors using dot product\n# How different are two players' shot directions on a key point?\ndef angle_between(v1, v2):\n    \"\"\"Returns angle in degrees between vectors v1 and v2.\"\"\"\n    cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n    # Clip to handle numerical errors\n    cos_angle = np.clip(cos_angle, -1, 1)\n    return np.degrees(np.arccos(cos_angle))\n\n# Compare player styles\nnadal_style = np.array([1, 0])      # pure baseline power\nfederer_style = np.array([1, 1])    # all-court game — serve and baseline\ndjokovic_style = np.array([0, 1])   # pure defensive consistency\nopposite_style = np.array([-1, 0])  # anti-power (extreme moonballing)\n\nprint(f\"Nadal vs Federer styles: {angle_between(nadal_style, federer_style):.1f} degrees apart\")\nprint(f\"Nadal vs Djokovic styles: {angle_between(nadal_style, djokovic_style):.1f} degrees apart\")\nprint(f\"Nadal vs anti-power style: {angle_between(nadal_style, opposite_style):.1f} degrees apart\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: Understanding the Dot Product Formula\n\nThere are **two equivalent ways** to define the dot product:\n\n**Definition 1 - Algebraic (how we compute it):**\n$$\\mathbf{a} \\cdot \\mathbf{b} = a_1 b_1 + a_2 b_2 + \\ldots + a_n b_n$$\n\n**Definition 2 - Geometric (what it means):**\n$$\\mathbf{a} \\cdot \\mathbf{b} = |\\mathbf{a}| \\cdot |\\mathbf{b}| \\cdot \\cos(\\theta)$$\n\nThese are mathematically proven to be equal (using the Law of Cosines).\n\n#### Breaking down the geometric formula:\n\n| Component | Meaning | Tennis Analogy | Range |\n|-----------|---------|---------------|-------|\n| $\\|\\mathbf{a}\\|$ | Length of vector a | How extreme a player's style strengths are | 0 to ∞ |\n| $\\|\\mathbf{b}\\|$ | Length of vector b | How demanding the surface characteristics are | 0 to ∞ |\n| $\\cos(\\theta)$ | \"Alignment factor\" based on angle | How well the player's style matches the surface demands | -1 to 1 |\n\n#### What does cos(θ) do? Think of it as a \"match score\":\n\n| Angle θ | cos(θ) | Tennis Meaning | Dot product |\n|---------|--------|---------------|-------------|\n| 0° | 1 | Player perfectly fits the surface (Nadal on clay) | Maximum positive |\n| 45° | 0.71 | Decent fit (an all-court player on any surface) | Positive |\n| 90° | 0 | Completely independent (serve speed vs. return depth) | Zero |\n| 135° | -0.71 | Poor fit (a serve-and-volley specialist on slow clay) | Negative |\n| 180° | -1 | Exact opposite of what's needed | Maximum negative |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Interactive visualization: How dot product changes with angle\n# Keep vector 'a' fixed, rotate vector 'b' around\n\na = np.array([2, 0])  # Fixed vector pointing right\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left plot: Show vectors at different angles\nangles_deg = [0, 45, 90, 135, 180]\ncolors = ['green', 'blue', 'orange', 'red', 'purple']\n\naxes[0].quiver(0, 0, a[0], a[1], angles='xy', scale_units='xy', scale=1, \n               color='black', width=0.03, label='a (fixed)')\n\nfor angle, color in zip(angles_deg, colors):\n    theta = np.radians(angle)\n    b = 1.5 * np.array([np.cos(theta), np.sin(theta)])  # |b| = 1.5\n    dot = a @ b\n    axes[0].quiver(0, 0, b[0], b[1], angles='xy', scale_units='xy', scale=1,\n                   color=color, width=0.02, alpha=0.7, label=f'θ={angle}°, a·b={dot:.2f}')\n\naxes[0].set_xlim(-3, 3)\naxes[0].set_ylim(-2, 2)\naxes[0].set_aspect('equal')\naxes[0].axhline(y=0, color='k', linewidth=0.5)\naxes[0].axvline(x=0, color='k', linewidth=0.5)\naxes[0].legend(loc='upper left', fontsize=9)\naxes[0].set_title('Vector b at different angles from a')\naxes[0].grid(True, alpha=0.3)\n\n# Right plot: Dot product as function of angle\nangles = np.linspace(0, 360, 100)\ndot_products = []\nfor angle in angles:\n    theta = np.radians(angle)\n    b = 1.5 * np.array([np.cos(theta), np.sin(theta)])\n    dot_products.append(a @ b)\n\naxes[1].plot(angles, dot_products, 'b-', linewidth=2)\naxes[1].axhline(y=0, color='k', linewidth=1)\naxes[1].set_xlabel('Angle θ (degrees)')\naxes[1].set_ylabel('Dot product (a · b)')\naxes[1].set_title('Dot product vs angle between vectors\\n|a|=2, |b|=1.5, so max = 2×1.5 = 3')\naxes[1].set_xticks([0, 45, 90, 135, 180, 225, 270, 315, 360])\naxes[1].grid(True, alpha=0.3)\n\n# Mark key points\nfor angle, color in zip(angles_deg, colors):\n    theta = np.radians(angle)\n    b = 1.5 * np.array([np.cos(theta), np.sin(theta)])\n    dot = a @ b\n    axes[1].scatter([angle], [dot], color=color, s=100, zorder=5)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key insight: The dot product follows a cosine curve!\")\nprint(\"This is because a·b = |a||b|cos(θ), and we're varying θ.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### The Projection Interpretation\n\nAnother powerful way to understand dot product: **projection**.\n\nThe dot product $\\mathbf{a} \\cdot \\mathbf{b}$ tells you: *\"How much of b points in the direction of a?\"*\n\nMore precisely:\n$$\\mathbf{a} \\cdot \\mathbf{b} = |\\mathbf{a}| \\times (\\text{length of b's shadow onto a})$$\n\nThis \"shadow\" is called the **scalar projection** of b onto a.\n\n**Tennis analogy**: Imagine a player's overall game has both a baseline power component and a net play component. The **projection** onto the baseline axis tells you: *\"How much of Nadal's game is pure baseline power?\"* The net play component is separate — it doesn't contribute to baseline dominance. Projection isolates the piece that matters for a given question.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualizing projection — player's all-court game projected onto baseline axis\nbaseline_axis = np.array([3, 0])       # the baseline power dimension\nplayer_game = np.array([2, 2])         # all-court player: equal baseline and net skills\n\n# Scalar projection of player game onto baseline: (a·b) / |a|\nscalar_proj = (baseline_axis @ player_game) / np.linalg.norm(baseline_axis)\n\n# Vector projection: scalar_proj * unit vector of baseline axis\nbaseline_unit = baseline_axis / np.linalg.norm(baseline_axis)\nvector_proj = scalar_proj * baseline_unit\n\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Draw vectors\nax.quiver(0, 0, baseline_axis[0], baseline_axis[1], angles='xy', scale_units='xy', scale=1,\n          color='blue', width=0.02, label=f'Baseline power axis')\nax.quiver(0, 0, player_game[0], player_game[1], angles='xy', scale_units='xy', scale=1,\n          color='red', width=0.02, label=f'Player all-court game')\n\n# Draw projection (baseline contribution)\nax.quiver(0, 0, vector_proj[0], vector_proj[1], angles='xy', scale_units='xy', scale=1,\n          color='green', width=0.025, label=f'Baseline contribution (projection)')\n\n# Draw dashed line from player game to its projection (net play component)\nax.plot([player_game[0], vector_proj[0]], [player_game[1], vector_proj[1]],\n        'k--', linewidth=1.5, alpha=0.5, label='Net play component')\n\n# Annotations\nax.annotate('', xy=(vector_proj[0], -0.3), xytext=(0, -0.3),\n            arrowprops=dict(arrowstyle='<->', color='green'))\nax.text(vector_proj[0]/2, -0.6, f'baseline power = {scalar_proj:.2f}', ha='center', fontsize=11, color='green')\n\nax.set_xlim(-1, 4)\nax.set_ylim(-1, 3)\nax.set_aspect('equal')\nax.axhline(y=0, color='k', linewidth=0.5)\nax.axvline(x=0, color='k', linewidth=0.5)\nax.legend(loc='upper left')\nax.set_title('Player Profile: How Much of the Game is Baseline Power?')\nax.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"Overall game strength: {np.linalg.norm(player_game):.2f} (total skill)\")\nprint(f\"Baseline power component: {scalar_proj:.2f}\")\nprint(f\"Net play component: {np.sqrt(np.linalg.norm(player_game)**2 - scalar_proj**2):.2f}\")\nprint(f\"\\nThis is why projections matter — they decompose a player's game into specific dimensions!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Why Dot Product Matters in Machine Learning (and Tennis)\n\nThe dot product appears everywhere because it answers: **\"How similar are these two vectors?\"**\n\n| Application | What the dot product computes | Tennis Parallel |\n|-------------|-------------------------------|----------------|\n| **Neural network layer** | `w · x + b` = \"How much does input match what this neuron looks for?\" | How well match stats match a known winning pattern |\n| **Word embeddings** | `word1 · word2` = \"How semantically similar?\" | How similar are two players' styles? |\n| **Attention (Transformers)** | `query · key` = \"How relevant is this key to this query?\" | \"Which past matches are most relevant to predicting this one?\" |\n| **Recommendation systems** | `user · item` = \"How much would this user like this item?\" | \"How well would Sinner perform at a new tournament?\" |\n| **Cosine similarity** | `(a · b) / (\\|a\\| \\|b\\|)` = Pure directional similarity | Comparing play styles regardless of overall ranking |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### 4. Vector Norm (Magnitude/Length)\n\nThe **L2 norm** (Euclidean length) of a vector:\n\n$$||\\mathbf{v}||_2 = \\sqrt{\\sum_{i=1}^{n} v_i^2}$$\n\n**Tennis analogy**: The norm is the **total magnitude** of a vector. For a shot velocity vector, it's the ball's actual speed off the racket. For a stats vector, it captures the overall \"intensity\" of a player's performance.\n\nOther norms used in ML:\n- **L1 norm**: $||\\mathbf{v}||_1 = \\sum |v_i|$ (Manhattan distance, used for sparsity)\n- **L∞ norm**: $||\\mathbf{v}||_\\infty = \\max |v_i|$"
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: What is a Vector Norm?\n\nA **norm** measures the \"size\" or \"length\" of a vector. Think of it as answering: *\"How far is this point from the origin?\"*\n\n#### The L2 (Euclidean) Norm - Most Common\n\n$$||\\mathbf{v}||_2 = \\sqrt{v_1^2 + v_2^2 + \\ldots + v_n^2}$$\n\nThis is just the **Pythagorean theorem** extended to n dimensions!\n\nFor a serve velocity `v = [3, 4]` m/s (3 m/s across court, 4 m/s deep into the box):\n\n$||v|| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5$ m/s actual serve speed\n\nThe speed gun reads 5 m/s — regardless of the split between lateral and depth components.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualizing the L2 norm: serve speed components\nv = np.array([3, 4])  # 3 m/s across court, 4 m/s deep\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\n# Draw the velocity vector\nax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1,\n          color='blue', width=0.02, label=f'Actual serve speed = {np.linalg.norm(v)} m/s')\n\n# Draw the right triangle (decomposed into lateral and depth)\nax.plot([0, v[0]], [0, 0], 'g-', linewidth=2, label=f'Across court = {v[0]} m/s')\nax.plot([v[0], v[0]], [0, v[1]], 'r-', linewidth=2, label=f'Depth = {v[1]} m/s')\n\n# Right angle marker\nax.plot([v[0]-0.2, v[0]-0.2, v[0]], [0, 0.2, 0.2], 'k-', linewidth=1)\n\n# Labels\nax.text(v[0]/2, -0.4, '3 m/s', ha='center', fontsize=14, color='green')\nax.text(v[0]+0.4, v[1]/2, '4 m/s', ha='center', fontsize=14, color='red')\nax.text(v[0]/2 - 0.5, v[1]/2 + 0.3, '5 m/s', ha='center', fontsize=14, color='blue')\n\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 6)\nax.set_aspect('equal')\nax.axhline(y=0, color='k', linewidth=0.5)\nax.axvline(x=0, color='k', linewidth=0.5)\nax.legend(loc='upper left')\nax.set_title('Serve Speed Components: L2 Norm = Pythagorean Theorem\\n||v|| = sqrt(3^2 + 4^2) = 5 m/s')\nax.grid(True, alpha=0.3)\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Comparing Different Norms\n\nDifferent norms measure \"size\" differently — and this matters when evaluating tennis performance:\n\n| Norm | Formula | Intuition | Tennis Analogy | Use in ML |\n|------|---------|-----------|---------------|-----------|\n| **L2** | $\\sqrt{\\sum v_i^2}$ | Straight-line distance | Overall shot power (Pythagorean: combines pace and spin) | Default distance, weight decay |\n| **L1** | $\\sum \\|v_i\\|$ | \"Taxicab\" distance | Total stat accumulation (aces + winners + break points won) | Sparsity (Lasso), makes weights exactly 0 |\n| **L∞** | $\\max \\|v_i\\|$ | Largest single component | Best single stat — your peak weapon (e.g., fastest serve) | Worst-case bounds |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualize \"unit balls\" - all points where ||v|| = 1 for different norms\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\ntheta = np.linspace(0, 2*np.pi, 100)\n\n# L2 norm: circle (x² + y² = 1)\nx_l2 = np.cos(theta)\ny_l2 = np.sin(theta)\naxes[0].plot(x_l2, y_l2, 'b-', linewidth=2)\naxes[0].fill(x_l2, y_l2, alpha=0.2)\naxes[0].set_title('L2 Norm (Euclidean)\\n||v||₂ = √(x² + y²) = 1\\nCircle')\naxes[0].set_xlabel('x')\naxes[0].set_ylabel('y')\n\n# L1 norm: diamond (|x| + |y| = 1)\nx_l1 = [1, 0, -1, 0, 1]\ny_l1 = [0, 1, 0, -1, 0]\naxes[1].plot(x_l1, y_l1, 'r-', linewidth=2)\naxes[1].fill(x_l1, y_l1, alpha=0.2, color='red')\naxes[1].set_title('L1 Norm (Manhattan)\\n||v||₁ = |x| + |y| = 1\\nDiamond')\naxes[1].set_xlabel('x')\naxes[1].set_ylabel('y')\n\n# L∞ norm: square (max(|x|, |y|) = 1)\nx_linf = [1, 1, -1, -1, 1]\ny_linf = [1, -1, -1, 1, 1]\naxes[2].plot(x_linf, y_linf, 'g-', linewidth=2)\naxes[2].fill(x_linf, y_linf, alpha=0.2, color='green')\naxes[2].set_title('L∞ Norm (Max)\\n||v||∞ = max(|x|, |y|) = 1\\nSquare')\naxes[2].set_xlabel('x')\naxes[2].set_ylabel('y')\n\nfor ax in axes:\n    ax.set_xlim(-1.5, 1.5)\n    ax.set_ylim(-1.5, 1.5)\n    ax.set_aspect('equal')\n    ax.axhline(y=0, color='k', linewidth=0.5)\n    ax.axvline(x=0, color='k', linewidth=0.5)\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Example with a specific vector\nv = np.array([3, 4])\nprint(f\"For v = {v}:\")\nprint(f\"  L2 norm: ||v||₂ = √(3² + 4²) = {np.linalg.norm(v, ord=2)}\")\nprint(f\"  L1 norm: ||v||₁ = |3| + |4| = {np.linalg.norm(v, ord=1)}\")\nprint(f\"  L∞ norm: ||v||∞ = max(|3|, |4|) = {np.linalg.norm(v, ord=np.inf)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Why Norms Matter in Machine Learning (and Tennis)\n\n| Use Case | How Norms are Used | Tennis Parallel |\n|----------|-------------------|----------------|\n| **Normalization** | Divide by norm to get unit vector: `v / \\|\\|v\\|\\|`. Isolates direction from magnitude. | Comparing playing *styles* regardless of ranking |\n| **Regularization** | Add `λ\\|\\|weights\\|\\|²` to loss. Keeps weights small → prevents overfitting. | Salary cap keeping player spending in check |\n| **Distance** | Distance between points: `\\|\\|a - b\\|\\|`. Used in k-NN, clustering. | Gap between players' service game percentages |\n| **Gradient clipping** | If `\\|\\|gradient\\|\\| > threshold`, scale it down. Prevents exploding gradients. | Shot clock — cap the time to prevent stalling |\n| **Embedding similarity** | Normalize embeddings so dot product = cosine similarity. | Comparing player styles regardless of era or competition level |\n\n#### Connecting Dot Product and Norm\n\nThe dot product of a vector with itself gives the **squared norm**:\n\n$$\\mathbf{v} \\cdot \\mathbf{v} = v_1^2 + v_2^2 + \\ldots = ||\\mathbf{v}||^2$$\n\nSo: $||\\mathbf{v}|| = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}}$\n\n*Speed² = lateral² + depth² — a serve's kinetic energy is proportional to the squared norm of its velocity!*",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Norms in tennis context: shot power components\nshot_power = np.array([3, 4])  # 3 units pace, 4 units topspin (heavy ball)\n\n# L2 norm (default) — total shot power\nl2_norm = np.linalg.norm(shot_power)\nprint(f\"Shot power [pace, topspin]: {shot_power}\")\nprint(f\"Total shot power (L2 norm): {l2_norm}\")  # Pythagorean: 5 total\n\n# L1 norm — sum of all components\nl1_norm = np.linalg.norm(shot_power, ord=1)\nprint(f\"Sum of components (L1 norm): {l1_norm}\")  # 3 + 4 = 7\n\n# L∞ norm — peak component\nlinf_norm = np.linalg.norm(shot_power, ord=np.inf)\nprint(f\"Best single component (L∞ norm): {linf_norm}\")  # max(3, 4) = 4\n\n# Unit vector (normalize) — isolate the style of the shot\nshot_unit = shot_power / np.linalg.norm(shot_power)\nprint(f\"\\nShot style direction (unit vector): {shot_unit}\")\nprint(f\"Magnitude of unit vector: {np.linalg.norm(shot_unit)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 2. Matrices\n\nA **matrix** is a 2D array of numbers. In deep learning:\n- Weight matrices connect layers\n- Batches of data are matrices (rows = samples, columns = features)\n- Attention scores form matrices\n\n### The Tennis Connection\n\nMatrices are everywhere in tennis analytics:\n- **Match stats**: Each row is a game or set, each column is a stat (aces, winners, errors) → a matrix of the entire match\n- **Tournament results**: Rows = players, columns = tournaments → a season performance matrix\n- **Coaching adjustments**: A matrix can represent how changing one tactical element affects multiple outcomes\n\n### Matrix as Linear Transformation\n\nA matrix transforms vectors from one space to another. In tennis terms: a coaching adjustment matrix takes a player's base tactical profile and transforms it into a new playing style."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Creating matrices — a match stats snapshot\n# Rows = games, Columns = [serve_speed_mph, first_serve_pct]\nmatch_stats_matrix = np.array([[125, 72],    # Game 1: big serving\n                               [118, 65],    # Game 2: slightly off\n                               [130, 80],    # Game 3: ace-fest\n                               [110, 58]])   # Game 4: double-fault trouble\n\nprint(f\"Match stats matrix (4 games x 2 stats):\\n{match_stats_matrix}\")\nprint(f\"Shape: {match_stats_matrix.shape}\")\nprint(f\"Number of games: {match_stats_matrix.shape[0]}\")\nprint(f\"Number of stats: {match_stats_matrix.shape[1]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Matrix-Vector Multiplication\n\nMatrix $\\mathbf{A}$ (m×n) times vector $\\mathbf{v}$ (n×1) produces vector (m×1):\n\n$$\\mathbf{Av} = \\begin{bmatrix} \\mathbf{a}_1 \\cdot \\mathbf{v} \\\\ \\mathbf{a}_2 \\cdot \\mathbf{v} \\\\ \\vdots \\\\ \\mathbf{a}_m \\cdot \\mathbf{v} \\end{bmatrix}$$\n\nEach element is a dot product of a row of A with vector v.\n\n**Tennis analogy**: Think of the matrix as a \"coaching adjustment\" and the vector as the player's current tactical profile. The matrix-vector multiplication produces the player's *new* tactical profile after the coaching change. Each row of the matrix defines how one output metric (e.g., serve effectiveness, rally win rate) depends on the input skills."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Matrix-vector multiplication: a \"grass court tactics\" coaching adjustment\n# This adjustment boosts serve power (x) by 2x, keeps baseline game (y) the same\ngrass_tactics = np.array([[2, 0],    # serve effectiveness doubled\n                          [0, 1]])   # baseline game unchanged\nbase_profile = np.array([1, 1])      # balanced player\n\nnew_profile = grass_tactics @ base_profile  # or np.dot(grass_tactics, base_profile)\nprint(f\"Grass tactics applied: {new_profile}\")\nprint(\"Serve power doubled, baseline unchanged — classic serve-and-volley adjustment!\")\n\nplot_vectors([base_profile, new_profile], ['blue', 'red'],\n             ['Base player (balanced)', 'Grass tactics (serve-heavy)'])\nplt.title('Matrix as Coaching Adjustment: Preparing for Wimbledon')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Rotation matrix — changing shot direction mid-rally\n# Redirecting a shot 90 degrees (down-the-line to cross-court)\ntheta = np.pi / 2  # 90 degrees\nshot_redirect = np.array([[np.cos(theta), -np.sin(theta)],\n                          [np.sin(theta),  np.cos(theta)]])\n\ndown_the_line = np.array([1, 0])  # shot heading straight down the line\ncross_court = shot_redirect @ down_the_line\n\nprint(f\"Shot redirect matrix:\\n{shot_redirect.round(3)}\")\nprint(f\"Down-the-line shot: {down_the_line}\")\nprint(f\"Cross-court redirect:  {cross_court.round(3)}\")\n\nplot_vectors([down_the_line, cross_court], ['blue', 'red'],\n             ['Down the line', 'Cross-court redirect (90 deg)'])\nplt.title('Rotation Matrix: Redirecting a Shot')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Visualizing Linear Transformations\n\nLet's see how different matrices transform a grid of points — like watching how a tactical change warps the entire performance envelope of a player's game."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transformation(A, title):\n",
    "    \"\"\"Visualize how matrix A transforms a unit square.\"\"\"\n",
    "    # Create a grid of points\n",
    "    n = 10\n",
    "    x = np.linspace(-1, 1, n)\n",
    "    y = np.linspace(-1, 1, n)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Original grid\n",
    "    for xi in x:\n",
    "        axes[0].plot([xi, xi], [-1, 1], 'b-', alpha=0.5)\n",
    "    for yi in y:\n",
    "        axes[0].plot([-1, 1], [yi, yi], 'b-', alpha=0.5)\n",
    "    # Highlight basis vectors\n",
    "    axes[0].quiver(0, 0, 1, 0, angles='xy', scale_units='xy', scale=1, color='red', width=0.02)\n",
    "    axes[0].quiver(0, 0, 0, 1, angles='xy', scale_units='xy', scale=1, color='green', width=0.02)\n",
    "    axes[0].set_xlim(-2, 2)\n",
    "    axes[0].set_ylim(-2, 2)\n",
    "    axes[0].set_aspect('equal')\n",
    "    axes[0].set_title('Original Space')\n",
    "    axes[0].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[0].axvline(x=0, color='k', linewidth=0.5)\n",
    "    \n",
    "    # Transformed grid\n",
    "    for xi in x:\n",
    "        points = np.array([[xi, yi] for yi in y])\n",
    "        transformed = (A @ points.T).T\n",
    "        axes[1].plot(transformed[:, 0], transformed[:, 1], 'b-', alpha=0.5)\n",
    "    for yi in y:\n",
    "        points = np.array([[xi, yi] for xi in x])\n",
    "        transformed = (A @ points.T).T\n",
    "        axes[1].plot(transformed[:, 0], transformed[:, 1], 'b-', alpha=0.5)\n",
    "    \n",
    "    # Transformed basis vectors\n",
    "    e1_transformed = A @ np.array([1, 0])\n",
    "    e2_transformed = A @ np.array([0, 1])\n",
    "    axes[1].quiver(0, 0, e1_transformed[0], e1_transformed[1], angles='xy', scale_units='xy', scale=1, color='red', width=0.02)\n",
    "    axes[1].quiver(0, 0, e2_transformed[0], e2_transformed[1], angles='xy', scale_units='xy', scale=1, color='green', width=0.02)\n",
    "    \n",
    "    axes[1].set_xlim(-2, 2)\n",
    "    axes[1].set_ylim(-2, 2)\n",
    "    axes[1].set_aspect('equal')\n",
    "    axes[1].set_title(f'After Transformation: {title}')\n",
    "    axes[1].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[1].axvline(x=0, color='k', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Matrix:\\n{A}\")\n",
    "    print(f\"Red basis vector [1,0] -> {e1_transformed}\")\n",
    "    print(f\"Green basis vector [0,1] -> {e2_transformed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Grass court tactics: boost serve power, sacrifice baseline rallying\ngrass_tactics = np.array([[1.5, 0],\n                          [0, 0.5]])\nplot_transformation(grass_tactics, \"Grass Tactics (1.5x serve, 0.5x rallying)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Shot redirect through a 30-degree angle change\ntheta = np.pi / 6  # 30 degrees\nredirect_30 = np.array([[np.cos(theta), -np.sin(theta)],\n                        [np.sin(theta),  np.cos(theta)]])\nplot_transformation(redirect_30, \"30 deg Shot Redirect (Rotation)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Wind effect: wind shears the ball's trajectory sideways\nwind_effect = np.array([[1, 0.5],   # depth gets a lateral push from wind\n                        [0, 1]])     # lateral component unaffected\nplot_transformation(wind_effect, \"Wind Shear Effect on Ball Trajectory\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: Understanding Matrices as Transformations\n\n**Key Insight**: A matrix doesn't just \"do math\" - it describes a geometric transformation. Every matrix is a machine that takes vectors in and outputs transformed vectors.\n\nIn tennis terms: a matrix is like a **coaching adjustment** to a player's game. Feed in the player's current tactical profile, and the matrix spits out the new one.\n\n#### What Do the Columns of a Matrix Mean?\n\nHere's the most important insight about matrices:\n\n> **The columns of a matrix tell you where the basis vectors land after transformation.**\n\nFor a 2D matrix $\\mathbf{A} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$:\n- **Column 1** $\\begin{bmatrix} a \\\\ c \\end{bmatrix}$ = where the vector $\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ (pure serve power) lands\n- **Column 2** $\\begin{bmatrix} b \\\\ d \\end{bmatrix}$ = where the vector $\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ (pure baseline consistency) lands\n\nThis means: **to design a transformation, just decide where you want the basis vectors to go!**\n\n*Imagine you're the head coach: \"I want pure serve power to also generate some net points (column 1), and pure baseline consistency to stay as consistency (column 2).\" You just designed a matrix!*",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Demonstration: Columns of a matrix = where basis vectors land\n# Let's verify this with an example\n\nA = np.array([[2, -1],\n              [1,  1]])\n\n# Standard basis vectors\ne1 = np.array([1, 0])  # Points right\ne2 = np.array([0, 1])  # Points up\n\n# Transform them\nAe1 = A @ e1\nAe2 = A @ e2\n\nprint(\"Matrix A:\")\nprint(A)\nprint(f\"\\nColumn 1 of A: {A[:, 0]}\")\nprint(f\"A @ [1,0] = {Ae1}\")\nprint(f\"Same? {np.allclose(A[:, 0], Ae1)}\")\n\nprint(f\"\\nColumn 2 of A: {A[:, 1]}\")\nprint(f\"A @ [0,1] = {Ae2}\")\nprint(f\"Same? {np.allclose(A[:, 1], Ae2)}\")\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Before transformation\naxes[0].quiver(0, 0, 1, 0, angles='xy', scale_units='xy', scale=1, color='red', width=0.02, label='e1 = [1,0]')\naxes[0].quiver(0, 0, 0, 1, angles='xy', scale_units='xy', scale=1, color='green', width=0.02, label='e2 = [0,1]')\naxes[0].set_xlim(-2, 3)\naxes[0].set_ylim(-2, 3)\naxes[0].set_aspect('equal')\naxes[0].axhline(y=0, color='k', linewidth=0.5)\naxes[0].axvline(x=0, color='k', linewidth=0.5)\naxes[0].grid(True, alpha=0.3)\naxes[0].legend()\naxes[0].set_title('BEFORE: Standard Basis Vectors')\n\n# After transformation\naxes[1].quiver(0, 0, Ae1[0], Ae1[1], angles='xy', scale_units='xy', scale=1, color='red', width=0.02, \n               label=f'A @ e1 = {Ae1} (Column 1)')\naxes[1].quiver(0, 0, Ae2[0], Ae2[1], angles='xy', scale_units='xy', scale=1, color='green', width=0.02, \n               label=f'A @ e2 = {Ae2} (Column 2)')\naxes[1].set_xlim(-2, 3)\naxes[1].set_ylim(-2, 3)\naxes[1].set_aspect('equal')\naxes[1].axhline(y=0, color='k', linewidth=0.5)\naxes[1].axvline(x=0, color='k', linewidth=0.5)\naxes[1].grid(True, alpha=0.3)\naxes[1].legend()\naxes[1].set_title('AFTER: Basis Vectors = Columns of A')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey insight: Reading the columns of A directly tells you the transformation!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Common 2D Transformation Matrices (with Tennis Intuition)\n\nOnce you understand \"columns = where basis vectors go,\" you can read or construct any transformation:\n\n| Transformation | Matrix | Tennis Intuition |\n|----------------|--------|-----------------|\n| **Identity** (do nothing) | $\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$ | Keep the player's game as-is |\n| **Scale by k** | $\\begin{bmatrix} k & 0 \\\\ 0 & k \\end{bmatrix}$ | Uniform improvement (better serve + better returns) |\n| **Scale x by a, y by b** | $\\begin{bmatrix} a & 0 \\\\ 0 & b \\end{bmatrix}$ | Grass tactics: boost serve (a), sacrifice rallying (b) |\n| **Rotate by θ** | $\\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix}$ | Shot redirect: change the ball's direction mid-rally |\n| **Reflect across x-axis** | $\\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}$ | Mirror the court — swap forehand/backhand targeting |\n| **Shear (horizontal)** | $\\begin{bmatrix} 1 & k \\\\ 0 & 1 \\end{bmatrix}$ | Wind effect: lateral force adds to shot depth |\n| **Project onto x-axis** | $\\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix}$ | Ignore baseline game entirely — only serve power matters |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Why Matrix Multiplication is Composition of Transformations\n\nWhen you multiply matrices $\\mathbf{AB}$, you're creating a new transformation that does **B first, then A**.\n\n**Think of it this way:**\n- To apply $\\mathbf{AB}$ to vector $\\mathbf{v}$: $(\\mathbf{AB})\\mathbf{v} = \\mathbf{A}(\\mathbf{B}\\mathbf{v})$\n- First B transforms v, then A transforms the result\n\n**Tennis analogy**: It's like applying multiple tactical adjustments in sequence. First the coach works on the serve (matrix B), then they adjust the return game (matrix A). The combined effect (AB) is a single matrix that captures both changes.\n\n**Why the \"backwards\" order?** Because we read left-to-right but function application is right-to-left: $f(g(x))$ applies g first, then f. Just like the coach who improves the serve first, then adjusts the return game — the final tactic AB reads \"return adjustment applied to serve improvement.\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Demonstration: Composing tactical adjustments\n# Step 1: Scale (Grass court tactics: 2x serve power, 0.5x baseline rallying)\n# Step 2: Rotate (player redirects shot 45 degrees mid-rally)\n\ntheta = np.pi / 4  # 45 degree redirect\nRedirect = np.array([[np.cos(theta), -np.sin(theta)],\n                     [np.sin(theta),  np.cos(theta)]])\n\nGrassTactics = np.array([[2.0, 0],\n                         [0, 0.5]])\n\n# Compose: GrassTactics first, then Redirect (remember: right-to-left!)\n# So we write: Redirect @ GrassTactics\nFullPlan = Redirect @ GrassTactics\n\nprint(\"Redirect (45 deg shot change):\")\nprint(Redirect.round(3))\nprint(\"\\nGrass Tactics (2x serve, 0.5x rallying):\")\nprint(GrassTactics)\nprint(\"\\nComposed (Redirect @ GrassTactics) — tactics first, then redirect:\")\nprint(FullPlan.round(3))\n\n# Visualize the composition\nfig, axes = plt.subplots(1, 4, figsize=(20, 5))\nv = np.array([1, 1])  # balanced player profile\n\naxes[0].quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='blue', width=0.02)\naxes[0].set_title('Base player [1, 1]')\n\nv_adjusted = GrassTactics @ v\naxes[1].quiver(0, 0, v_adjusted[0], v_adjusted[1], angles='xy', scale_units='xy', scale=1, color='green', width=0.02)\naxes[1].set_title(f'After grass tactics: {v_adjusted}')\n\nv_adjusted_redirected = Redirect @ v_adjusted\naxes[2].quiver(0, 0, v_adjusted_redirected[0], v_adjusted_redirected[1], angles='xy', scale_units='xy', scale=1, color='red', width=0.02)\naxes[2].set_title(f'Then redirect shot:\\n{v_adjusted_redirected.round(3)}')\n\nv_composed = FullPlan @ v\naxes[3].quiver(0, 0, v_composed[0], v_composed[1], angles='xy', scale_units='xy', scale=1, color='purple', width=0.02)\naxes[3].set_title(f'Composed matrix:\\n{v_composed.round(3)}')\n\nfor ax in axes:\n    ax.set_xlim(-3, 3)\n    ax.set_ylim(-2, 2)\n    ax.set_aspect('equal')\n    ax.axhline(y=0, color='k', linewidth=0.5)\n    ax.axvline(x=0, color='k', linewidth=0.5)\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTwo-step result: {v_adjusted_redirected.round(6)}\")\nprint(f\"Composed result: {v_composed.round(6)}\")\nprint(f\"Same? {np.allclose(v_adjusted_redirected, v_composed)}\")\nprint(\"\\nKey insight: (Redirect @ GrassTactics) @ v = Redirect @ (GrassTactics @ v)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Matrix-Matrix Multiplication\n\nIf $\\mathbf{A}$ is (m×n) and $\\mathbf{B}$ is (n×p), then $\\mathbf{AB}$ is (m×p).\n\n**Key insight**: Matrix multiplication = composition of transformations.\n\nIf A is the shot redirect and B is the grass court tactics, then AB does both — grass tactics first, then the redirect. One single matrix captures the combined effect of multiple coaching adjustments."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "B = np.array([[5, 6],\n",
    "              [7, 8]])\n",
    "\n",
    "C = A @ B\n",
    "print(f\"A:\\n{A}\\n\")\n",
    "print(f\"B:\\n{B}\\n\")\n",
    "print(f\"A @ B:\\n{C}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Implement matrix multiplication from scratch\n",
    "def matmul(A, B):\n",
    "    \"\"\"\n",
    "    Multiply matrices A and B.\n",
    "    A: (m, n) matrix\n",
    "    B: (n, p) matrix\n",
    "    Returns: (m, p) matrix\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    n2, p = B.shape\n",
    "    assert n == n2, f\"Incompatible dimensions: {A.shape} and {B.shape}\"\n",
    "    \n",
    "    # TODO: Implement this!\n",
    "    # Hint: C[i,j] = sum over k of A[i,k] * B[k,j]\n",
    "    C = np.zeros((m, p))\n",
    "    \n",
    "    # Your code here\n",
    "    for i in range(m):\n",
    "        for j in range(p):\n",
    "            for k in range(n):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "    \n",
    "    return C\n",
    "\n",
    "# Test your implementation\n",
    "result = matmul(A, B)\n",
    "expected = A @ B\n",
    "print(f\"Your result:\\n{result}\")\n",
    "print(f\"Expected:\\n{expected}\")\n",
    "print(f\"Correct: {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Properties\n",
    "\n",
    "#### Transpose\n",
    "Swap rows and columns: $(\\mathbf{A}^T)_{ij} = \\mathbf{A}_{ji}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "print(f\"A (2x3):\\n{A}\\n\")\n",
    "print(f\"A^T (3x2):\\n{A.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Identity Matrix\nThe \"do nothing\" transformation. $\\mathbf{IA} = \\mathbf{AI} = \\mathbf{A}$\n\n*Like keeping a player's game plan unchanged — no tactical adjustments applied.*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.eye(3)  # 3x3 identity matrix\n",
    "print(f\"Identity matrix:\\n{I}\")\n",
    "\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "print(f\"\\nA @ I = A: {np.allclose(A @ I, A)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Matrix Inverse\n\nThe inverse $\\mathbf{A}^{-1}$ \"undoes\" the transformation: $\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}$\n\nNot all matrices have inverses (singular matrices).\n\n**Tennis analogy**: If a matrix represents a coaching adjustment, the inverse is the adjustment that **reverts** the player back to their baseline style. Added an aggressive net-rushing tactic? The inverse matrix takes it away. But some changes are irreversible — if you mentally broke a player's confidence (collapsed all performance to zero), there's no inverse that brings it back. That's a singular matrix."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[4, 7],\n",
    "              [2, 6]])\n",
    "\n",
    "A_inv = np.linalg.inv(A)\n",
    "print(f\"A:\\n{A}\\n\")\n",
    "print(f\"A^(-1):\\n{A_inv}\\n\")\n",
    "print(f\"A @ A^(-1):\\n{(A @ A_inv).round(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# A singular matrix (no inverse) — like a tactic that destroys information\n# This projects everything onto one line: baseline = 2 * serve\nsingular = np.array([[1, 2],\n                     [2, 4]])  # Row 2 = 2 * Row 1\n\nprint(f\"Determinant: {np.linalg.det(singular)}\")\nprint(\"Determinant = 0 → this matrix is singular (no inverse)\")\nprint(\"It collapses 2D space into a line — like a player who can only hit one shot!\")\n# np.linalg.inv(singular)  # This would raise an error"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 3. Tensors\n\n**Tensors** are generalizations to higher dimensions:\n- Scalar: 0D tensor (a single match duration: 2h 34m)\n- Vector: 1D tensor (one player's stats at one moment)\n- Matrix: 2D tensor (one player's full-match stats: games x stat categories)\n- 3D tensor: all players' full-match stats (players x games x stats)\n- 4D tensor: all players across all tournaments (tournaments x players x games x stats)\n\nIn deep learning, we constantly work with tensors. In tennis, the data is naturally high-dimensional — and tensors are how we organize it."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tensors in NumPy — tennis data at every scale\nmatch_duration = np.array(154.5)                   # 0D: a single match duration (minutes)\npoint_stats = np.array([125, 68.5, 32])            # 1D: one moment [serve_speed, first_serve_pct, winners]\none_match = np.random.rand(24, 5)                  # 2D: 24 games × 5 stats\nall_players = np.random.rand(20, 24, 5)            # 3D: 20 players × 24 games × 5 stats\nfull_tournament = np.random.rand(7, 20, 24, 5)     # 4D: 7 rounds × 20 players × 24 × 5\n\nprint(f\"Match duration shape:  {match_duration.shape}, ndim: {match_duration.ndim}  (scalar)\")\nprint(f\"Point stats shape:     {point_stats.shape}, ndim: {point_stats.ndim}  (vector)\")\nprint(f\"One match shape:       {one_match.shape}, ndim: {one_match.ndim}  (matrix)\")\nprint(f\"All players shape:     {all_players.shape}, ndim: {all_players.ndim}  (3D tensor)\")\nprint(f\"Full tournament shape: {full_tournament.shape}, ndim: {full_tournament.ndim}  (4D tensor)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Broadcasting\n\nNumPy's broadcasting allows operations on arrays of different shapes. This is crucial for efficient ML code.\n\n**Tennis analogy**: Suppose you have a matrix of match stats for 2 players across 3 tournaments. You want to subtract each player's *average* to see who improved. Broadcasting lets you subtract a vector from a matrix naturally."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Broadcasting examples with tennis data\n# Match durations for 2 players across 3 tournaments (minutes)\nmatch_times = np.array([[91.5, 82.3, 78.1],    # Player A: AO, RG, Wimbledon\n                        [92.1, 83.0, 77.8]])    # Player B: AO, RG, Wimbledon\n\n# Scalar broadcast: convert to seconds\nprint(f\"Match times in seconds:\\n{match_times * 60}\\n\")\n\n# Row vector broadcast: add surface-specific fatigue penalty per tournament\nfatigue_penalty = np.array([5.0, 8.0, 3.0])  # AO hard, RG clay (longer rallies), Wimbledon grass\nprint(f\"After fatigue penalties:\\n{match_times + fatigue_penalty}\\n\")\n\n# Column vector broadcast: player-specific fitness adjustment\nfitness_adj = np.array([[0.5], [0.8]])  # Player A fitter, Player B less so\nprint(f\"After fitness adjustment:\\n{match_times + fitness_adj}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 4. Eigenvalues and Eigenvectors\n\nFor a square matrix $\\mathbf{A}$, an **eigenvector** $\\mathbf{v}$ and **eigenvalue** $\\lambda$ satisfy:\n\n$$\\mathbf{Av} = \\lambda\\mathbf{v}$$\n\n**Meaning**: When you apply transformation A to eigenvector v, it only scales (by λ), doesn't change direction.\n\n### The Tennis Connection\n\nEvery player has certain **fundamental play style axes** — directions where adjusting something only amplifies or diminishes that style without redirecting it. For example:\n- A player might have a \"baseline grinding axis\" — more court coverage and consistency scales their grinding game without affecting their net play\n- And a \"serve-and-volley axis\" — more serve speed and net skills scales their attacking game without much effect on rallying\n\nThese natural axes are the **eigenvectors**. The **eigenvalues** tell you how sensitive the player's game is along each axis. A large eigenvalue means a small coaching tweak has a big effect in that direction.\n\n**Applications in ML**:\n- PCA (Principal Component Analysis)\n- Understanding neural network dynamics\n- Spectral clustering"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple example: a player's performance transformation\n# This matrix boosts serve power more than baseline consistency\nplayer_perf = np.array([[3, 1],\n                        [0, 2]])\n\neigenvalues, eigenvectors = np.linalg.eig(player_perf)\n\nprint(f\"Player performance matrix:\\n{player_perf}\\n\")\nprint(f\"Eigenvalues (sensitivity along natural axes): {eigenvalues}\")\nprint(f\"Eigenvectors (natural play style axes, as columns):\\n{eigenvectors}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify: Av = λv (the eigenvector equation)\nA = player_perf\nfor i in range(len(eigenvalues)):\n    λ = eigenvalues[i]\n    v = eigenvectors[:, i]  # Column i is eigenvector i\n    \n    Av = A @ v\n    λv = λ * v\n    \n    print(f\"\\nNatural axis {i+1}: {v}\")\n    print(f\"Sensitivity (eigenvalue): {λ}\")\n    print(f\"A @ v = {Av}\")\n    print(f\"λ * v = {λv}\")\n    print(f\"Only scaled, not rotated: {np.allclose(Av, λv)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize eigenvectors: they don't change direction under transformation\n# Like finding the \"natural play style axes\" of a player's game matrix\nA = np.array([[2, 1],\n              [1, 2]])\n\neigenvalues, eigenvectors = np.linalg.eig(A)\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\n# Plot many style vectors and their transformations\nfor theta in np.linspace(0, 2*np.pi, 16, endpoint=False):\n    v = np.array([np.cos(theta), np.sin(theta)])\n    Av = A @ v\n    ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, \n              color='blue', alpha=0.3, width=0.01)\n    ax.quiver(0, 0, Av[0], Av[1], angles='xy', scale_units='xy', scale=1, \n              color='red', alpha=0.3, width=0.01)\n\n# Highlight eigenvectors — the natural axes\nfor i in range(2):\n    v = eigenvectors[:, i]\n    Av = A @ v\n    ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, \n              color='blue', width=0.02, label=f'natural axis {i+1}' if i == 0 else '')\n    ax.quiver(0, 0, Av[0], Av[1], angles='xy', scale_units='xy', scale=1, \n              color='red', width=0.02, label=f'after transformation' if i == 0 else '')\n\nax.set_xlim(-4, 4)\nax.set_ylim(-4, 4)\nax.set_aspect('equal')\nax.axhline(y=0, color='k', linewidth=0.5)\nax.axvline(x=0, color='k', linewidth=0.5)\nax.set_title('Blue: Original, Red: Transformed\\nEigenvectors (thick) only scale — they are the natural play style axes')\nax.legend()\nplt.show()\n\nprint(f\"Eigenvalues: {eigenvalues}\")\nprint(\"The eigenvectors (thick lines) stay on the same line after transformation!\")\nprint(\"Most directions get rotated AND scaled — eigenvectors are the special ones that only scale.\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: The Intuition Behind Eigenvectors\n\n**The Big Picture**: Eigenvectors are the \"special directions\" of a transformation - directions that only get stretched or shrunk, never rotated.\n\nIn tennis: imagine you tweak a player's training regimen (that's the matrix). Most aspects of performance change in complicated ways — more fitness helps endurance but may reduce explosive serve speed. But there are **natural axes** where the effect is pure: push along this axis and you just get \"more\" (or \"less\") of the same thing.\n\n> **Eigenvector intuition**: \"I'm a direction that this matrix only scales, never rotates. Apply the matrix to me, and I just get longer or shorter.\"\n\n#### Breaking Down the Equation\n\n$$\\mathbf{Av} = \\lambda\\mathbf{v}$$\n\n| Component | Meaning | Tennis Analogy |\n|-----------|---------|---------------|\n| $\\mathbf{A}$ | The transformation matrix | The player's training/tactical characteristics |\n| $\\mathbf{v}$ | An eigenvector (special direction) | A fundamental play style axis |\n| $\\lambda$ | The eigenvalue (how much v gets scaled) | Sensitivity — how much the player responds along that axis |\n| $\\mathbf{Av}$ | The result of transforming v | Performance after the training is applied |\n| $\\lambda\\mathbf{v}$ | Same direction as v, just scaled | Same style, just amplified or diminished |\n\n#### What the Eigenvalue Tells You\n\n| Eigenvalue λ | Geometric meaning | Tennis Meaning |\n|--------------|-------------------|---------------|\n| λ > 1 | Eigenvector gets stretched | High sensitivity — small training input, big performance gain |\n| 0 < λ < 1 | Eigenvector gets shrunk | Diminishing returns along this axis |\n| λ = 1 | Eigenvector unchanged | This style axis is immune to the training change |\n| λ = 0 | Eigenvector collapses to zero | Training completely kills this performance dimension |\n| λ < 0 | Eigenvector flips and scales | Perverse effect — more input makes things worse |\n| Complex λ | Rotation is involved | Oscillatory behavior (e.g., streaky form swings!) |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Why Eigenvectors Matter in Machine Learning (and Tennis)\n\n| Application | How Eigenvectors are Used | Tennis Parallel |\n|------------|---------------------------|----------------|\n| **PCA** | Eigenvectors of covariance matrix = directions of maximum variance | Finding the main axes of player performance variation |\n| **Spectral Clustering** | Eigenvectors of graph Laplacian reveal cluster structure | Grouping similar tournaments (clay Slams vs. grass Slams vs. hard court Masters) |\n| **PageRank** | Dominant eigenvector gives importance scores | Ranking players by head-to-head dominance |\n| **Neural Network Dynamics** | Eigenvalues of weight matrices affect gradient flow. >1 = exploding, <1 = vanishing. | Serve speed: too hard = double fault, too soft = gets crushed |\n| **Covariance Analysis** | Eigenvectors show directions of correlation in data | Which performance metrics are most correlated? |\n| **Matrix Powers** | $A^n$ easy via eigendecomposition | Predicting long-term ranking trends |\n\n#### The PCA Connection\n\n**PCA finds eigenvectors of the covariance matrix.**\n\nImagine you have match stats across hundreds of matches: serve speed, first serve %, winners, unforced errors, break points... The covariance matrix captures how these all vary together. Its eigenvectors point in the directions of **maximum variation** — maybe the first principal component is \"overall aggressiveness\" and the second is \"consistency vs. risk-taking style.\"\n\nThe eigenvector with the **largest eigenvalue** = direction of **maximum variance** = the factor that explains the most performance difference between players.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 5. Singular Value Decomposition (SVD)\n\nSVD decomposes ANY matrix (not just square) into:\n\n$$\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T$$\n\nWhere:\n- $\\mathbf{U}$: Left singular vectors (orthonormal)\n- $\\mathbf{\\Sigma}$: Diagonal matrix of singular values (non-negative, sorted descending)\n- $\\mathbf{V}^T$: Right singular vectors (orthonormal)\n\n### The Tennis Connection\n\nThink of a **player × surface performance matrix** — rows are players, columns are surfaces/tournaments, entries are average win rates. SVD decomposes this into:\n- $\\mathbf{U}$: **Player profiles** — each player described by hidden factors (e.g., \"clay skill,\" \"grass skill,\" \"mental toughness\")\n- $\\mathbf{\\Sigma}$: **Importance** of each factor\n- $\\mathbf{V}^T$: **Surface/tournament profiles** — how much each surface demands each factor\n\nThis is exactly how Netflix recommends movies, but we're predicting *which surfaces and tournaments each player would dominate*.\n\n**Applications in ML**:\n- Dimensionality reduction (PCA uses SVD)\n- Image compression\n- Recommender systems\n- Latent semantic analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD example\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9],\n",
    "              [10, 11, 12]])\n",
    "\n",
    "U, s, Vt = np.linalg.svd(A)\n",
    "\n",
    "print(f\"Original A shape: {A.shape}\")\n",
    "print(f\"U shape: {U.shape}\")\n",
    "print(f\"Singular values: {s}\")\n",
    "print(f\"V^T shape: {Vt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct A from SVD\n",
    "# Need to create the full Sigma matrix\n",
    "Sigma = np.zeros((U.shape[0], Vt.shape[0]))\n",
    "np.fill_diagonal(Sigma, s)\n",
    "\n",
    "A_reconstructed = U @ Sigma @ Vt\n",
    "print(f\"Original A:\\n{A}\\n\")\n",
    "print(f\"Reconstructed:\\n{A_reconstructed.round(10)}\\n\")\n",
    "print(f\"Reconstruction accurate: {np.allclose(A, A_reconstructed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-Rank Approximation\n",
    "\n",
    "By keeping only the top k singular values, we get the best rank-k approximation of A.\n",
    "\n",
    "This is the foundation of dimensionality reduction!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: Understanding SVD Geometrically\n\nSVD reveals the hidden structure of any matrix. Think of it as answering: *\"What are the fundamental building blocks of this transformation?\"*\n\n$$\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T$$\n\n#### What Each Component Represents\n\n| Component | Shape | What it represents | Tennis Analogy |\n|-----------|-------|-------------------|---------------|\n| $\\mathbf{V}^T$ | (n x n) | Input rotation | Rotate from \"tournament features\" to hidden factors |\n| $\\mathbf{\\Sigma}$ | (m x n) | Scaling | How important each hidden factor is |\n| $\\mathbf{U}$ | (m x m) | Output rotation | Rotate from hidden factors to \"player profiles\" |\n\n**The key insight**: ANY matrix transformation can be decomposed into: **rotate → scale → rotate**.\n\nIn tennis terms: you can understand any player-surface performance matrix as: (1) find the hidden skill factors that matter, (2) weight them by importance, (3) map them to specific players.\n\n#### Why Singular Values are Sorted by Importance\n\nThe singular values in $\\Sigma$ are always sorted: $\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r \\geq 0$\n\n**Why sorted?** Because they represent how much the matrix \"stretches\" space in each direction:\n- $\\sigma_1$ = the most important factor (maybe \"overall talent level\")\n- $\\sigma_2$ = second most important (maybe \"surface specialization\")\n- Small $\\sigma_i$ = \"noise\" (random variation that doesn't represent real skill)\n\nThis ordering is why keeping only the top-k singular values gives the **best** rank-k approximation! Keep the signal, drop the noise.\n\n#### The Connection to PCA\n\nPCA and SVD are deeply connected:\n\n| If you have... | PCA finds... | Which equals... |\n|----------------|--------------|-----------------|\n| Data matrix $\\mathbf{X}$ (centered) | Eigenvectors of $\\mathbf{X}^T\\mathbf{X}$ | Right singular vectors $\\mathbf{V}$ from SVD of $\\mathbf{X}$ |\n| Principal components | $\\mathbf{X} \\cdot \\text{eigenvectors}$ | $\\mathbf{U} \\cdot \\Sigma$ from SVD |\n| Variance explained | Eigenvalues / total | $\\sigma_i^2 / \\sum \\sigma_j^2$ |\n\n**Bottom line**: PCA is just SVD on centered data! In practice, PCA is often computed using SVD because it's more numerically stable.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_rank_approx(A, k):\n",
    "    \"\"\"Return rank-k approximation of matrix A using SVD.\"\"\"\n",
    "    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "    return U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]\n",
    "\n",
    "# Example with random matrix\n",
    "np.random.seed(42)\n",
    "A = np.random.rand(10, 8)\n",
    "\n",
    "print(f\"Original matrix shape: {A.shape}\")\n",
    "print(f\"Full rank: {np.linalg.matrix_rank(A)}\")\n",
    "\n",
    "for k in [1, 2, 4, 8]:\n",
    "    A_k = low_rank_approx(A, k)\n",
    "    error = np.linalg.norm(A - A_k, 'fro')  # Frobenius norm\n",
    "    print(f\"Rank-{k} approximation error: {error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Practical Exercise: Image Compression with SVD\n\nLet's compress an image using SVD — just like how tennis analytics teams compress massive match statistics datasets to find the essential patterns, throwing away noise while keeping the signal."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample grayscale image (or load one)\n",
    "# We'll create a simple pattern\n",
    "x = np.linspace(-3, 3, 200)\n",
    "y = np.linspace(-3, 3, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "image = np.sin(X) * np.cos(Y) + 0.5 * np.sin(2*X) * np.cos(2*Y)\n",
    "image = (image - image.min()) / (image.max() - image.min())  # Normalize to [0, 1]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(f'Original Image ({image.shape[0]}×{image.shape[1]})')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress with different ranks\n",
    "U, s, Vt = np.linalg.svd(image, full_matrices=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "ranks = [1, 5, 10, 20, 50, 100]\n",
    "for ax, k in zip(axes.flat, ranks):\n",
    "    compressed = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]\n",
    "    \n",
    "    # Calculate compression ratio\n",
    "    original_size = image.shape[0] * image.shape[1]\n",
    "    compressed_size = k * (image.shape[0] + image.shape[1] + 1)\n",
    "    ratio = original_size / compressed_size\n",
    "    \n",
    "    ax.imshow(compressed, cmap='gray')\n",
    "    ax.set_title(f'Rank {k}\\nCompression: {ratio:.1f}x')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot singular values\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(s, 'b-')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Singular Value')\n",
    "plt.title('Singular Values')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.cumsum(s**2) / np.sum(s**2), 'b-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Variance Explained')\n",
    "plt.title('Cumulative Variance')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95%')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercises\n\n### Exercise 1: Implement Matrix Operations\nImplement the following functions without using NumPy's built-in functions.\n\n*Think of it as building your own match analytics toolkit from scratch — every Grand Slam broadcast now shows real-time stats, and someone had to code those tools!*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose(A):\n",
    "    \"\"\"Return the transpose of matrix A.\"\"\"\n",
    "    m, n = A.shape\n",
    "    result = np.zeros((n, m))\n",
    "    # TODO: Implement\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            result[j, i] = A[i, j]\n",
    "    return result\n",
    "\n",
    "def dot_product(a, b):\n",
    "    \"\"\"Return the dot product of vectors a and b.\"\"\"\n",
    "    assert len(a) == len(b)\n",
    "    result = 0\n",
    "    # TODO: Implement\n",
    "    for i in range(len(a)):\n",
    "        result += a[i] * b[i]\n",
    "    return result\n",
    "\n",
    "def matrix_vector_mult(A, v):\n",
    "    \"\"\"Return A @ v.\"\"\"\n",
    "    m, n = A.shape\n",
    "    assert n == len(v)\n",
    "    result = np.zeros(m)\n",
    "    # TODO: Implement\n",
    "    for i in range(m):\n",
    "        result[i] = dot_product(A[i], v)\n",
    "    return result\n",
    "\n",
    "# Test\n",
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "v = np.array([1, 2, 3])\n",
    "\n",
    "print(f\"transpose(A) correct: {np.allclose(transpose(A), A.T)}\")\n",
    "print(f\"dot_product([1,2,3], [4,5,6]) = {dot_product(np.array([1,2,3]), np.array([4,5,6]))}\")\n",
    "print(f\"matrix_vector_mult correct: {np.allclose(matrix_vector_mult(A, v), A @ v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2: Tennis Transformation Explorer\nCreate different transformation matrices and visualize their effects on a player's tactical profile.\n\nTry to create:\n1. **Clay court tactics**: Scale baseline consistency way up, serve power slightly down\n2. **Mirror targeting**: Reflect the player's shot selection (swap forehand/backhand side targeting)\n3. **All-court special**: Rotate 45 deg then scale (change direction + amplify)\n4. **Project onto serve**: Ignore everything except serve power"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Create and visualize these tennis transformations:\n# 1. Clay court tactics (high baseline consistency, lower serve power)\n# 2. Mirror targeting (reflect across x-axis — swap forehand/backhand targeting)\n# 3. All-court special (rotate 45° then scale by 2)\n# 4. Project onto serve (x-axis projection — ignore baseline game)\n\n# Example: Clay court tactics — boost baseline consistency, sacrifice serve power\nclay_tactics = np.array([[0.7, 0],    # reduce serve power to 70%\n                         [0, 1.5]])   # boost baseline consistency by 50%\nplot_transformation(clay_tactics, \"Clay Tactics (0.7x serve, 1.5x baseline)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 3: Build a Player-Surface Performance Predictor\n\nUse SVD for matrix factorization to predict how players would perform on surfaces they haven't played much on — the same math Netflix uses to recommend movies!\n\n**Scenario**: You have a player × surface/tournament rating matrix where each entry is a performance score (1-5). Some entries are missing (player hasn't competed there much). Can SVD help predict them?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Player-Surface performance matrix (players × tournaments)\n# Scores 1-5 (5 = dominant, 1 = struggled). 0 = limited/no data.\n#                       AusOpen  RolandG  Wimbledon  USOpen  IndianWells\nperformance = np.array([\n    [5, 3, 0, 4, 5],   # Djokovic: dominant everywhere, unknown at Wimbledon (hypothetical)\n    [4, 0, 0, 4, 3],   # Sinner: quick, unknown at RG & Wimbledon\n    [2, 5, 0, 3, 2],   # Nadal: clay king, struggles on fast courts\n    [0, 2, 5, 4, 0],   # Federer: grass master, unknown at AO & Indian Wells\n    [0, 0, 4, 0, 3],   # Alcaraz: limited data\n    [3, 4, 3, 5, 4]    # Medvedev: consistent everywhere\n])\n\nplayers = ['DJO', 'SIN', 'NAD', 'FED', 'ALC', 'MED']\ntournaments = ['AusOpen', 'RolandG', 'Wimbledon', 'USOpen', 'IndianWells']\n\nprint(\"Player-Tournament Performance (0 = unknown):\")\nprint(f\"{'':>5}\", end='')\nfor t in tournaments:\n    print(f\"{t:>12}\", end='')\nprint()\nfor i, p in enumerate(players):\n    print(f\"{p:>5}\", end='')\n    for j in range(len(tournaments)):\n        val = performance[i, j]\n        print(f\"{'?' if val == 0 else val:>12}\", end='')\n    print()\n\n# Step 1: Fill missing values with player's average (simple imputation)\nperf_filled = performance.copy().astype(float)\nfor i in range(performance.shape[0]):\n    row = performance[i]\n    mean = row[row > 0].mean()\n    perf_filled[i, row == 0] = mean\n\nprint(\"\\nFilled with player averages:\")\nprint(perf_filled.round(2))\n\n# Step 2: SVD low-rank approximation — find hidden \"skill factors\"\nk = 2  # 2 latent factors (maybe \"baseline endurance\" and \"serve-and-volley skill\")\nU, s, Vt = np.linalg.svd(perf_filled, full_matrices=False)\npredicted = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]\n\nprint(f\"\\nPredicted performance (rank-{k} — 2 hidden skill factors):\")\nprint(predicted.round(2))\n\n# Step 3: Show predictions for originally missing entries\nprint(\"\\n--- PREDICTIONS FOR UNKNOWN TOURNAMENTS ---\")\nfor i in range(performance.shape[0]):\n    for j in range(performance.shape[1]):\n        if performance[i, j] == 0:\n            print(f\"  {players[i]} at {tournaments[j]}: {predicted[i, j]:.1f}/5\")\n\nprint(\"\\nThe SVD found hidden factors and used them to predict!\")\nprint(f\"Singular values: {s[:k].round(2)} — these are the importance of each hidden factor\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Summary\n\n### Key Concepts (with the Tennis Lens)\n\n1. **Vectors** represent data points, weights, and gradients — or a player's match stats, serve velocity, and shot force vectors\n2. **Dot product** measures alignment — how well a player's style matches a surface, or how similar two players' games are\n3. **Norms** measure magnitude — overall shot power, total stat accumulation, or peak single weapon\n4. **Matrices** are linear transformations — coaching adjustments, shot redirects, or data organization\n5. **Matrix multiplication** composes transformations — applying multiple tactical changes in sequence\n6. **Eigenvectors** reveal the \"natural axes\" of a transformation — the player's fundamental play style dimensions\n7. **SVD** decomposes any matrix into rotate→scale→rotate — revealing hidden structure like player skill factors\n\n### Connection to Deep Learning (and Tennis Strategy)\n\n| Deep Learning | Tennis Parallel |\n|--------------|----------------|\n| **Forward pass**: Matrix multiplications + activations | Coaching adjustments + nonlinear mental/physical effects |\n| **Weights**: Learned transformation matrices | Optimized tactical parameters |\n| **Backprop**: Chain rule on matrix operations | Sensitivity analysis: which tactical change had the most effect? |\n| **Embeddings**: Low-dimensional representations (SVD) | Player/surface profiles from sparse tournament results |\n| **Attention**: Dot products between vectors | \"Which past matches are most relevant to this situation?\" |\n\n### Checklist\n- [ ] I can perform vector operations (addition, dot product, norm) — and explain them with forces on a tennis ball\n- [ ] I understand matrices as linear transformations — like coaching adjustments\n- [ ] I can multiply matrices and understand shape compatibility\n- [ ] I know what eigenvalues/eigenvectors represent — the natural play style axes\n- [ ] I can use SVD for dimensionality reduction — and to predict player performance at new tournaments"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to **Part 1.2: Calculus Refresher** where we'll cover:\n",
    "- Derivatives and the chain rule\n",
    "- Gradients and gradient descent\n",
    "- The mathematical foundation of backpropagation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}