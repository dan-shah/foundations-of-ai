{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Part 5.2: Language Models â€” The Formula 1 Edition\n\nLanguage models are the foundation of modern AI. At their core, they do something deceptively simple: **predict the next token in a sequence**. Yet from this single objective emerges the ability to write essays, translate languages, answer questions, and even reason about code. In notebook 17 you built a Transformer from scratch -- now we'll see how that architecture becomes GPT, BERT, and the large language models that are reshaping every field.\n\n**F1 analogy:** A language model is like an F1 race prediction system. Given everything that has happened so far in a race -- \"Safety car deployed on lap 12, Verstappen pits on lap 13, rain starts on lap 14...\" -- it predicts the most likely next event. Just as a language model assigns probabilities to every possible next word, a race predictor assigns probabilities to every possible next event: pit stop, overtake, mechanical failure, safety car. The better the model, the better its predictions.\n\n---\n\n## Learning Objectives\n\nBy the end of this notebook, you should be able to:\n\n- [ ] Explain what a language model is and why next-token prediction is so powerful\n- [ ] Implement Byte Pair Encoding (BPE) tokenization from scratch\n- [ ] Describe the GPT (decoder-only) architecture and why it suits generation\n- [ ] Describe the BERT (encoder-only) architecture and why it suits understanding\n- [ ] Train a small character-level GPT and generate text from it\n- [ ] Explain scaling laws and how model size, data, and compute interact\n- [ ] Choose the right architecture (GPT vs BERT) for a given task",
   "id": "cell-0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "id": "cell-1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 1. What Is a Language Model?\n\n### Intuitive Explanation\n\nImagine someone starts a sentence: *\"The cat sat on the ___\"*. Your brain instantly predicts likely next words: \"mat\", \"chair\", \"roof\". You assign high probability to sensible continuations and low probability to nonsensical ones like \"democracy\" or \"purple\".\n\nA **language model** does exactly this -- it learns a probability distribution over the next token given all previous tokens:\n\n$$P(w_t \\mid w_1, w_2, \\ldots, w_{t-1})$$\n\n#### Breaking down the formula:\n\n| Component | Meaning | Intuition | F1 Analogy |\n|-----------|---------|----------|------------|\n| $w_t$ | The next token to predict | The word we're guessing | The next race event to predict |\n| $w_1, \\ldots, w_{t-1}$ | All previous tokens | The context we've seen so far | Everything that has happened in the race so far |\n| $P(\\cdot \\mid \\cdot)$ | Conditional probability | How likely is this word *given* what came before? | How likely is a pit stop *given* the current race state? |\n\n**What this means:** A language model is a probability machine. Given any prefix of text, it outputs a probability for every possible next token in its vocabulary. The better the model, the higher probability it assigns to tokens that actually make sense.\n\n**F1 analogy:** Think of the language model as a race prediction engine. Given the sequence of events so far -- \"Lap 1: Verstappen leads, Lap 2: Norris overtakes Leclerc, Lap 3: ...\" -- it assigns a probability to every possible next event. The model trained on thousands of historical races learns patterns: safety cars often trigger pit stops, tire degradation increases over stint length, drivers on fresh tires tend to overtake. This is *autoregressive prediction* applied to racing.\n\n### Why This Is Useful\n\nLanguage modeling isn't just about prediction -- it's a gateway to many capabilities:\n\n| Capability | How LM Enables It | F1 Parallel |\n|------------|------------------|-------------|\n| **Text Generation** | Sample from the predicted distribution repeatedly | Generate race commentary lap by lap |\n| **Translation** | Model P(target language \\| source language) | Translate telemetry data into strategy calls |\n| **Summarization** | Generate condensed version conditioned on original | Race report: 58 laps summarized in 3 paragraphs |\n| **Question Answering** | Generate answer conditioned on context + question | \"When should we pit?\" given current race state |\n| **Code Writing** | Code is just another language to model | Generate strategy algorithms from requirements |\n| **Reasoning** | Chain-of-thought emerges from predicting logical next steps | Multi-step strategic reasoning about race scenarios |",
   "id": "cell-2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Brief History: From N-grams to Transformers\n\n| Era | Approach | Key Idea | Limitation | F1 Parallel |\n|-----|----------|----------|------------|-------------|\n| 1990s-2000s | **N-gram models** | Count word sequences in data | Fixed context window (2-5 words) | Strategy based on last 2-3 laps only |\n| 2010-2015 | **RNN/LSTM** | Recurrent hidden state carries context | Sequential processing, vanishing gradients | Pit wall processing one car at a time |\n| 2017-present | **Transformers** | Self-attention over all positions | Quadratic cost in sequence length | Full parallel telemetry processing |\n| 2018+ | **Large LMs (GPT, BERT)** | Scale Transformers + massive data | Compute-intensive training | Strategy AI trained on decades of race data |",
   "id": "cell-3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Next-token probability distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: probability distribution for a predictable context\n",
    "context1 = '\"The cat sat on the ___\"'\n",
    "words1 = ['mat', 'floor', 'chair', 'roof', 'table', 'bed', 'dog', 'moon', 'idea', 'purple']\n",
    "probs1 = [0.30, 0.18, 0.15, 0.10, 0.08, 0.06, 0.05, 0.03, 0.03, 0.02]\n",
    "\n",
    "colors1 = ['#2ecc71' if p > 0.1 else '#3498db' if p > 0.04 else '#e74c3c' for p in probs1]\n",
    "bars1 = axes[0].barh(range(len(words1)), probs1, color=colors1, edgecolor='white', linewidth=0.5)\n",
    "axes[0].set_yticks(range(len(words1)))\n",
    "axes[0].set_yticklabels(words1, fontsize=11)\n",
    "axes[0].set_xlabel('Probability', fontsize=12)\n",
    "axes[0].set_title(f'Next word after:\\n{context1}', fontsize=13, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "for i, p in enumerate(probs1):\n",
    "    axes[0].text(p + 0.005, i, f'{p:.0%}', va='center', fontsize=10)\n",
    "\n",
    "# Right: probability distribution for an ambiguous context\n",
    "context2 = '\"I need to go to the ___\"'\n",
    "words2 = ['store', 'bank', 'doctor', 'office', 'gym', 'park', 'school', 'airport', 'dentist', 'library']\n",
    "probs2 = [0.15, 0.13, 0.12, 0.11, 0.10, 0.09, 0.08, 0.08, 0.07, 0.07]\n",
    "\n",
    "colors2 = ['#2ecc71' if p > 0.1 else '#3498db' if p > 0.04 else '#e74c3c' for p in probs2]\n",
    "bars2 = axes[1].barh(range(len(words2)), probs2, color=colors2, edgecolor='white', linewidth=0.5)\n",
    "axes[1].set_yticks(range(len(words2)))\n",
    "axes[1].set_yticklabels(words2, fontsize=11)\n",
    "axes[1].set_xlabel('Probability', fontsize=12)\n",
    "axes[1].set_title(f'Next word after:\\n{context2}', fontsize=13, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "for i, p in enumerate(probs2):\n",
    "    axes[1].text(p + 0.005, i, f'{p:.0%}', va='center', fontsize=10)\n",
    "\n",
    "fig.suptitle('Language Models Output Probability Distributions Over Next Tokens', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: More context = more peaked (confident) distributions\")\n",
    "print(\"The model's job is to learn these distributions from data\")"
   ],
   "id": "cell-4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 2. Tokenization\n\n### Intuitive Explanation\n\nBefore a language model can process text, it needs to break text into discrete units called **tokens**. But how should we split text?\n\n**The vocabulary problem:** If we use whole words, we need a massive vocabulary (English has 170,000+ words, plus names, technical terms, misspellings...). If we use single characters, sequences become very long and the model must learn spelling from scratch.\n\n**Subword tokenization** is the elegant middle ground: common words stay whole (\"the\", \"and\"), while rare words are split into meaningful pieces (\"un\" + \"happi\" + \"ness\").\n\n**F1 analogy:** Tokenization is like breaking radio messages into meaningful units. The message \"Box box box, we're switching to hards\" could be tokenized at different granularities:\n- **Character-level:** B, o, x, (space), b, o, x, ... -- too fine-grained, loses meaning\n- **Word-level:** \"Box\", \"box\", \"box\", \"we're\", \"switching\", \"to\", \"hards\" -- works but cannot handle rare compound terms like \"undercut-opportunity\"\n- **Subword (BPE):** \"Box\", \"box\", \"box\", \"we\", \"'re\", \"switch\", \"ing\", \"to\", \"hard\", \"s\" -- the sweet spot\n\n| Approach | Example: \"unhappiness\" | Vocab Size | Pros | Cons |\n|----------|----------------------|------------|------|------|\n| **Character-level** | u, n, h, a, p, p, i, n, e, s, s | ~256 | Handles any text | Very long sequences |\n| **Word-level** | unhappiness | 100K+ | Semantically meaningful | Can't handle unseen words |\n| **Subword (BPE)** | un, happi, ness | 30K-50K | Best of both worlds | Requires training |",
   "id": "cell-5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Byte Pair Encoding (BPE): The Dominant Tokenization Algorithm\n\nBPE is beautifully simple. It starts with individual characters and repeatedly merges the most frequent pair:\n\n**Algorithm:**\n1. Start with a vocabulary of all individual characters in the text\n2. Count all adjacent pairs of tokens in the text\n3. Merge the most frequent pair into a new token\n4. Repeat steps 2-3 until desired vocabulary size is reached\n\n**What this means:** BPE discovers the natural building blocks of language. Frequent subwords like \"ing\", \"tion\", \"un\" emerge automatically from the data. Common words end up as single tokens, while rare words get split into recognizable pieces.\n\n**F1 analogy:** BPE is like how F1 engineers develop shorthand for common telemetry patterns. At first, everything is described in raw terms (\"throttle 100%, brake 0%, speed increasing\"). Over time, frequent patterns get their own names: \"full traction zone,\" \"brake point,\" \"power unit deployment.\" The most common patterns become single tokens in the team's vocabulary, while rare events are still described by combining known sub-patterns.\n\n### Deep Dive: Why BPE Works So Well\n\n| Property | Why It Matters | F1 Parallel |\n|----------|---------------|-------------|\n| **Data-driven** | Learns from actual text, not handcrafted rules | Vocabulary emerges from real race data, not imposed |\n| **Open vocabulary** | Any text can be encoded (falls back to characters) | Any radio message can be parsed, even unusual ones |\n| **Compression** | Common patterns get short representations | \"Box box box\" becomes one token with enough data |\n| **Morphological** | Naturally discovers prefixes, suffixes, stems | Discovers \"under-cut\", \"over-cut\", \"out-braked\" |\n| **Language-agnostic** | Works for English, Chinese, code, math -- anything | Works for team radio, timing data, technical reports |\n\n#### Common Misconceptions\n\n| Misconception | Reality |\n|---------------|--------|\n| BPE splits on word boundaries | BPE operates on characters/bytes, ignoring word boundaries |\n| All words are single tokens | Only frequent words; rare ones are split |\n| Tokenization is trivial | It significantly impacts model performance |",
   "id": "cell-6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: BPE step by step on a small corpus\n",
    "def visualize_bpe_steps(corpus, num_merges=8):\n",
    "    \"\"\"\n",
    "    Run BPE on a small corpus and visualize each merge step.\n",
    "    \n",
    "    Args:\n",
    "        corpus: list of words (each word is a list of characters + end marker)\n",
    "        num_merges: number of merge operations to perform\n",
    "    \"\"\"\n",
    "    # Initialize: split each word into characters with end-of-word marker\n",
    "    word_freqs = {}\n",
    "    for word, freq in corpus:\n",
    "        word_freqs[tuple(word)] = freq\n",
    "    \n",
    "    merge_history = []\n",
    "    \n",
    "    for step in range(num_merges):\n",
    "        # Count all adjacent pairs\n",
    "        pair_counts = Counter()\n",
    "        for word, freq in word_freqs.items():\n",
    "            for i in range(len(word) - 1):\n",
    "                pair_counts[(word[i], word[i+1])] += freq\n",
    "        \n",
    "        if not pair_counts:\n",
    "            break\n",
    "        \n",
    "        # Find most frequent pair\n",
    "        best_pair = pair_counts.most_common(1)[0]\n",
    "        pair, count = best_pair\n",
    "        merged = pair[0] + pair[1]\n",
    "        \n",
    "        merge_history.append((pair, count, merged))\n",
    "        \n",
    "        # Apply merge\n",
    "        new_word_freqs = {}\n",
    "        for word, freq in word_freqs.items():\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                if i < len(word) - 1 and word[i] == pair[0] and word[i+1] == pair[1]:\n",
    "                    new_word.append(merged)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word_freqs[tuple(new_word)] = freq\n",
    "        word_freqs = new_word_freqs\n",
    "    \n",
    "    return merge_history, word_freqs\n",
    "\n",
    "# Small corpus with frequencies\n",
    "corpus = [\n",
    "    (list(\"low\") + [\"_\"], 5),\n",
    "    (list(\"lower\") + [\"_\"], 2),\n",
    "    (list(\"newest\") + [\"_\"], 6),\n",
    "    (list(\"widest\") + [\"_\"], 3),\n",
    "    (list(\"new\") + [\"_\"], 2),\n",
    "]\n",
    "\n",
    "merge_history, final_vocab = visualize_bpe_steps(corpus, num_merges=10)\n",
    "\n",
    "# Visualize merge history\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "steps = range(1, len(merge_history) + 1)\n",
    "labels = [f'\"{h[0][0]}\" + \"{h[0][1]}\" \\u2192 \"{h[2]}\"' for h in merge_history]\n",
    "counts = [h[1] for h in merge_history]\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(merge_history)))\n",
    "bars = ax.barh(range(len(merge_history)), counts, color=colors, edgecolor='white')\n",
    "\n",
    "ax.set_yticks(range(len(merge_history)))\n",
    "ax.set_yticklabels([f'Step {i+1}: {labels[i]}' for i in range(len(merge_history))], fontsize=11)\n",
    "ax.set_xlabel('Pair Frequency in Corpus', fontsize=12)\n",
    "ax.set_title('BPE Merge Operations (Most Frequent Pairs First)', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "for i, c in enumerate(counts):\n",
    "    ax.text(c + 0.1, i, str(c), va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCorpus words: low(5), lower(2), newest(6), widest(3), new(2)\")\n",
    "print(\"\\nFinal tokenization of each word:\")\n",
    "for word, freq in final_vocab.items():\n",
    "    print(f\"  {''.join(word):15s} (frequency: {freq})\")"
   ],
   "id": "cell-7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing BPE from Scratch"
   ],
   "id": "cell-8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBPE:\n",
    "    \"\"\"\n",
    "    A simple Byte Pair Encoding tokenizer built from scratch.\n",
    "    \n",
    "    This implements the core BPE algorithm: start with characters,\n",
    "    iteratively merge the most frequent adjacent pair.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=256):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.merges = []  # List of (pair, merged_token) in order\n",
    "        self.vocab = {}   # token -> index\n",
    "    \n",
    "    def train(self, text, verbose=False):\n",
    "        \"\"\"\n",
    "        Train BPE on a text corpus.\n",
    "        \n",
    "        Args:\n",
    "            text: Training text string\n",
    "            verbose: If True, print each merge step\n",
    "        \"\"\"\n",
    "        # Split text into words, add end-of-word marker\n",
    "        words = text.split()\n",
    "        word_freqs = Counter(words)\n",
    "        \n",
    "        # Initialize: each word as tuple of characters + end marker\n",
    "        splits = {}\n",
    "        for word, freq in word_freqs.items():\n",
    "            splits[tuple(list(word) + [\"</w>\"])] = freq\n",
    "        \n",
    "        # Build initial character vocabulary\n",
    "        chars = set()\n",
    "        for word in splits:\n",
    "            chars.update(word)\n",
    "        \n",
    "        num_merges = self.vocab_size - len(chars)\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            # Count pairs\n",
    "            pair_counts = Counter()\n",
    "            for word, freq in splits.items():\n",
    "                for j in range(len(word) - 1):\n",
    "                    pair_counts[(word[j], word[j+1])] += freq\n",
    "            \n",
    "            if not pair_counts:\n",
    "                break\n",
    "            \n",
    "            # Find best pair\n",
    "            best_pair = pair_counts.most_common(1)[0][0]\n",
    "            merged = best_pair[0] + best_pair[1]\n",
    "            \n",
    "            if verbose and i < 15:\n",
    "                count = pair_counts[best_pair]\n",
    "                print(f\"Merge {i+1}: '{best_pair[0]}' + '{best_pair[1]}' -> '{merged}' (freq: {count})\")\n",
    "            \n",
    "            self.merges.append((best_pair, merged))\n",
    "            \n",
    "            # Apply merge\n",
    "            new_splits = {}\n",
    "            for word, freq in splits.items():\n",
    "                new_word = []\n",
    "                j = 0\n",
    "                while j < len(word):\n",
    "                    if j < len(word) - 1 and word[j] == best_pair[0] and word[j+1] == best_pair[1]:\n",
    "                        new_word.append(merged)\n",
    "                        j += 2\n",
    "                    else:\n",
    "                        new_word.append(word[j])\n",
    "                        j += 1\n",
    "                new_splits[tuple(new_word)] = freq\n",
    "            splits = new_splits\n",
    "        \n",
    "        # Build vocabulary\n",
    "        all_tokens = set()\n",
    "        for word in splits:\n",
    "            all_tokens.update(word)\n",
    "        self.vocab = {token: idx for idx, token in enumerate(sorted(all_tokens))}\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nFinal vocabulary size: {len(self.vocab)}\")\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize a string using learned merges.\n",
    "        \n",
    "        Args:\n",
    "            text: Input string\n",
    "            \n",
    "        Returns:\n",
    "            List of tokens\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        all_tokens = []\n",
    "        \n",
    "        for word in words:\n",
    "            tokens = list(word) + [\"</w>\"]\n",
    "            \n",
    "            # Apply each merge rule in order\n",
    "            for (a, b), merged in self.merges:\n",
    "                new_tokens = []\n",
    "                i = 0\n",
    "                while i < len(tokens):\n",
    "                    if i < len(tokens) - 1 and tokens[i] == a and tokens[i+1] == b:\n",
    "                        new_tokens.append(merged)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_tokens.append(tokens[i])\n",
    "                        i += 1\n",
    "                tokens = new_tokens\n",
    "            \n",
    "            all_tokens.extend(tokens)\n",
    "        \n",
    "        return all_tokens\n",
    "\n",
    "# Train on a small corpus\n",
    "training_text = \"\"\"the cat sat on the mat the cat sat on the hat\n",
    "the dog sat on the log the dog sat on the rug\n",
    "a new cat and a new dog sat on a new mat\n",
    "the newest cat sat on the newest mat happily\n",
    "the lower cat and the lowest dog were unhappy\"\"\"\n",
    "\n",
    "print(\"Training BPE tokenizer...\")\n",
    "print(\"=\" * 50)\n",
    "bpe = SimpleBPE(vocab_size=60)\n",
    "bpe.train(training_text, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\\nTokenizing example sentences:\")\n",
    "test_sentences = [\"the cat sat\", \"newest dog\", \"unhappy cat\"]\n",
    "for sent in test_sentences:\n",
    "    tokens = bpe.tokenize(sent)\n",
    "    print(f\"  '{sent}' -> {tokens}\")"
   ],
   "id": "cell-9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: How \"unhappiness\" gets tokenized at different vocab sizes\n",
    "fig, axes = plt.subplots(4, 1, figsize=(12, 8))\n",
    "\n",
    "# Simulate different tokenization granularities\n",
    "tokenizations = [\n",
    "    (\"Character-level\\n(vocab ~30)\", list(\"unhappiness\"), '#e74c3c'),\n",
    "    (\"Small subword\\n(vocab ~100)\", [\"un\", \"h\", \"app\", \"in\", \"ess\"], '#e67e22'),\n",
    "    (\"Medium subword\\n(vocab ~1000)\", [\"un\", \"happi\", \"ness\"], '#2ecc71'),\n",
    "    (\"Large subword\\n(vocab ~50000)\", [\"unhappiness\"], '#3498db'),\n",
    "]\n",
    "\n",
    "for ax, (label, tokens, color) in zip(axes, tokenizations):\n",
    "    # Draw token boxes\n",
    "    x_pos = 0\n",
    "    for token in tokens:\n",
    "        width = len(token) * 0.8 + 0.4\n",
    "        rect = plt.Rectangle((x_pos, 0.1), width, 0.8, \n",
    "                            facecolor=color, edgecolor='black', alpha=0.7, linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x_pos + width/2, 0.5, token, ha='center', va='center', \n",
    "               fontsize=13, fontweight='bold', color='white')\n",
    "        x_pos += width + 0.15\n",
    "    \n",
    "    ax.set_xlim(-0.3, 12)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_ylabel(label, fontsize=10, rotation=0, labelpad=100, va='center')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    \n",
    "    # Show token count\n",
    "    ax.text(11.5, 0.5, f'{len(tokens)} tokens', fontsize=12, va='center', \n",
    "           fontweight='bold', color=color)\n",
    "\n",
    "fig.suptitle('Tokenizing \"unhappiness\" at Different Vocabulary Sizes', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Trade-off: Smaller vocab = longer sequences, Larger vocab = shorter sequences\")\n",
    "print(\"Most modern LLMs use 30K-100K subword tokens (the sweet spot)\")"
   ],
   "id": "cell-10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Special Tokens\n\nLanguage models use special tokens to mark structure and boundaries:\n\n| Token | Name | Purpose | Used By | F1 Parallel |\n|-------|------|---------|--------|-------------|\n| `[PAD]` | Padding | Fill shorter sequences to equal length in a batch | All models | Padding short stints to match longest stint length |\n| `[UNK]` | Unknown | Represent tokens not in vocabulary | Word-level models | \"Unknown flag condition\" -- never seen before |\n| `[BOS]` / `<s>` | Beginning of Sequence | Mark where text starts | GPT, generation models | Race start / lights out |\n| `[EOS]` / `</s>` | End of Sequence | Signal the model to stop generating | GPT, generation models | Chequered flag |\n| `[CLS]` | Classification | Aggregate representation for classification tasks | BERT | Race summary token |\n| `[SEP]` | Separator | Separate two sentences in a pair | BERT | Separating qualifying data from race data |\n| `[MASK]` | Mask | Placeholder for masked language modeling | BERT | Hidden event: \"On lap 15, [MASK] pitted\" -- predict who |\n\n**What this means:** Special tokens are the \"punctuation\" of the model's internal language. They tell the model where sequences start and end, how to separate inputs, and where to output predictions.\n\n### Tokenization Comparison Table\n\n| Method | How It Works | Vocab Size | Used By |\n|--------|-------------|------------|--------|\n| **BPE** | Merge most frequent character pairs | 30K-50K | GPT-2, GPT-3, GPT-4, LLaMA |\n| **WordPiece** | Like BPE but uses likelihood, not frequency | 30K | BERT, DistilBERT |\n| **SentencePiece** | BPE/Unigram on raw text (no pre-tokenization) | 32K-256K | T5, LLaMA, multilingual models |\n| **Tiktoken** | BPE with byte-level fallback | 100K | GPT-4, Claude |",
   "id": "cell-11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 3. GPT Architecture (Decoder-Only)\n\n### Intuitive Explanation\n\nGPT stands for **Generative Pre-trained Transformer**. It uses only the **decoder** part of the Transformer (from notebook 17) to generate text left-to-right.\n\n**Why decoder-only works for generation:** When you write a sentence, you write one word at a time, left to right. Each word depends only on what came before it. A decoder-only model mirrors this natural process with **causal masking** -- each position can only attend to positions before it (and itself), never to future positions.\n\n**Key insight:** The same architecture that predicts the next token during training can generate text at inference time by repeatedly sampling from its predictions.\n\n**F1 analogy:** GPT is like a **race commentary generator** or a **lap time predictor from history**. Given everything that has happened so far in the race, it predicts the next event -- and then feeds that prediction back in to predict the event after that. \"Verstappen leads lap 1\" -> predicts \"Norris closes gap in sector 2\" -> predicts \"DRS enabled on lap 3\" -> and so on. It can only look backward (causal masking), just as a commentator can only describe what has already happened when predicting what comes next.\n\n### Architecture Overview\n\nThe GPT architecture is a stack of identical decoder blocks:\n\n```\nInput Text: \"The cat sat\"\n    |\n    v\n[Token Embedding] + [Positional Encoding]\n    |\n    v\n[Decoder Block 1] -- Masked Self-Attention -> Feed-Forward\n    |\n    v\n[Decoder Block 2] -- Masked Self-Attention -> Feed-Forward\n    |\n    v\n   ...N blocks...\n    |\n    v\n[Layer Norm]\n    |\n    v\n[Linear Head] -> Vocabulary-sized logits\n    |\n    v\nP(\"on\") = 0.35, P(\"down\") = 0.12, ...\n```",
   "id": "cell-12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: GPT Architecture and Causal Masking\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Causal masking pattern\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\"]\n",
    "n = len(tokens)\n",
    "\n",
    "# Create causal mask (lower triangular)\n",
    "mask = np.tril(np.ones((n, n)))\n",
    "\n",
    "im = axes[0].imshow(mask, cmap='Blues', vmin=0, vmax=1)\n",
    "axes[0].set_xticks(range(n))\n",
    "axes[0].set_yticks(range(n))\n",
    "axes[0].set_xticklabels(tokens, fontsize=12)\n",
    "axes[0].set_yticklabels(tokens, fontsize=12)\n",
    "axes[0].set_xlabel('Key (can attend to)', fontsize=12)\n",
    "axes[0].set_ylabel('Query (predicting from)', fontsize=12)\n",
    "axes[0].set_title('GPT: Causal (Left-to-Right) Mask', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        color = 'white' if mask[i, j] > 0.5 else 'black'\n",
    "        symbol = '\\u2713' if mask[i, j] > 0.5 else '\\u2717'\n",
    "        axes[0].text(j, i, symbol, ha='center', va='center', fontsize=16, \n",
    "                   color=color, fontweight='bold')\n",
    "\n",
    "# Right: What each position predicts\n",
    "axes[1].set_xlim(0, 10)\n",
    "axes[1].set_ylim(-0.5, 5.5)\n",
    "axes[1].set_title('GPT: Each Position Predicts Next Token', fontsize=13, fontweight='bold')\n",
    "\n",
    "predictions = [\n",
    "    (\"The\", \"cat\", '#3498db'),\n",
    "    (\"The cat\", \"sat\", '#2ecc71'),\n",
    "    (\"The cat sat\", \"on\", '#e67e22'),\n",
    "    (\"The cat sat on\", \"the\", '#9b59b6'),\n",
    "    (\"The cat sat on the\", \"mat\", '#e74c3c'),\n",
    "]\n",
    "\n",
    "for i, (context, pred, color) in enumerate(predictions):\n",
    "    y = 4.5 - i\n",
    "    # Context box\n",
    "    axes[1].add_patch(plt.Rectangle((0.2, y - 0.2), 4.5, 0.4, \n",
    "                     facecolor=color, alpha=0.3, edgecolor=color, linewidth=2))\n",
    "    axes[1].text(2.45, y, context, ha='center', va='center', fontsize=11)\n",
    "    # Arrow\n",
    "    axes[1].annotate('', xy=(6.5, y), xytext=(5.0, y),\n",
    "                    arrowprops=dict(arrowstyle='->', color=color, lw=2))\n",
    "    # Prediction\n",
    "    axes[1].text(7.3, y, f'\\u2192 \"{pred}\"', ha='left', va='center', \n",
    "               fontsize=12, fontweight='bold', color=color)\n",
    "\n",
    "axes[1].set_xticks([])\n",
    "axes[1].set_yticks([])\n",
    "axes[1].spines['top'].set_visible(False)\n",
    "axes[1].spines['right'].set_visible(False)\n",
    "axes[1].spines['bottom'].set_visible(False)\n",
    "axes[1].spines['left'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left: Each row shows which positions that token can attend to\")\n",
    "print(\"Right: GPT is trained so every position predicts the NEXT token\")\n",
    "print(\"\\nThis is why GPT is autoregressive: it generates one token at a time, left to right\")"
   ],
   "id": "cell-13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Implementing a Mini-GPT Model\n\nLet's build a small GPT model. This uses the same components from notebook 17 (multi-head attention, feed-forward layers) but arranged as a **decoder-only** model with causal masking.\n\n**F1 framing:** We are building a mini race-event predictor. Feed in the sequence of events, and it predicts the next one -- all using causal attention so it can only look at what has already happened, never peek into the future.",
   "id": "cell-14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head self-attention with causal (autoregressive) masking.\n",
    "    Each position can only attend to itself and earlier positions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, max_seq_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.W_qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        self.W_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Causal mask: lower triangular matrix\n",
    "        # Register as buffer so it moves to GPU with model but isn't a parameter\n",
    "        mask = torch.tril(torch.ones(max_seq_len, max_seq_len))\n",
    "        self.register_buffer('mask', mask.view(1, 1, max_seq_len, max_seq_len))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Compute Q, K, V in one projection\n",
    "        qkv = self.W_qkv(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention with causal mask\n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        scores = scores.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = attn @ v\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.W_out(out)\n",
    "\n",
    "\n",
    "class GPTBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single GPT decoder block: LayerNorm -> Causal Attention -> LayerNorm -> FFN\n",
    "    Uses pre-norm (LayerNorm before attention) like GPT-2.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, d_ff=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))   # Residual + attention\n",
    "        x = x + self.ffn(self.ln2(x))     # Residual + FFN\n",
    "        return x\n",
    "\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    A minimal GPT language model.\n",
    "    \n",
    "    Architecture: Token Embed + Pos Embed -> N x GPTBlock -> LayerNorm -> Linear\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Number of tokens in vocabulary\n",
    "        d_model: Embedding dimension\n",
    "        n_heads: Number of attention heads\n",
    "        n_layers: Number of decoder blocks\n",
    "        max_seq_len: Maximum sequence length\n",
    "        dropout: Dropout rate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=128, n_heads=4, n_layers=4, \n",
    "                 max_seq_len=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[\n",
    "            GPTBlock(d_model, n_heads, dropout=dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.ln_final = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying: share weights between token embedding and output head\n",
    "        self.lm_head.weight = self.token_embedding.weight\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights following GPT-2 conventions.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            idx: Token indices, shape (batch, seq_len)\n",
    "            targets: Target token indices for loss computation\n",
    "            \n",
    "        Returns:\n",
    "            logits: Shape (batch, seq_len, vocab_size)\n",
    "            loss: Cross-entropy loss (if targets provided)\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.max_seq_len, f\"Sequence length {T} exceeds max {self.max_seq_len}\"\n",
    "        \n",
    "        # Token + positional embeddings\n",
    "        positions = torch.arange(T, device=idx.device).unsqueeze(0)\n",
    "        x = self.token_embedding(idx) + self.position_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_final(x)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate tokens autoregressively.\n",
    "        \n",
    "        Args:\n",
    "            idx: Starting token indices, shape (batch, seq_len)\n",
    "            max_new_tokens: Number of new tokens to generate\n",
    "            temperature: Controls randomness (higher = more random)\n",
    "            top_k: If set, only sample from top-k most likely tokens\n",
    "            \n",
    "        Returns:\n",
    "            Token indices including generated tokens\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop to max_seq_len if needed\n",
    "            idx_cond = idx if idx.size(1) <= self.max_seq_len else idx[:, -self.max_seq_len:]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature  # Take last position, apply temperature\n",
    "            \n",
    "            # Optional top-k filtering\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "            \n",
    "            # Sample from distribution\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "# Create a mini-GPT and inspect it\n",
    "model = MiniGPT(vocab_size=100, d_model=64, n_heads=4, n_layers=2, max_seq_len=32)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Mini-GPT Architecture\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Vocabulary size: 100\")\n",
    "print(f\"Embedding dimension: 64\")\n",
    "print(f\"Attention heads: 4\")\n",
    "print(f\"Decoder layers: 2\")\n",
    "print(f\"Max sequence length: 32\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print()\n",
    "print(\"Model structure:\")\n",
    "print(model)"
   ],
   "id": "cell-15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Comparison: Decoder-Only vs Encoder-Decoder\n\n| Aspect | Decoder-Only (GPT) | Encoder-Decoder (T5, Original Transformer) | F1 Parallel |\n|--------|--------------------|-----------------------------------------|-------------|\n| **Architecture** | Only decoder blocks with causal masking | Separate encoder (bidirectional) + decoder (causal) | Commentary generator vs. telemetry-to-strategy translator |\n| **Attention** | Causal self-attention only | Encoder: full self-attention; Decoder: causal + cross-attention | Only past events visible vs. full data + sequential output |\n| **Training** | Predict next token | Map input sequence to output sequence | Predict next lap event vs. translate telemetry to strategy |\n| **Input/Output** | Single text stream | Separate input and output streams | One continuous race narrative vs. separate input/output |\n| **Best for** | Generation, completion, chat | Translation, summarization (structured input->output) | Race commentary, predictions vs. data translation tasks |\n| **Examples** | GPT-1/2/3/4, LLaMA, Claude | T5, BART, original Transformer | -- |\n\n**Key insight:** Decoder-only models turn out to be surprisingly versatile. By framing any task as \"continue this text,\" GPT can do translation, Q&A, summarization, and more -- all as text generation.\n\n### Deep Dive: GPT Scaling History\n\n| Model | Year | Parameters | Training Data | Context Length | Key Innovation |\n|-------|------|-----------|--------------|----------------|---------------|\n| **GPT-1** | 2018 | 117M | BookCorpus (5GB) | 512 | Proved unsupervised pre-training works |\n| **GPT-2** | 2019 | 1.5B | WebText (40GB) | 1024 | Zero-shot task transfer |\n| **GPT-3** | 2020 | 175B | 300B tokens | 2048 | In-context learning, few-shot prompting |\n| **GPT-4** | 2023 | ~1.8T (est.) | ~13T tokens (est.) | 8K-128K | Multimodal, instruction following |\n\n#### Key Insight\n\nEach generation didn't change the core architecture much -- it primarily **scaled up** parameters, data, and compute. The Transformer architecture from 2017 has proven remarkably scalable. In F1 terms, the fundamental car concept (ground effect, for example) stays the same -- the teams that win are the ones that invest the most in refining and developing that concept.",
   "id": "cell-16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 4. BERT Architecture (Encoder-Only)\n\n### Intuitive Explanation\n\nWhile GPT generates text left-to-right, BERT takes a completely different approach: it reads text **bidirectionally** -- looking at both left and right context simultaneously.\n\n**Analogy:** Imagine filling in a blank in a sentence: *\"The [MASK] chased the mouse.\"* You need to see both what comes before AND after the blank to determine the answer is \"cat.\" This is exactly what BERT does -- it's trained to fill in randomly masked tokens.\n\n**F1 analogy:** BERT is like understanding the **full context of a race situation**. When analyzing \"On lap 32, [MASK] made a critical overtake at turn 4,\" you need to see both the preceding context (who was in position, what the gaps were) and the following context (the overtake was into P3, the driver was on fresh softs) to determine it was Norris. GPT can only look left; BERT sees the whole picture -- that is why BERT excels at *understanding* rather than *generating*.\n\n**Why encoder-only works for understanding:** Many NLP tasks don't require generating text -- they require *understanding* it:\n- Is this email spam or not? (classification)\n- What is the sentiment of this review? (sentiment analysis)  \n- Which word does \"it\" refer to? (coreference resolution)\n- Where is the answer in this paragraph? (extractive QA)\n\nFor all these tasks, looking at the **full context** (both directions) gives better understanding than only looking left-to-right.\n\n### BERT's Two Training Objectives\n\n| Objective | How It Works | What It Teaches | F1 Parallel |\n|-----------|-------------|----------------|-------------|\n| **Masked Language Modeling (MLM)** | Randomly mask 15% of tokens, predict them | Deep bidirectional understanding of language | Predict hidden race events from full context |\n| **Next Sentence Prediction (NSP)** | Given two sentences, predict if B follows A | Understanding relationships between sentences | \"Did this strategy call follow that telemetry reading?\" |\n\n#### MLM Details\n- 15% of tokens are selected for prediction\n- Of those: 80% replaced with [MASK], 10% replaced with random token, 10% kept unchanged\n- The model must predict the original token for all selected positions\n\n**What this means:** MLM forces BERT to build rich representations that capture meaning from both directions. Unlike GPT which only predicts forward, BERT sees the full picture.",
   "id": "cell-17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: BERT vs GPT Attention Patterns\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "tokens = [\"The\", \"cat\", \"[MASK]\", \"the\", \"mouse\"]\n",
    "n = len(tokens)\n",
    "\n",
    "# GPT: Causal mask (lower triangular)\n",
    "causal_mask = np.tril(np.ones((n, n)))\n",
    "im1 = axes[0].imshow(causal_mask, cmap='Oranges', vmin=0, vmax=1.5)\n",
    "axes[0].set_xticks(range(n))\n",
    "axes[0].set_yticks(range(n))\n",
    "axes[0].set_xticklabels(tokens, fontsize=10, rotation=45, ha='right')\n",
    "axes[0].set_yticklabels(tokens, fontsize=10)\n",
    "axes[0].set_title('GPT: Causal Attention\\n(left-to-right only)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Keys (attends to)', fontsize=11)\n",
    "axes[0].set_ylabel('Queries', fontsize=11)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        symbol = '\\u2713' if causal_mask[i, j] > 0 else ''\n",
    "        color = 'white' if causal_mask[i, j] > 0 else 'gray'\n",
    "        axes[0].text(j, i, symbol, ha='center', va='center', fontsize=14, \n",
    "                   color=color, fontweight='bold')\n",
    "\n",
    "# BERT: Full bidirectional attention\n",
    "full_mask = np.ones((n, n))\n",
    "im2 = axes[1].imshow(full_mask, cmap='Blues', vmin=0, vmax=1.5)\n",
    "axes[1].set_xticks(range(n))\n",
    "axes[1].set_yticks(range(n))\n",
    "axes[1].set_xticklabels(tokens, fontsize=10, rotation=45, ha='right')\n",
    "axes[1].set_yticklabels(tokens, fontsize=10)\n",
    "axes[1].set_title('BERT: Bidirectional Attention\\n(sees everything)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Keys (attends to)', fontsize=11)\n",
    "axes[1].set_ylabel('Queries', fontsize=11)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        axes[1].text(j, i, '\\u2713', ha='center', va='center', fontsize=14, \n",
    "                   color='white', fontweight='bold')\n",
    "\n",
    "# Side comparison diagram\n",
    "ax = axes[2]\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.set_title('Context Available for \"[MASK]\"', fontsize=13, fontweight='bold')\n",
    "\n",
    "# GPT context for position 2\n",
    "ax.add_patch(plt.Rectangle((0.5, 6), 9, 1.5, facecolor='#e67e22', alpha=0.2, \n",
    "             edgecolor='#e67e22', linewidth=2))\n",
    "ax.text(5, 7.2, 'GPT at position 3', ha='center', fontsize=12, fontweight='bold', color='#e67e22')\n",
    "ax.text(1.5, 6.5, '\"The\"  \"cat\"', ha='left', fontsize=11, color='#e67e22')\n",
    "ax.text(5.5, 6.5, '\\u2190 can only see these', ha='left', fontsize=10, color='#e67e22', style='italic')\n",
    "\n",
    "# BERT context for [MASK]\n",
    "ax.add_patch(plt.Rectangle((0.5, 3), 9, 1.5, facecolor='#2980b9', alpha=0.2, \n",
    "             edgecolor='#2980b9', linewidth=2))\n",
    "ax.text(5, 4.2, 'BERT at [MASK]', ha='center', fontsize=12, fontweight='bold', color='#2980b9')\n",
    "ax.text(1.2, 3.5, '\"The\" \"cat\" ... \"the\" \"mouse\"', ha='left', fontsize=11, color='#2980b9')\n",
    "ax.text(7, 3.5, '\\u2190 sees ALL', ha='left', fontsize=10, color='#2980b9', style='italic')\n",
    "\n",
    "# Verdict\n",
    "ax.text(5, 1.5, 'BERT: \"chased\" (confident)', ha='center', fontsize=12, \n",
    "       fontweight='bold', color='#2980b9')\n",
    "ax.text(5, 0.7, 'GPT: \"sat\"? \"ate\"? (uncertain)', ha='center', fontsize=12, \n",
    "       fontweight='bold', color='#e67e22')\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"BERT sees both left and right context => better understanding\")\n",
    "print(\"GPT sees only left context => suited for generation\")"
   ],
   "id": "cell-18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### GPT vs BERT: Complete Comparison\n\nThis is one of the most important architectural distinctions in modern NLP:\n\n| Aspect | GPT (Decoder-Only) | BERT (Encoder-Only) | F1 Parallel |\n|--------|--------------------|-----------------|-------------|\n| **Direction** | Left-to-right (autoregressive) | Bidirectional | Predicting next lap vs. analyzing full race |\n| **Attention mask** | Causal (triangular) | Full (no mask) | Only past events vs. full race context |\n| **Training objective** | Predict next token | Predict masked tokens + NSP | Predict next event vs. fill in hidden events |\n| **Pre-training task** | Language modeling | Masked language modeling | Race commentary vs. race analysis |\n| **Output** | Next-token probabilities | Contextualized embeddings | \"What happens next?\" vs. \"What does this data mean?\" |\n| **Generation** | Natural (sample next token) | Unnatural (not designed for it) | Generates commentary vs. not a generator |\n| **Understanding** | Limited (only left context) | Superior (full context) | Partial picture vs. full picture |\n| **Fine-tuning** | Prompt-based / instruction-tuning | Add classification head on [CLS] | Adapt via prompts vs. add decision layer |\n| **Parameters (base)** | GPT-2: 117M - 1.5B | BERT-base: 110M, BERT-large: 340M | -- |\n| **Best for** | Text generation, chatbots, coding | Classification, NER, QA, search | Commentary, prediction vs. classification, analysis |\n\n### When to Use Which?\n\n| Task | Best Architecture | Why | F1 Framing |\n|------|------------------|-----|------------|\n| Chatbot / dialogue | **GPT** | Needs to generate fluent responses | Race commentary / team radio generation |\n| Text classification | **BERT** | Needs to understand full document | \"Was this a good strategy?\" (yes/no) |\n| Code generation | **GPT** | Code is written left-to-right | Strategy algorithm generation |\n| Named entity recognition | **BERT** | Needs bidirectional context for each token | Identify drivers, teams, circuits in text |\n| Creative writing | **GPT** | Generation task | Generate race preview articles |\n| Semantic search | **BERT** | Needs rich sentence embeddings | Find similar race situations in the archive |\n| Translation | **Encoder-Decoder** | Structured input-to-output mapping | Telemetry-to-English translation |\n| Summarization | **GPT** or **Encoder-Decoder** | Generation with input understanding | Post-race summary generation |\n\n#### Key Insight\n\nThe trend in 2023-2024+ has been toward **large decoder-only models** (GPT-4, Claude, LLaMA, Gemini). It turns out that with enough scale and the right training, decoder-only models can match or exceed BERT-style models even on understanding tasks. But BERT-style models remain popular for efficient, focused applications where generation isn't needed.",
   "id": "cell-19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 5. Training a Small Language Model\n\n### Intuitive Explanation\n\nLet's put theory into practice by training a **character-level GPT** on a small dataset. Character-level means each token is a single character -- this keeps our vocabulary tiny and training fast, while still demonstrating all the core concepts.\n\n**F1 framing:** Think of this as training a tiny race commentator. We feed it examples of text, and it learns to predict what character comes next -- eventually generating coherent-looking sequences. With a small model and dataset, we will not get Brundle-quality commentary, but we will see the fundamental mechanism in action.\n\nWe'll:\n1. Prepare a simple text dataset\n2. Create character-level tokenization\n3. Train our MiniGPT model\n4. Generate text and explore temperature sampling",
   "id": "cell-20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data: simple repetitive patterns for fast learning\n",
    "training_text = \"\"\"\n",
    "the cat sat on the mat. the cat ran to the hat.\n",
    "the dog sat on the log. the dog ran to the fog.\n",
    "a big cat sat on a big mat. a small dog sat on a small log.\n",
    "the cat and the dog sat on the mat.\n",
    "the cat saw the dog. the dog saw the cat.\n",
    "\"\"\" * 100  # Repeat for more training data\n",
    "\n",
    "# Character-level tokenization\n",
    "chars = sorted(list(set(training_text)))\n",
    "vocab_size = len(chars)\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Encode the text\n",
    "def encode(text):\n",
    "    return [char_to_idx[ch] for ch in text]\n",
    "\n",
    "def decode(indices):\n",
    "    return ''.join([idx_to_char[i] for i in indices])\n",
    "\n",
    "data = torch.tensor(encode(training_text), dtype=torch.long)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Characters: {repr(''.join(chars))}\")\n",
    "print(f\"Total characters in training data: {len(data):,}\")\n",
    "print(f\"\\nExample encoding: 'cat' -> {encode('cat')}\")\n",
    "print(f\"Example decoding: {encode('cat')} -> '{decode(encode('cat'))}'\")"
   ],
   "id": "cell-21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader for training\n",
    "def get_batch(data, batch_size, block_size):\n",
    "    \"\"\"\n",
    "    Get a random batch of training examples.\n",
    "    \n",
    "    Args:\n",
    "        data: Full training data as tensor\n",
    "        batch_size: Number of sequences per batch\n",
    "        block_size: Length of each sequence\n",
    "    \n",
    "    Returns:\n",
    "        x: Input sequences (batch_size, block_size)\n",
    "        y: Target sequences (batch_size, block_size) - shifted by 1\n",
    "    \"\"\"\n",
    "    ix = torch.randint(len(data) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# Test the batch function\n",
    "x, y = get_batch(data, batch_size=4, block_size=16)\n",
    "print(f\"Input batch shape: {x.shape}\")\n",
    "print(f\"Target batch shape: {y.shape}\")\n",
    "print(f\"\\nExample input:  '{decode(x[0].tolist())}'\")\n",
    "print(f\"Example target: '{decode(y[0].tolist())}'\")\n",
    "print(\"\\nNotice: Target is the input shifted by one character (next-char prediction)\")"
   ],
   "id": "cell-22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Model hyperparameters\n",
    "block_size = 32\n",
    "batch_size = 64\n",
    "d_model = 64\n",
    "n_heads = 4\n",
    "n_layers = 3\n",
    "learning_rate = 3e-4\n",
    "n_steps = 2000\n",
    "\n",
    "# Create model\n",
    "model = MiniGPT(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_layers,\n",
    "    max_seq_len=block_size,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "samples_during_training = []\n",
    "\n",
    "print(f\"Training MiniGPT ({sum(p.numel() for p in model.parameters()):,} parameters)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for step in range(n_steps):\n",
    "    # Get batch\n",
    "    x, y = get_batch(data, batch_size, block_size)\n",
    "    \n",
    "    # Forward pass\n",
    "    logits, loss = model(x, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Log progress\n",
    "    if step % 400 == 0 or step == n_steps - 1:\n",
    "        # Generate a sample\n",
    "        model.eval()\n",
    "        prompt = encode(\"the \")\n",
    "        prompt_tensor = torch.tensor([prompt], dtype=torch.long)\n",
    "        generated = model.generate(prompt_tensor, max_new_tokens=40, temperature=0.8)\n",
    "        sample = decode(generated[0].tolist())\n",
    "        samples_during_training.append((step, loss.item(), sample))\n",
    "        model.train()\n",
    "        \n",
    "        print(f\"Step {step:4d} | Loss: {loss.item():.4f} | Sample: '{sample[:50]}...'\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ],
   "id": "cell-23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(losses, alpha=0.3, color='blue', label='Raw loss')\n",
    "# Smoothed loss\n",
    "window = 50\n",
    "smoothed = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "axes[0].plot(range(window-1, len(losses)), smoothed, color='red', linewidth=2, label='Smoothed')\n",
    "axes[0].set_xlabel('Training Step', fontsize=12)\n",
    "axes[0].set_ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Generation quality over time\n",
    "ax = axes[1]\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Generation Quality During Training', fontsize=14, fontweight='bold')\n",
    "\n",
    "y_pos = 0.9\n",
    "for step, loss, sample in samples_during_training:\n",
    "    # Color based on loss\n",
    "    color = plt.cm.RdYlGn(1 - min(loss / 3, 1))\n",
    "    truncated = sample[:45] + '...' if len(sample) > 45 else sample\n",
    "    ax.text(0.02, y_pos, f\"Step {step}:\", fontsize=10, fontweight='bold', \n",
    "           transform=ax.transAxes)\n",
    "    ax.text(0.15, y_pos, f\"'{truncated}'\", fontsize=9, \n",
    "           color=color, transform=ax.transAxes, family='monospace')\n",
    "    y_pos -= 0.15\n",
    "\n",
    "ax.text(0.02, 0.05, \"Green = low loss (good), Red = high loss (bad)\", \n",
    "       fontsize=10, style='italic', transform=ax.transAxes)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Watch how generation quality improves as loss decreases!\")"
   ],
   "id": "cell-24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Temperature Sampling\n\n**Temperature** controls the randomness of generation:\n\n$$P(token) = \\frac{\\exp(logit_i / T)}{\\sum_j \\exp(logit_j / T)}$$\n\n| Temperature | Effect | Use Case | F1 Parallel |\n|-------------|--------|----------|-------------|\n| T < 1.0 | Sharper distribution, more deterministic | Factual answers, code | Conservative strategy: \"Stay on plan, no surprises\" |\n| T = 1.0 | Original distribution | General use | Balanced strategy: weigh all options fairly |\n| T > 1.0 | Flatter distribution, more random | Creative writing | Aggressive strategy: \"Consider the unlikely -- maybe a 3-stop works?\" |\n\n**What this means:** Low temperature makes the model \"confident\" (picks high-probability tokens), while high temperature makes it \"creative\" (considers unlikely tokens).\n\n**F1 analogy:** Temperature is like the aggressiveness dial on a strategy computer. Low temperature (conservative) = the strategy sticks to the obvious call, like a safe one-stop. High temperature (aggressive) = the strategy considers wild alternatives, like an early pit under a virtual safety car or switching to inters on a drying track. Sometimes the aggressive call wins the race.",
   "id": "cell-25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Temperature effect on probability distribution\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# Simulated logits for next character\n",
    "logits = np.array([2.5, 2.0, 1.5, 0.5, 0.0, -0.5, -1.0, -2.0])\n",
    "chars_viz = ['a', 't', ' ', 'e', 'o', 'n', 's', 'd']\n",
    "\n",
    "temperatures = [0.3, 0.7, 1.0, 2.0]\n",
    "\n",
    "for ax, temp in zip(axes, temperatures):\n",
    "    # Apply temperature and softmax\n",
    "    scaled = logits / temp\n",
    "    probs = np.exp(scaled) / np.sum(np.exp(scaled))\n",
    "    \n",
    "    colors = plt.cm.Blues(probs / max(probs))\n",
    "    bars = ax.bar(chars_viz, probs, color=colors, edgecolor='black')\n",
    "    \n",
    "    ax.set_ylim(0, 0.8)\n",
    "    ax.set_xlabel('Character', fontsize=11)\n",
    "    ax.set_ylabel('Probability' if temp == 0.3 else '', fontsize=11)\n",
    "    ax.set_title(f'T = {temp}', fontsize=13, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Annotate top prob\n",
    "    max_idx = np.argmax(probs)\n",
    "    ax.text(max_idx, probs[max_idx] + 0.02, f'{probs[max_idx]:.2f}', \n",
    "           ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "fig.suptitle('Effect of Temperature on Token Probability Distribution', \n",
    "            fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"T=0.3: Very peaked (deterministic) -> always picks 'a'\")\n",
    "print(\"T=2.0: Very flat (random) -> might pick any character\")"
   ],
   "id": "cell-26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate at different temperatures\n",
    "model.eval()\n",
    "\n",
    "prompt = \"the cat \"\n",
    "prompt_tokens = torch.tensor([encode(prompt)], dtype=torch.long)\n",
    "\n",
    "print(\"Generating from prompt: '\" + prompt + \"'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for temp in [0.3, 0.5, 0.8, 1.0, 1.5]:\n",
    "    print(f\"\\nTemperature = {temp}:\")\n",
    "    for i in range(3):\n",
    "        generated = model.generate(prompt_tokens.clone(), max_new_tokens=50, temperature=temp)\n",
    "        text = decode(generated[0].tolist())\n",
    "        print(f\"  {i+1}. '{text}'\")"
   ],
   "id": "cell-27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 6. Scaling Laws\n\n### Intuitive Explanation\n\nOne of the most remarkable discoveries in deep learning is that language model performance follows **predictable scaling laws**. As you increase model size, training data, or compute, performance improves in a smooth, predictable way.\n\nThe key insight from OpenAI and DeepMind research:\n\n$$L(N, D, C) \\approx \\left(\\frac{N_c}{N}\\right)^{\\alpha_N} + \\left(\\frac{D_c}{D}\\right)^{\\alpha_D} + \\left(\\frac{C_c}{C}\\right)^{\\alpha_C}$$\n\nWhere:\n- $L$ = Loss (lower is better)\n- $N$ = Number of parameters\n- $D$ = Dataset size (tokens)\n- $C$ = Compute (FLOPs)\n\n**What this means:** If you want a 10x better model, you can predict exactly how much bigger it needs to be, how much more data you need, or how much more compute you need.\n\n**F1 analogy:** Scaling laws in AI are like development budgets in F1. The relationship between spending and performance is remarkably predictable: more wind tunnel hours, more CFD simulations, more development tokens all improve lap time in a smooth, power-law fashion. Just as F1 teams can predict \"X million in aero development yields Y tenths per lap,\" AI labs can predict \"X more compute yields Y reduction in loss.\" The cost cap in F1 is essentially a compute budget constraint.",
   "id": "cell-28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Scaling laws\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Parameters scaling\n",
    "params = np.logspace(6, 12, 100)  # 1M to 1T\n",
    "loss_params = 10 * (params / 1e6) ** (-0.076)  # Approximate scaling\n",
    "\n",
    "axes[0].loglog(params, loss_params, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Parameters', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Scaling with Model Size', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Mark some famous models\n",
    "models_p = [\n",
    "    ('GPT-2', 1.5e9, 3.5),\n",
    "    ('GPT-3', 175e9, 2.0),\n",
    "    ('GPT-4', 1.8e12, 1.2),\n",
    "]\n",
    "for name, size, loss in models_p:\n",
    "    axes[0].scatter([size], [loss], s=100, zorder=5, edgecolors='black')\n",
    "    axes[0].annotate(name, (size, loss), xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "# Data scaling\n",
    "data_size = np.logspace(9, 13, 100)  # 1B to 10T tokens\n",
    "loss_data = 8 * (data_size / 1e9) ** (-0.095)\n",
    "\n",
    "axes[1].loglog(data_size, loss_data, 'g-', linewidth=2)\n",
    "axes[1].set_xlabel('Training Tokens', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].set_title('Scaling with Dataset Size', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Compute scaling\n",
    "compute = np.logspace(17, 25, 100)  # FLOPs\n",
    "loss_compute = 20 * (compute / 1e17) ** (-0.05)\n",
    "\n",
    "axes[2].loglog(compute, loss_compute, 'r-', linewidth=2)\n",
    "axes[2].set_xlabel('Compute (FLOPs)', fontsize=12)\n",
    "axes[2].set_ylabel('Loss', fontsize=12)\n",
    "axes[2].set_title('Scaling with Compute', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "fig.suptitle('Neural Scaling Laws: Predictable Performance Improvements', \n",
    "            fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: Performance improves predictably on log-log scale\")\n",
    "print(\"This allows researchers to plan training runs and predict outcomes\")"
   ],
   "id": "cell-29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline of model sizes\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "models = [\n",
    "    (2018.5, 'GPT-1', 0.117, 'OpenAI'),\n",
    "    (2018.8, 'BERT', 0.34, 'Google'),\n",
    "    (2019.2, 'GPT-2', 1.5, 'OpenAI'),\n",
    "    (2020.5, 'GPT-3', 175, 'OpenAI'),\n",
    "    (2022.0, 'PaLM', 540, 'Google'),\n",
    "    (2022.3, 'Chinchilla', 70, 'DeepMind'),\n",
    "    (2023.0, 'GPT-4', 1800, 'OpenAI'),\n",
    "    (2023.2, 'LLaMA', 65, 'Meta'),\n",
    "    (2023.7, 'Llama-2', 70, 'Meta'),\n",
    "    (2024.0, 'Gemini Ultra', 1500, 'Google'),\n",
    "]\n",
    "\n",
    "colors = {'OpenAI': '#2ecc71', 'Google': '#3498db', 'DeepMind': '#9b59b6', 'Meta': '#e74c3c'}\n",
    "\n",
    "for year, name, params, company in models:\n",
    "    ax.scatter([year], [params], s=200, c=colors[company], alpha=0.7, \n",
    "              edgecolors='black', linewidth=2, zorder=5)\n",
    "    offset = 15 if params < 100 else -25\n",
    "    ax.annotate(f'{name}\\n({params}B)', (year, params), \n",
    "               xytext=(0, offset), textcoords='offset points',\n",
    "               ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Year', fontsize=12)\n",
    "ax.set_ylabel('Parameters (Billions)', fontsize=12)\n",
    "ax.set_title('The Race to Scale: Language Model Parameter Growth', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, which='both')\n",
    "ax.set_xlim(2018, 2024.5)\n",
    "\n",
    "# Legend\n",
    "for company, color in colors.items():\n",
    "    ax.scatter([], [], c=color, s=100, label=company, edgecolors='black')\n",
    "ax.legend(loc='upper left', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"From 117M (GPT-1) to 1.8T (GPT-4) in 5 years: 15,000x increase!\")\n",
    "print(\"\\nBut Chinchilla showed: it's not just about size, data matters too\")"
   ],
   "id": "cell-30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Key Scaling Insights\n\n| Discovery | Implication | F1 Parallel |\n|-----------|------------|-------------|\n| **Power law scaling** | Can predict performance before training | Predict lap time improvement from development spend |\n| **Compute-optimal training** | Balance model size and data (Chinchilla) | Balance aero and PU development budgets |\n| **Emergent abilities** | Some capabilities appear suddenly at scale | Sudden breakthrough in car concept (e.g., double diffuser) |\n| **Diminishing returns** | Each 10x costs more for same improvement | Last tenth of a second costs more than the first |\n| **Architecture matters less** | At scale, different architectures converge | At the front, all cars converge on similar concepts |\n\n### The Chinchilla Finding\n\nDeepMind's Chinchilla paper (2022) showed that many models were **undertrained** -- they had too many parameters for the amount of data they saw. The optimal ratio is roughly:\n\n$$\\text{Tokens} \\approx 20 \\times \\text{Parameters}$$\n\n**What this means:** A 70B parameter model trained on 1.4T tokens (Chinchilla) outperformed the 280B parameter Gopher trained on 300B tokens. **More data can be better than more parameters.**\n\n**F1 analogy:** This is like discovering that a team with a smaller budget but more testing days beats a team with a bigger budget but fewer track sessions. The car is only as good as the data used to develop it. Chinchilla showed that \"testing\" (training data) matters at least as much as \"car complexity\" (parameters).",
   "id": "cell-31"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercises\n\n### Exercise 1: Extend BPE with Encoding\n\nAdd an `encode()` method to our SimpleBPE class that returns token indices.\n\n**F1 framing:** Build a radio message encoder. Given the BPE vocabulary learned from race communications, encode any new team radio message into its token sequence -- turning \"Box box box, switch to mediums\" into a sequence of integer IDs the strategy computer can process.",
   "id": "cell-32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Add encode method to SimpleBPE\n",
    "def bpe_encode(self, text):\n",
    "    \"\"\"\n",
    "    Tokenize and convert to indices.\n",
    "    \n",
    "    Args:\n",
    "        text: Input string\n",
    "        \n",
    "    Returns:\n",
    "        List of token indices\n",
    "    \"\"\"\n",
    "    # TODO: Implement this!\n",
    "    # 1. Call self.tokenize(text) to get tokens\n",
    "    # 2. Convert each token to its index using self.vocab\n",
    "    # Hint: Handle unknown tokens gracefully\n",
    "    pass\n",
    "\n",
    "# Test:\n",
    "# SimpleBPE.encode = bpe_encode\n",
    "# indices = bpe.encode(\"the cat sat\")\n",
    "# print(f\"Encoded: {indices}\")"
   ],
   "id": "cell-33"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2: Top-p (Nucleus) Sampling\n\nImplement top-p sampling, which samples from the smallest set of tokens whose cumulative probability exceeds p.\n\n**F1 framing:** Top-p sampling is like a strategy engineer who considers only the most likely scenarios until they cover, say, 90% of probable outcomes. With p=0.9, if \"pit on this lap\" (60%) and \"pit next lap\" (25%) and \"stay out\" (10%) cover 95%, the engineer ignores the remaining 5% of wild scenarios. Implement this \"nucleus\" of likely strategy options.",
   "id": "cell-34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Implement top-p sampling\n",
    "def sample_top_p(logits, p=0.9):\n",
    "    \"\"\"\n",
    "    Sample from the nucleus (top-p) of the distribution.\n",
    "    \n",
    "    Args:\n",
    "        logits: Raw model outputs (vocab_size,)\n",
    "        p: Cumulative probability threshold\n",
    "        \n",
    "    Returns:\n",
    "        Sampled token index\n",
    "    \"\"\"\n",
    "    # TODO: Implement this!\n",
    "    # 1. Convert logits to probabilities with softmax\n",
    "    # 2. Sort probabilities in descending order\n",
    "    # 3. Compute cumulative sum\n",
    "    # 4. Find cutoff where cumsum > p\n",
    "    # 5. Zero out probabilities below cutoff\n",
    "    # 6. Renormalize and sample\n",
    "    pass\n",
    "\n",
    "# Test:\n",
    "# logits = torch.randn(100)\n",
    "# sampled = sample_top_p(logits, p=0.9)\n",
    "# print(f\"Sampled token: {sampled}\")"
   ],
   "id": "cell-35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 3: Calculate Perplexity\n\nPerplexity is the standard metric for language models: $PPL = \\exp(\\text{average cross-entropy loss})$\n\n**F1 framing:** Perplexity measures how \"surprised\" the model is by the actual sequence of events. A race predictor with perplexity 10 on a race means it was, on average, as uncertain as choosing between 10 equally likely next events. A perplexity of 2 means the model is almost always choosing between just 2 options -- much better calibrated. Calculate this metric for our trained model.",
   "id": "cell-36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Calculate perplexity\n",
    "def calculate_perplexity(model, text, encode_fn, block_size=32):\n",
    "    \"\"\"\n",
    "    Calculate perplexity of a model on given text.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained language model\n",
    "        text: Text string to evaluate\n",
    "        encode_fn: Function to convert text to token indices\n",
    "        block_size: Sequence length for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        Perplexity value (lower is better)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this!\n",
    "    # 1. Encode the text\n",
    "    # 2. Split into sequences of block_size\n",
    "    # 3. Compute average loss over all sequences\n",
    "    # 4. Return exp(average_loss)\n",
    "    pass\n",
    "\n",
    "# Test:\n",
    "# ppl = calculate_perplexity(model, \"the cat sat on the mat\", encode)\n",
    "# print(f\"Perplexity: {ppl:.2f}\")"
   ],
   "id": "cell-37"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Summary\n\n### Key Concepts\n\n- **Language Model**: Predicts $P(w_t | w_1, ..., w_{t-1})$ -- the probability of the next token given context\n- **Tokenization**: Breaking text into discrete units; BPE is the dominant modern approach\n- **GPT (Decoder-Only)**: Left-to-right generation with causal masking\n- **BERT (Encoder-Only)**: Bidirectional understanding with masked language modeling\n- **Temperature**: Controls randomness in generation (low = deterministic, high = creative)\n- **Scaling Laws**: Performance improves predictably with parameters, data, and compute\n\n### Connection to Deep Learning\n\n| Concept | Application | F1 Parallel |\n|---------|------------|-------------|\n| Next-token prediction | Foundation of ChatGPT, Claude, and all modern LLMs | Predicting the next event in a race sequence |\n| BPE tokenization | Used by GPT-4, LLaMA, and most production models | Breaking radio messages into meaningful units |\n| Causal masking | Enables autoregressive text generation | Real-time decisions based only on past events |\n| Bidirectional attention | Powers search engines and classification systems | Full race situation analysis (BERT-style understanding) |\n| Temperature sampling | Controls creativity in AI writing assistants | Conservative vs. aggressive strategy dial |\n| Scaling laws | Guide billion-dollar training decisions | Development budget allocation in F1 |\n\n### Checklist\n\n- [ ] I can explain why next-token prediction leads to general intelligence\n- [ ] I understand how BPE tokenization works and why it's used\n- [ ] I can implement a simple GPT model from scratch\n- [ ] I know when to use GPT vs BERT for different tasks\n- [ ] I understand how temperature affects generation\n- [ ] I can explain the key findings of scaling laws research\n\n---\n\n## Next Steps\n\nNow that you understand language model architectures, the next notebook covers **Embeddings** -- how to represent words, sentences, and concepts as vectors that capture meaning, and how these representations power search, recommendation, and RAG systems.",
   "id": "cell-38"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}