{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.3: Probability & Statistics for Deep Learning\n",
    "\n",
    "Probability and statistics are essential for understanding:\n",
    "- How models make predictions (probabilistic outputs)\n",
    "- How we train models (maximum likelihood)\n",
    "- How we measure uncertainty and information\n",
    "\n",
    "## Learning Objectives\n",
    "- [ ] Work with common probability distributions\n",
    "- [ ] Apply Bayes' theorem to update beliefs\n",
    "- [ ] Derive MLE estimators for simple distributions\n",
    "- [ ] Calculate entropy and KL divergence\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.special import comb\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Probability Basics\n",
    "\n",
    "### Random Variables\n",
    "\n",
    "A **random variable** is a variable whose value is determined by a random process.\n",
    "\n",
    "- **Discrete**: Takes on countable values (e.g., coin flips, dice rolls)\n",
    "- **Continuous**: Takes on any value in a range (e.g., height, temperature)\n",
    "\n",
    "### Probability Distributions\n",
    "\n",
    "A **probability distribution** describes the likelihood of each possible outcome.\n",
    "\n",
    "- **PMF** (Probability Mass Function): For discrete variables, $P(X = x)$\n",
    "- **PDF** (Probability Density Function): For continuous variables, $f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: What is a Probability Distribution?\n\nA probability distribution answers a fundamental question: **\"What outcomes are possible, and how likely is each one?\"**\n\nThink of it as a complete recipe for uncertainty:\n- It lists every possible outcome\n- It assigns a probability (or density) to each outcome\n- All probabilities sum to 1 (something must happen!)\n\n**The Key Insight**: A distribution captures *everything* we know about a random process. Once you have the distribution, you can compute any probability, expectation, or uncertainty measure.\n\n#### Discrete vs Continuous Distributions\n\n| Aspect | Discrete | Continuous |\n|--------|----------|------------|\n| **Possible values** | Countable (finite or infinite) | Uncountable (any value in a range) |\n| **Probability function** | PMF: P(X = x) gives exact probability | PDF: f(x) gives density, not probability |\n| **Finding probabilities** | Sum: P(a ≤ X ≤ b) = Σ P(X = x) | Integrate: P(a ≤ X ≤ b) = ∫f(x)dx |\n| **Examples** | Coin flips, dice, word counts | Height, temperature, neural network weights |\n| **ML applications** | Classification labels, token IDs | Regression targets, latent variables |\n\n**Important**: For continuous distributions, P(X = x) = 0 for any specific value! We can only ask about ranges.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Common Distributions\n",
    "\n",
    "### 2.1 Bernoulli Distribution\n",
    "\n",
    "Models a single binary outcome (success/failure, yes/no, 1/0).\n",
    "\n",
    "$$P(X = 1) = p, \\quad P(X = 0) = 1 - p$$\n",
    "\n",
    "**In ML**: Binary classification outputs, dropout masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli distribution\n",
    "p = 0.7  # Probability of success\n",
    "\n",
    "# Generate samples\n",
    "samples = np.random.binomial(1, p, size=1000)\n",
    "\n",
    "print(f\"Bernoulli(p={p})\")\n",
    "print(f\"Mean (theoretical): {p}\")\n",
    "print(f\"Mean (empirical): {samples.mean():.3f}\")\n",
    "print(f\"Variance (theoretical): {p * (1-p):.3f}\")\n",
    "print(f\"Variance (empirical): {samples.var():.3f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar([0, 1], [1-p, p], width=0.4, alpha=0.7)\n",
    "plt.xticks([0, 1], ['Failure (0)', 'Success (1)'])\n",
    "plt.ylabel('Probability')\n",
    "plt.title(f'Bernoulli Distribution (p={p})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Binomial Distribution\n",
    "\n",
    "Number of successes in $n$ independent Bernoulli trials.\n",
    "\n",
    "$$P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}$$\n",
    "\n",
    "**In ML**: Counting successes in multiple trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binomial distribution\n",
    "n = 20  # Number of trials\n",
    "p = 0.3  # Probability of success\n",
    "\n",
    "# PMF\n",
    "k = np.arange(0, n+1)\n",
    "pmf = stats.binom.pmf(k, n, p)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(k, pmf, alpha=0.7)\n",
    "plt.xlabel('Number of Successes (k)')\n",
    "plt.ylabel('P(X = k)')\n",
    "plt.title(f'Binomial Distribution (n={n}, p={p})')\n",
    "plt.axvline(x=n*p, color='red', linestyle='--', label=f'Mean = np = {n*p}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean: E[X] = np = {n*p}\")\n",
    "print(f\"Variance: Var[X] = np(1-p) = {n*p*(1-p):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Categorical Distribution\n",
    "\n",
    "Generalization of Bernoulli to $K$ categories.\n",
    "\n",
    "$$P(X = k) = p_k, \\quad \\sum_{k=1}^K p_k = 1$$\n",
    "\n",
    "**In ML**: Multi-class classification (softmax output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical distribution (e.g., softmax output)\n",
    "categories = ['Cat', 'Dog', 'Bird', 'Fish']\n",
    "probabilities = [0.4, 0.35, 0.15, 0.1]\n",
    "\n",
    "# Generate samples\n",
    "samples = np.random.choice(len(categories), size=1000, p=probabilities)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(categories, probabilities, alpha=0.7, color='steelblue')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Categorical Distribution (True)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "empirical = [np.mean(samples == i) for i in range(len(categories))]\n",
    "plt.bar(categories, empirical, alpha=0.7, color='coral')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Empirical Distribution (1000 samples)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Gaussian (Normal) Distribution\n",
    "\n",
    "The most important continuous distribution.\n",
    "\n",
    "$$f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "**In ML**: \n",
    "- Weight initialization\n",
    "- Noise in VAEs\n",
    "- Regression targets\n",
    "- Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Different means\n",
    "x = np.linspace(-6, 10, 200)\n",
    "for mu in [-2, 0, 2, 4]:\n",
    "    axes[0].plot(x, stats.norm.pdf(x, mu, 1), label=f'μ={mu}, σ=1')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('Effect of Mean (μ)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Different standard deviations\n",
    "x = np.linspace(-8, 8, 200)\n",
    "for sigma in [0.5, 1, 2, 3]:\n",
    "    axes[1].plot(x, stats.norm.pdf(x, 0, sigma), label=f'μ=0, σ={sigma}')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].set_title('Effect of Standard Deviation (σ)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 68-95-99.7 rule\n",
    "mu, sigma = 0, 1\n",
    "x = np.linspace(-4, 4, 200)\n",
    "y = stats.norm.pdf(x, mu, sigma)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, 'b-', linewidth=2)\n",
    "\n",
    "# Fill regions\n",
    "plt.fill_between(x, y, where=(x >= -3) & (x <= 3), alpha=0.2, color='blue', label='99.7% (±3σ)')\n",
    "plt.fill_between(x, y, where=(x >= -2) & (x <= 2), alpha=0.3, color='blue', label='95% (±2σ)')\n",
    "plt.fill_between(x, y, where=(x >= -1) & (x <= 1), alpha=0.4, color='blue', label='68% (±1σ)')\n",
    "\n",
    "plt.xlabel('x (in standard deviations)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Standard Normal Distribution - The 68-95-99.7 Rule')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Verify with scipy\n",
    "print(\"Probability within:\")\n",
    "print(f\"  ±1σ: {stats.norm.cdf(1) - stats.norm.cdf(-1):.4f} (68.27%)\")\n",
    "print(f\"  ±2σ: {stats.norm.cdf(2) - stats.norm.cdf(-2):.4f} (95.45%)\")\n",
    "print(f\"  ±3σ: {stats.norm.cdf(3) - stats.norm.cdf(-3):.4f} (99.73%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Multivariate Gaussian\n",
    "\n",
    "Extension to multiple dimensions:\n",
    "\n",
    "$$f(\\mathbf{x}) = \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T \\Sigma^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})\\right)$$\n",
    "\n",
    "Where:\n",
    "- $\\boldsymbol{\\mu}$: Mean vector\n",
    "- $\\Sigma$: Covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Choosing the Right Distribution: A Decision Guide\n\n| Distribution | Use When | Parameters | Example in ML |\n|--------------|----------|------------|---------------|\n| **Bernoulli** | Single yes/no outcome | p (success probability) | Binary classification output, dropout mask |\n| **Binomial** | Count of successes in n trials | n (trials), p (success prob) | Number of correct predictions in batch |\n| **Categorical** | Single choice from K options | p₁, p₂, ..., pₖ (probabilities) | Softmax output, token prediction |\n| **Multinomial** | Counts across K categories | n (trials), p₁...pₖ | Word counts in document (bag of words) |\n| **Gaussian** | Continuous value, symmetric uncertainty | μ (mean), σ (std dev) | Regression targets, weight initialization |\n| **Multivariate Gaussian** | Multiple correlated continuous values | μ (mean vector), Σ (covariance) | VAE latent space, GP predictions |\n\n**The Pattern**: \n- Bernoulli/Binomial are for binary outcomes (yes/no)\n- Categorical/Multinomial are for multi-class outcomes  \n- Gaussian is for continuous outcomes with symmetric uncertainty\n\n**Key ML Connection**: The distribution you choose for your model's output determines your loss function:\n- Categorical output → Cross-entropy loss\n- Gaussian output → MSE loss (equivalent to assuming Gaussian noise)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Gaussian with different covariance structures\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Generate grid for contour plots\n",
    "x = np.linspace(-4, 4, 100)\n",
    "y = np.linspace(-4, 4, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "pos = np.dstack((X, Y))\n",
    "\n",
    "# Different covariance matrices\n",
    "covariances = [\n",
    "    (np.array([[1, 0], [0, 1]]), 'Spherical\\n(Independent)'),\n",
    "    (np.array([[2, 0], [0, 0.5]]), 'Diagonal\\n(Different variances)'),\n",
    "    (np.array([[1, 0.8], [0.8, 1]]), 'Full\\n(Correlated)')\n",
    "]\n",
    "\n",
    "mean = np.array([0, 0])\n",
    "\n",
    "for ax, (cov, title) in zip(axes, covariances):\n",
    "    rv = stats.multivariate_normal(mean, cov)\n",
    "    Z = rv.pdf(pos)\n",
    "    \n",
    "    ax.contour(X, Y, Z, levels=10, cmap='viridis')\n",
    "    \n",
    "    # Draw samples\n",
    "    samples = rv.rvs(size=200)\n",
    "    ax.scatter(samples[:, 0], samples[:, 1], alpha=0.3, s=10, color='red')\n",
    "    \n",
    "    ax.set_xlabel('x₁')\n",
    "    ax.set_ylabel('x₂')\n",
    "    ax.set_title(f'{title}\\nΣ = {cov.tolist()}')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim(-4, 4)\n",
    "    ax.set_ylim(-4, 4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Expected Value and Variance\n",
    "\n",
    "### Expected Value (Mean)\n",
    "\n",
    "The \"average\" outcome weighted by probability:\n",
    "\n",
    "- Discrete: $E[X] = \\sum_x x \\cdot P(X = x)$\n",
    "- Continuous: $E[X] = \\int x \\cdot f(x) dx$\n",
    "\n",
    "### Variance\n",
    "\n",
    "Measures spread around the mean:\n",
    "\n",
    "$$\\text{Var}(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: Understanding Each Term in Bayes' Theorem\n\n$$P(\\text{hypothesis}|\\text{data}) = \\frac{P(\\text{data}|\\text{hypothesis}) \\cdot P(\\text{hypothesis})}{P(\\text{data})}$$\n\nLet's break down what each term really means:\n\n| Term | Name | Meaning | Example (Disease Testing) |\n|------|------|---------|---------------------------|\n| **P(H)** | Prior | Your belief *before* seeing any evidence | 1% of population has disease |\n| **P(D\\|H)** | Likelihood | How probable is this evidence *if* hypothesis is true? | 95% chance of positive test *if* you have disease |\n| **P(D)** | Evidence (Marginal) | Total probability of seeing this evidence | Overall rate of positive tests |\n| **P(H\\|D)** | Posterior | Updated belief *after* seeing evidence | Probability you have disease *given* positive test |\n\n**The Core Insight**: Bayes' theorem is a *belief update* mechanism:\n```\nNew Belief = (How well evidence supports hypothesis) × (Old Belief) / (How common is this evidence)\n```\n\n**Why the denominator matters**: P(D) normalizes everything. If positive tests are common (many false positives), a positive test is less informative.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visual: How Bayes' Theorem Updates Beliefs\n# Let's visualize the belief update process step by step\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Setup\nP_disease = 0.01\nP_positive_given_disease = 0.95      # True positive rate\nP_positive_given_no_disease = 0.05   # False positive rate\n\n# Imagine 10,000 people\nn_people = 10000\nn_sick = int(n_people * P_disease)\nn_healthy = n_people - n_sick\n\n# Among sick people\nsick_test_positive = int(n_sick * P_positive_given_disease)\nsick_test_negative = n_sick - sick_test_positive\n\n# Among healthy people  \nhealthy_test_positive = int(n_healthy * P_positive_given_no_disease)\nhealthy_test_negative = n_healthy - healthy_test_positive\n\n# Plot 1: Prior - Population breakdown\nax = axes[0, 0]\nax.bar(['Sick', 'Healthy'], [n_sick, n_healthy], color=['red', 'green'], alpha=0.7)\nax.set_ylabel('Number of People')\nax.set_title(f'Step 1: PRIOR\\n{n_people:,} people: {n_sick} sick (1%), {n_healthy} healthy (99%)')\nax.set_ylim(0, n_people * 1.1)\nfor i, v in enumerate([n_sick, n_healthy]):\n    ax.text(i, v + 200, str(v), ha='center', fontweight='bold')\n\n# Plot 2: Likelihood - Test results by group\nax = axes[0, 1]\nx = np.arange(2)\nwidth = 0.35\nbars1 = ax.bar(x - width/2, [sick_test_positive, healthy_test_positive], width, \n               label='Test Positive', color='orange', alpha=0.7)\nbars2 = ax.bar(x + width/2, [sick_test_negative, healthy_test_negative], width,\n               label='Test Negative', color='blue', alpha=0.7)\nax.set_xticks(x)\nax.set_xticklabels(['Sick (100)', 'Healthy (9900)'])\nax.set_ylabel('Number of People')\nax.set_title('Step 2: LIKELIHOOD\\nHow the test performs on each group')\nax.legend()\n\n# Plot 3: Evidence - All positive tests\nax = axes[1, 0]\nax.bar(['True Positives\\n(Sick + Positive)', 'False Positives\\n(Healthy + Positive)'], \n       [sick_test_positive, healthy_test_positive], \n       color=['red', 'green'], alpha=0.7)\ntotal_positive = sick_test_positive + healthy_test_positive\nax.set_ylabel('Number of People')\nax.set_title(f'Step 3: EVIDENCE\\nAll positive tests: {total_positive} total\\n'\n             f'P(positive) = {total_positive/n_people:.2%}')\nfor i, v in enumerate([sick_test_positive, healthy_test_positive]):\n    ax.text(i, v + 10, str(v), ha='center', fontweight='bold')\n\n# Plot 4: Posterior - Among positive tests, who is actually sick?\nax = axes[1, 1]\nposterior = sick_test_positive / total_positive\nax.bar(['Actually Sick', 'Actually Healthy'], \n       [sick_test_positive, healthy_test_positive],\n       color=['red', 'green'], alpha=0.7)\nax.set_ylabel('Number of People (with positive test)')\nax.set_title(f'Step 4: POSTERIOR\\nAmong {total_positive} positive tests:\\n'\n             f'P(sick|positive) = {sick_test_positive}/{total_positive} = {posterior:.1%}')\nfor i, v in enumerate([sick_test_positive, healthy_test_positive]):\n    pct = v / total_positive * 100\n    ax.text(i, v + 10, f'{v} ({pct:.1f}%)', ha='center', fontweight='bold')\n\nplt.tight_layout()\nplt.suptitle('Bayes Theorem: Why a 95% Accurate Test Gives Only 16% Confidence', \n             fontsize=14, fontweight='bold', y=1.02)\nplt.show()\n\nprint(\"\\nThe Counterintuitive Result Explained:\")\nprint(\"=\" * 50)\nprint(f\"Even though the test is 95% accurate:\")\nprint(f\"  - Out of {n_sick} sick people: {sick_test_positive} test positive\")\nprint(f\"  - Out of {n_healthy} healthy people: {healthy_test_positive} ALSO test positive (false positives)\")\nprint(f\"\\nTotal positive tests: {total_positive}\")\nprint(f\"True positives: {sick_test_positive} ({sick_test_positive/total_positive:.1%})\")\nprint(f\"False positives: {healthy_test_positive} ({healthy_test_positive/total_positive:.1%})\")\nprint(f\"\\nThe false positives OVERWHELM the true positives because\")\nprint(f\"healthy people vastly outnumber sick people!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing expected value for a discrete distribution\n",
    "# Example: Unfair die\n",
    "outcomes = np.array([1, 2, 3, 4, 5, 6])\n",
    "probabilities = np.array([0.1, 0.1, 0.1, 0.2, 0.2, 0.3])  # Biased toward higher numbers\n",
    "\n",
    "# Expected value\n",
    "expected_value = np.sum(outcomes * probabilities)\n",
    "print(f\"E[X] = Σ x·P(x) = {expected_value}\")\n",
    "\n",
    "# Variance\n",
    "variance = np.sum((outcomes - expected_value)**2 * probabilities)\n",
    "print(f\"Var(X) = E[(X - E[X])²] = {variance:.4f}\")\n",
    "print(f\"Std(X) = √Var(X) = {np.sqrt(variance):.4f}\")\n",
    "\n",
    "# Verify with sampling\n",
    "samples = np.random.choice(outcomes, size=10000, p=probabilities)\n",
    "print(f\"\\nEmpirical mean: {samples.mean():.4f}\")\n",
    "print(f\"Empirical variance: {samples.var():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Bayes' Theorem\n",
    "\n",
    "Bayes' theorem tells us how to update beliefs given new evidence:\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\n",
    "\n",
    "In ML terms:\n",
    "\n",
    "$$P(\\text{hypothesis}|\\text{data}) = \\frac{P(\\text{data}|\\text{hypothesis}) \\cdot P(\\text{hypothesis})}{P(\\text{data})}$$\n",
    "\n",
    "Or:\n",
    "\n",
    "$$\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Evidence}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Bayes' Theorem in Machine Learning\n\nBayesian thinking is fundamental to many ML techniques:\n\n| Application | Prior P(H) | Likelihood P(D\\|H) | Posterior P(H\\|D) |\n|-------------|------------|-------------------|-------------------|\n| **Naive Bayes Classifier** | Class frequencies in training data | P(features\\|class) assumed independent | P(class\\|features) for prediction |\n| **Bayesian Neural Networks** | Prior on weights (e.g., Gaussian) | P(data\\|weights) from network output | Distribution over weights given data |\n| **Bayesian Optimization** | GP prior over objective function | Observations so far | Updated belief about function |\n| **Spam Filtering** | Base rate of spam emails | P(words\\|spam) and P(words\\|ham) | P(spam\\|email content) |\n| **A/B Testing** | Prior belief about conversion rates | Observed clicks/conversions | Updated belief about which variant wins |\n\n**The Bayesian vs Frequentist Perspective**:\n- **Frequentist**: Parameters are fixed, unknown constants. We estimate them.\n- **Bayesian**: Parameters have probability distributions. We update our beliefs.\n\nIn deep learning, we're usually frequentist (point estimates via SGD), but Bayesian methods give us uncertainty quantification.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: The Intuition Behind Maximum Likelihood\n\n**The Core Question**: Given observed data, what parameters would have made this data *most probable*?\n\nImagine you flip a coin 10 times and get 7 heads. What's the \"most likely\" value of p (probability of heads)?\n\n**MLE answers**: Find the p that maximizes P(7 heads in 10 flips | p)\n\nThe answer is p = 0.7, because:\n- If p = 0.5, getting 7 heads is somewhat unlikely\n- If p = 0.9, getting only 7 heads (not 9) is unlikely\n- p = 0.7 makes our observed data most probable\n\n**Why Log-Likelihood?**\n1. Products become sums: log(a × b × c) = log(a) + log(b) + log(c)\n2. Numerical stability: Avoids underflow when multiplying many small probabilities\n3. Same maximum: log is monotonic, so argmax is preserved\n\n**The Profound Connection to Loss Functions**:\n\nFor classification with softmax outputs:\n$$\\text{Minimize Cross-Entropy} = \\text{Maximize Log-Likelihood}$$\n\nThey're the same optimization! When you train with cross-entropy loss, you're doing MLE.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic example: Medical testing\n",
    "# Disease affects 1% of population\n",
    "# Test is 95% accurate (both sensitivity and specificity)\n",
    "\n",
    "P_disease = 0.01  # Prior: probability of having disease\n",
    "P_positive_given_disease = 0.95  # Sensitivity (true positive rate)\n",
    "P_positive_given_no_disease = 0.05  # False positive rate (1 - specificity)\n",
    "\n",
    "# P(positive) = P(positive|disease)P(disease) + P(positive|no disease)P(no disease)\n",
    "P_positive = P_positive_given_disease * P_disease + P_positive_given_no_disease * (1 - P_disease)\n",
    "\n",
    "# Bayes' theorem: P(disease|positive)\n",
    "P_disease_given_positive = (P_positive_given_disease * P_disease) / P_positive\n",
    "\n",
    "print(\"Medical Testing Example\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Prior P(disease) = {P_disease:.2%}\")\n",
    "print(f\"Test sensitivity = {P_positive_given_disease:.2%}\")\n",
    "print(f\"Test specificity = {1 - P_positive_given_no_disease:.2%}\")\n",
    "print()\n",
    "print(f\"P(positive test) = {P_positive:.4f}\")\n",
    "print(f\"P(disease | positive test) = {P_disease_given_positive:.2%}\")\n",
    "print()\n",
    "print(\"Surprising! Even with a positive test from a 95% accurate test,\")\n",
    "print(f\"there's only a {P_disease_given_positive:.1%} chance of actually having the disease!\")\n",
    "print(\"This is because the disease is rare (low prior).\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Demonstrating: Cross-Entropy Loss = Negative Log-Likelihood\n# This shows they're mathematically equivalent!\n\nprint(\"Cross-Entropy Loss vs Negative Log-Likelihood\")\nprint(\"=\" * 50)\n\n# Imagine a 3-class classification problem\n# True label is class 0, model outputs these probabilities:\ntrue_class = 0\nmodel_probs = np.array([0.7, 0.2, 0.1])  # Model is fairly confident\n\n# Method 1: Cross-Entropy Loss (what we use in practice)\n# CE = -sum(y_true * log(y_pred)) where y_true is one-hot\none_hot = np.array([1, 0, 0])  # One-hot encoding of true class\ncross_entropy = -np.sum(one_hot * np.log(model_probs))\nprint(f\"\\nCross-Entropy Loss: -sum(y_true * log(y_pred))\")\nprint(f\"  = -({one_hot[0]} * log({model_probs[0]:.2f}) + {one_hot[1]} * log({model_probs[1]:.2f}) + {one_hot[2]} * log({model_probs[2]:.2f}))\")\nprint(f\"  = -{np.log(model_probs[0]):.4f}\")\nprint(f\"  = {cross_entropy:.4f}\")\n\n# Method 2: Negative Log-Likelihood (MLE perspective)\n# NLL = -log(P(true_class))\nneg_log_likelihood = -np.log(model_probs[true_class])\nprint(f\"\\nNegative Log-Likelihood: -log(P(true_class))\")\nprint(f\"  = -log({model_probs[true_class]:.2f})\")\nprint(f\"  = {neg_log_likelihood:.4f}\")\n\nprint(f\"\\nThey're identical! CE = NLL = {cross_entropy:.4f}\")\nprint(\"\\nThis means: Training with cross-entropy loss is doing MLE!\")\nprint(\"We're finding network weights that maximize P(correct labels | inputs)\")\n\n# Show how loss changes with confidence\nprint(\"\\n\" + \"=\" * 50)\nprint(\"How loss varies with model confidence:\")\nprobs_for_true_class = [0.1, 0.3, 0.5, 0.7, 0.9, 0.99]\nprint(f\"{'P(true class)':<15} {'Cross-Entropy Loss':<20}\")\nprint(\"-\" * 35)\nfor p in probs_for_true_class:\n    loss = -np.log(p)\n    print(f\"{p:<15.2f} {loss:<20.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how posterior changes with prior\n",
    "priors = np.linspace(0.001, 0.5, 100)\n",
    "sensitivity = 0.95\n",
    "specificity = 0.95\n",
    "\n",
    "posteriors = []\n",
    "for prior in priors:\n",
    "    p_positive = sensitivity * prior + (1 - specificity) * (1 - prior)\n",
    "    posterior = (sensitivity * prior) / p_positive\n",
    "    posteriors.append(posterior)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(priors * 100, np.array(posteriors) * 100, 'b-', linewidth=2)\n",
    "plt.xlabel('Prior P(disease) [%]')\n",
    "plt.ylabel('Posterior P(disease|positive) [%]')\n",
    "plt.title('Posterior vs Prior (95% accurate test)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark some key points\n",
    "for prior in [0.01, 0.1, 0.5]:\n",
    "    p_positive = sensitivity * prior + (1 - specificity) * (1 - prior)\n",
    "    posterior = (sensitivity * prior) / p_positive\n",
    "    plt.scatter([prior * 100], [posterior * 100], s=100, zorder=5)\n",
    "    plt.annotate(f'({prior*100:.0f}%, {posterior*100:.1f}%)', \n",
    "                 (prior * 100 + 1, posterior * 100 - 3))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Interactive visualization: The Likelihood Surface\n# Shows how likelihood changes as we vary parameters\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Generate data from known distribution\nnp.random.seed(42)\ntrue_mu, true_sigma = 3.0, 1.5\ndata = np.random.normal(true_mu, true_sigma, size=30)\n\n# Plot 1: 1D likelihood for mu (sigma fixed at true value)\nmus = np.linspace(0, 6, 100)\nlog_likelihoods_mu = [np.sum(stats.norm.logpdf(data, mu, true_sigma)) for mu in mus]\n\naxes[0].plot(mus, log_likelihoods_mu, 'b-', linewidth=2)\naxes[0].axvline(x=data.mean(), color='red', linestyle='--', label=f'MLE: {data.mean():.2f}')\naxes[0].axvline(x=true_mu, color='green', linestyle=':', label=f'True: {true_mu}')\naxes[0].set_xlabel('mu')\naxes[0].set_ylabel('Log-Likelihood')\naxes[0].set_title('Likelihood Slice (sigma fixed)')\naxes[0].legend()\n\n# Plot 2: 1D likelihood for sigma (mu fixed at true value)\nsigmas = np.linspace(0.5, 4, 100)\nlog_likelihoods_sigma = [np.sum(stats.norm.logpdf(data, true_mu, sigma)) for sigma in sigmas]\n\naxes[1].plot(sigmas, log_likelihoods_sigma, 'b-', linewidth=2)\naxes[1].axvline(x=data.std(), color='red', linestyle='--', label=f'MLE: {data.std():.2f}')\naxes[1].axvline(x=true_sigma, color='green', linestyle=':', label=f'True: {true_sigma}')\naxes[1].set_xlabel('sigma')\naxes[1].set_ylabel('Log-Likelihood')\naxes[1].set_title('Likelihood Slice (mu fixed)')\naxes[1].legend()\n\n# Plot 3: 2D likelihood surface\nmus_2d = np.linspace(1, 5, 50)\nsigmas_2d = np.linspace(0.5, 3, 50)\nMU, SIGMA = np.meshgrid(mus_2d, sigmas_2d)\n\nLL = np.zeros_like(MU)\nfor i in range(len(sigmas_2d)):\n    for j in range(len(mus_2d)):\n        LL[i, j] = np.sum(stats.norm.logpdf(data, MU[i, j], SIGMA[i, j]))\n\ncontour = axes[2].contourf(MU, SIGMA, LL, levels=30, cmap='viridis')\naxes[2].scatter([data.mean()], [data.std()], color='red', s=150, marker='*', \n                label=f'MLE', zorder=5, edgecolors='white')\naxes[2].scatter([true_mu], [true_sigma], color='white', s=100, marker='o',\n                label=f'True', zorder=5, edgecolors='black')\naxes[2].set_xlabel('mu')\naxes[2].set_ylabel('sigma')\naxes[2].set_title('2D Log-Likelihood Surface')\naxes[2].legend()\nplt.colorbar(contour, ax=axes[2], label='Log-Likelihood')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key Observations:\")\nprint(\"1. The likelihood surface has a clear peak (the MLE)\")\nprint(\"2. As we move away from the MLE, likelihood decreases\")\nprint(\"3. Gradient ascent on this surface finds the MLE\")\nprint(\"4. This is exactly what neural network training does!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: Understanding Entropy\n\nEntropy has several intuitive interpretations that all lead to the same formula:\n\n**Interpretation 1: Average Surprise**\n- \"Surprise\" of an event = -log P(event)\n- Rare events (low probability) are more surprising\n- Entropy = average surprise across all possible outcomes\n- H(X) = E[-log P(X)] = \"How surprised will I be on average?\"\n\n**Interpretation 2: Uncertainty**  \n- How uncertain are we about the outcome?\n- Maximum entropy = maximum uncertainty (uniform distribution)\n- Zero entropy = complete certainty (deterministic)\n\n**Interpretation 3: Information Content (Bits)**\n- \"How many yes/no questions do I need to identify the outcome?\"\n- Fair coin: 1 bit (one yes/no question: \"Was it heads?\")\n- Fair 4-sided die: 2 bits (\"Is it 1 or 2?\" then \"Is it the first of those two?\")\n- Biased distributions need fewer questions on average (can ask about likely outcomes first)\n\n**Why log base 2?** \n- Gives entropy in \"bits\" - the number of binary questions\n- log base e gives \"nats\" (natural units)\n- They're proportional: 1 nat = 1.44 bits",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualizing Entropy as \"Average Surprise\"\n# Surprise of an event = -log2(P(event))\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Plot 1: Surprise function\nprobs = np.linspace(0.01, 1, 100)\nsurprise = -np.log2(probs)\n\naxes[0].plot(probs, surprise, 'b-', linewidth=2)\naxes[0].set_xlabel('Probability P(x)')\naxes[0].set_ylabel('Surprise = -log2(P(x))')\naxes[0].set_title('Surprise Function')\naxes[0].grid(True, alpha=0.3)\naxes[0].annotate('Rare events\\n(high surprise)', xy=(0.1, 3.3), fontsize=10)\naxes[0].annotate('Common events\\n(low surprise)', xy=(0.7, 0.8), fontsize=10)\n\n# Plot 2: Entropy for different distributions\ndistributions = {\n    'Certain\\n[1,0,0,0]': [1, 0, 0, 0],\n    'Skewed\\n[0.7,0.2,0.1,0]': [0.7, 0.2, 0.1, 0],\n    'Moderate\\n[0.4,0.3,0.2,0.1]': [0.4, 0.3, 0.2, 0.1],\n    'Uniform\\n[0.25,0.25,0.25,0.25]': [0.25, 0.25, 0.25, 0.25],\n}\n\nnames = list(distributions.keys())\nentropies = [entropy(p) for p in distributions.values()]\n\nbars = axes[1].bar(range(len(names)), entropies, color=['darkblue', 'blue', 'steelblue', 'lightblue'])\naxes[1].set_xticks(range(len(names)))\naxes[1].set_xticklabels(names, fontsize=9)\naxes[1].set_ylabel('Entropy (bits)')\naxes[1].set_title('Entropy of Different Distributions')\naxes[1].set_ylim(0, 2.5)\nfor i, v in enumerate(entropies):\n    axes[1].text(i, v + 0.1, f'{v:.2f}', ha='center', fontweight='bold')\n\n# Plot 3: Why uniform has maximum entropy\n# Show entropy vs \"peakedness\" of distribution\n# Using parameterized softmax: p_i = exp(alpha * x_i) / sum(exp(alpha * x_j))\nalphas = np.linspace(0, 5, 50)\nscores = np.array([1, 2, 3, 4])  # Base preferences\n\nentropy_values = []\nfor alpha in alphas:\n    if alpha == 0:\n        probs = np.ones(4) / 4  # Uniform\n    else:\n        logits = alpha * scores\n        probs = np.exp(logits - logits.max())\n        probs = probs / probs.sum()\n    entropy_values.append(entropy(probs))\n\naxes[2].plot(alphas, entropy_values, 'b-', linewidth=2)\naxes[2].set_xlabel('Temperature (lower = peakier)')\naxes[2].set_ylabel('Entropy (bits)')\naxes[2].set_title('Entropy vs Distribution Sharpness\\n(Like softmax temperature)')\naxes[2].axhline(y=2, color='r', linestyle='--', alpha=0.5, label='Max entropy (uniform)')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Entropy Reference Table\n\n| Distribution | Formula | Entropy (bits) | Interpretation |\n|--------------|---------|----------------|----------------|\n| Fair coin | [0.5, 0.5] | 1.00 | 1 yes/no question needed |\n| Biased coin (90/10) | [0.9, 0.1] | 0.47 | Less than 1 question on average |\n| Certain outcome | [1, 0] | 0.00 | No uncertainty, no questions needed |\n| Fair 4-sided die | [0.25, 0.25, 0.25, 0.25] | 2.00 | 2 yes/no questions needed |\n| Fair 8-sided die | [1/8] * 8 | 3.00 | 3 yes/no questions needed |\n| Fair N-sided die | [1/N] * N | log2(N) | log2(N) questions needed |\n\n**Pattern**: For a uniform distribution over N outcomes, entropy = log2(N) bits.\n\n**Why Maximum Entropy = Uniform?**\n- Mathematically: Proven via Lagrange multipliers (maximizing H subject to sum = 1)\n- Intuitively: Any preference toward one outcome reduces average surprise\n- Philosophically: Maximum entropy = maximum ignorance = all outcomes equally plausible",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier\n",
    "\n",
    "A simple but effective classifier using Bayes' theorem:\n",
    "\n",
    "$$P(y|x_1, ..., x_n) \\propto P(y) \\prod_{i=1}^n P(x_i|y)$$\n",
    "\n",
    "The \"naive\" assumption is that features are conditionally independent given the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Naive Bayes from scratch\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.class_priors = {}\n",
    "        self.feature_params = {}  # (class, feature) -> (mean, std)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit Gaussian Naive Bayes.\"\"\"\n",
    "        classes = np.unique(y)\n",
    "        n_samples = len(y)\n",
    "        \n",
    "        for c in classes:\n",
    "            # Class prior\n",
    "            self.class_priors[c] = np.sum(y == c) / n_samples\n",
    "            \n",
    "            # Feature parameters (Gaussian)\n",
    "            X_c = X[y == c]\n",
    "            for j in range(X.shape[1]):\n",
    "                self.feature_params[(c, j)] = (X_c[:, j].mean(), X_c[:, j].std() + 1e-6)\n",
    "                \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Compute class probabilities.\"\"\"\n",
    "        classes = list(self.class_priors.keys())\n",
    "        n_samples = X.shape[0]\n",
    "        probs = np.zeros((n_samples, len(classes)))\n",
    "        \n",
    "        for i, c in enumerate(classes):\n",
    "            # Start with log prior\n",
    "            log_prob = np.log(self.class_priors[c])\n",
    "            \n",
    "            # Add log likelihood for each feature\n",
    "            for j in range(X.shape[1]):\n",
    "                mean, std = self.feature_params[(c, j)]\n",
    "                log_prob += stats.norm.logpdf(X[:, j], mean, std)\n",
    "            \n",
    "            probs[:, i] = log_prob\n",
    "        \n",
    "        # Convert to probabilities (softmax of log probs)\n",
    "        probs = np.exp(probs - probs.max(axis=1, keepdims=True))\n",
    "        probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        return probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        classes = list(self.class_priors.keys())\n",
    "        return np.array([classes[i] for i in probs.argmax(axis=1)])\n",
    "\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Class 0: centered at (0, 0)\n",
    "X0 = np.random.randn(n_samples // 2, 2) + np.array([0, 0])\n",
    "# Class 1: centered at (3, 3)\n",
    "X1 = np.random.randn(n_samples // 2, 2) + np.array([3, 3])\n",
    "\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.array([0] * (n_samples // 2) + [1] * (n_samples // 2))\n",
    "\n",
    "# Train\n",
    "clf = NaiveBayesClassifier()\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(X)\n",
    "accuracy = np.mean(y_pred == y)\n",
    "print(f\"Training accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Visualize decision boundary\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', label='Class 0', edgecolors='k')\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], c='red', label='Class 1', edgecolors='k')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Naive Bayes Decision Boundary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: Understanding KL Divergence\n\nKL divergence measures how \"different\" one distribution is from another. Here are multiple ways to understand it:\n\n**Interpretation 1: Extra Bits for Wrong Encoding**\n- Suppose you design a code optimized for distribution Q\n- But the true data comes from distribution P\n- KL(P || Q) = extra bits needed because you used the wrong distribution\n- If P = Q, you need exactly H(P) bits (optimal)\n- If P != Q, you need H(P) + KL(P||Q) bits (suboptimal)\n\n**Interpretation 2: Information Lost**\n- KL(P || Q) measures information lost when Q is used to approximate P\n- It's the \"distance\" from Q to P (but not symmetric!)\n\n**Interpretation 3: The Fundamental Relationship**\n$$D_{KL}(P || Q) = H(P, Q) - H(P) = \\text{Cross-Entropy} - \\text{Entropy}$$\n\nThis tells us:\n- Cross-entropy = cost of using Q to encode P\n- Entropy = minimum possible cost (using P itself)\n- KL divergence = the \"wasted\" bits from using Q instead of P\n\n**Why Not Symmetric?**\n- KL(P || Q): Cost of using Q when truth is P\n- KL(Q || P): Cost of using P when truth is Q\n- These are different questions!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualizing the KL = CrossEntropy - Entropy relationship\n# And demonstrating asymmetry\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Example distributions\nP = np.array([0.6, 0.3, 0.1])  # True distribution\nQ = np.array([0.33, 0.33, 0.34])  # Approximation (roughly uniform)\n\n# Calculate all quantities\nH_P = entropy(P)\nH_P_Q = cross_entropy(P, Q)  # Cross-entropy\nKL_P_Q = kl_divergence(P, Q)\n\n# Plot 1: Bar chart showing the relationship\nquantities = ['H(P)\\nEntropy', 'KL(P||Q)\\nDivergence', 'H(P,Q)\\nCross-Entropy']\nvalues = [H_P, KL_P_Q, H_P_Q]\ncolors = ['green', 'red', 'blue']\n\nbars = axes[0].bar(quantities, values, color=colors, alpha=0.7)\naxes[0].set_ylabel('Bits')\naxes[0].set_title('H(P,Q) = H(P) + KL(P||Q)')\n\n# Add value labels\nfor bar, val in zip(bars, values):\n    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05, \n                 f'{val:.3f}', ha='center', fontweight='bold')\n\n# Verify the relationship\naxes[0].text(0.5, 0.85, f'{H_P:.3f} + {KL_P_Q:.3f} = {H_P + KL_P_Q:.3f}', \n             transform=axes[0].transAxes, ha='center', fontsize=11,\n             bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n\n# Plot 2: Asymmetry visualization\nP = np.array([0.9, 0.1])  # Peaked distribution\nQ = np.array([0.5, 0.5])  # Uniform distribution\n\nkl_pq = kl_divergence(P, Q)\nkl_qp = kl_divergence(Q, P)\n\nx = np.arange(2)\nwidth = 0.35\n\naxes[1].bar(x - width/2, P, width, label='P (peaked)', color='blue', alpha=0.7)\naxes[1].bar(x + width/2, Q, width, label='Q (uniform)', color='orange', alpha=0.7)\naxes[1].set_xticks(x)\naxes[1].set_xticklabels(['Outcome 1', 'Outcome 2'])\naxes[1].set_ylabel('Probability')\naxes[1].set_title(f'KL Asymmetry\\nKL(P||Q)={kl_pq:.3f}, KL(Q||P)={kl_qp:.3f}')\naxes[1].legend()\n\n# Plot 3: Why asymmetry matters in practice\n# KL(P||Q) penalizes Q=0 where P>0 (infinity!)\n# KL(Q||P) penalizes P=0 where Q>0 (infinity!)\n\naxes[2].text(0.5, 0.85, 'KL(P || Q) vs KL(Q || P)', fontsize=14, fontweight='bold',\n             ha='center', transform=axes[2].transAxes)\n\nexplanation = \"\"\"\nKL(P || Q): \"How bad is Q as a model of P?\"\n- Averages over P (true distribution)\n- Catastrophic if Q gives 0 probability \n  where P has probability (log(0) = -inf!)\n- Used in: VAE loss, variational inference\n\nKL(Q || P): \"How bad is P as a model of Q?\"  \n- Averages over Q (approximate distribution)\n- Catastrophic if P gives 0 probability\n  where Q has probability\n- Used in: Reverse KL for mode-seeking\n\nIn classification:\n- P = true labels (one-hot), Q = model predictions\n- Cross-entropy = H(P) + KL(P||Q) = KL(P||Q)\n  (since H(P) = 0 for one-hot)\n\"\"\"\n\naxes[2].text(0.05, 0.75, explanation, fontsize=9, transform=axes[2].transAxes,\n             verticalalignment='top', fontfamily='monospace')\naxes[2].axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key Insight for Classification:\")\nprint(\"When true labels are one-hot, H(P) = 0\")\nprint(\"So: Cross-Entropy Loss = KL(true || predicted)\")\nprint(\"Minimizing cross-entropy = minimizing KL divergence!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Demonstrating KL divergence in Knowledge Distillation\n# Soft labels from teacher contain more information than hard labels\n\n# Imagine a 5-class image classification problem\nclasses = ['cat', 'dog', 'bird', 'car', 'plane']\n\n# Hard label (ground truth)\nhard_label = np.array([1, 0, 0, 0, 0])  # True class is 'cat'\n\n# Teacher model's soft prediction (trained, accurate)\n# Notice: teacher thinks it could be dog (similar to cat)\nteacher_soft = np.array([0.7, 0.2, 0.05, 0.03, 0.02])\n\n# Student predictions at different training stages\nstudent_untrained = np.array([0.2, 0.2, 0.2, 0.2, 0.2])  # Uniform (no knowledge)\nstudent_partial = np.array([0.5, 0.15, 0.15, 0.1, 0.1])  # Learning\nstudent_trained = np.array([0.68, 0.18, 0.07, 0.04, 0.03])  # Well-trained\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot 1: Compare distributions\nx = np.arange(len(classes))\nwidth = 0.2\n\naxes[0].bar(x - 1.5*width, hard_label, width, label='Hard Label', color='red', alpha=0.7)\naxes[0].bar(x - 0.5*width, teacher_soft, width, label='Teacher (soft)', color='blue', alpha=0.7)\naxes[0].bar(x + 0.5*width, student_partial, width, label='Student (learning)', color='green', alpha=0.7)\naxes[0].bar(x + 1.5*width, student_trained, width, label='Student (trained)', color='purple', alpha=0.7)\n\naxes[0].set_xticks(x)\naxes[0].set_xticklabels(classes)\naxes[0].set_ylabel('Probability')\naxes[0].set_title('Knowledge Distillation: Soft vs Hard Labels')\naxes[0].legend()\n\n# Plot 2: KL divergences\nstudents = {\n    'Untrained': student_untrained,\n    'Partial': student_partial, \n    'Trained': student_trained\n}\n\n# KL from hard labels (what standard cross-entropy uses)\nkl_hard = [kl_divergence(hard_label, s) for s in students.values()]\n\n# KL from teacher soft labels (knowledge distillation)\nkl_soft = [kl_divergence(teacher_soft, s) for s in students.values()]\n\nx = np.arange(len(students))\nwidth = 0.35\n\naxes[1].bar(x - width/2, kl_hard, width, label='KL(Hard || Student)', color='red', alpha=0.7)\naxes[1].bar(x + width/2, kl_soft, width, label='KL(Teacher || Student)', color='blue', alpha=0.7)\naxes[1].set_xticks(x)\naxes[1].set_xticklabels(list(students.keys()))\naxes[1].set_ylabel('KL Divergence (bits)')\naxes[1].set_title('Loss: Hard Labels vs Knowledge Distillation')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Why Soft Labels Help:\")\nprint(\"=\" * 50)\nprint(\"Hard label only says: 'This is a cat'\")\nprint(\"Soft label says: 'This is a cat, but somewhat similar to dog,\")\nprint(\"                  not similar to car or plane'\")\nprint(\"\\nThe relationships between classes ('dark knowledge') help\")\nprint(\"the student generalize better!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### KL Divergence in Machine Learning Applications\n\n| Application | P (True/Target) | Q (Approximate/Model) | What KL Measures |\n|-------------|-----------------|----------------------|------------------|\n| **Classification Loss** | One-hot labels | Softmax predictions | How wrong are predictions |\n| **VAE Loss** | Posterior q(z\\|x) | Prior p(z), usually N(0,1) | How far latent code is from prior |\n| **Knowledge Distillation** | Teacher softmax | Student softmax | How well student mimics teacher |\n| **Policy Gradient (PPO)** | Old policy | New policy | Prevents too-large policy updates |\n| **Variational Inference** | True posterior | Variational approx | Quality of approximation |\n\n**The VAE Loss Decomposition**:\n$$\\mathcal{L}_{VAE} = \\underbrace{-\\mathbb{E}_{q(z|x)}[\\log p(x|z)]}_{\\text{Reconstruction Loss}} + \\underbrace{D_{KL}(q(z|x) || p(z))}_{\\text{Regularization}}$$\n\nThe KL term pulls the encoder's latent distribution toward the prior, enabling generation.\n\n**Knowledge Distillation**:\n- Teacher: Large, accurate model with \"soft\" predictions\n- Student: Small model learning to match teacher\n- Loss = KL(Teacher || Student) on softmax outputs\n- Student learns teacher's \"dark knowledge\" (relationships between classes)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "MLE finds parameters that maximize the probability of observing the data:\n",
    "\n",
    "$$\\hat{\\theta}_{MLE} = \\arg\\max_\\theta P(\\text{data}|\\theta) = \\arg\\max_\\theta \\prod_i P(x_i|\\theta)$$\n",
    "\n",
    "In practice, we maximize the **log-likelihood** (easier to work with):\n",
    "\n",
    "$$\\hat{\\theta}_{MLE} = \\arg\\max_\\theta \\sum_i \\log P(x_i|\\theta)$$\n",
    "\n",
    "**Key insight**: Minimizing cross-entropy loss = maximizing log-likelihood!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE for Gaussian parameters\n",
    "# True parameters\n",
    "mu_true = 5.0\n",
    "sigma_true = 2.0\n",
    "\n",
    "# Generate data\n",
    "n_samples = 100\n",
    "data = np.random.normal(mu_true, sigma_true, n_samples)\n",
    "\n",
    "# MLE estimates (can be derived analytically)\n",
    "mu_mle = data.mean()  # Sample mean\n",
    "sigma_mle = data.std()  # Sample std (biased, but MLE)\n",
    "\n",
    "print(f\"True parameters: μ = {mu_true}, σ = {sigma_true}\")\n",
    "print(f\"MLE estimates:   μ̂ = {mu_mle:.3f}, σ̂ = {sigma_mle:.3f}\")\n",
    "\n",
    "# Visualize\n",
    "x = np.linspace(mu_true - 4*sigma_true, mu_true + 4*sigma_true, 100)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data, bins=20, density=True, alpha=0.6, label='Data histogram')\n",
    "plt.plot(x, stats.norm.pdf(x, mu_true, sigma_true), 'g-', linewidth=2, \n",
    "         label=f'True: N({mu_true}, {sigma_true}²)')\n",
    "plt.plot(x, stats.norm.pdf(x, mu_mle, sigma_mle), 'r--', linewidth=2,\n",
    "         label=f'MLE: N({mu_mle:.2f}, {sigma_mle:.2f}²)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.title('MLE for Gaussian Distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the likelihood function\n",
    "def log_likelihood(mu, sigma, data):\n",
    "    \"\"\"Compute log-likelihood of data under N(mu, sigma^2).\"\"\"\n",
    "    return np.sum(stats.norm.logpdf(data, mu, sigma))\n",
    "\n",
    "# Create grid of parameters\n",
    "mus = np.linspace(3, 7, 50)\n",
    "sigmas = np.linspace(1, 4, 50)\n",
    "MU, SIGMA = np.meshgrid(mus, sigmas)\n",
    "\n",
    "# Compute log-likelihood at each point\n",
    "LL = np.zeros_like(MU)\n",
    "for i in range(len(sigmas)):\n",
    "    for j in range(len(mus)):\n",
    "        LL[i, j] = log_likelihood(MU[i, j], SIGMA[i, j], data)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(MU, SIGMA, LL, levels=30, cmap='viridis')\n",
    "plt.colorbar(label='Log-Likelihood')\n",
    "plt.scatter([mu_mle], [sigma_mle], color='red', s=200, marker='*', \n",
    "            label=f'MLE: ({mu_mle:.2f}, {sigma_mle:.2f})', zorder=5)\n",
    "plt.scatter([mu_true], [sigma_true], color='white', s=100, marker='o',\n",
    "            label=f'True: ({mu_true}, {sigma_true})', zorder=5)\n",
    "plt.xlabel('μ')\n",
    "plt.ylabel('σ')\n",
    "plt.title('Log-Likelihood Surface')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE for Bernoulli (Coin Flip)\n",
    "\n",
    "If we observe $k$ heads in $n$ flips, the MLE estimate is simply:\n",
    "\n",
    "$$\\hat{p}_{MLE} = \\frac{k}{n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE for coin flip\n",
    "p_true = 0.7\n",
    "n_flips = 50\n",
    "flips = np.random.binomial(1, p_true, n_flips)\n",
    "k = flips.sum()  # Number of heads\n",
    "\n",
    "p_mle = k / n_flips\n",
    "\n",
    "print(f\"True p: {p_true}\")\n",
    "print(f\"Observed: {k} heads in {n_flips} flips\")\n",
    "print(f\"MLE estimate: p̂ = {p_mle:.3f}\")\n",
    "\n",
    "# Visualize likelihood function\n",
    "p_values = np.linspace(0.01, 0.99, 100)\n",
    "likelihoods = [stats.binom.pmf(k, n_flips, p) for p in p_values]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(p_values, likelihoods, 'b-', linewidth=2)\n",
    "plt.axvline(x=p_mle, color='red', linestyle='--', label=f'MLE: p̂ = {p_mle:.3f}')\n",
    "plt.axvline(x=p_true, color='green', linestyle=':', label=f'True: p = {p_true}')\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('Likelihood P(data|p)')\n",
    "plt.title(f'Likelihood Function for {k} heads in {n_flips} flips')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Information Theory\n",
    "\n",
    "Information theory quantifies information and uncertainty.\n",
    "\n",
    "### Entropy\n",
    "\n",
    "Entropy measures the \"uncertainty\" or \"information content\" of a distribution:\n",
    "\n",
    "$$H(X) = -\\sum_x P(x) \\log P(x) = -E[\\log P(X)]$$\n",
    "\n",
    "**Properties**:\n",
    "- Higher entropy = more uncertainty\n",
    "- Uniform distribution has maximum entropy\n",
    "- Deterministic variable has entropy 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    \"\"\"Compute entropy of a discrete distribution.\"\"\"\n",
    "    p = np.array(p)\n",
    "    p = p[p > 0]  # Avoid log(0)\n",
    "    return -np.sum(p * np.log2(p))\n",
    "\n",
    "# Examples\n",
    "print(\"Entropy examples (in bits):\")\n",
    "print(f\"Fair coin [0.5, 0.5]: H = {entropy([0.5, 0.5]):.4f} bits\")\n",
    "print(f\"Biased coin [0.9, 0.1]: H = {entropy([0.9, 0.1]):.4f} bits\")\n",
    "print(f\"Certain [1.0, 0.0]: H = {entropy([1.0, 0.0]):.4f} bits\")\n",
    "print(f\"Fair die [1/6]*6: H = {entropy([1/6]*6):.4f} bits\")\n",
    "print(f\"Uniform 8-sided: H = {entropy([1/8]*8):.4f} bits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy of binary distribution as function of p\n",
    "p_values = np.linspace(0.001, 0.999, 100)\n",
    "entropies = [-p * np.log2(p) - (1-p) * np.log2(1-p) for p in p_values]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(p_values, entropies, 'b-', linewidth=2)\n",
    "plt.xlabel('p (probability of heads)')\n",
    "plt.ylabel('Entropy (bits)')\n",
    "plt.title('Binary Entropy Function H(p)')\n",
    "plt.axhline(y=1, color='r', linestyle='--', alpha=0.5, label='Maximum = 1 bit')\n",
    "plt.axvline(x=0.5, color='g', linestyle='--', alpha=0.5, label='p = 0.5')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Maximum entropy occurs at p = 0.5 (maximum uncertainty)\")\n",
    "print(\"Entropy = 0 when p = 0 or p = 1 (no uncertainty)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy\n",
    "\n",
    "Cross-entropy measures the \"cost\" of using distribution $Q$ to encode samples from distribution $P$:\n",
    "\n",
    "$$H(P, Q) = -\\sum_x P(x) \\log Q(x) = -E_P[\\log Q(X)]$$\n",
    "\n",
    "**In ML**: Cross-entropy loss measures how well predicted probabilities $Q$ match true labels $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(p, q):\n",
    "    \"\"\"Compute cross-entropy H(P, Q).\"\"\"\n",
    "    p = np.array(p)\n",
    "    q = np.array(q)\n",
    "    # Avoid log(0) by clipping\n",
    "    q = np.clip(q, 1e-10, 1.0)\n",
    "    return -np.sum(p * np.log2(q))\n",
    "\n",
    "# Example: True distribution vs predictions\n",
    "p_true = np.array([1, 0, 0])  # True class is 0\n",
    "\n",
    "predictions = [\n",
    "    ([0.9, 0.05, 0.05], \"Confident correct\"),\n",
    "    ([0.6, 0.2, 0.2], \"Less confident\"),\n",
    "    ([0.33, 0.33, 0.34], \"Uniform (uncertain)\"),\n",
    "    ([0.1, 0.45, 0.45], \"Confident wrong\"),\n",
    "]\n",
    "\n",
    "print(\"Cross-entropy loss for different predictions:\")\n",
    "print(f\"True label: class 0 (one-hot: {p_true})\\n\")\n",
    "\n",
    "for q, desc in predictions:\n",
    "    ce = cross_entropy(p_true, q)\n",
    "    print(f\"{desc}\")\n",
    "    print(f\"  Prediction: {q}\")\n",
    "    print(f\"  Cross-entropy: {ce:.4f} bits\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL Divergence\n",
    "\n",
    "KL divergence measures how different distribution $Q$ is from $P$:\n",
    "\n",
    "$$D_{KL}(P || Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)} = H(P, Q) - H(P)$$\n",
    "\n",
    "**Properties**:\n",
    "- $D_{KL}(P || Q) \\geq 0$ (always non-negative)\n",
    "- $D_{KL}(P || Q) = 0$ if and only if $P = Q$\n",
    "- Not symmetric: $D_{KL}(P || Q) \\neq D_{KL}(Q || P)$\n",
    "\n",
    "**In ML**: Used in VAEs, knowledge distillation, regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    \"\"\"Compute KL divergence D_KL(P || Q).\"\"\"\n",
    "    p = np.array(p)\n",
    "    q = np.array(q)\n",
    "    # Only sum where p > 0\n",
    "    mask = p > 0\n",
    "    q = np.clip(q, 1e-10, 1.0)\n",
    "    return np.sum(p[mask] * np.log2(p[mask] / q[mask]))\n",
    "\n",
    "# Compare two distributions\n",
    "p = np.array([0.4, 0.3, 0.2, 0.1])\n",
    "q1 = np.array([0.35, 0.35, 0.2, 0.1])  # Similar to P\n",
    "q2 = np.array([0.1, 0.2, 0.3, 0.4])    # Reversed\n",
    "q3 = np.array([0.25, 0.25, 0.25, 0.25])  # Uniform\n",
    "\n",
    "print(f\"P = {p}\")\n",
    "print(f\"\\nKL divergences:\")\n",
    "print(f\"D_KL(P || Q1) where Q1 = {q1}: {kl_divergence(p, q1):.4f} bits\")\n",
    "print(f\"D_KL(P || Q2) where Q2 = {q2}: {kl_divergence(p, q2):.4f} bits\")\n",
    "print(f\"D_KL(P || Q3) where Q3 = {q3}: {kl_divergence(p, q3):.4f} bits\")\n",
    "\n",
    "print(f\"\\nNote asymmetry:\")\n",
    "print(f\"D_KL(P || Q2) = {kl_divergence(p, q2):.4f}\")\n",
    "print(f\"D_KL(Q2 || P) = {kl_divergence(q2, p):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize KL divergence between two Gaussians\n",
    "def kl_gaussian(mu1, sigma1, mu2, sigma2):\n",
    "    \"\"\"KL divergence between two univariate Gaussians.\"\"\"\n",
    "    return (np.log(sigma2/sigma1) + \n",
    "            (sigma1**2 + (mu1 - mu2)**2) / (2 * sigma2**2) - 0.5)\n",
    "\n",
    "# P is N(0, 1), vary Q\n",
    "mu_p, sigma_p = 0, 1\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Vary mean\n",
    "mus = np.linspace(-3, 3, 100)\n",
    "kls = [kl_gaussian(mu_p, sigma_p, mu, sigma_p) for mu in mus]\n",
    "\n",
    "axes[0].plot(mus, kls, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('μ_Q')\n",
    "axes[0].set_ylabel('D_KL(P || Q)')\n",
    "axes[0].set_title('KL Divergence: P = N(0,1), Q = N(μ,1)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Vary std\n",
    "sigmas = np.linspace(0.1, 4, 100)\n",
    "kls = [kl_gaussian(mu_p, sigma_p, mu_p, sigma) for sigma in sigmas]\n",
    "\n",
    "axes[1].plot(sigmas, kls, 'r-', linewidth=2)\n",
    "axes[1].axvline(x=1, color='g', linestyle='--', alpha=0.5, label='σ_Q = σ_P')\n",
    "axes[1].set_xlabel('σ_Q')\n",
    "axes[1].set_ylabel('D_KL(P || Q)')\n",
    "axes[1].set_title('KL Divergence: P = N(0,1), Q = N(0,σ)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Bayesian Coin Inference\n",
    "\n",
    "Use Bayes' theorem to update beliefs about a coin's bias after observing flips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian inference for coin bias\n",
    "# Prior: Beta(a, b) distribution over p\n",
    "# Posterior after k heads, n-k tails: Beta(a + k, b + n - k)\n",
    "\n",
    "def plot_beta_posterior(a_prior, b_prior, n_heads, n_tails):\n",
    "    \"\"\"Plot prior and posterior distributions.\"\"\"\n",
    "    p = np.linspace(0, 1, 100)\n",
    "    \n",
    "    # Prior\n",
    "    prior = stats.beta.pdf(p, a_prior, b_prior)\n",
    "    \n",
    "    # Posterior\n",
    "    a_post = a_prior + n_heads\n",
    "    b_post = b_prior + n_tails\n",
    "    posterior = stats.beta.pdf(p, a_post, b_post)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(p, prior, 'b--', linewidth=2, label=f'Prior: Beta({a_prior}, {b_prior})')\n",
    "    plt.plot(p, posterior, 'r-', linewidth=2, \n",
    "             label=f'Posterior: Beta({a_post}, {b_post})')\n",
    "    plt.axvline(x=n_heads/(n_heads + n_tails) if (n_heads + n_tails) > 0 else 0.5, \n",
    "                color='g', linestyle=':', label=f'MLE: {n_heads/(n_heads + n_tails):.3f}')\n",
    "    plt.xlabel('p (probability of heads)')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'Bayesian Inference: {n_heads} heads, {n_tails} tails')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Posterior statistics\n",
    "    post_mean = a_post / (a_post + b_post)\n",
    "    print(f\"Posterior mean: {post_mean:.4f}\")\n",
    "    print(f\"95% credible interval: [{stats.beta.ppf(0.025, a_post, b_post):.4f}, \"\n",
    "          f\"{stats.beta.ppf(0.975, a_post, b_post):.4f}]\")\n",
    "\n",
    "# Start with uniform prior (no prior knowledge)\n",
    "# Update after observing data\n",
    "plot_beta_posterior(a_prior=1, b_prior=1, n_heads=7, n_tails=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with different priors and data\n",
    "# What happens with:\n",
    "# 1. A strong prior belief that coin is fair: Beta(10, 10)\n",
    "# 2. More data: 70 heads, 30 tails\n",
    "# 3. Conflicting prior and data\n",
    "\n",
    "# Your experiments here:\n",
    "plot_beta_posterior(a_prior=10, b_prior=10, n_heads=7, n_tails=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Softmax Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax.\"\"\"\n",
    "    x_shifted = x - np.max(x, axis=-1, keepdims=True)\n",
    "    exp_x = np.exp(x_shifted)\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(logits, labels):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss.\n",
    "    \n",
    "    Args:\n",
    "        logits: Raw model outputs (before softmax), shape (batch_size, num_classes)\n",
    "        labels: True class indices, shape (batch_size,)\n",
    "    \n",
    "    Returns:\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    # 1. Apply softmax to get probabilities\n",
    "    # 2. Extract probability of true class\n",
    "    # 3. Return negative log probability (averaged over batch)\n",
    "    \n",
    "    probs = softmax(logits)\n",
    "    batch_size = len(labels)\n",
    "    # Get probability assigned to correct class for each sample\n",
    "    correct_probs = probs[np.arange(batch_size), labels]\n",
    "    # Negative log likelihood\n",
    "    loss = -np.mean(np.log(correct_probs + 1e-10))\n",
    "    return loss\n",
    "\n",
    "# Test\n",
    "logits = np.array([[2.0, 1.0, 0.1],   # Should predict class 0\n",
    "                   [0.1, 2.5, 0.3],   # Should predict class 1\n",
    "                   [0.2, 0.3, 3.0]])  # Should predict class 2\n",
    "labels = np.array([0, 1, 2])\n",
    "\n",
    "loss = cross_entropy_loss(logits, labels)\n",
    "print(f\"Logits:\\n{logits}\")\n",
    "print(f\"Softmax probabilities:\\n{softmax(logits).round(4)}\")\n",
    "print(f\"True labels: {labels}\")\n",
    "print(f\"Cross-entropy loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Information Gain for Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(parent_labels, left_labels, right_labels):\n",
    "    \"\"\"\n",
    "    Compute information gain from a split.\n",
    "    \n",
    "    IG = H(parent) - weighted_avg(H(left), H(right))\n",
    "    \"\"\"\n",
    "    def label_entropy(labels):\n",
    "        \"\"\"Compute entropy of label distribution.\"\"\"\n",
    "        if len(labels) == 0:\n",
    "            return 0\n",
    "        _, counts = np.unique(labels, return_counts=True)\n",
    "        probs = counts / len(labels)\n",
    "        return entropy(probs)\n",
    "    \n",
    "    n = len(parent_labels)\n",
    "    n_left = len(left_labels)\n",
    "    n_right = len(right_labels)\n",
    "    \n",
    "    h_parent = label_entropy(parent_labels)\n",
    "    h_left = label_entropy(left_labels)\n",
    "    h_right = label_entropy(right_labels)\n",
    "    \n",
    "    weighted_child = (n_left/n) * h_left + (n_right/n) * h_right\n",
    "    \n",
    "    return h_parent - weighted_child\n",
    "\n",
    "# Example: Splitting data based on a feature\n",
    "# Parent has mixed classes\n",
    "parent = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "# Good split: separates classes well\n",
    "left_good = np.array([0, 0, 0, 0])\n",
    "right_good = np.array([1, 1, 1, 1, 1, 1])\n",
    "\n",
    "# Bad split: doesn't separate well\n",
    "left_bad = np.array([0, 0, 1, 1, 1])\n",
    "right_bad = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "print(f\"Parent entropy: {entropy([0.4, 0.6]):.4f} bits\")\n",
    "print(f\"\\nGood split information gain: {information_gain(parent, left_good, right_good):.4f} bits\")\n",
    "print(f\"Bad split information gain: {information_gain(parent, left_bad, right_bad):.4f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Probability Distributions**: Bernoulli, Categorical, Gaussian are fundamental to ML\n",
    "2. **Bayes' Theorem**: Updates beliefs given evidence (prior × likelihood = posterior)\n",
    "3. **Maximum Likelihood**: Find parameters that maximize P(data|params)\n",
    "4. **Entropy**: Measures uncertainty in a distribution\n",
    "5. **Cross-Entropy**: The loss function for classification\n",
    "6. **KL Divergence**: Measures difference between distributions\n",
    "\n",
    "### Connection to Deep Learning\n",
    "\n",
    "- **Classification**: Softmax outputs a categorical distribution, trained with cross-entropy\n",
    "- **Regression**: Often assumes Gaussian noise, uses MSE (= MLE for Gaussian)\n",
    "- **VAEs**: Use KL divergence to regularize latent distributions\n",
    "- **Dropout**: Samples from Bernoulli to create masks\n",
    "- **Bayesian NN**: Treat weights as distributions, use Bayes' theorem\n",
    "\n",
    "### Checklist\n",
    "- [ ] I understand common probability distributions\n",
    "- [ ] I can apply Bayes' theorem\n",
    "- [ ] I understand MLE and its connection to loss functions\n",
    "- [ ] I can compute entropy and KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to **Part 2: Python Foundations** where we'll cover:\n",
    "- Python OOP for deep learning\n",
    "- NumPy deep dive\n",
    "- Building the tools we'll use throughout the course"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}