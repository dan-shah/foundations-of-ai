{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Part 8.2: LLM Inference Optimization — The Formula 1 Edition\n\nTraining a model is expensive, but **inference** is where the real cost lives. Every user query, every API call, every token generated costs compute. At scale, a model that's 2x faster or uses half the memory saves millions of dollars.\n\n**F1 analogy:** Training is like the off-season: months in the wind tunnel and simulation farm, building the best car you can. But inference is race day — and race day never stops. Every lap, every corner, every radio message needs to be processed in real time. The pit wall can't wait 30 seconds for a strategy recommendation while the driver is screaming \"What do I do?!\" into the radio. LLM inference optimization is the engineering that takes a brilliant but slow simulation model and makes it fast enough for the live pit wall. Quantization is like reducing telemetry precision from float32 to int8 for faster pit wall processing. KV caching is like remembering computations from already-processed laps. The goal: real-time strategy calls (low latency) without sacrificing the quality of post-race analysis (high throughput).\n\nLLM inference has unique challenges: models are enormous (7B-400B+ parameters), generation is sequential (each token depends on the last), and users expect low latency. This notebook covers the key techniques that make production LLM serving feasible.\n\n## Learning Objectives\n\n- [ ] Understand the inference bottleneck: why LLM generation is slow\n- [ ] Implement model quantization (INT8, INT4) from scratch\n- [ ] Build a KV cache to eliminate redundant computation\n- [ ] Understand speculative decoding for faster generation\n- [ ] Implement continuous batching for throughput optimization\n- [ ] Build a model distillation pipeline (teacher → student)\n- [ ] Compare optimization techniques on speed, memory, and quality"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom collections import defaultdict\nimport time\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nnp.random.seed(42)\ntorch.manual_seed(42)\n\nprint(\"Part 8.2: LLM Inference Optimization — The Formula 1 Edition\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "---\n\n## 1. The Inference Bottleneck\n\nLLM inference has two phases:\n\n1. **Prefill**: Process the entire prompt in parallel (compute-bound)\n2. **Decode**: Generate tokens one at a time, each depending on all previous (memory-bound)\n\nThe decode phase is the bottleneck because:\n- Each token requires loading the **entire model** from memory\n- Generation is inherently **sequential** (can't parallelize across tokens)\n- Most time is spent moving weights from memory to compute, not doing math\n\n| Phase | Compute Pattern | Bottleneck | Tokens/sec | F1 Parallel |\n|-------|----------------|------------|------------|-------------|\n| Prefill | Matrix multiply (batched) | Compute | High (thousands) | Loading the full race history into the strategy model at the start of a stint — slow once, but done in bulk |\n| Decode | Matrix-vector product | Memory bandwidth | Low (tens) | Real-time strategy calls — each decision requires reloading the entire model for one new data point |\n\n**F1 analogy:** The prefill phase is like the pit wall loading 20 laps of telemetry history at once to initialize the strategy model before a stint — a big batch operation. The decode phase is like generating second-by-second strategy recommendations during the race: each new prediction depends on all previous ones, and you need the entire model in memory for each tick. The bottleneck is bandwidth, not compute — just like an F1 data link where the radio channel bandwidth limits how fast the pit wall can receive new telemetry, not how fast the computers can process it."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the inference pipeline\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Prefill vs Decode phases\n",
    "ax = axes[0]\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.axis('off')\n",
    "ax.set_title('LLM Inference Phases', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Prefill\n",
    "box = mpatches.FancyBboxPatch((0.5, 3), 5, 2, boxstyle=\"round,pad=0.2\",\n",
    "                               facecolor='#3498db', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(box)\n",
    "ax.text(3, 4.3, 'PREFILL', ha='center', fontsize=12, fontweight='bold', color='white')\n",
    "ax.text(3, 3.6, 'Process entire prompt\\nin parallel (fast)', ha='center', fontsize=9, color='white')\n",
    "\n",
    "# Decode tokens\n",
    "for i in range(5):\n",
    "    x = 7 + i * 1.3\n",
    "    box = mpatches.FancyBboxPatch((x, 3), 1, 2, boxstyle=\"round,pad=0.1\",\n",
    "                                   facecolor='#e74c3c', edgecolor='black', linewidth=1.5)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x + 0.5, 4, f'T{i+1}', ha='center', fontsize=10, fontweight='bold', color='white')\n",
    "\n",
    "ax.text(10.2, 3.3, 'DECODE: one token at a time (slow)', ha='center', fontsize=9, color='gray')\n",
    "\n",
    "ax.annotate('', xy=(6.8, 4), xytext=(5.5, 4),\n",
    "           arrowprops=dict(arrowstyle='->', lw=2, color='gray'))\n",
    "\n",
    "# Prompt tokens\n",
    "for i in range(6):\n",
    "    ax.text(0.8 + i * 0.8, 1.5, f'p{i}', ha='center', fontsize=9,\n",
    "           bbox=dict(boxstyle='round', facecolor='#ecf0f1', edgecolor='gray'))\n",
    "ax.text(3, 0.8, 'Input prompt tokens', ha='center', fontsize=9, color='gray')\n",
    "\n",
    "# Time breakdown\n",
    "ax = axes[1]\n",
    "categories = ['7B Model', '13B Model', '70B Model']\n",
    "prefill_times = [0.05, 0.1, 0.5]  # seconds for 512 token prompt\n",
    "decode_times = [2.0, 3.8, 20.0]   # seconds for 256 output tokens\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "w = 0.35\n",
    "ax.bar(x - w/2, prefill_times, w, label='Prefill (512 tokens)', color='#3498db', edgecolor='black')\n",
    "ax.bar(x + w/2, decode_times, w, label='Decode (256 tokens)', color='#e74c3c', edgecolor='black')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.set_ylabel('Time (seconds)', fontsize=11)\n",
    "ax.set_title('Prefill vs Decode Time', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": "---\n\n## 2. Quantization\n\n**Quantization** reduces the precision of model weights from 32-bit floats to 8-bit or 4-bit integers. This cuts memory usage by 4-8x and speeds up inference (less data to move from memory).\n\n### How It Works\n\nMap floating-point values to a smaller integer range:\n$$x_\\text{quant} = \\text{round}\\left(\\frac{x - \\text{zero\\_point}}{\\text{scale}}\\right)$$\n$$x_\\text{dequant} = x_\\text{quant} \\times \\text{scale} + \\text{zero\\_point}$$\n\n| Precision | Bits | Memory per 1B params | Typical Quality Loss | F1 Parallel |\n|-----------|------|---------------------|--------------------|-------------|\n| FP32 | 32 | 4 GB | Baseline | Full-precision telemetry — every sensor at maximum resolution |\n| FP16/BF16 | 16 | 2 GB | Negligible | Halving sensor precision — still captures all meaningful variation |\n| INT8 | 8 | 1 GB | <1% degradation | Rounding tire temps to nearest degree, throttle to nearest percent — negligible impact |\n| INT4 (GPTQ/AWQ) | 4 | 0.5 GB | 1-3% degradation | Coarse-graining telemetry for quick pit wall displays — loses subtle detail but shows the big picture |\n\n**F1 analogy:** Quantization is exactly what F1 teams do when transmitting telemetry from car to pit wall. The car's ECU records data at extreme precision (float32), but the radio link has limited bandwidth. So the team quantizes: tire temperature doesn't need 7 decimal places — rounding to the nearest degree (INT8) loses almost nothing. Throttle position can be sent as a percentage (0-100) instead of a 32-bit float. The pit wall gets 4x more data channels within the same bandwidth, with barely any loss in decision quality."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantizer:\n",
    "    \"\"\"Weight quantization from scratch.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def quantize_absmax(weights, n_bits=8):\n",
    "        \"\"\"Absmax (symmetric) quantization.\n",
    "        \n",
    "        Maps [-max, max] to [-2^(n-1), 2^(n-1)-1]\n",
    "        \"\"\"\n",
    "        qmax = 2**(n_bits - 1) - 1\n",
    "        scale = weights.abs().max() / qmax\n",
    "        quantized = torch.round(weights / scale).clamp(-qmax, qmax).to(torch.int8)\n",
    "        return quantized, scale\n",
    "    \n",
    "    @staticmethod\n",
    "    def dequantize_absmax(quantized, scale):\n",
    "        \"\"\"Dequantize absmax back to float.\"\"\"\n",
    "        return quantized.float() * scale\n",
    "    \n",
    "    @staticmethod\n",
    "    def quantize_zeropoint(weights, n_bits=8):\n",
    "        \"\"\"Zero-point (asymmetric) quantization.\n",
    "        \n",
    "        Maps [min, max] to [0, 2^n - 1]\n",
    "        \"\"\"\n",
    "        qmin, qmax = 0, 2**n_bits - 1\n",
    "        w_min, w_max = weights.min(), weights.max()\n",
    "        \n",
    "        scale = (w_max - w_min) / (qmax - qmin)\n",
    "        zero_point = torch.round(-w_min / scale).clamp(qmin, qmax)\n",
    "        \n",
    "        quantized = torch.round(weights / scale + zero_point).clamp(qmin, qmax).to(torch.uint8)\n",
    "        return quantized, scale, zero_point\n",
    "    \n",
    "    @staticmethod\n",
    "    def dequantize_zeropoint(quantized, scale, zero_point):\n",
    "        \"\"\"Dequantize zero-point back to float.\"\"\"\n",
    "        return (quantized.float() - zero_point) * scale\n",
    "    \n",
    "    @staticmethod\n",
    "    def quantize_per_channel(weights, n_bits=8):\n",
    "        \"\"\"Per-channel quantization (one scale per output channel).\n",
    "        \n",
    "        Better quality than per-tensor because each channel has its own range.\n",
    "        \"\"\"\n",
    "        qmax = 2**(n_bits - 1) - 1\n",
    "        # Scale per output channel (row)\n",
    "        scales = weights.abs().max(dim=1, keepdim=True).values / qmax\n",
    "        scales = scales.clamp(min=1e-8)  # Avoid division by zero\n",
    "        quantized = torch.round(weights / scales).clamp(-qmax, qmax).to(torch.int8)\n",
    "        return quantized, scales.squeeze()\n",
    "    \n",
    "    @staticmethod\n",
    "    def dequantize_per_channel(quantized, scales):\n",
    "        return quantized.float() * scales.unsqueeze(1)\n",
    "\n",
    "\n",
    "# Create a sample weight matrix (simulating a linear layer)\n",
    "torch.manual_seed(42)\n",
    "weights = torch.randn(256, 512) * 0.02  # Typical weight scale\n",
    "\n",
    "quantizer = Quantizer()\n",
    "\n",
    "# Quantize with different methods\n",
    "q8_absmax, scale_8 = quantizer.quantize_absmax(weights, n_bits=8)\n",
    "dq8_absmax = quantizer.dequantize_absmax(q8_absmax, scale_8)\n",
    "\n",
    "q4_absmax, scale_4 = quantizer.quantize_absmax(weights, n_bits=4)\n",
    "dq4_absmax = quantizer.dequantize_absmax(q4_absmax, scale_4)\n",
    "\n",
    "q8_pc, scales_pc = quantizer.quantize_per_channel(weights, n_bits=8)\n",
    "dq8_pc = quantizer.dequantize_per_channel(q8_pc, scales_pc)\n",
    "\n",
    "# Measure quantization error\n",
    "def quantization_error(original, dequantized):\n",
    "    mse = ((original - dequantized) ** 2).mean().item()\n",
    "    max_err = (original - dequantized).abs().max().item()\n",
    "    snr = 10 * math.log10(original.var().item() / mse) if mse > 0 else float('inf')\n",
    "    return {'mse': mse, 'max_error': max_err, 'snr_db': snr}\n",
    "\n",
    "print(\"Quantization Results\\n\")\n",
    "print(f\"Original weights: {weights.shape}, dtype={weights.dtype}\")\n",
    "print(f\"Memory: {weights.numel() * 4 / 1024:.1f} KB (FP32)\\n\")\n",
    "\n",
    "methods = [\n",
    "    ('INT8 absmax', dq8_absmax, weights.numel() * 1),\n",
    "    ('INT4 absmax', dq4_absmax, weights.numel() * 0.5),\n",
    "    ('INT8 per-channel', dq8_pc, weights.numel() * 1),\n",
    "]\n",
    "\n",
    "print(f\"{'Method':>20} {'MSE':>12} {'Max Error':>12} {'SNR (dB)':>10} {'Memory (KB)':>12} {'Compression':>12}\")\n",
    "print(\"-\" * 80)\n",
    "for name, dq, mem_bytes in methods:\n",
    "    err = quantization_error(weights, dq)\n",
    "    mem_kb = mem_bytes / 1024\n",
    "    orig_kb = weights.numel() * 4 / 1024\n",
    "    print(f\"{name:>20} {err['mse']:>12.2e} {err['max_error']:>12.4f} {err['snr_db']:>10.1f} \"\n",
    "          f\"{mem_kb:>12.1f} {orig_kb/mem_kb:>11.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize quantization effects\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Weight distribution: original vs quantized\n",
    "ax = axes[0, 0]\n",
    "ax.hist(weights.flatten().numpy(), bins=100, alpha=0.6, label='FP32 original',\n",
    "       color='#3498db', edgecolor='black', density=True)\n",
    "ax.hist(dq8_absmax.flatten().numpy(), bins=100, alpha=0.6, label='INT8 dequantized',\n",
    "       color='#e74c3c', edgecolor='black', density=True)\n",
    "ax.set_xlabel('Weight Value', fontsize=10)\n",
    "ax.set_ylabel('Density', fontsize=10)\n",
    "ax.set_title('Weight Distribution: FP32 vs INT8', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# Quantization error distribution\n",
    "ax = axes[0, 1]\n",
    "error_8 = (weights - dq8_absmax).flatten().numpy()\n",
    "error_4 = (weights - dq4_absmax).flatten().numpy()\n",
    "ax.hist(error_8, bins=100, alpha=0.6, label='INT8 error', color='#2ecc71', density=True)\n",
    "ax.hist(error_4, bins=100, alpha=0.6, label='INT4 error', color='#e74c3c', density=True)\n",
    "ax.set_xlabel('Quantization Error', fontsize=10)\n",
    "ax.set_ylabel('Density', fontsize=10)\n",
    "ax.set_title('Quantization Error Distribution', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# Scatter: original vs quantized values\n",
    "ax = axes[1, 0]\n",
    "sample = np.random.choice(weights.numel(), 2000, replace=False)\n",
    "ax.scatter(weights.flatten()[sample].numpy(), dq8_absmax.flatten()[sample].numpy(),\n",
    "          alpha=0.2, s=5, color='#3498db', label='INT8')\n",
    "ax.scatter(weights.flatten()[sample].numpy(), dq4_absmax.flatten()[sample].numpy(),\n",
    "          alpha=0.2, s=5, color='#e74c3c', label='INT4')\n",
    "lim = 0.06\n",
    "ax.plot([-lim, lim], [-lim, lim], 'k--', alpha=0.3)\n",
    "ax.set_xlabel('Original (FP32)', fontsize=10)\n",
    "ax.set_ylabel('Dequantized', fontsize=10)\n",
    "ax.set_title('Original vs Dequantized Values', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# Memory comparison\n",
    "ax = axes[1, 1]\n",
    "model_sizes_gb = [7, 13, 70]  # Billion params\n",
    "precisions = ['FP32', 'FP16', 'INT8', 'INT4']\n",
    "multipliers = [4, 2, 1, 0.5]  # Bytes per param\n",
    "\n",
    "x = np.arange(len(model_sizes_gb))\n",
    "w = 0.2\n",
    "colors = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c']\n",
    "\n",
    "for i, (prec, mult, color) in enumerate(zip(precisions, multipliers, colors)):\n",
    "    mem = [s * mult for s in model_sizes_gb]\n",
    "    ax.bar(x + i * w, mem, w, label=prec, color=color, edgecolor='black', alpha=0.8)\n",
    "\n",
    "ax.set_xticks(x + 1.5 * w)\n",
    "ax.set_xticklabels([f'{s}B params' for s in model_sizes_gb])\n",
    "ax.set_ylabel('Memory (GB)', fontsize=10)\n",
    "ax.set_title('Model Memory by Precision', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "---\n\n## 3. KV Cache\n\nThe **KV cache** is the single most important optimization for autoregressive generation. Without it, generating token $t$ requires recomputing attention for all $t-1$ previous tokens.\n\n### The Problem\nIn self-attention, we compute:\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\nWhen generating token $t$, the keys and values for tokens $1$ through $t-1$ haven't changed — but without caching, we recompute them every time.\n\n### The Solution\nCache the K and V matrices from previous tokens. When generating token $t$:\n1. Compute Q, K, V for **only the new token**\n2. Append new K, V to the cache\n3. Attend to the full cached K, V\n\nThis reduces per-token compute from O(t^2) to O(t).\n\n**F1 analogy:** The KV cache is like the pit wall's running memory of the race. Without a cache, every time the strategist needs to make a call, they'd have to re-analyze every lap from the start — recalculating tire degradation curves, fuel load effects, and track evolution from scratch. With a KV cache, all that analysis is stored. When lap 42 comes in, the pit wall only computes the *new* lap's contribution and appends it to the existing analysis. The result is identical, but the wall clock time drops from \"re-process 42 laps\" to \"process 1 new lap.\" That's the difference between a strategy call taking 30 seconds and taking 1 second."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedAttention(nn.Module):\n",
    "    \"\"\"Self-attention with KV cache for efficient generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, cache=None):\n",
    "        \"\"\"Forward with optional KV cache.\n",
    "        \n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "            cache: tuple of (cached_K, cached_V) or None\n",
    "        \n",
    "        Returns:\n",
    "            output, (new_K, new_V)\n",
    "        \"\"\"\n",
    "        B, T, _ = x.shape\n",
    "        \n",
    "        Q = self.q_proj(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        K = self.k_proj(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        V = self.v_proj(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # Append to cache\n",
    "        if cache is not None:\n",
    "            cached_K, cached_V = cache\n",
    "            K = torch.cat([cached_K, K], dim=2)\n",
    "            V = torch.cat([cached_V, V], dim=2)\n",
    "        \n",
    "        # Standard attention\n",
    "        scale = math.sqrt(self.d_head)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / scale\n",
    "        \n",
    "        # Causal mask\n",
    "        seq_len_k = K.shape[2]\n",
    "        seq_len_q = Q.shape[2]\n",
    "        mask = torch.triu(torch.ones(seq_len_q, seq_len_k, device=x.device),\n",
    "                         diagonal=seq_len_k - seq_len_q + 1).bool()\n",
    "        scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, self.d_model)\n",
    "        \n",
    "        return self.out_proj(out), (K, V)\n",
    "\n",
    "\n",
    "# Benchmark: with vs without KV cache\n",
    "d_model = 128\n",
    "n_heads = 4\n",
    "attn = CachedAttention(d_model, n_heads)\n",
    "attn.eval()\n",
    "\n",
    "# Simulate generating 32 tokens\n",
    "gen_len = 32\n",
    "prompt = torch.randn(1, 8, d_model)  # 8-token prompt\n",
    "\n",
    "# WITHOUT cache: recompute everything each step\n",
    "start = time.perf_counter()\n",
    "all_tokens_no_cache = prompt.clone()\n",
    "with torch.no_grad():\n",
    "    for i in range(gen_len):\n",
    "        out, _ = attn(all_tokens_no_cache, cache=None)\n",
    "        new_token = out[:, -1:, :]  # Take last token output\n",
    "        all_tokens_no_cache = torch.cat([all_tokens_no_cache, new_token], dim=1)\n",
    "time_no_cache = time.perf_counter() - start\n",
    "\n",
    "# WITH cache: only process new token each step\n",
    "start = time.perf_counter()\n",
    "with torch.no_grad():\n",
    "    # Prefill: process prompt\n",
    "    out, cache = attn(prompt, cache=None)\n",
    "    generated = [out[:, -1:, :]]\n",
    "    \n",
    "    # Decode: one token at a time\n",
    "    for i in range(gen_len - 1):\n",
    "        out, cache = attn(generated[-1], cache=cache)\n",
    "        generated.append(out)\n",
    "time_with_cache = time.perf_counter() - start\n",
    "\n",
    "print(\"KV Cache Benchmark\\n\")\n",
    "print(f\"  Without cache: {time_no_cache*1000:.1f}ms\")\n",
    "print(f\"  With cache:    {time_with_cache*1000:.1f}ms\")\n",
    "print(f\"  Speedup:       {time_no_cache/time_with_cache:.1f}x\")\n",
    "print(f\"  Cache size:    K={cache[0].shape}, V={cache[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize KV cache mechanics and scaling\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Computation per step: with vs without cache\n",
    "ax = axes[0]\n",
    "seq_lens = np.arange(1, 65)\n",
    "# Without cache: O(t^2) total compute at step t\n",
    "no_cache_ops = seq_lens ** 2\n",
    "# With cache: O(t) compute at step t\n",
    "cache_ops = seq_lens\n",
    "\n",
    "ax.plot(seq_lens, no_cache_ops, 'r-', linewidth=2, label='Without KV cache (O(t²))')\n",
    "ax.plot(seq_lens, cache_ops, 'g-', linewidth=2, label='With KV cache (O(t))')\n",
    "ax.fill_between(seq_lens, cache_ops, no_cache_ops, alpha=0.15, color='red', label='Saved compute')\n",
    "ax.set_xlabel('Sequence Position', fontsize=11)\n",
    "ax.set_ylabel('Compute Operations', fontsize=11)\n",
    "ax.set_title('Per-Step Compute: Cache vs No Cache', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# KV cache memory usage\n",
    "ax = axes[1]\n",
    "seq_lengths = [512, 1024, 2048, 4096, 8192, 16384]\n",
    "# Cache size per layer = 2 * seq_len * d_model * batch_size (K and V)\n",
    "# For 7B model: ~32 layers, d_model=4096\n",
    "d = 4096\n",
    "n_layers = 32\n",
    "batch_sizes = [1, 4, 16]\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    cache_gb = [2 * sl * d * n_layers * bs * 2 / 1e9 for sl in seq_lengths]  # FP16\n",
    "    ax.plot(seq_lengths, cache_gb, 'o-', linewidth=2, markersize=6, label=f'Batch size {bs}')\n",
    "\n",
    "ax.set_xlabel('Sequence Length', fontsize=11)\n",
    "ax.set_ylabel('KV Cache Memory (GB)', fontsize=11)\n",
    "ax.set_title('KV Cache Memory (7B model, FP16)', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": "---\n\n## 4. Speculative Decoding\n\n**Speculative decoding** uses a small, fast \"draft\" model to propose multiple tokens at once, then verifies them with the large model in a single forward pass.\n\n### How It Works\n\n1. **Draft**: Small model generates K candidate tokens quickly\n2. **Verify**: Large model scores all K tokens in parallel (one forward pass)\n3. **Accept/reject**: Keep tokens where draft agrees with large model\n4. **Resample**: If rejected at position i, sample from large model's distribution there\n\nThe key insight: the large model does the **same amount of work** regardless of how many draft tokens are accepted, but each accepted token saves a full decode step.\n\n**F1 analogy:** Speculative decoding is like having a junior strategist (the draft model) sitting next to the chief strategist (the large model). The junior quickly sketches out the next 5 laps of strategy: \"stay out, stay out, pit lap 35, mediums, undercut.\" The chief strategist then reviews all 5 calls in one look — much faster than making each call independently. If the junior got the first 3 right, great — that saved 3 rounds of deliberation. If call 4 was wrong, the chief overrides from that point. The junior is fast but approximate; the chief is slow but authoritative. Together, they're faster than the chief alone."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeculativeDecoder:\n",
    "    \"\"\"Speculative decoding simulation.\"\"\"\n",
    "    \n",
    "    def __init__(self, target_model_time_ms, draft_model_time_ms, acceptance_rate=0.7):\n",
    "        \"\"\"\n",
    "        target_model_time_ms: Time for one forward pass of large model\n",
    "        draft_model_time_ms: Time for one forward pass of small model\n",
    "        acceptance_rate: Probability draft token matches target\n",
    "        \"\"\"\n",
    "        self.target_time = target_model_time_ms\n",
    "        self.draft_time = draft_model_time_ms\n",
    "        self.acceptance_rate = acceptance_rate\n",
    "    \n",
    "    def simulate_normal_decoding(self, n_tokens):\n",
    "        \"\"\"Standard autoregressive decoding.\"\"\"\n",
    "        return n_tokens * self.target_time\n",
    "    \n",
    "    def simulate_speculative(self, n_tokens, k=5):\n",
    "        \"\"\"Speculative decoding with k draft tokens.\"\"\"\n",
    "        total_time = 0\n",
    "        generated = 0\n",
    "        n_verify_calls = 0\n",
    "        n_draft_calls = 0\n",
    "        \n",
    "        while generated < n_tokens:\n",
    "            # Draft: generate k tokens with small model\n",
    "            total_time += k * self.draft_time\n",
    "            n_draft_calls += k\n",
    "            \n",
    "            # Verify: one forward pass of large model for all k tokens\n",
    "            total_time += self.target_time\n",
    "            n_verify_calls += 1\n",
    "            \n",
    "            # Accept tokens until first rejection\n",
    "            accepted = 0\n",
    "            for _ in range(k):\n",
    "                if np.random.random() < self.acceptance_rate:\n",
    "                    accepted += 1\n",
    "                else:\n",
    "                    accepted += 1  # We still get 1 token from large model at rejection point\n",
    "                    break\n",
    "            \n",
    "            generated += accepted\n",
    "        \n",
    "        return total_time, n_verify_calls, n_draft_calls\n",
    "\n",
    "\n",
    "# Compare decoding strategies\n",
    "decoder = SpeculativeDecoder(\n",
    "    target_model_time_ms=50,  # Large model: 50ms per token\n",
    "    draft_model_time_ms=5,    # Small model: 5ms per token\n",
    "    acceptance_rate=0.7\n",
    ")\n",
    "\n",
    "n_tokens = 256\n",
    "\n",
    "normal_time = decoder.simulate_normal_decoding(n_tokens)\n",
    "\n",
    "print(f\"Generating {n_tokens} tokens\\n\")\n",
    "print(f\"Normal decoding: {normal_time:.0f}ms ({n_tokens} target calls)\")\n",
    "\n",
    "# Try different k values\n",
    "results = []\n",
    "for k in [2, 3, 5, 8, 12]:\n",
    "    times = []\n",
    "    for _ in range(50):  # Average over runs\n",
    "        t, n_verify, n_draft = decoder.simulate_speculative(n_tokens, k=k)\n",
    "        times.append(t)\n",
    "    avg_time = np.mean(times)\n",
    "    speedup = normal_time / avg_time\n",
    "    results.append({'k': k, 'time': avg_time, 'speedup': speedup})\n",
    "    print(f\"Speculative (k={k:>2}): {avg_time:.0f}ms -> {speedup:.2f}x speedup\")\n",
    "\n",
    "# Also vary acceptance rate\n",
    "print(\"\\nEffect of acceptance rate (k=5):\")\n",
    "for rate in [0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    decoder.acceptance_rate = rate\n",
    "    times = [decoder.simulate_speculative(n_tokens, k=5)[0] for _ in range(50)]\n",
    "    avg = np.mean(times)\n",
    "    print(f\"  acceptance={rate:.0%}: {avg:.0f}ms -> {normal_time/avg:.2f}x speedup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize speculative decoding\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Speedup vs k\n",
    "ax = axes[0]\n",
    "ks = [r['k'] for r in results]\n",
    "speedups = [r['speedup'] for r in results]\n",
    "ax.plot(ks, speedups, 'bo-', linewidth=2, markersize=8)\n",
    "ax.axhline(y=1, color='gray', linestyle='--', alpha=0.5, label='No speedup')\n",
    "ax.set_xlabel('Draft Length (k)', fontsize=11)\n",
    "ax.set_ylabel('Speedup vs Normal Decoding', fontsize=11)\n",
    "ax.set_title('Speculative Decoding Speedup', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Speedup vs acceptance rate\n",
    "ax = axes[1]\n",
    "rates = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "for k in [3, 5, 8]:\n",
    "    speedups_rate = []\n",
    "    for rate in rates:\n",
    "        decoder.acceptance_rate = rate\n",
    "        times = [decoder.simulate_speculative(n_tokens, k=k)[0] for _ in range(30)]\n",
    "        speedups_rate.append(normal_time / np.mean(times))\n",
    "    ax.plot(rates, speedups_rate, 'o-', linewidth=2, markersize=6, label=f'k={k}')\n",
    "\n",
    "ax.set_xlabel('Acceptance Rate', fontsize=11)\n",
    "ax.set_ylabel('Speedup', fontsize=11)\n",
    "ax.set_title('Speedup vs Draft Quality', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": "---\n\n## 5. Continuous Batching\n\nIn naive batching, all requests in a batch must wait for the longest request to finish. **Continuous batching** (also called \"inflight batching\") allows new requests to join and completed requests to leave the batch dynamically.\n\n| Approach | Throughput | Latency | GPU Utilization | F1 Parallel |\n|----------|-----------|---------|----------------|-------------|\n| No batching | Low | Optimal per-request | Very low | Processing one car's data at a time — other 19 cars wait idle |\n| Static batching | Medium | Worst-case per-batch | Medium | Processing all 20 cars together, but waiting until the slowest finishes before starting the next batch |\n| Continuous batching | High | Near-optimal | High | Processing multiple cars' data simultaneously, with each car's analysis completing and freeing resources independently |\n\n**F1 analogy:** Continuous batching is how a modern F1 pit wall actually processes data from all 20 cars on the grid. Without batching, the system would analyze Car 1's telemetry, then Car 2's, then Car 3's — serialized and slow. Static batching would process all 20 cars together but wait for the most complex analysis (say, the car on a complex mixed strategy) to finish before starting any new work. Continuous batching is the real-world approach: as soon as Car 7's simple \"stay out\" analysis finishes, that compute slot immediately starts processing new data, even while Car 14's complex undercut calculation is still running."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchingSimulator:\n",
    "    \"\"\"Simulate different batching strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self, time_per_token_ms=10, max_batch_size=16):\n",
    "        self.time_per_token = time_per_token_ms\n",
    "        self.max_batch = max_batch_size\n",
    "    \n",
    "    def no_batching(self, requests):\n",
    "        \"\"\"Process requests one at a time.\"\"\"\n",
    "        results = []\n",
    "        current_time = 0\n",
    "        \n",
    "        for req in requests:\n",
    "            start = max(current_time, req['arrival'])\n",
    "            duration = req['output_tokens'] * self.time_per_token\n",
    "            end = start + duration\n",
    "            results.append({\n",
    "                'latency': end - req['arrival'],\n",
    "                'start': start, 'end': end\n",
    "            })\n",
    "            current_time = end\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def static_batching(self, requests, batch_size=4):\n",
    "        \"\"\"Process requests in fixed-size batches.\"\"\"\n",
    "        results = [None] * len(requests)\n",
    "        current_time = 0\n",
    "        \n",
    "        for i in range(0, len(requests), batch_size):\n",
    "            batch = requests[i:i+batch_size]\n",
    "            batch_start = max(current_time, max(r['arrival'] for r in batch))\n",
    "            \n",
    "            # All requests must wait for the longest one\n",
    "            max_tokens = max(r['output_tokens'] for r in batch)\n",
    "            # Batched: ~same time as single request (parallel on GPU)\n",
    "            duration = max_tokens * self.time_per_token\n",
    "            batch_end = batch_start + duration\n",
    "            \n",
    "            for j, req in enumerate(batch):\n",
    "                results[i + j] = {\n",
    "                    'latency': batch_end - req['arrival'],\n",
    "                    'start': batch_start, 'end': batch_end\n",
    "                }\n",
    "            current_time = batch_end\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def continuous_batching(self, requests, max_batch=8):\n",
    "        \"\"\"Process requests with continuous batching.\"\"\"\n",
    "        results = [None] * len(requests)\n",
    "        active = []  # (request_idx, tokens_remaining)\n",
    "        queue = list(range(len(requests)))\n",
    "        current_time = 0\n",
    "        \n",
    "        while queue or active:\n",
    "            # Add new requests to batch\n",
    "            while queue and len(active) < max_batch:\n",
    "                idx = queue[0]\n",
    "                if requests[idx]['arrival'] <= current_time:\n",
    "                    queue.pop(0)\n",
    "                    active.append((idx, requests[idx]['output_tokens']))\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            if not active:\n",
    "                if queue:\n",
    "                    current_time = requests[queue[0]]['arrival']\n",
    "                continue\n",
    "            \n",
    "            # Process one step for all active requests\n",
    "            current_time += self.time_per_token\n",
    "            \n",
    "            new_active = []\n",
    "            for idx, remaining in active:\n",
    "                if remaining <= 1:\n",
    "                    # Request complete\n",
    "                    results[idx] = {\n",
    "                        'latency': current_time - requests[idx]['arrival'],\n",
    "                        'end': current_time\n",
    "                    }\n",
    "                else:\n",
    "                    new_active.append((idx, remaining - 1))\n",
    "            active = new_active\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Simulate requests\n",
    "np.random.seed(42)\n",
    "n_requests = 20\n",
    "requests = []\n",
    "for i in range(n_requests):\n",
    "    requests.append({\n",
    "        'arrival': i * 50,  # 50ms between arrivals\n",
    "        'output_tokens': np.random.randint(10, 100),\n",
    "    })\n",
    "\n",
    "sim = BatchingSimulator(time_per_token_ms=10)\n",
    "\n",
    "no_batch_results = sim.no_batching(requests)\n",
    "static_results = sim.static_batching(requests, batch_size=4)\n",
    "continuous_results = sim.continuous_batching(requests, max_batch=8)\n",
    "\n",
    "print(\"Batching Strategy Comparison\\n\")\n",
    "for name, results in [('No batching', no_batch_results),\n",
    "                       ('Static (bs=4)', static_results),\n",
    "                       ('Continuous (max=8)', continuous_results)]:\n",
    "    latencies = [r['latency'] for r in results if r is not None]\n",
    "    total_time = max(r['end'] for r in results if r is not None)\n",
    "    throughput = n_requests / (total_time / 1000)  # req/sec\n",
    "    print(f\"  {name:>25}: avg latency={np.mean(latencies):.0f}ms, \"\n",
    "          f\"p99={np.percentile(latencies, 99):.0f}ms, \"\n",
    "          f\"throughput={throughput:.1f} req/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize batching comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Latency distribution\n",
    "ax = axes[0]\n",
    "strategies = [\n",
    "    ('No batching', no_batch_results, '#e74c3c'),\n",
    "    ('Static', static_results, '#f39c12'),\n",
    "    ('Continuous', continuous_results, '#2ecc71'),\n",
    "]\n",
    "\n",
    "for name, results, color in strategies:\n",
    "    latencies = sorted([r['latency'] for r in results if r is not None])\n",
    "    ax.plot(range(len(latencies)), latencies, 'o-', color=color, linewidth=2,\n",
    "           markersize=4, label=name)\n",
    "\n",
    "ax.set_xlabel('Request (sorted by latency)', fontsize=11)\n",
    "ax.set_ylabel('Latency (ms)', fontsize=11)\n",
    "ax.set_title('Per-Request Latency', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Summary bars\n",
    "ax = axes[1]\n",
    "names = ['No\\nbatching', 'Static\\n(bs=4)', 'Continuous\\n(max=8)']\n",
    "avg_latencies = []\n",
    "throughputs = []\n",
    "\n",
    "for _, results, _ in strategies:\n",
    "    latencies = [r['latency'] for r in results if r is not None]\n",
    "    total_time = max(r['end'] for r in results if r is not None)\n",
    "    avg_latencies.append(np.mean(latencies))\n",
    "    throughputs.append(n_requests / (total_time / 1000))\n",
    "\n",
    "x = np.arange(len(names))\n",
    "w = 0.35\n",
    "colors_bar = ['#3498db', '#e74c3c']\n",
    "ax.bar(x - w/2, avg_latencies, w, label='Avg Latency (ms)', color='#3498db', edgecolor='black')\n",
    "ax2 = ax.twinx()\n",
    "ax2.bar(x + w/2, throughputs, w, label='Throughput (req/s)', color='#2ecc71', edgecolor='black')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(names)\n",
    "ax.set_ylabel('Avg Latency (ms)', fontsize=10, color='#3498db')\n",
    "ax2.set_ylabel('Throughput (req/s)', fontsize=10, color='#2ecc71')\n",
    "ax.set_title('Batching Strategy Comparison', fontsize=13, fontweight='bold')\n",
    "ax.legend(loc='upper left', fontsize=9)\n",
    "ax2.legend(loc='upper right', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": "---\n\n## 6. Knowledge Distillation\n\n**Distillation** trains a small \"student\" model to mimic a large \"teacher\" model. The student learns from the teacher's soft probability distributions, which carry more information than hard labels.\n\n$$\\mathcal{L}_{\\text{distill}} = \\alpha \\cdot T^2 \\cdot \\text{KL}\\left(\\sigma\\left(\\frac{z_s}{T}\\right) \\| \\sigma\\left(\\frac{z_t}{T}\\right)\\right) + (1-\\alpha) \\cdot \\text{CE}(y, z_s)$$\n\nwhere $T$ is the temperature and $\\alpha$ balances distillation vs. hard-label loss.\n\n**F1 analogy:** Distillation is like the relationship between a team's full CFD simulation and the simplified model that runs on the pit wall during a race. The full CFD (teacher) takes hours to compute aerodynamic loads for a single configuration — far too slow for race day. But by having the simplified model (student) learn from thousands of CFD outputs, the pit wall model captures the *essence* of the full simulation's knowledge. The temperature parameter controls how much \"soft\" insight transfers: at high temperature, the student learns not just that \"low downforce is best for Monza\" but *how much better* it is than medium downforce, and how close medium is to high — the full ranking, not just the winner."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationTrainer:\n",
    "    \"\"\"Knowledge distillation from teacher to student model.\"\"\"\n",
    "    \n",
    "    def __init__(self, teacher, student, temperature=4.0, alpha=0.7):\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def distillation_loss(self, student_logits, teacher_logits, targets):\n",
    "        \"\"\"Compute combined distillation + hard label loss.\"\"\"\n",
    "        T = self.temperature\n",
    "        \n",
    "        # Soft target loss (KL divergence on softened distributions)\n",
    "        student_soft = F.log_softmax(student_logits / T, dim=-1)\n",
    "        teacher_soft = F.softmax(teacher_logits / T, dim=-1)\n",
    "        soft_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (T * T)\n",
    "        \n",
    "        # Hard target loss\n",
    "        hard_loss = F.cross_entropy(student_logits, targets)\n",
    "        \n",
    "        return self.alpha * soft_loss + (1 - self.alpha) * hard_loss\n",
    "    \n",
    "    def train_step(self, x, targets, optimizer):\n",
    "        \"\"\"One training step.\"\"\"\n",
    "        self.teacher.eval()\n",
    "        self.student.train()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            teacher_logits = self.teacher(x)\n",
    "        \n",
    "        student_logits = self.student(x)\n",
    "        loss = self.distillation_loss(student_logits, teacher_logits, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "# Create teacher (large) and student (small) models\n",
    "n_classes = 10\n",
    "input_dim = 64\n",
    "\n",
    "teacher = nn.Sequential(\n",
    "    nn.Linear(input_dim, 256), nn.ReLU(),\n",
    "    nn.Linear(256, 128), nn.ReLU(),\n",
    "    nn.Linear(128, n_classes)\n",
    ")\n",
    "\n",
    "student = nn.Sequential(\n",
    "    nn.Linear(input_dim, 32), nn.ReLU(),\n",
    "    nn.Linear(32, n_classes)\n",
    ")\n",
    "\n",
    "student_no_distill = nn.Sequential(\n",
    "    nn.Linear(input_dim, 32), nn.ReLU(),\n",
    "    nn.Linear(32, n_classes)\n",
    ")\n",
    "# Copy initial weights so comparison is fair\n",
    "student_no_distill.load_state_dict(student.state_dict())\n",
    "\n",
    "teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "student_params = sum(p.numel() for p in student.parameters())\n",
    "print(f\"Teacher: {teacher_params:,} params\")\n",
    "print(f\"Student: {student_params:,} params ({student_params/teacher_params:.1%} of teacher)\")\n",
    "\n",
    "# Generate synthetic dataset\n",
    "n_train = 500\n",
    "X = torch.randn(n_train, input_dim)\n",
    "# Teacher generates \"ground truth\"\n",
    "teacher.eval()\n",
    "with torch.no_grad():\n",
    "    y = teacher(X).argmax(dim=-1)\n",
    "\n",
    "# Train teacher to convergence first\n",
    "teacher_opt = torch.optim.Adam(teacher.parameters(), lr=1e-3)\n",
    "teacher.train()\n",
    "for _ in range(200):\n",
    "    loss = F.cross_entropy(teacher(X), y)\n",
    "    teacher_opt.zero_grad()\n",
    "    loss.backward()\n",
    "    teacher_opt.step()\n",
    "\n",
    "teacher.eval()\n",
    "teacher_acc = (teacher(X).argmax(dim=-1) == y).float().mean().item()\n",
    "print(f\"\\nTeacher accuracy: {teacher_acc:.1%}\")\n",
    "\n",
    "# Train student WITH distillation\n",
    "distiller = DistillationTrainer(teacher, student, temperature=4.0, alpha=0.7)\n",
    "student_opt = torch.optim.Adam(student.parameters(), lr=1e-3)\n",
    "\n",
    "distill_losses = []\n",
    "for epoch in range(200):\n",
    "    loss = distiller.train_step(X, y, student_opt)\n",
    "    distill_losses.append(loss)\n",
    "\n",
    "# Train student WITHOUT distillation (hard labels only)\n",
    "no_distill_opt = torch.optim.Adam(student_no_distill.parameters(), lr=1e-3)\n",
    "no_distill_losses = []\n",
    "student_no_distill.train()\n",
    "for epoch in range(200):\n",
    "    logits = student_no_distill(X)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    no_distill_opt.zero_grad()\n",
    "    loss.backward()\n",
    "    no_distill_opt.step()\n",
    "    no_distill_losses.append(loss.item())\n",
    "\n",
    "# Evaluate\n",
    "student.eval()\n",
    "student_no_distill.eval()\n",
    "distill_acc = (student(X).argmax(dim=-1) == y).float().mean().item()\n",
    "no_distill_acc = (student_no_distill(X).argmax(dim=-1) == y).float().mean().item()\n",
    "\n",
    "print(f\"Student (with distillation): {distill_acc:.1%}\")\n",
    "print(f\"Student (without distillation): {no_distill_acc:.1%}\")\n",
    "print(f\"\\nDistillation advantage: {distill_acc - no_distill_acc:+.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distillation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training loss comparison\n",
    "ax = axes[0]\n",
    "w = 10\n",
    "smooth_distill = [np.mean(distill_losses[max(0,i-w):i+1]) for i in range(len(distill_losses))]\n",
    "smooth_no = [np.mean(no_distill_losses[max(0,i-w):i+1]) for i in range(len(no_distill_losses))]\n",
    "ax.plot(smooth_distill, linewidth=2, label='With distillation', color='#2ecc71')\n",
    "ax.plot(smooth_no, linewidth=2, label='Without distillation', color='#e74c3c')\n",
    "ax.set_xlabel('Epoch', fontsize=11)\n",
    "ax.set_ylabel('Loss', fontsize=11)\n",
    "ax.set_title('Student Training Loss', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy comparison bar chart\n",
    "ax = axes[1]\n",
    "models = ['Teacher\\n(large)', 'Student +\\nDistillation', 'Student\\n(hard labels)']\n",
    "accs = [teacher_acc, distill_acc, no_distill_acc]\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "params = [teacher_params, student_params, student_params]\n",
    "\n",
    "bars = ax.bar(models, accs, color=colors, edgecolor='black', alpha=0.8)\n",
    "for bar, acc, p in zip(bars, accs, params):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{acc:.1%}\\n({p:,} params)', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Accuracy', fontsize=11)\n",
    "ax.set_title('Model Comparison', fontsize=13, fontweight='bold')\n",
    "ax.set_ylim(0, 1.15)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": "---\n\n## 7. Optimization Techniques Compared\n\nLet's compare all optimization techniques on the dimensions that matter — like comparing different car development strategies on lap time, reliability, and cost.\n\n**F1 analogy:** Just as an F1 team evaluates upgrades on multiple dimensions (lap time gain vs. weight vs. reliability vs. cost), inference optimizations must be evaluated on speed, memory, quality, and implementation complexity. The best teams stack multiple optimizations, just as the best serving systems combine quantization + KV cache + continuous batching."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison\n",
    "techniques = {\n",
    "    'Baseline (FP32)':     {'memory': 1.0, 'speed': 1.0, 'quality': 1.0, 'complexity': 1},\n",
    "    'FP16':                {'memory': 0.5, 'speed': 1.5, 'quality': 0.99, 'complexity': 1},\n",
    "    'INT8 Quant':          {'memory': 0.25, 'speed': 2.0, 'quality': 0.97, 'complexity': 2},\n",
    "    'INT4 Quant':          {'memory': 0.125, 'speed': 2.5, 'quality': 0.93, 'complexity': 3},\n",
    "    'KV Cache':            {'memory': 1.1, 'speed': 3.0, 'quality': 1.0, 'complexity': 2},\n",
    "    'Speculative':         {'memory': 1.3, 'speed': 2.5, 'quality': 1.0, 'complexity': 4},\n",
    "    'Continuous Batch':    {'memory': 1.0, 'speed': 2.0, 'quality': 1.0, 'complexity': 3},\n",
    "    'Distillation':        {'memory': 0.3, 'speed': 3.0, 'quality': 0.90, 'complexity': 4},\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Speed vs Memory tradeoff\n",
    "ax = axes[0]\n",
    "for name, vals in techniques.items():\n",
    "    color = plt.cm.viridis(vals['quality'])\n",
    "    size = (1.1 - vals['memory']) * 200 + 50  # Bigger = less memory\n",
    "    ax.scatter(vals['memory'], vals['speed'], s=size, c=[vals['quality']],\n",
    "              cmap='RdYlGn', vmin=0.85, vmax=1.0, edgecolors='black', linewidth=1, zorder=5)\n",
    "    ax.annotate(name, (vals['memory'], vals['speed']),\n",
    "               textcoords='offset points', xytext=(8, 5), fontsize=8)\n",
    "\n",
    "ax.set_xlabel('Memory (relative to baseline)', fontsize=11)\n",
    "ax.set_ylabel('Speed (relative to baseline)', fontsize=11)\n",
    "ax.set_title('Speed vs Memory (color=quality)', fontsize=13, fontweight='bold')\n",
    "ax.axhline(y=1, color='gray', linestyle='--', alpha=0.3)\n",
    "ax.axvline(x=1, color='gray', linestyle='--', alpha=0.3)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Bar comparison\n",
    "ax = axes[1]\n",
    "names = list(techniques.keys())\n",
    "x = np.arange(len(names))\n",
    "w = 0.2\n",
    "\n",
    "metrics = ['speed', 'quality']\n",
    "colors_bar = ['#3498db', '#2ecc71']\n",
    "labels = ['Speed', 'Quality']\n",
    "\n",
    "for i, (metric, color, label) in enumerate(zip(metrics, colors_bar, labels)):\n",
    "    vals = [techniques[n][metric] for n in names]\n",
    "    ax.bar(x + i * w, vals, w, label=label, color=color, edgecolor='black', alpha=0.8)\n",
    "\n",
    "ax.set_xticks(x + w/2)\n",
    "ax.set_xticklabels(names, rotation=40, ha='right', fontsize=8)\n",
    "ax.set_ylabel('Relative to Baseline', fontsize=10)\n",
    "ax.set_title('Speed and Quality by Technique', fontsize=13, fontweight='bold')\n",
    "ax.axhline(y=1, color='gray', linestyle='--', alpha=0.3)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": "---\n\n## Exercises\n\n### Exercise 1: Group Quantization\n\nImplement **group quantization** where weights are quantized in groups of 128 (each group has its own scale). Compare error against per-tensor and per-channel quantization at INT4 precision.\n\n**F1 scenario:** Different sections of a telemetry trace have wildly different ranges — brake pressure spikes to 200 bar in braking zones but sits at 0 on straights. Group quantization is like having separate precision scales for each track sector: high-range encoding for braking zones, fine-grained encoding for smooth straights. Implement this and show it reduces quantization error compared to one-size-fits-all encoding."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Hint: Reshape weights into groups of 128, quantize each group independently,\n",
    "# then reshape back. Compare MSE against per-tensor and per-channel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": "### Exercise 2: PagedAttention Simulator\n\nImplement a simplified version of **PagedAttention** (used in vLLM). Instead of pre-allocating KV cache for max sequence length, allocate fixed-size pages on demand. Show memory savings compared to naive pre-allocation.\n\n**F1 scenario:** Pre-allocating KV cache for max sequence length is like reserving pit wall memory for a full 78-lap race for every car, even if some cars retire on lap 1. PagedAttention allocates memory in fixed-size \"pages\" on demand — like reserving pit wall compute for each car only as their race progresses. Show the memory savings when cars (requests) have varying race lengths (sequence lengths)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Hint: Create a page table that maps sequence positions to memory pages.\n",
    "# Track allocated vs wasted memory compared to contiguous allocation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": "### Exercise 3: Distillation with Temperature Sweep\n\nRun distillation experiments at temperatures T = 1, 2, 4, 8, 16. Plot student accuracy vs temperature. What temperature works best and why?\n\n**F1 scenario:** Temperature in distillation controls how much nuance transfers from the full CFD simulation to the pit wall model. Low T (T=1) gives sharp \"this setup is best\" signals. High T (T=16) gives soft \"here's the full ranking of all setups with their relative merits.\" Find the sweet spot where the pit wall model learns the most useful knowledge from the CFD teacher."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": "---\n\n## Summary\n\n### Key Concepts\n\n| Concept | What It Does | F1 Parallel |\n|---------|-------------|-------------|\n| **Memory-bound decoding** | Most inference time is moving weights, not computing | Pit wall bandwidth limit — the data link, not the CPU, is the bottleneck |\n| **Quantization** (INT8/INT4) | Reduces memory 4-8x with minimal quality loss | Reducing telemetry precision from float32 to int8 for faster pit wall processing |\n| **KV cache** | Eliminates redundant computation, O(t^2) to O(t) | Caching computed analysis for already-processed laps instead of re-analyzing from scratch |\n| **Speculative decoding** | Small draft model proposes, large model verifies in parallel | Junior strategist proposes next 5 calls, chief reviews them all at once |\n| **Continuous batching** | Dynamically adds/removes requests from GPU batches | Processing all 20 cars simultaneously, each completing independently |\n| **Distillation** | Small student learns from large teacher's soft outputs | Pit wall model trained on thousands of full CFD simulation results |\n| **Stacking** | Production systems combine all techniques together | The complete pit wall stack: compressed telemetry + cached history + parallel processing |\n\n### The Optimization Stack\n\nIn production, you don't pick one technique — you layer them, just like an F1 team layers every marginal gain:\n1. **Distillation** -> Smaller model (like distilling a full CFD sim into a real-time pit wall model)\n2. **Quantization** -> Less memory per parameter (like compressing telemetry for faster radio transmission)\n3. **KV cache** -> Less redundant compute (like keeping a running race analysis instead of starting from scratch)\n4. **Continuous batching** -> Higher throughput (like processing all cars' data simultaneously)\n5. **Speculative decoding** -> Lower latency (like having a junior strategist pre-draft calls for the chief to approve)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": "---\n\n## Next Steps\n\nOptimizing inference is about making models fast — getting the strategy model from the simulation farm to the live pit wall. But building reliable ML systems requires more than fast models — it requires **experiment tracking, reproducibility, and systematic model management**. In **Notebook 27: ML Systems & Experiment Tracking**, we'll build the infrastructure that makes ML development systematic — the factory behind the race team."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}