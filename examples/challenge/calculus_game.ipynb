{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculus Challenge Arena\n",
    "\n",
    "Welcome, challenger! This notebook tests your calculus knowledge through progressively harder challenges. Each concept is a **World** with multiple **Levels** and a **Boss Level**.\n",
    "\n",
    "**Rules:**\n",
    "- Try each challenge before looking at hints\n",
    "- Hints cost points ‚Äî use them wisely!\n",
    "- Solutions explain the concept AFTER you've struggled with it\n",
    "- Bonus challenges are optional but earn extra points"
   ],
   "id": "cell-0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Tracker\n",
    "\n",
    "| Level | Challenge | Type | Points | Status |\n",
    "|-------|-----------|------|--------|--------|\n",
    "| 1.1 | The Slope Finder | Compute | /10 | ‚¨ú |\n",
    "| 1.2 | Shape Predictor | Predict | /15 | ‚¨ú |\n",
    "| 1.3 | Derivative Detective | Debug | /20 | ‚¨ú |\n",
    "| üèÜ | Boss: Activation Function Analysis | Connect | /30 | ‚¨ú |\n",
    "| 2.1 | Chain Reaction | Compute | /10 | ‚¨ú |\n",
    "| 2.2 | The Missing Link | Debug | /15 | ‚¨ú |\n",
    "| üèÜ | Boss: Backprop Simulator | Construct | /30 | ‚¨ú |\n",
    "| 3.1 | Downhill Runner | Compute | /10 | ‚¨ú |\n",
    "| 3.2 | Rate Tuner | Predict | /15 | ‚¨ú |\n",
    "| 3.3 | Valley Finder | Optimize | /20 | ‚¨ú |\n",
    "| üèÜ | Boss: The Optimizer | Construct | /30 | ‚¨ú |\n",
    "| üëë | Final Boss: Train a Model | Construct | /50 | ‚¨ú |\n",
    "| | **Total** | | **/255** | |"
   ],
   "id": "cell-1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def numerical_derivative(f, x, h=1e-5):\n",
    "    \"\"\"Helper: compute derivative numerically.\"\"\"\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "def check_answer(yours, correct, tolerance=0.01):\n",
    "    \"\"\"Check if your answer is close enough.\"\"\"\n",
    "    if abs(yours - correct) < tolerance:\n",
    "        print(f\"\\u2705 Correct! Your answer: {yours:.4f}, Expected: {correct:.4f}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"\\u274c Not quite. Your answer: {yours:.4f}, Expected: {correct:.4f}\")\n",
    "        return False"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World 1: Derivatives"
   ],
   "id": "cell-3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1.1: The Slope Finder\n",
    "\n",
    "**Type:** Compute | **Points:** 10 | **Difficulty:** Easy\n",
    "\n",
    "### Mission\n",
    "\n",
    "Find the derivative of f(x) = 3x¬≤ + 2x - 5 and evaluate it at x = 2.\n",
    "\n",
    "### Intel\n",
    "\n",
    "The **power rule** says: if f(x) = x^n, then f'(x) = n * x^(n-1).\n",
    "For a sum: take the derivative of each term separately.\n",
    "\n",
    "### Hints\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1 (Conceptual Nudge)</summary>\n",
    "\n",
    "Break the function into three terms and find the derivative of each one separately.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2 (Methodological Clue)</summary>\n",
    "\n",
    "- d/dx[3x¬≤] = 3 * 2x = 6x\n",
    "- d/dx[2x] = 2\n",
    "- d/dx[-5] = 0\n",
    "\n",
    "Now combine and plug in x = 2.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 3 (Detailed Walkthrough)</summary>\n",
    "\n",
    "f'(x) = 6x + 2. At x = 2: f'(2) = 6(2) + 2 = 14.\n",
    "\n",
    "</details>"
   ],
   "id": "cell-4"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Level 1.1 ‚Äî YOUR SOLUTION\n",
    "# Write your derivative function and evaluate at x = 2\n",
    "\n",
    "def f(x):\n",
    "    return 3*x**2 + 2*x - 5\n",
    "\n",
    "# TODO: Replace None with the derivative function\n",
    "def f_prime(x):\n",
    "    return 6*x + 2  # YOUR ANSWER\n",
    "\n",
    "answer = f_prime(2)\n",
    "print(f\"f'(2) = {answer}\")\n",
    "check_answer(answer, 14.0)\n",
    "\n",
    "# Verify with numerical derivative\n",
    "print(f\"Numerical check: {numerical_derivative(f, 2):.4f}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1.2: Shape Predictor\n",
    "\n",
    "**Type:** Predict | **Points:** 15 | **Difficulty:** Medium\n",
    "\n",
    "### Mission\n",
    "\n",
    "Without computing, predict: where does f(x) = x¬≥ - 3x have zero slope? Then verify by finding the derivative, setting it to zero, and plotting both the function and its derivative.\n",
    "\n",
    "### Intel\n",
    "\n",
    "A function has **zero slope** at points where f'(x) = 0. These are called **critical points**. At these points the function is either at a local maximum, a local minimum, or an inflection point. Think about the shape of a cubic curve ‚Äî it rises, flattens, dips, flattens, then rises again.\n",
    "\n",
    "### Hints\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1 (Conceptual Nudge)</summary>\n",
    "\n",
    "A cubic like x¬≥ - 3x has an S-shape. It has one local max and one local min. Where do those occur?\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2 (Methodological Clue)</summary>\n",
    "\n",
    "f'(x) = 3x¬≤ - 3. Set this equal to zero: 3x¬≤ - 3 = 0. Can you solve for x?\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 3 (Detailed Walkthrough)</summary>\n",
    "\n",
    "3x¬≤ - 3 = 0 means x¬≤ = 1 means x = +1 or x = -1. At x = -1 the function has a local max, at x = 1 it has a local min.\n",
    "\n",
    "</details>"
   ],
   "id": "cell-6"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Level 1.2 ‚Äî Verification\n",
    "# f(x) = x¬≥ - 3x, f'(x) = 3x¬≤ - 3\n",
    "# Set f'(x) = 0: 3x¬≤ - 3 = 0 -> x¬≤ = 1 -> x = +/-1\n",
    "\n",
    "f = lambda x: x**3 - 3*x\n",
    "f_prime = lambda x: 3*x**2 - 3\n",
    "\n",
    "x = np.linspace(-3, 3, 200)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.plot(x, f(x), 'b-', linewidth=2)\n",
    "ax1.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax1.scatter([1, -1], [f(1), f(-1)], color='red', s=100, zorder=5)\n",
    "ax1.set_title('f(x) = x\\u00b3 - 3x')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(x, f_prime(x), 'r-', linewidth=2)\n",
    "ax2.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax2.scatter([1, -1], [0, 0], color='red', s=100, zorder=5)\n",
    "ax2.set_title(\"f'(x) = 3x\\u00b2 - 3 (zero at x = \\u00b11)\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Critical points at x = -1 and x = 1\")\n",
    "print(f\"f'(-1) = {f_prime(-1):.1f}, f'(1) = {f_prime(1):.1f} ‚Äî both zero! \\u2705\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1.3: Derivative Detective\n",
    "\n",
    "**Type:** Debug | **Points:** 20 | **Difficulty:** Hard\n",
    "\n",
    "### Mission\n",
    "\n",
    "This code computes the derivative of sin(x) but has a bug. Find and fix it.\n",
    "\n",
    "```python\n",
    "# BUGGY CODE:\n",
    "import numpy as np\n",
    "def my_derivative(f, x, h=1e-5):\n",
    "    return (f(x + h) - f(x)) / h  # Something is wrong here...\n",
    "```\n",
    "\n",
    "The function gives a result, but it is significantly less accurate than it should be. Can you figure out why and fix it?\n",
    "\n",
    "### Intel\n",
    "\n",
    "There are multiple ways to approximate a derivative numerically. The most common are:\n",
    "- **Forward difference:** (f(x+h) - f(x)) / h\n",
    "- **Centered difference:** (f(x+h) - f(x-h)) / (2h)\n",
    "\n",
    "One of these is significantly more accurate than the other. The error in the forward difference is O(h), while the centered difference has error O(h¬≤).\n",
    "\n",
    "### Hints\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1 (Conceptual Nudge)</summary>\n",
    "\n",
    "The bug is about accuracy, not about getting a completely wrong answer. Compare the error of the buggy code vs. what a better method would give.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2 (Methodological Clue)</summary>\n",
    "\n",
    "The forward difference only looks at one side of x. A centered difference looks at both sides and gives a much better approximation of the slope at x.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 3 (Detailed Walkthrough)</summary>\n",
    "\n",
    "Replace `(f(x + h) - f(x)) / h` with `(f(x + h) - f(x - h)) / (2 * h)`. This changes the error from O(h) to O(h¬≤), making it roughly 100,000x more accurate for h = 1e-5.\n",
    "\n",
    "</details>"
   ],
   "id": "cell-8"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Level 1.3 ‚Äî The Bug: Forward difference vs centered difference\n",
    "\n",
    "# Buggy version (forward difference)\n",
    "def bad_derivative(f, x, h=1e-5):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "# Fixed version (centered difference - much more accurate!)\n",
    "def good_derivative(f, x, h=1e-5):\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "x0 = 1.0\n",
    "exact = np.cos(x0)  # derivative of sin(x) is cos(x)\n",
    "bad = bad_derivative(np.sin, x0)\n",
    "good = good_derivative(np.sin, x0)\n",
    "\n",
    "print(f\"Exact derivative of sin(x) at x={x0}: {exact:.10f}\")\n",
    "print(f\"Forward difference (buggy):            {bad:.10f}  error: {abs(bad-exact):.2e}\")\n",
    "print(f\"Centered difference (fixed):           {good:.10f}  error: {abs(good-exact):.2e}\")\n",
    "print(f\"\\nCentered difference is ~{abs(bad-exact)/abs(good-exact):.0f}x more accurate! \\u2705\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boss Level: Activation Function Analysis\n",
    "\n",
    "**Type:** Connect | **Points:** 30 | **Difficulty:** Boss\n",
    "\n",
    "### Mission\n",
    "\n",
    "The sigmoid, ReLU, and tanh are all \"activation functions\" used in neural networks. Your mission:\n",
    "\n",
    "1. Compute and plot each function AND its derivative.\n",
    "2. Then explain: why might a neural network prefer ReLU over sigmoid?\n",
    "\n",
    "### Intel\n",
    "\n",
    "- **Sigmoid:** sigma(x) = 1 / (1 + e^(-x)), outputs between 0 and 1\n",
    "- **ReLU:** relu(x) = max(0, x), outputs 0 or positive\n",
    "- **Tanh:** tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x)), outputs between -1 and 1\n",
    "\n",
    "In neural networks, gradients flow backward through layers. If a derivative is consistently small, gradients shrink to near-zero ‚Äî this is the **vanishing gradient problem**.\n",
    "\n",
    "### Hints\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1 (Conceptual Nudge)</summary>\n",
    "\n",
    "Plot the derivatives of all three. What is the maximum value each derivative can reach? Which one lets the largest gradients through?\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2 (Methodological Clue)</summary>\n",
    "\n",
    "- Sigmoid derivative: sigma(x) * (1 - sigma(x)), max value = 0.25\n",
    "- ReLU derivative: 0 when x < 0, 1 when x > 0\n",
    "- Tanh derivative: 1 - tanh(x)¬≤, max value = 1.0\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 3 (Detailed Walkthrough)</summary>\n",
    "\n",
    "Sigmoid's derivative maxes at 0.25, so in every layer the gradient is multiplied by at most 0.25. After 10 layers: 0.25^10 is near zero. ReLU's derivative is either 0 or 1 ‚Äî gradients pass through unchanged when active. This is why deep networks prefer ReLU.\n",
    "\n",
    "</details>"
   ],
   "id": "cell-10"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Boss Level 1 Solution: Activation Function Analysis\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "sigmoid_d = lambda x: sigmoid(x) * (1 - sigmoid(x))\n",
    "relu = lambda x: np.maximum(0, x)\n",
    "relu_d = lambda x: (x > 0).astype(float)\n",
    "tanh_fn = lambda x: np.tanh(x)\n",
    "tanh_d = lambda x: 1 - np.tanh(x)**2\n",
    "\n",
    "x = np.linspace(-5, 5, 200)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "for ax, fn, name in zip(axes[0], [sigmoid, relu, tanh_fn], ['Sigmoid', 'ReLU', 'Tanh']):\n",
    "    ax.plot(x, fn(x), 'b-', linewidth=2)\n",
    "    ax.set_title(name, fontsize=13)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='gray', alpha=0.3)\n",
    "\n",
    "for ax, fn, name in zip(axes[1], [sigmoid_d, relu_d, tanh_d], [\"Sigmoid'\", \"ReLU'\", \"Tanh'\"]):\n",
    "    ax.plot(x, fn(x), 'r-', linewidth=2)\n",
    "    ax.set_title(f'{name} (derivative)', fontsize=13)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Activation Functions and Their Derivatives', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Key insight: Sigmoid's derivative maxes at 0.25 ‚Äî gradients shrink!\")\n",
    "print(\"ReLU's derivative is 0 or 1 ‚Äî gradients pass through unchanged.\")\n",
    "print(\"This is why ReLU helps avoid the 'vanishing gradient' problem. \\u2705\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World 2: Chain Rule"
   ],
   "id": "cell-12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 2.1: Chain Reaction\n",
    "\n",
    "**Type:** Compute | **Points:** 10 | **Difficulty:** Easy\n",
    "\n",
    "### Mission\n",
    "\n",
    "Find dy/dx for y = (2x + 1)¬≥ at x = 2.\n",
    "\n",
    "### Intel\n",
    "\n",
    "The **chain rule** handles compositions: if y = f(g(x)), then dy/dx = f'(g(x)) * g'(x).\n",
    "\n",
    "Think of it as \"derivative of the outer, times derivative of the inner.\"\n",
    "\n",
    "### Hints\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1 (Conceptual Nudge)</summary>\n",
    "\n",
    "Identify the outer function and the inner function. What is \"outer\" and what is \"inner\" in (2x + 1)¬≥?\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2 (Methodological Clue)</summary>\n",
    "\n",
    "- Outer: u¬≥, derivative = 3u¬≤\n",
    "- Inner: u = 2x + 1, derivative = 2\n",
    "- Chain rule: 3(2x+1)¬≤ * 2 = 6(2x+1)¬≤\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 3 (Detailed Walkthrough)</summary>\n",
    "\n",
    "dy/dx = 6(2x + 1)¬≤. At x = 2: 6(2*2 + 1)¬≤ = 6(5)¬≤ = 6 * 25 = 150.\n",
    "\n",
    "</details>"
   ],
   "id": "cell-13"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Level 2.1 Solution\n",
    "# y = (2x + 1)¬≥ -> outer: u¬≥, inner: 2x + 1\n",
    "# dy/dx = 3u¬≤ * 2 = 6(2x + 1)¬≤\n",
    "y = lambda x: (2*x + 1)**3\n",
    "dy_dx = lambda x: 6 * (2*x + 1)**2\n",
    "\n",
    "x_val = 2\n",
    "print(f\"y = (2x + 1)\\u00b3 at x = {x_val}\")\n",
    "print(f\"Chain rule: dy/dx = 6(2({x_val}) + 1)\\u00b2 = {dy_dx(x_val)}\")\n",
    "print(f\"Numerical:  {numerical_derivative(y, x_val):.4f}\")\n",
    "check_answer(dy_dx(x_val), numerical_derivative(y, x_val))"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 2.2: The Missing Link\n",
    "\n",
    "**Type:** Debug | **Points:** 15 | **Difficulty:** Medium\n",
    "\n",
    "### Mission\n",
    "\n",
    "A student differentiated sin(x¬≤) and got cos(x¬≤). What did they forget? Find the correct derivative and verify numerically.\n",
    "\n",
    "### Intel\n",
    "\n",
    "When you have a function inside a function ‚Äî like sin(something) ‚Äî you need the chain rule. The derivative is NOT just the derivative of the outer function. You must multiply by the derivative of the inner function too.\n",
    "\n",
    "### Hints\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1 (Conceptual Nudge)</summary>\n",
    "\n",
    "The student only took the derivative of the outer function (sin -> cos). What about the inner function (x¬≤)?\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2 (Methodological Clue)</summary>\n",
    "\n",
    "Chain rule: d/dx[sin(x¬≤)] = cos(x¬≤) * d/dx[x¬≤]. What is d/dx[x¬≤]?\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 3 (Detailed Walkthrough)</summary>\n",
    "\n",
    "d/dx[sin(x¬≤)] = cos(x¬≤) * 2x. The student forgot to multiply by 2x (the derivative of the inner function x¬≤).\n",
    "\n",
    "</details>"
   ],
   "id": "cell-15"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Level 2.2 Solution\n",
    "# f(x) = sin(x¬≤)\n",
    "# WRONG: f'(x) = cos(x¬≤) ‚Äî forgot the chain rule!\n",
    "# RIGHT: f'(x) = cos(x¬≤) * 2x ‚Äî must multiply by derivative of inner function\n",
    "\n",
    "f = lambda x: np.sin(x**2)\n",
    "wrong_answer = lambda x: np.cos(x**2)\n",
    "right_answer = lambda x: np.cos(x**2) * 2 * x\n",
    "\n",
    "x0 = 1.5\n",
    "exact = numerical_derivative(f, x0)\n",
    "print(f\"f(x) = sin(x\\u00b2) at x = {x0}\")\n",
    "print(f\"Wrong (forgot chain rule): {wrong_answer(x0):.6f}\")\n",
    "print(f\"Right (with chain rule):   {right_answer(x0):.6f}\")\n",
    "print(f\"Numerical verification:    {exact:.6f}\")\n",
    "print(f\"\\nThe student forgot to multiply by the derivative of the INNER function (2x)!\")\n",
    "check_answer(right_answer(x0), exact)"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boss Level: Backprop Simulator\n",
    "\n",
    "**Type:** Construct | **Points:** 30 | **Difficulty:** Boss\n",
    "\n",
    "### Mission\n",
    "\n",
    "Build a simple computational graph. Given L = (sigmoid(wx + b) - y)¬≤, compute dL/dw step by step using the chain rule. This is exactly what happens inside neural network training ‚Äî **backpropagation** is just the chain rule applied to a computation graph!\n",
    "\n",
    "Use w = 0.5, x = 2.0, b = 0.1, y = 1.0.\n",
    "\n",
    "### Intel\n",
    "\n",
    "The computation graph for L = (sigmoid(wx + b) - y)¬≤ has four steps:\n",
    "1. z = wx + b (linear combination)\n",
    "2. a = sigmoid(z) (activation)\n",
    "3. e = a - y (error)\n",
    "4. L = e¬≤ (squared loss)\n",
    "\n",
    "To find dL/dw, apply the chain rule through each step: dL/dw = dL/de * de/da * da/dz * dz/dw.\n",
    "\n",
    "### Hints\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1 (Conceptual Nudge)</summary>\n",
    "\n",
    "Work forward first to get all the values (z, a, e, L). Then work backward, computing each local derivative.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2 (Methodological Clue)</summary>\n",
    "\n",
    "The local derivatives are:\n",
    "- dL/de = 2e = 2(a - y)\n",
    "- de/da = 1\n",
    "- da/dz = a(1 - a) (sigmoid derivative)\n",
    "- dz/dw = x\n",
    "\n",
    "Multiply them all together.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 3 (Detailed Walkthrough)</summary>\n",
    "\n",
    "Forward: z = 0.5*2 + 0.1 = 1.1, a = sigmoid(1.1) = 0.7503, L = (0.7503 - 1)¬≤ = 0.0624.\n",
    "Backward: dL/da = 2(0.7503 - 1) = -0.4995, da/dz = 0.7503 * 0.2497 = 0.1874, dz/dw = 2.0.\n",
    "dL/dw = -0.4995 * 0.1874 * 2.0 = -0.1872.\n",
    "\n",
    "</details>"
   ],
   "id": "cell-17"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Boss Level 2: Manual Backpropagation\n",
    "# L = (sigmoid(wx + b) - y)¬≤\n",
    "w, x_val, b, y_val = 0.5, 2.0, 0.1, 1.0\n",
    "\n",
    "# Forward pass\n",
    "z = w * x_val + b              # z = wx + b\n",
    "a = 1 / (1 + np.exp(-z))      # a = sigmoid(z)\n",
    "L = (a - y_val)**2             # L = (a - y)¬≤\n",
    "\n",
    "print(\"Forward pass:\")\n",
    "print(f\"  z = w*x + b = {w}*{x_val} + {b} = {z}\")\n",
    "print(f\"  a = sigmoid({z:.1f}) = {a:.6f}\")\n",
    "print(f\"  L = (a - y)\\u00b2 = ({a:.6f} - {y_val})\\u00b2 = {L:.6f}\")\n",
    "\n",
    "# Backward pass (chain rule!)\n",
    "dL_da = 2 * (a - y_val)       # d/da[(a-y)¬≤]\n",
    "da_dz = a * (1 - a)           # sigmoid derivative\n",
    "dz_dw = x_val                  # d/dw[wx + b] = x\n",
    "\n",
    "dL_dw = dL_da * da_dz * dz_dw  # CHAIN RULE: multiply through!\n",
    "\n",
    "print(f\"\\nBackward pass (chain rule):\")\n",
    "print(f\"  dL/da = 2(a - y) = {dL_da:.6f}\")\n",
    "print(f\"  da/dz = a(1-a) = {da_dz:.6f}\")\n",
    "print(f\"  dz/dw = x = {dz_dw}\")\n",
    "print(f\"  dL/dw = {dL_da:.6f} \\u00d7 {da_dz:.6f} \\u00d7 {dz_dw} = {dL_dw:.6f}\")\n",
    "\n",
    "# Verify numerically\n",
    "def loss(w_val):\n",
    "    z = w_val * x_val + b\n",
    "    a = 1 / (1 + np.exp(-z))\n",
    "    return (a - y_val)**2\n",
    "\n",
    "numerical = numerical_derivative(loss, w)\n",
    "print(f\"\\nNumerical verification: {numerical:.6f}\")\n",
    "check_answer(dL_dw, numerical)"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World 3: Gradient Descent"
   ],
   "id": "cell-19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 3.1: Downhill Runner\n",
    "\n",
    "**Type:** Compute | **Points:** 10 | **Difficulty:** Easy\n",
    "\n",
    "### Mission\n",
    "\n",
    "Implement gradient descent to minimize f(x) = (x - 3)¬≤. Start at x = 10, use learning_rate = 0.2, and run for 20 steps. Where do you end up?\n",
    "\n",
    "### Intel\n",
    "\n",
    "Gradient descent updates a parameter by moving it in the direction opposite to the gradient (derivative): x_new = x_old - learning_rate * f'(x_old).\n",
    "\n",
    "Think of it as rolling a ball downhill. The gradient tells you which way is \"up,\" so you go the opposite direction. The learning rate controls how big your steps are.\n",
    "\n",
    "### Hints\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1 (Conceptual Nudge)</summary>\n",
    "\n",
    "What is f'(x) for f(x) = (x - 3)¬≤? Use the power rule (or chain rule).\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2 (Methodological Clue)</summary>\n",
    "\n",
    "f'(x) = 2(x - 3). The update rule is: x = x - 0.2 * 2(x - 3) = x - 0.4(x - 3).\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 3 (Detailed Walkthrough)</summary>\n",
    "\n",
    "Starting at x = 10:\n",
    "- Step 1: x = 10 - 0.4*(10-3) = 10 - 2.8 = 7.2\n",
    "- Step 2: x = 7.2 - 0.4*(7.2-3) = 7.2 - 1.68 = 5.52\n",
    "- ...continues approaching 3.0\n",
    "\n",
    "</details>"
   ],
   "id": "cell-20"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Level 3.1 Solution\n",
    "f = lambda x: (x - 3)**2\n",
    "df = lambda x: 2 * (x - 3)\n",
    "\n",
    "x = 10.0\n",
    "lr = 0.2\n",
    "history = [x]\n",
    "for step in range(20):\n",
    "    x = x - lr * df(x)\n",
    "    history.append(x)\n",
    "\n",
    "print(f\"Start: x = 10.0\")\n",
    "print(f\"After 20 steps: x = {x:.6f} (target: 3.0)\")\n",
    "check_answer(x, 3.0, tolerance=0.01)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "t = np.linspace(-1, 12, 100)\n",
    "plt.plot(t, (t-3)**2, 'b-', alpha=0.3)\n",
    "plt.plot(history, [(h-3)**2 for h in history], 'ro-', markersize=4)\n",
    "plt.title('Gradient Descent Path')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history, 'g.-')\n",
    "plt.axhline(y=3, color='r', linestyle='--', label='target')\n",
    "plt.title('x value over steps')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('x')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 3.2: Rate Tuner\n",
    "\n",
    "**Type:** Predict | **Points:** 15 | **Difficulty:** Medium\n",
    "\n",
    "### Mission\n",
    "\n",
    "Predict what happens with learning rates 0.01, 0.5, and 1.1 on f(x) = x¬≤, starting at x = 5. Then verify with code and plots.\n",
    "\n",
    "### Intel\n",
    "\n",
    "The learning rate is the most important hyperparameter in gradient descent. Too small and you waste time. Too large and you overshoot the minimum and may even diverge (the values get larger instead of smaller). There is a sweet spot in between.\n",
    "\n",
    "### Hints\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1 (Conceptual Nudge)</summary>\n",
    "\n",
    "Think about what happens at each step. With lr = 0.01, the step size is 0.01 * 2x. With lr = 1.1, the step size is 1.1 * 2x. For x = 5, that is a step of 11 ‚Äî past zero and out the other side!\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2 (Methodological Clue)</summary>\n",
    "\n",
    "For f(x) = x¬≤, the update is x_new = x - lr * 2x = x(1 - 2*lr).\n",
    "- lr = 0.01: x_new = 0.98x (slow convergence)\n",
    "- lr = 0.5: x_new = 0 (instant convergence!)\n",
    "- lr = 1.1: x_new = -1.2x (oscillates and grows!)\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 3 (Detailed Walkthrough)</summary>\n",
    "\n",
    "For f(x) = x¬≤, convergence requires |1 - 2*lr| < 1, which means 0 < lr < 1. So lr = 0.01 converges (slowly), lr = 0.5 converges (instantly), and lr = 1.1 diverges.\n",
    "\n",
    "</details>"
   ],
   "id": "cell-22"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Level 3.2 Solution: Learning Rate Effects\n",
    "f = lambda x: x**2\n",
    "df = lambda x: 2*x\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for ax, lr, desc in zip(axes, [0.01, 0.5, 1.1], ['Too Slow', 'Just Right', 'Diverges!']):\n",
    "    x = 5.0\n",
    "    path = [x]\n",
    "    for _ in range(30):\n",
    "        x = x - lr * df(x)\n",
    "        path.append(x)\n",
    "        if abs(x) > 1e6:  # Divergence detection\n",
    "            break\n",
    "    \n",
    "    t = np.linspace(-6, 6, 100)\n",
    "    ax.plot(t, t**2, 'b-', alpha=0.3)\n",
    "    path_clipped = [p for p in path if abs(p) < 10]\n",
    "    ax.plot(path_clipped, [p**2 for p in path_clipped], 'ro-', markersize=3)\n",
    "    ax.set_title(f'lr={lr} ‚Äî {desc}', fontsize=12)\n",
    "    ax.set_ylim(-2, 40)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Learning Rate Comparison', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"lr=0.01: Converges but very slowly\")\n",
    "print(\"lr=0.5:  Converges in one step! (perfect for x\\u00b2)\")\n",
    "print(\"lr=1.1:  Diverges! Each step overshoots more than the last\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 3.3: Valley Finder\n",
    "\n",
    "**Type:** Optimize | **Points:** 20 | **Difficulty:** Hard\n",
    "\n",
    "### Mission\n",
    "\n",
    "Use 2D gradient descent to find the minimum of f(x, y) = x¬≤ + 3y¬≤. Start at (4, 3) with learning rate 0.1. Run 50 steps and plot the path on a contour plot.\n",
    "\n",
    "### Intel\n",
    "\n",
    "In 2D, the gradient is a vector of partial derivatives: grad f = [df/dx, df/dy]. You update both x and y simultaneously. The contour plot shows \"level curves\" ‚Äî points with the same function value. The minimum is at the center of the contours.\n",
    "\n",
    "### Hints\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1 (Conceptual Nudge)</summary>\n",
    "\n",
    "What are the partial derivatives of f(x, y) = x¬≤ + 3y¬≤? Take the derivative with respect to x (treating y as constant), then with respect to y (treating x as constant).\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2 (Methodological Clue)</summary>\n",
    "\n",
    "df/dx = 2x, df/dy = 6y. So the gradient is [2x, 6y]. Note that the gradient in the y-direction is 3x larger ‚Äî the function is steeper in y. This means y will converge faster.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 3 (Detailed Walkthrough)</summary>\n",
    "\n",
    "Update: [x, y] = [x, y] - 0.1 * [2x, 6y].\n",
    "Step 1: [4, 3] - 0.1 * [8, 18] = [3.2, 1.2].\n",
    "The path curves toward the x-axis first (y converges faster), then slides along toward the origin.\n",
    "\n",
    "</details>"
   ],
   "id": "cell-24"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Level 3.3 Solution: 2D Gradient Descent\n",
    "def f_2d(p):\n",
    "    return p[0]**2 + 3*p[1]**2\n",
    "\n",
    "def grad_2d(p):\n",
    "    return np.array([2*p[0], 6*p[1]])\n",
    "\n",
    "point = np.array([4.0, 3.0])\n",
    "lr = 0.1\n",
    "path = [point.copy()]\n",
    "\n",
    "for _ in range(50):\n",
    "    point = point - lr * grad_2d(point)\n",
    "    path.append(point.copy())\n",
    "\n",
    "path = np.array(path)\n",
    "print(f\"Start: ({path[0][0]:.1f}, {path[0][1]:.1f})\")\n",
    "print(f\"End:   ({path[-1][0]:.4f}, {path[-1][1]:.4f})\")\n",
    "print(f\"Minimum at: (0, 0), f = {f_2d(path[-1]):.6f}\")\n",
    "\n",
    "# Contour plot\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-4, 4, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = X**2 + 3*Y**2\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.5)\n",
    "plt.plot(path[:, 0], path[:, 1], 'r.-', markersize=4, label='GD path')\n",
    "plt.plot(4, 3, 'go', markersize=10, label='Start')\n",
    "plt.plot(0, 0, 'r*', markersize=15, label='Minimum')\n",
    "plt.title('2D Gradient Descent on f(x,y) = x\\u00b2 + 3y\\u00b2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boss Level: The Optimizer\n",
    "\n",
    "**Type:** Construct | **Points:** 30 | **Difficulty:** Boss\n",
    "\n",
    "### Mission\n",
    "\n",
    "Build a complete gradient descent optimizer that works for ANY 1D function. Your optimizer should:\n",
    "\n",
    "1. Accept any function f(x) ‚Äî compute its derivative numerically\n",
    "2. Run gradient descent from a starting point\n",
    "3. Track and plot the optimization path\n",
    "4. Detect convergence (stop when the gradient is small enough)\n",
    "\n",
    "Test it on f(x) = x‚Å¥ - 3x¬≤ + 2 ‚Äî a function with multiple local minima!\n",
    "\n",
    "### Intel\n",
    "\n",
    "A robust optimizer needs:\n",
    "- A numerical derivative function (you built one in Level 1.3!)\n",
    "- A convergence criterion: stop when |f'(x)| < epsilon\n",
    "- A maximum iteration count to prevent infinite loops\n",
    "- History tracking for visualization\n",
    "\n",
    "The function x‚Å¥ - 3x¬≤ + 2 has two local minima. Depending on your starting point, gradient descent will find different ones!\n",
    "\n",
    "### Hints\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1 (Conceptual Nudge)</summary>\n",
    "\n",
    "Your optimizer function should take f, x_start, lr, and max_steps as arguments. Use the centered difference formula for the numerical derivative.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2 (Methodological Clue)</summary>\n",
    "\n",
    "The convergence check: if abs(numerical_derivative(f, x)) < 1e-6, the gradient is essentially zero and you can stop. Return the full history so you can plot it.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 3 (Detailed Walkthrough)</summary>\n",
    "\n",
    "```python\n",
    "def optimize(f, x_start, lr=0.01, max_steps=1000, tol=1e-6):\n",
    "    x = x_start\n",
    "    history = [x]\n",
    "    for i in range(max_steps):\n",
    "        grad = numerical_derivative(f, x)\n",
    "        if abs(grad) < tol:\n",
    "            break\n",
    "        x = x - lr * grad\n",
    "        history.append(x)\n",
    "    return x, history\n",
    "```\n",
    "\n",
    "</details>"
   ],
   "id": "cell-26"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Boss Level 3 Solution: Universal 1D Optimizer\n",
    "\n",
    "def optimize(f, x_start, lr=0.01, max_steps=1000, tol=1e-6):\n",
    "    \"\"\"Gradient descent optimizer for any 1D function.\"\"\"\n",
    "    x = x_start\n",
    "    history = [x]\n",
    "    for i in range(max_steps):\n",
    "        grad = numerical_derivative(f, x)\n",
    "        if abs(grad) < tol:\n",
    "            print(f\"Converged after {i} steps!\")\n",
    "            break\n",
    "        x = x - lr * grad\n",
    "        history.append(x)\n",
    "    return x, history\n",
    "\n",
    "# Test on f(x) = x‚Å¥ - 3x¬≤ + 2\n",
    "f = lambda x: x**4 - 3*x**2 + 2\n",
    "\n",
    "# Try two starting points to find different minima\n",
    "x_min1, hist1 = optimize(f, x_start=2.0, lr=0.01)\n",
    "x_min2, hist2 = optimize(f, x_start=-2.0, lr=0.01)\n",
    "\n",
    "print(f\"\\nStarting at x=2.0:  found minimum at x = {x_min1:.4f}, f(x) = {f(x_min1):.4f}\")\n",
    "print(f\"Starting at x=-2.0: found minimum at x = {x_min2:.4f}, f(x) = {f(x_min2):.4f}\")\n",
    "\n",
    "# Visualization\n",
    "t = np.linspace(-2.5, 2.5, 200)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(t, f(t), 'b-', linewidth=2, alpha=0.5)\n",
    "ax1.plot(hist1, [f(x) for x in hist1], 'ro-', markersize=2, label=f'Start x=2.0')\n",
    "ax1.plot(hist2, [f(x) for x in hist2], 'g^-', markersize=2, label=f'Start x=-2.0')\n",
    "ax1.plot(x_min1, f(x_min1), 'r*', markersize=15)\n",
    "ax1.plot(x_min2, f(x_min2), 'g*', markersize=15)\n",
    "ax1.set_title('f(x) = x\\u2074 - 3x\\u00b2 + 2 with GD paths')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('f(x)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(hist1, 'r.-', label='Start x=2.0')\n",
    "ax2.plot(hist2, 'g.-', label='Start x=-2.0')\n",
    "ax2.set_title('Convergence: x value over steps')\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('x')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\nKey insight: Same function, different starting points -> different minima!\")\n",
    "print(\"This is why initialization matters in neural network training. \\u2705\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Boss: Train a Model\n",
    "\n",
    "**Type:** Construct | **Points:** 50 | **Difficulty:** Final Boss\n",
    "\n",
    "### Mission\n",
    "\n",
    "Train a linear model y = wx + b to fit noisy data using gradient descent. You must implement everything from scratch:\n",
    "\n",
    "1. **Forward pass:** Compute predictions y_pred = w*x + b\n",
    "2. **Loss computation:** Mean Squared Error = mean((y_pred - y_true)¬≤)\n",
    "3. **Gradient computation:** Use the chain rule to find dL/dw and dL/db\n",
    "4. **Parameter update:** Apply gradient descent to update w and b\n",
    "\n",
    "The true relationship is y = 2.5x + 1.0 with some noise. Can your optimizer recover these values?\n",
    "\n",
    "### Intel\n",
    "\n",
    "This is where everything comes together:\n",
    "- **Derivatives** (World 1): You need to compute gradients of the loss\n",
    "- **Chain rule** (World 2): The loss depends on predictions, which depend on w and b\n",
    "- **Gradient descent** (World 3): You update parameters using the gradients\n",
    "\n",
    "For MSE loss L = mean((wx + b - y)¬≤):\n",
    "- dL/dw = mean(2 * (wx + b - y) * x)\n",
    "- dL/db = mean(2 * (wx + b - y))\n",
    "\n",
    "### Hints\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1 (Conceptual Nudge)</summary>\n",
    "\n",
    "Initialize w = 0 and b = 0. Run the forward pass, compute the loss, then compute the gradients and update. Repeat for 100 epochs.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2 (Methodological Clue)</summary>\n",
    "\n",
    "The gradients come from the chain rule applied to L = mean((wx + b - y)¬≤):\n",
    "- dL/dw = mean(2 * error * x) where error = (y_pred - y_true)\n",
    "- dL/db = mean(2 * error)\n",
    "Use lr = 0.01 for stable convergence.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 3 (Detailed Walkthrough)</summary>\n",
    "\n",
    "```python\n",
    "for epoch in range(100):\n",
    "    y_pred = w * X + b\n",
    "    loss = np.mean((y_pred - y_true)**2)\n",
    "    dL_dw = np.mean(2 * (y_pred - y_true) * X)\n",
    "    dL_db = np.mean(2 * (y_pred - y_true))\n",
    "    w = w - lr * dL_dw\n",
    "    b = b - lr * dL_db\n",
    "```\n",
    "\n",
    "</details>"
   ],
   "id": "cell-28"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Final Boss Solution: Train a Linear Model\n",
    "np.random.seed(42)\n",
    "X = np.random.uniform(-3, 3, 50)\n",
    "y_true = 2.5 * X + 1.0 + np.random.normal(0, 0.5, 50)\n",
    "\n",
    "# Initialize parameters\n",
    "w, b = 0.0, 0.0\n",
    "lr = 0.01\n",
    "n_epochs = 100\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = w * X + b\n",
    "    \n",
    "    # Loss (Mean Squared Error)\n",
    "    loss = np.mean((y_pred - y_true)**2)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Gradients (chain rule!)\n",
    "    dL_dw = np.mean(2 * (y_pred - y_true) * X)\n",
    "    dL_db = np.mean(2 * (y_pred - y_true))\n",
    "    \n",
    "    # Update (gradient descent!)\n",
    "    w = w - lr * dL_dw\n",
    "    b = b - lr * dL_db\n",
    "\n",
    "print(f\"Learned: y = {w:.3f}x + {b:.3f}\")\n",
    "print(f\"True:    y = 2.500x + 1.000\")\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.scatter(X, y_true, alpha=0.5, label='Data')\n",
    "x_line = np.linspace(-3, 3, 100)\n",
    "ax1.plot(x_line, w*x_line + b, 'r-', linewidth=2, label=f'Learned: {w:.2f}x + {b:.2f}')\n",
    "ax1.plot(x_line, 2.5*x_line + 1.0, 'g--', linewidth=2, label='True: 2.5x + 1.0')\n",
    "ax1.legend()\n",
    "ax1.set_title('Linear Regression via Gradient Descent')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(losses)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss (MSE)')\n",
    "ax2.set_title('Training Loss')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n--- FINAL BOSS CLEARED! ---\")\n",
    "print(f\"You combined derivatives, chain rule, and gradient descent\")\n",
    "print(f\"to train a model from scratch. This is the foundation of\")\n",
    "print(f\"ALL neural network training. \\u2705\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Summary\n",
    "\n",
    "Fill in your scores below!\n",
    "\n",
    "### Scoring Rules\n",
    "- **Full points:** Solved without any hints\n",
    "- **-2 points:** Used Hint 1\n",
    "- **-5 points:** Used Hint 2\n",
    "- **-8 points:** Used Hint 3 (or looked at the solution)\n",
    "\n",
    "| Level | Challenge | Max Points | Your Score |\n",
    "|-------|-----------|-----------|------------|\n",
    "| 1.1 | The Slope Finder | 10 | |\n",
    "| 1.2 | Shape Predictor | 15 | |\n",
    "| 1.3 | Derivative Detective | 20 | |\n",
    "| Boss 1 | Activation Function Analysis | 30 | |\n",
    "| 2.1 | Chain Reaction | 10 | |\n",
    "| 2.2 | The Missing Link | 15 | |\n",
    "| Boss 2 | Backprop Simulator | 30 | |\n",
    "| 3.1 | Downhill Runner | 10 | |\n",
    "| 3.2 | Rate Tuner | 15 | |\n",
    "| 3.3 | Valley Finder | 20 | |\n",
    "| Boss 3 | The Optimizer | 30 | |\n",
    "| Final Boss | Train a Model | 50 | |\n",
    "| | **Total** | **255** | |\n",
    "\n",
    "### Achievement Levels\n",
    "\n",
    "| Score Range | Title | What It Means |\n",
    "|-------------|-------|---------------|\n",
    "| 230-255 | Calculus Grand Master | You crushed it with minimal hints. Ready for advanced ML. |\n",
    "| 180-229 | Calculus Warrior | Strong fundamentals. Review the hints you used ‚Äî they highlight your growth areas. |\n",
    "| 120-179 | Calculus Apprentice | Good start! Re-try the Boss Levels after reviewing World solutions. |\n",
    "| 60-119 | Calculus Explorer | You are building the right instincts. Work through the hints carefully and try again. |\n",
    "| 0-59 | Calculus Newcomer | No shame ‚Äî everyone starts somewhere. Focus on World 1, master it, then move on. |\n",
    "\n",
    "### What to Do Next\n",
    "\n",
    "- **Scored 200+?** Try modifying the Final Boss to use a non-linear model (quadratic regression).\n",
    "- **Struggled with chain rule?** Go back to World 2 and trace through the backprop example step by step.\n",
    "- **Gradient descent clicked?** Explore momentum and Adam optimizer ‚Äî they build on exactly these ideas.\n",
    "- **Want more?** Every deep learning framework (PyTorch, TensorFlow) does exactly what you did in the Final Boss ‚Äî just at a much larger scale."
   ],
   "id": "cell-30"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}