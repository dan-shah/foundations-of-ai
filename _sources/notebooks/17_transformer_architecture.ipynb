{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Part 6.1: Transformer Architecture\n\nThe Transformer is the architecture behind GPT, BERT, T5, and virtually every state-of-the-art model in NLP, vision, and beyond. Introduced in the landmark 2017 paper *\"Attention Is All You Need\"*, it replaced recurrent networks with a design built entirely on the attention mechanisms you learned in the previous notebook. In this notebook, you will understand every piece of the Transformer and build one from scratch.\n\n**F1 analogy:** Think of the Transformer as F1's modern strategy computer -- the system that ingests all available data streams (tire wear, fuel load, weather, gap to rivals) **in parallel** and produces real-time strategy calls. Older RNN-based models are like the pre-radio era when information traveled sequentially through pit boards, one lap at a time. The Transformer processes everything at once, just as a modern F1 strategy wall sees every car's telemetry simultaneously.\n\n**Prerequisites:** Notebook 16 (Attention Mechanisms) -- you should be comfortable with Q/K/V attention, self-attention, multi-head attention, and causal masking.\n\n---",
   "id": "cell-0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n\n- [ ] Explain why Transformers replaced RNNs and the key innovations of the architecture\n- [ ] Implement sinusoidal positional encoding and explain why position information is needed\n- [ ] Build an Encoder block from scratch (multi-head attention + Add&Norm + FFN)\n- [ ] Build a Decoder block from scratch (masked self-attention + cross-attention + FFN)\n- [ ] Assemble a complete Transformer model in PyTorch\n- [ ] Train a Transformer on a simple sequence task and visualize attention patterns\n- [ ] Analyze parameter counts and explain why Transformers scale so well\n\n---"
   ],
   "id": "cell-1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport math\n\n%matplotlib inline\nplt.style.use('seaborn-v0_8-whitegrid')\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nprint(\"Setup complete!\")\nprint(f\"PyTorch version: {torch.__version__}\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 1. \"Attention Is All You Need\" -- The Big Picture\n\n### Intuitive Explanation\n\nBefore Transformers, sequence models were dominated by **Recurrent Neural Networks** (RNNs, LSTMs, GRUs). These process tokens one at a time, left to right, passing a hidden state from step to step like a game of telephone. This sequential nature creates two fundamental problems:\n\n1. **No parallelization:** You cannot process token 5 until you have finished tokens 1-4. Training is painfully slow.\n2. **Long-range dependencies fade:** Information from early tokens must survive through many sequential steps. Even with LSTMs, signals degrade over long distances.\n\nThe Transformer's key insight is radical: **throw away recurrence entirely.** Instead of processing tokens sequentially, let every token attend to every other token simultaneously using attention. This means:\n\n- **Full parallelization:** All positions are processed at once during training\n- **Direct connections:** Token 1 can directly attend to token 100 with no intermediaries\n- **Constant path length:** Information travels in O(1) steps, not O(n)\n\n**F1 analogy:** Imagine an RNN as a team that relays information via pit boards -- the driver sees one message per lap, and by lap 50 the message from lap 1 has been distorted through 49 handoffs. A Transformer is like modern F1 radio and telemetry: the strategy wall can instantly access data from *any* lap, *any* sector, *any* car -- all at once, with no degradation. That is why Transformers dominate: they process all positions in parallel, just as a modern F1 strategy system processes all data streams simultaneously.",
   "id": "cell-3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Comparison: RNN vs Transformer\n\n| Property | RNN/LSTM | Transformer | F1 Parallel |\n|----------|----------|-------------|-------------|\n| **Processing** | Sequential (one token at a time) | Parallel (all tokens at once) | Pit boards (one message/lap) vs. full telemetry dashboard |\n| **Long-range dependencies** | Degrades over distance | Direct attention at any distance | Forgetting early stint data vs. instant lap-1 recall |\n| **Path length** | O(n) between distant tokens | O(1) between any two tokens | Information relay chain vs. direct radio link |\n| **Training speed** | Slow (sequential bottleneck) | Fast (GPU-parallelizable) | Slow debriefs vs. real-time analytics |\n| **Memory** | Fixed hidden state size | Grows with sequence length (O(n^2)) | Limited pit board vs. full data storage |\n| **Positional info** | Built-in (sequential processing) | Must be explicitly added | Implicit lap count vs. explicit position encoding |\n\n**The trade-off:** Transformers use O(n^2) memory for attention (every token attends to every other), but this is well worth it for the parallelization and quality gains.",
   "id": "cell-4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### The High-Level Architecture\n\nThe original Transformer has an **encoder-decoder** structure:\n\n```\nINPUT TOKENS                          OUTPUT TOKENS\n     |                                      |\n  [Embedding + Positional Encoding]    [Embedding + Positional Encoding]\n     |                                      |\n  ┌──────────────┐                    ┌──────────────┐\n  │  Encoder      │                    │  Decoder      │\n  │  Block x N    │──────────────────▶│  Block x N    │\n  │               │  (cross-attention) │               │\n  └──────────────┘                    └──────────────┘\n                                           |\n                                    [Linear + Softmax]\n                                           |\n                                    OUTPUT PROBABILITIES\n```\n\n**Encoder:** Reads the full input and creates rich contextual representations.\n**Decoder:** Generates output one token at a time, attending to both its own previous outputs AND the encoder's representations.\n\n**F1 analogy:** The encoder is like the **telemetry and data acquisition system** -- it ingests all raw sensor data (tire temps, throttle traces, GPS coordinates) and builds a rich, contextual picture of the car's state. The decoder is the **strategy engineer** who translates that encoded picture into actionable calls: \"Box this lap,\" \"Switch to hards,\" \"Push now.\" Cross-attention is the bridge -- how the strategy engineer *reads* the telemetry to make decisions.\n\nNot all modern models use both sides:\n- **GPT** uses only the decoder (autoregressive language modeling)\n- **BERT** uses only the encoder (bidirectional understanding)\n- **T5, BART** use the full encoder-decoder",
   "id": "cell-5"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualization: RNN vs Transformer information flow\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# --- RNN: Sequential information flow ---\nax = axes[0]\nn_tokens = 6\npositions = np.arange(n_tokens)\n\n# Draw tokens\nfor i in positions:\n    circle = plt.Circle((i, 0), 0.3, fill=True, color='steelblue', alpha=0.8)\n    ax.add_patch(circle)\n    ax.text(i, 0, f't{i+1}', ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n\n# Draw sequential arrows\nfor i in range(n_tokens - 1):\n    ax.annotate('', xy=(i+0.7, 0), xytext=(i+0.3, 0),\n                arrowprops=dict(arrowstyle='->', color='red', lw=2))\n\n# Show fading signal\nfor i in positions:\n    alpha = max(0.1, 1.0 - i * 0.18)\n    ax.add_patch(plt.Circle((i, 0), 0.3, fill=False, edgecolor='red', \n                             linewidth=2, alpha=alpha))\n\nax.set_xlim(-0.8, n_tokens - 0.2)\nax.set_ylim(-1.5, 1.5)\nax.set_title('RNN: Sequential Processing\\n(signal fades over distance)', fontsize=13, fontweight='bold')\nax.set_aspect('equal')\nax.axis('off')\n\n# --- Transformer: All-to-all attention ---\nax = axes[1]\n\n# Draw tokens\nfor i in positions:\n    circle = plt.Circle((i, 0), 0.3, fill=True, color='steelblue', alpha=0.8)\n    ax.add_patch(circle)\n    ax.text(i, 0, f't{i+1}', ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n\n# Draw attention connections (all-to-all)\nfor i in positions:\n    for j in positions:\n        if i != j:\n            ax.plot([i, j], [0, 0], color='green', alpha=0.15, lw=1.5)\n\n# Highlight a specific long-range connection\nax.annotate('', xy=(5-0.3, 0.15), xytext=(0+0.3, 0.15),\n            arrowprops=dict(arrowstyle='->', color='green', lw=2.5))\nax.text(2.5, 0.5, 'Direct connection!', ha='center', fontsize=10, color='green', fontweight='bold')\n\nax.set_xlim(-0.8, n_tokens - 0.2)\nax.set_ylim(-1.5, 1.5)\nax.set_title('Transformer: Parallel Attention\\n(direct path between any tokens)', fontsize=13, fontweight='bold')\nax.set_aspect('equal')\nax.axis('off')\n\nplt.tight_layout()\nplt.savefig('rnn_vs_transformer.png', dpi=100, bbox_inches='tight')\nplt.show()\nprint(\"Left: RNN processes sequentially -- information from t1 must pass through every step to reach t6.\")\nprint(\"Right: Transformer connects all tokens directly via attention -- t1 and t6 are just one step apart.\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 2. Positional Encoding\n\n### The Problem: Attention Has No Sense of Order\n\nHere is a crucial insight: **self-attention is permutation invariant.** If you shuffle the input tokens, the attention outputs are also shuffled in exactly the same way -- the mechanism itself does not know or care about token order.\n\nThink about it: `Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V`. The computation depends only on the *content* of the vectors, not their *position*. The sentence \"the cat sat on the mat\" and \"mat the on sat cat the\" would produce identical attention patterns (just rearranged).\n\nBut word order matters enormously! \"The dog bit the man\" and \"The man bit the dog\" have very different meanings. We need to inject position information somehow.\n\n**The solution:** Add a **positional encoding** to each token embedding before feeding it into the Transformer. Each position gets a unique signal that the model can use to determine where tokens are in the sequence.\n\n**F1 analogy:** Positional encoding is like **lap number and track position encoding**. Raw telemetry data (speed, throttle, brake) means nothing without knowing *when* and *where* it was recorded. A speed of 320 km/h means something very different on lap 1 (opening lap with cold tires) versus lap 50 (late-race tire degradation), and at the start/finish straight versus the apex of turn 1. Just as the strategy computer tags every data point with its lap number and track sector, the Transformer tags every token with its position in the sequence.",
   "id": "cell-7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Sinusoidal Positional Encoding\n\nThe original Transformer paper uses a clever mathematical approach: encode each position using sine and cosine waves at different frequencies.\n\n$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n\n$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n\n#### Breaking down the formula:\n\n| Component | Meaning | Intuition | F1 Analogy |\n|-----------|---------|-----------|------------|\n| $pos$ | Position in the sequence (0, 1, 2, ...) | Which token are we encoding? | Lap number in the race |\n| $i$ | Dimension index (0, 1, ..., d/2) | Which \"frequency channel\"? | Different telemetry channels (speed, tire temp, fuel) |\n| $d_{model}$ | Model dimension (e.g., 512) | Total embedding size | Total number of sensor channels |\n| $10000^{2i/d_{model}}$ | Wavelength scaling | Low dimensions = fast waves, high dimensions = slow waves | Fast-changing data (throttle) vs. slow-changing data (fuel load) |\n\n**What this means:** Each position is encoded as a point on many sine/cosine waves of different frequencies. Low-dimension pairs oscillate rapidly (capturing fine-grained position), while high-dimension pairs oscillate slowly (capturing broad position). Together they create a **unique fingerprint** for every position.\n\n**F1 analogy:** Think of this as encoding race position using multiple time scales simultaneously. Fast-oscillating dimensions capture *which sector* of the lap you are in (changes every few seconds). Slow-oscillating dimensions capture *which stint* you are in (changes every 15-25 laps). Together, they uniquely identify any moment in the race.\n\n**Why sin/cos?** Two key reasons:\n1. **Unique encoding:** Every position gets a distinct vector\n2. **Relative positions are linear:** $PE_{pos+k}$ can be expressed as a linear function of $PE_{pos}$ (rotation in 2D), so the model can easily learn to attend to relative offsets",
   "id": "cell-8"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Implement sinusoidal positional encoding\ndef get_positional_encoding(max_len, d_model):\n    \"\"\"\n    Generate sinusoidal positional encoding.\n    \n    Args:\n        max_len: Maximum sequence length\n        d_model: Model dimension (must be even)\n    \n    Returns:\n        Tensor of shape (max_len, d_model)\n    \"\"\"\n    pe = torch.zeros(max_len, d_model)\n    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n    \n    # Compute the division term: 10000^(2i/d_model)\n    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n    \n    # Even dimensions: sin, Odd dimensions: cos\n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    \n    return pe\n\n# Generate positional encoding\nmax_len = 100\nd_model = 64\npe = get_positional_encoding(max_len, d_model)\nprint(f\"Positional encoding shape: {pe.shape}\")\nprint(f\"\\nFirst position (pos=0):\")\nprint(f\"  First 8 values: {pe[0, :8].numpy().round(3)}\")\nprint(f\"\\nSecond position (pos=1):\")\nprint(f\"  First 8 values: {pe[1, :8].numpy().round(3)}\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-9"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualization: Positional Encoding Heatmap\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Full heatmap\nax = axes[0]\nim = ax.imshow(pe[:50, :].numpy(), cmap='RdBu', aspect='auto', interpolation='nearest')\nax.set_xlabel('Embedding Dimension', fontsize=11)\nax.set_ylabel('Position', fontsize=11)\nax.set_title('Positional Encoding Heatmap\\n(first 50 positions)', fontsize=13, fontweight='bold')\nplt.colorbar(im, ax=ax, shrink=0.8)\n\n# Individual dimensions showing different frequencies\nax = axes[1]\npositions = np.arange(max_len)\ndims_to_show = [0, 1, 10, 11, 30, 31]\ncolors = ['blue', 'blue', 'green', 'green', 'red', 'red']\nstyles = ['-', '--', '-', '--', '-', '--']\n\nfor dim, color, style in zip(dims_to_show, colors, styles):\n    label = f'dim {dim} ({\"sin\" if dim % 2 == 0 else \"cos\"})'\n    ax.plot(positions, pe[:, dim].numpy(), color=color, linestyle=style, \n            alpha=0.7, label=label, linewidth=1.5)\n\nax.set_xlabel('Position', fontsize=11)\nax.set_ylabel('Encoding Value', fontsize=11)\nax.set_title('Different Dimensions = Different Frequencies', fontsize=13, fontweight='bold')\nax.legend(fontsize=8, loc='upper right')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('positional_encoding.png', dpi=100, bbox_inches='tight')\nplt.show()\n\nprint(\"Left: Each row is one position's encoding vector. Notice the wave patterns across dimensions.\")\nprint(\"Right: Low dimensions (blue) oscillate fast, high dimensions (red) oscillate slowly.\")\nprint(\"This creates a unique 'fingerprint' for every position.\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-10"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualization: Each position has a unique encoding\n# Show this by computing pairwise distances between position encodings\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Cosine similarity between positions\npe_norm = pe / pe.norm(dim=1, keepdim=True)\ncos_sim = (pe_norm @ pe_norm.T).numpy()\n\nax = axes[0]\nn_show = 50\nim = ax.imshow(cos_sim[:n_show, :n_show], cmap='viridis', aspect='equal')\nax.set_xlabel('Position', fontsize=11)\nax.set_ylabel('Position', fontsize=11)\nax.set_title('Cosine Similarity Between\\nPosition Encodings', fontsize=13, fontweight='bold')\nplt.colorbar(im, ax=ax, shrink=0.8)\n\n# Show that nearby positions are more similar\nax = axes[1]\nref_positions = [0, 10, 25, 40]\ncolors = ['blue', 'green', 'orange', 'red']\nfor ref_pos, color in zip(ref_positions, colors):\n    similarities = cos_sim[ref_pos, :n_show]\n    ax.plot(range(n_show), similarities, color=color, alpha=0.8, \n            label=f'Similarity to pos {ref_pos}', linewidth=1.5)\n\nax.set_xlabel('Position', fontsize=11)\nax.set_ylabel('Cosine Similarity', fontsize=11)\nax.set_title('Nearby Positions Are More Similar', fontsize=13, fontweight='bold')\nax.legend(fontsize=9)\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('positional_uniqueness.png', dpi=100, bbox_inches='tight')\nplt.show()\n\nprint(\"Left: Each position has a unique encoding (no two rows are identical).\")\nprint(\"Right: Similarity peaks at the reference position and decays with distance.\")\nprint(\"This smooth decay means the model can learn relative position relationships.\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Learned vs Sinusoidal Positional Embeddings\n\n| Approach | How it works | Pros | Cons | F1 Analogy |\n|----------|-------------|------|------|------------|\n| **Sinusoidal** (original paper) | Fixed sin/cos functions | Can extrapolate to longer sequences; no extra parameters | Slightly less flexible | Fixed GPS coordinates -- works for any circuit length |\n| **Learned** (BERT, GPT-2) | Trainable embedding per position | Can adapt to data patterns | Fixed max length; more parameters | Team-specific track maps tuned from practice data |\n| **Relative** (Transformer-XL, RoPE) | Encode distance between tokens, not absolute position | Better generalization; handles variable lengths | More complex implementation | Gap-to-car-ahead (relative) vs. absolute lap time |\n\nIn practice, learned embeddings are most common in modern models. **RoPE** (Rotary Position Embedding), used in LLaMA and many recent models, applies rotation matrices that naturally encode relative positions -- a clever evolution of the sinusoidal idea.",
   "id": "cell-12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: Why Sinusoidal Encoding Enables Relative Position\n\n#### Key Insight\n\nFor any fixed offset $k$, the encoding at position $pos + k$ can be written as a **linear transformation** of the encoding at position $pos$. Specifically, for each pair of dimensions $(2i, 2i+1)$:\n\n$$\\begin{bmatrix} PE_{pos+k, 2i} \\\\ PE_{pos+k, 2i+1} \\end{bmatrix} = \\begin{bmatrix} \\cos(k \\cdot \\omega_i) & \\sin(k \\cdot \\omega_i) \\\\ -\\sin(k \\cdot \\omega_i) & \\cos(k \\cdot \\omega_i) \\end{bmatrix} \\begin{bmatrix} PE_{pos, 2i} \\\\ PE_{pos, 2i+1} \\end{bmatrix}$$\n\nThis is a **rotation matrix!** Moving from position $pos$ to $pos + k$ is a rotation in each 2D subspace. Since rotations are linear, the model can learn to compute relative positions using simple linear operations (which is exactly what attention does).\n\n#### Common Misconceptions\n\n| Misconception | Reality |\n|---------------|---------|\n| \"Positional encoding tells the model the exact position\" | It provides a signal the model *learns* to interpret; the encoding itself is just numbers |\n| \"You need sinusoidal encodings specifically\" | Learned encodings work just as well; the key is having *some* position signal |\n| \"Positional encoding is added only once\" | It is added once at the input, but residual connections carry it through all layers |"
   ],
   "id": "cell-13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 3. The Encoder Block\n\n### Intuitive Explanation\n\nThe encoder's job is to read the input sequence and build **rich, contextual representations** of each token. It does this by stacking identical blocks, each containing two sub-layers:\n\n1. **Multi-Head Self-Attention:** Each token looks at all other tokens to understand context (you built this in Notebook 16)\n2. **Position-wise Feed-Forward Network (FFN):** Each token is independently transformed through a small neural network\n\nAround each sub-layer, there are two critical additions:\n- **Residual connection:** Add the input back to the output (skip connection)\n- **Layer normalization:** Normalize the result for stable training\n\n**F1 analogy:** The encoder block is like a **telemetry processing pipeline**. Self-attention is the step where every sensor reading is cross-referenced with every other sensor reading -- \"How does tire temperature relate to lap time? How does throttle application correlate with tire wear?\" The FFN is where each sensor reading is independently refined -- converting raw voltage into meaningful engineering units. The residual connection ensures the original raw signal is never lost, and layer normalization keeps all signals on comparable scales regardless of whether you are measuring temperature in Celsius or pressure in bar.\n\n```\nInput (for each token)\n  │\n  ├──────────────────────┐\n  │                      │ (residual)\n  ▼                      │\nMulti-Head Self-Attention │\n  │                      │\n  ▼                      │\n  + ◄────────────────────┘\n  │\nLayerNorm\n  │\n  ├──────────────────────┐\n  │                      │ (residual)\n  ▼                      │\nFeed-Forward Network     │\n  │                      │\n  ▼                      │\n  + ◄────────────────────┘\n  │\nLayerNorm\n  │\n  ▼\nOutput (same shape as input)\n```",
   "id": "cell-14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Add & Norm: Residual Connections + Layer Normalization\n\n**Residual connections** solve the vanishing gradient problem in deep networks. Instead of learning $F(x)$, we learn $F(x) + x$. If the sub-layer has nothing useful to add, gradients can still flow through the identity path.\n\n**Layer normalization** normalizes across the feature dimension (not the batch dimension like BatchNorm). For each token independently:\n\n$$\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma + \\epsilon} \\cdot \\gamma + \\beta$$\n\n| Component | What it does | Why it matters | F1 Parallel |\n|-----------|-------------|----------------|-------------|\n| **Residual connection** | Adds input to sub-layer output | Enables training of very deep networks | Baseline setup preserved -- new adjustments are *added* on top |\n| **Layer normalization** | Normalizes each token's features | Stabilizes training, reduces sensitivity to scale | Normalizing across different race conditions -- wet vs. dry, hot vs. cold track |\n| **Together (Add & Norm)** | `LayerNorm(x + SubLayer(x))` | The standard \"wrapper\" around every sub-layer | Adjust setup, then normalize so all readings are comparable |\n\n**F1 analogy:** Layer normalization is like normalizing telemetry across different race conditions. A tire temperature of 100C means something different at Bahrain (50C ambient) versus Spa (15C ambient). LayerNorm ensures that the model sees standardized signals regardless of the \"ambient conditions\" of the data, just as engineers normalize readings before comparing across sessions.",
   "id": "cell-15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### The Feed-Forward Network (FFN)\n\nThe FFN is applied to each position **independently** (the same network, same weights, for every token). It consists of two linear transformations with a ReLU (or GELU) activation in between:\n\n$$\\text{FFN}(x) = W_2 \\cdot \\text{ReLU}(W_1 x + b_1) + b_2$$\n\nThe inner dimension is typically **4x** the model dimension (e.g., $d_{model} = 512 \\rightarrow d_{ff} = 2048$).\n\n**Why is the FFN needed?** Self-attention is powerful but it only computes **weighted averages** of value vectors -- which is a linear operation. The FFN adds:\n1. **Nonlinearity** (via ReLU/GELU) -- critical for learning complex functions\n2. **Per-position processing** -- each token can independently transform its representation\n3. **Memory** -- recent research suggests FFN layers act as key-value memories, storing factual knowledge\n\n| Sub-layer | Operation | Capacity | F1 Parallel |\n|-----------|-----------|----------|-------------|\n| Self-Attention | Mixes information **across** tokens | Contextual understanding | Cross-referencing all sensor channels with each other |\n| FFN | Transforms each token **independently** | Feature extraction, knowledge storage | Per-sensor signal processing and calibration |",
   "id": "cell-16"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Implement the core components of an Encoder Block\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"\n    Multi-head attention (from Notebook 16, now as a reusable module).\n    \"\"\"\n    def __init__(self, d_model, n_heads):\n        super().__init__()\n        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n        \n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        \n        # Linear projections for Q, K, V, and output\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n    \n    def forward(self, query, key, value, mask=None):\n        \"\"\"\n        Args:\n            query: (batch, seq_len_q, d_model)\n            key: (batch, seq_len_k, d_model)\n            value: (batch, seq_len_k, d_model)\n            mask: Optional mask (broadcastable to attention shape)\n        \n        Returns:\n            output: (batch, seq_len_q, d_model)\n            attention_weights: (batch, n_heads, seq_len_q, seq_len_k)\n        \"\"\"\n        batch_size = query.size(0)\n        \n        # Project and reshape to (batch, n_heads, seq_len, d_k)\n        Q = self.W_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n        K = self.W_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n        V = self.W_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n        \n        # Scaled dot-product attention\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        \n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n        \n        attention_weights = F.softmax(scores, dim=-1)\n        context = torch.matmul(attention_weights, V)\n        \n        # Concatenate heads and project\n        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        output = self.W_o(context)\n        \n        return output, attention_weights\n\n\nclass PositionwiseFFN(nn.Module):\n    \"\"\"\n    Position-wise Feed-Forward Network.\n    Two linear layers with ReLU activation. Applied independently to each position.\n    \"\"\"\n    def __init__(self, d_model, d_ff):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        return self.linear2(self.relu(self.linear1(x)))\n\n\nclass EncoderBlock(nn.Module):\n    \"\"\"\n    One Transformer Encoder block.\n    \n    Contains:\n    1. Multi-head self-attention + Add & Norm\n    2. Position-wise FFN + Add & Norm\n    \"\"\"\n    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.self_attention = MultiHeadAttention(d_model, n_heads)\n        self.ffn = PositionwiseFFN(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        \"\"\"\n        Args:\n            x: (batch, seq_len, d_model)\n            mask: Optional attention mask\n        \n        Returns:\n            output: (batch, seq_len, d_model)\n        \"\"\"\n        # Sub-layer 1: Multi-head self-attention + Add & Norm\n        attn_output, attn_weights = self.self_attention(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))   # Residual + LayerNorm\n        \n        # Sub-layer 2: FFN + Add & Norm\n        ffn_output = self.ffn(x)\n        x = self.norm2(x + self.dropout(ffn_output))    # Residual + LayerNorm\n        \n        return x\n\n# Test the encoder block\nd_model = 64\nn_heads = 4\nd_ff = 256\nbatch_size = 2\nseq_len = 10\n\nencoder_block = EncoderBlock(d_model, n_heads, d_ff)\nx = torch.randn(batch_size, seq_len, d_model)\noutput = encoder_block(x)\n\nprint(f\"Encoder Block\")\nprint(f\"  Input shape:  {x.shape}\")\nprint(f\"  Output shape: {output.shape}\")\nprint(f\"  Same shape: {x.shape == output.shape}\")\nprint(f\"\\nParameter count:\")\ntotal_params = sum(p.numel() for p in encoder_block.parameters())\nfor name, param in encoder_block.named_parameters():\n    print(f\"  {name}: {param.shape} ({param.numel():,} params)\")\nprint(f\"  TOTAL: {total_params:,} parameters\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-17"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualization: Data flow through an Encoder Block\nfig, ax = plt.subplots(figsize=(10, 8))\nax.set_xlim(0, 10)\nax.set_ylim(0, 12)\nax.axis('off')\nax.set_title('Data Flow Through One Encoder Block', fontsize=15, fontweight='bold', pad=20)\n\n# Helper to draw boxes\ndef draw_box(ax, x, y, w, h, text, color, fontsize=10):\n    rect = plt.Rectangle((x-w/2, y-h/2), w, h, linewidth=2, \n                          edgecolor=color, facecolor=color, alpha=0.2)\n    ax.add_patch(rect)\n    ax.text(x, y, text, ha='center', va='center', fontsize=fontsize, fontweight='bold')\n\ndef draw_arrow(ax, x1, y1, x2, y2, color='black'):\n    ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n                arrowprops=dict(arrowstyle='->', color=color, lw=2))\n\n# Input\ndraw_box(ax, 5, 11, 3, 0.6, 'Input: (batch, seq_len, d_model)', 'gray')\ndraw_arrow(ax, 5, 10.7, 5, 9.7)\n\n# Self-Attention\ndraw_box(ax, 5, 9.3, 4, 0.8, 'Multi-Head Self-Attention', 'steelblue')\ndraw_arrow(ax, 5, 8.9, 5, 8.1)\n\n# Add & Norm 1\ndraw_box(ax, 5, 7.7, 3.5, 0.6, 'Add & LayerNorm', 'green')\n# Residual connection 1\nax.annotate('', xy=(3.2, 7.7), xytext=(3.2, 10.3),\n            arrowprops=dict(arrowstyle='->', color='red', lw=2, linestyle='--'))\nax.text(2.4, 9.0, 'residual', fontsize=9, color='red', rotation=90, va='center')\ndraw_arrow(ax, 5, 7.4, 5, 6.5)\n\n# FFN\ndraw_box(ax, 5, 6.1, 4.5, 0.8, f'Feed-Forward Network\\n(d_model -> d_ff -> d_model)', 'orange')\ndraw_arrow(ax, 5, 5.7, 5, 4.8)\n\n# Add & Norm 2\ndraw_box(ax, 5, 4.4, 3.5, 0.6, 'Add & LayerNorm', 'green')\n# Residual connection 2\nax.annotate('', xy=(3.2, 4.4), xytext=(3.2, 7.1),\n            arrowprops=dict(arrowstyle='->', color='red', lw=2, linestyle='--'))\nax.text(2.4, 5.7, 'residual', fontsize=9, color='red', rotation=90, va='center')\ndraw_arrow(ax, 5, 4.1, 5, 3.2)\n\n# Output\ndraw_box(ax, 5, 2.8, 3, 0.6, 'Output: (batch, seq_len, d_model)', 'gray')\n\n# Annotations\nax.text(8.5, 9.3, 'Each token attends\\nto all other tokens', fontsize=9, \n        ha='center', style='italic', color='steelblue')\nax.text(8.5, 6.1, 'Same MLP applied\\nto each token\\nindependently', fontsize=9,\n        ha='center', style='italic', color='darkorange')\nax.text(8.5, 7.7, 'Stabilizes gradients', fontsize=9,\n        ha='center', style='italic', color='green')\n\nplt.tight_layout()\nplt.savefig('encoder_block.png', dpi=100, bbox_inches='tight')\nplt.show()\n\nprint(\"Key insight: the input and output have the SAME shape.\")\nprint(\"This means encoder blocks can be stacked -- the output of one is the input to the next.\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Stacking N Encoder Blocks\n\nThe original Transformer uses **N = 6** identical encoder blocks stacked on top of each other. Each block refines the representations:\n\n- **Early layers** tend to capture syntactic patterns (word relationships, grammar)\n- **Middle layers** develop semantic understanding (meaning, context)\n- **Later layers** create task-specific representations\n\n**F1 analogy:** Think of stacking encoder blocks like the stages of telemetry analysis. Early layers capture raw patterns (\"the car braked here\"). Middle layers build understanding (\"the driver is struggling with understeer in sector 2\"). Later layers produce race-specific insights (\"pit window opens in 3 laps based on current degradation\"). Each layer builds on the one before, just as each stage of analysis adds deeper interpretation.\n\nBecause input and output shapes are identical, stacking is trivial:\n\n```python\nfor block in encoder_blocks:\n    x = block(x)  # Same shape in, same shape out\n```\n\nEach layer builds on the representations from the previous layer, creating increasingly abstract and contextual embeddings.",
   "id": "cell-19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 4. The Decoder Block\n\n### Intuitive Explanation\n\nThe decoder generates the output sequence one token at a time. It is similar to the encoder but has an important extra sub-layer and a critical constraint:\n\n1. **Masked Multi-Head Self-Attention:** The decoder attends to its own previous outputs, but with a **causal mask** -- each position can only attend to earlier positions (no peeking at future tokens!)\n2. **Cross-Attention:** The decoder attends to the encoder's output. Queries come from the decoder; keys and values come from the encoder. This is how the decoder \"reads\" the input.\n3. **Feed-Forward Network:** Same as in the encoder.\n\n**F1 analogy:** The decoder is the **strategy engineer making real-time calls**. Masked self-attention means the strategist can only consider decisions already made (you cannot un-pit the car) -- each decision is based only on what has happened before, not on future events. Cross-attention is how the strategist reads the full telemetry picture (the encoder output) to inform the next decision. The causal mask is like the fundamental constraint of racing: decisions must be made in real time, with no knowledge of the future.\n\n```\nDecoder Input (shifted right)\n  │\n  ├──────────────────────┐\n  │                      │ (residual)\n  ▼                      │\nMasked Multi-Head         │\nSelf-Attention            │\n(causal: no future)      │\n  │                      │\n  ▼                      │\n  + ◄────────────────────┘\n  │\nLayerNorm\n  │\n  ├──────────────────────┐\n  │                      │ (residual)\n  ▼                      │\nMulti-Head Cross-Attention│\n(Q: decoder, K/V: encoder)│\n  │                      │\n  ▼                      │\n  + ◄────────────────────┘\n  │\nLayerNorm\n  │\n  ├──────────────────────┐\n  │                      │ (residual)\n  ▼                      │\nFeed-Forward Network     │\n  │                      │\n  ▼                      │\n  + ◄────────────────────┘\n  │\nLayerNorm\n  │\n  ▼\nOutput\n```",
   "id": "cell-20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Cross-Attention: The Bridge Between Encoder and Decoder\n\nCross-attention is the mechanism that connects the encoder and decoder. It works exactly like the self-attention you already know, but with a twist:\n\n| Attention Type | Queries from | Keys/Values from | Purpose | F1 Parallel |\n|---------------|-------------|-----------------|---------|-------------|\n| **Encoder self-attention** | Encoder input | Encoder input | Each input token attends to all input tokens | Telemetry sensors cross-referencing each other |\n| **Decoder masked self-attention** | Decoder input | Decoder input (masked) | Each output token attends to previous output tokens | Strategy history: what calls have we already made? |\n| **Cross-attention** | Decoder | **Encoder output** | Each output token attends to all input tokens | Strategy engineer reading telemetry to inform next call |\n\n**What this means:** When the decoder generates each output token, it can \"look back\" at the entire input sequence through cross-attention. For example, in translation, when generating the French word \"chat\", the cross-attention heads might focus on the English word \"cat\" in the encoder output.\n\n**F1 analogy:** Cross-attention is how the strategy engineer (decoder) reads the telemetry data (encoder output). When deciding whether to pit, the strategy engineer's query (\"Should we pit?\") attends to all the encoded telemetry signals -- tire degradation, weather forecast, gap to competitors -- and builds a decision from the most relevant data. Different attention heads might focus on different factors: one head on tire data, another on competitor positions, another on weather.\n\nThis is the same encoder-decoder attention you learned about in Notebook 16, now integrated into the Transformer architecture.",
   "id": "cell-21"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class DecoderBlock(nn.Module):\n    \"\"\"\n    One Transformer Decoder block.\n    \n    Contains:\n    1. Masked multi-head self-attention + Add & Norm\n    2. Multi-head cross-attention + Add & Norm  \n    3. Position-wise FFN + Add & Norm\n    \"\"\"\n    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n        super().__init__()\n        # Three sub-layers\n        self.self_attention = MultiHeadAttention(d_model, n_heads)\n        self.cross_attention = MultiHeadAttention(d_model, n_heads)\n        self.ffn = PositionwiseFFN(d_model, d_ff)\n        \n        # Three layer norms (one per sub-layer)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n        \"\"\"\n        Args:\n            x: Decoder input (batch, tgt_len, d_model)\n            encoder_output: Encoder output (batch, src_len, d_model)\n            src_mask: Mask for encoder output (optional)\n            tgt_mask: Causal mask for decoder self-attention\n        \n        Returns:\n            output: (batch, tgt_len, d_model)\n        \"\"\"\n        # Sub-layer 1: Masked self-attention\n        attn_output, _ = self.self_attention(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        # Sub-layer 2: Cross-attention (Q from decoder, K/V from encoder)\n        cross_output, cross_weights = self.cross_attention(x, encoder_output, encoder_output, src_mask)\n        x = self.norm2(x + self.dropout(cross_output))\n        \n        # Sub-layer 3: FFN\n        ffn_output = self.ffn(x)\n        x = self.norm3(x + self.dropout(ffn_output))\n        \n        return x\n\n# Test the decoder block\ndecoder_block = DecoderBlock(d_model, n_heads, d_ff)\n\n# Decoder input (target sequence, slightly shorter for demonstration)\ntgt_len = 8\ndecoder_input = torch.randn(batch_size, tgt_len, d_model)\nencoder_output = torch.randn(batch_size, seq_len, d_model)  # From the encoder\n\n# Create causal mask for decoder self-attention\ncausal_mask = torch.tril(torch.ones(tgt_len, tgt_len)).unsqueeze(0).unsqueeze(0)  # (1, 1, tgt_len, tgt_len)\n\ndecoder_output = decoder_block(decoder_input, encoder_output, tgt_mask=causal_mask)\n\nprint(f\"Decoder Block\")\nprint(f\"  Decoder input shape:  {decoder_input.shape}\")\nprint(f\"  Encoder output shape: {encoder_output.shape}\")\nprint(f\"  Decoder output shape: {decoder_output.shape}\")\nprint(f\"\\nCausal mask (tokens can only attend to earlier positions):\")\nprint(causal_mask[0, 0].numpy().astype(int))\nprint(f\"\\nParameter count:\")\ntotal_params = sum(p.numel() for p in decoder_block.parameters())\nfor name, param in decoder_block.named_parameters():\n    print(f\"  {name}: {param.shape} ({param.numel():,} params)\")\nprint(f\"  TOTAL: {total_params:,} parameters\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-22"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualization: Data flow through one Decoder Block\nfig, ax = plt.subplots(figsize=(10, 10))\nax.set_xlim(0, 12)\nax.set_ylim(0, 15)\nax.axis('off')\nax.set_title('Data Flow Through One Decoder Block', fontsize=15, fontweight='bold', pad=20)\n\n# Decoder Input\ndraw_box(ax, 5, 14, 3.5, 0.6, 'Decoder Input (shifted right)', 'gray')\ndraw_arrow(ax, 5, 13.7, 5, 12.7)\n\n# Masked Self-Attention\ndraw_box(ax, 5, 12.3, 4.5, 0.8, 'Masked Multi-Head Self-Attention', 'steelblue')\ndraw_arrow(ax, 5, 11.9, 5, 11.1)\n\n# Add & Norm 1\ndraw_box(ax, 5, 10.7, 3.5, 0.6, 'Add & LayerNorm', 'green')\nax.annotate('', xy=(2.8, 10.7), xytext=(2.8, 13.4),\n            arrowprops=dict(arrowstyle='->', color='red', lw=2, linestyle='--'))\nax.text(2.0, 12.0, 'residual', fontsize=9, color='red', rotation=90, va='center')\ndraw_arrow(ax, 5, 10.4, 5, 9.5)\n\n# Cross-Attention\ndraw_box(ax, 5, 9.1, 4.5, 0.8, 'Multi-Head Cross-Attention', 'purple')\ndraw_arrow(ax, 5, 8.7, 5, 7.9)\n\n# Encoder output arrow into cross-attention\ndraw_box(ax, 10, 9.1, 2.5, 0.8, 'Encoder\\nOutput', 'orange')\ndraw_arrow(ax, 8.75, 9.1, 7.25, 9.1)\nax.text(8.0, 9.6, 'K, V', fontsize=10, fontweight='bold', color='purple')\nax.text(4.0, 9.65, 'Q', fontsize=10, fontweight='bold', color='purple')\n\n# Add & Norm 2\ndraw_box(ax, 5, 7.5, 3.5, 0.6, 'Add & LayerNorm', 'green')\nax.annotate('', xy=(2.8, 7.5), xytext=(2.8, 10.1),\n            arrowprops=dict(arrowstyle='->', color='red', lw=2, linestyle='--'))\nax.text(2.0, 8.7, 'residual', fontsize=9, color='red', rotation=90, va='center')\ndraw_arrow(ax, 5, 7.2, 5, 6.3)\n\n# FFN\ndraw_box(ax, 5, 5.9, 4.5, 0.8, 'Feed-Forward Network', 'orange')\ndraw_arrow(ax, 5, 5.5, 5, 4.6)\n\n# Add & Norm 3\ndraw_box(ax, 5, 4.2, 3.5, 0.6, 'Add & LayerNorm', 'green')\nax.annotate('', xy=(2.8, 4.2), xytext=(2.8, 6.9),\n            arrowprops=dict(arrowstyle='->', color='red', lw=2, linestyle='--'))\nax.text(2.0, 5.5, 'residual', fontsize=9, color='red', rotation=90, va='center')\ndraw_arrow(ax, 5, 3.9, 5, 3.0)\n\n# Output\ndraw_box(ax, 5, 2.6, 3, 0.6, 'Decoder Output', 'gray')\n\n# Annotations\nax.text(8.5, 12.3, 'Causal mask:\\ncan only see\\npast tokens', fontsize=9,\n        ha='center', style='italic', color='steelblue')\nax.text(8.0, 7.5, '\"Read\" the\\ninput sequence', fontsize=9,\n        ha='center', style='italic', color='purple')\n\nplt.tight_layout()\nplt.savefig('decoder_block.png', dpi=100, bbox_inches='tight')\nplt.show()\n\nprint(\"The decoder block has THREE sub-layers (vs encoder's TWO).\")\nprint(\"The extra sub-layer is cross-attention, which connects the decoder to the encoder.\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Encoder Block vs Decoder Block\n\n| Feature | Encoder Block | Decoder Block | F1 Parallel |\n|---------|--------------|---------------|-------------|\n| **Sub-layers** | 2 (self-attention + FFN) | 3 (masked self-attn + cross-attn + FFN) | Data processing vs. data processing + strategy lookup + decision |\n| **Self-attention** | Bidirectional (sees all tokens) | Causal (only sees past tokens) | Full session replay vs. real-time racing (no future knowledge) |\n| **Cross-attention** | None | Attends to encoder output | N/A vs. reading the telemetry dashboard |\n| **Residual + LayerNorm** | Around each sub-layer | Around each sub-layer | Signal preservation at every stage |\n| **Output shape** | Same as input | Same as input | Same data dimensions throughout |\n| **Parameter count** | ~4 * d_model^2 | ~6 * d_model^2 (extra attention) | Leaner processing vs. fuller decision system |",
   "id": "cell-24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## 5. The Full Transformer\n\n### Putting It All Together\n\nNow we assemble every piece into a complete Transformer. Here is the full architecture:\n\n```\nSOURCE TOKENS                              TARGET TOKENS (shifted right)\n     │                                           │\n┌────▼──────────────────┐                 ┌──────▼────────────────┐\n│ Input Embedding       │                 │ Output Embedding      │\n│    +                  │                 │    +                  │\n│ Positional Encoding   │                 │ Positional Encoding   │\n└────┬──────────────────┘                 └──────┬────────────────┘\n     │                                           │\n     │  ┌─────────────────────┐                  │  ┌─────────────────────┐\n     └─▶│ Encoder Block 1     │           ┌─────▶│ Decoder Block 1     │◄─ encoder output\n        │   - Self-Attention  │           │      │   - Masked Self-Attn │\n        │   - Add & Norm      │           │      │   - Cross-Attention  │\n        │   - FFN             │           │      │   - Add & Norm       │\n        │   - Add & Norm      │           │      │   - FFN              │\n        └─────────┬───────────┘           │      │   - Add & Norm       │\n                  │                       │      └─────────┬───────────┘\n        ┌─────────▼───────────┐           │                │\n        │ Encoder Block 2     │           │      ┌─────────▼───────────┐\n        │       ...           │           │      │ Decoder Block 2     │\n        └─────────┬───────────┘           │      │       ...           │\n                  │                       │      └─────────┬───────────┘\n        ┌─────────▼───────────┐           │                │\n        │ Encoder Block N     │───────────┘      ┌─────────▼───────────┐\n        └─────────────────────┘                  │ Decoder Block N     │\n                                                 └─────────┬───────────┘\n                                                           │\n                                                 ┌─────────▼───────────┐\n                                                 │ Linear (d_model →   │\n                                                 │         vocab_size) │\n                                                 │ Softmax             │\n                                                 └─────────┬───────────┘\n                                                           │\n                                                  OUTPUT PROBABILITIES\n```\n\nEvery encoder block feeds into the **same** final encoder output. Every decoder block receives this encoder output for cross-attention."
   ],
   "id": "cell-25"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class PositionalEncoding(nn.Module):\n    \"\"\"\n    Adds sinusoidal positional encoding to token embeddings.\n    Registered as a buffer (not a parameter) since it's fixed.\n    \"\"\"\n    def __init__(self, d_model, max_len=5000, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)  # (1, max_len, d_model) for broadcasting\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x):\n        \"\"\"Add positional encoding to input embeddings.\"\"\"\n        x = x + self.pe[:, :x.size(1), :]\n        return self.dropout(x)\n\n\nclass Transformer(nn.Module):\n    \"\"\"\n    Complete Transformer model (encoder-decoder).\n    \n    Built from scratch using the components defined above.\n    \"\"\"\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=64, n_heads=4,\n                 n_layers=2, d_ff=256, max_len=100, dropout=0.1):\n        super().__init__()\n        \n        self.d_model = d_model\n        \n        # Embeddings\n        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, max_len, dropout)\n        \n        # Encoder stack\n        self.encoder_blocks = nn.ModuleList([\n            EncoderBlock(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)\n        ])\n        \n        # Decoder stack\n        self.decoder_blocks = nn.ModuleList([\n            DecoderBlock(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)\n        ])\n        \n        # Final output projection\n        self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n    \n    def encode(self, src, src_mask=None):\n        \"\"\"Run the encoder stack.\"\"\"\n        x = self.src_embedding(src) * math.sqrt(self.d_model)  # Scale embedding\n        x = self.positional_encoding(x)\n        \n        for block in self.encoder_blocks:\n            x = block(x, src_mask)\n        \n        return x\n    \n    def decode(self, tgt, encoder_output, src_mask=None, tgt_mask=None):\n        \"\"\"Run the decoder stack.\"\"\"\n        x = self.tgt_embedding(tgt) * math.sqrt(self.d_model)  # Scale embedding\n        x = self.positional_encoding(x)\n        \n        for block in self.decoder_blocks:\n            x = block(x, encoder_output, src_mask, tgt_mask)\n        \n        return x\n    \n    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n        \"\"\"\n        Full forward pass.\n        \n        Args:\n            src: Source token IDs (batch, src_len)\n            tgt: Target token IDs (batch, tgt_len)\n            src_mask: Optional source mask\n            tgt_mask: Causal mask for target\n        \n        Returns:\n            logits: (batch, tgt_len, tgt_vocab_size)\n        \"\"\"\n        encoder_output = self.encode(src, src_mask)\n        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n        logits = self.output_projection(decoder_output)\n        return logits\n    \n    @staticmethod\n    def generate_causal_mask(size):\n        \"\"\"Generate a causal (lower-triangular) mask.\"\"\"\n        mask = torch.tril(torch.ones(size, size)).unsqueeze(0).unsqueeze(0)\n        return mask  # (1, 1, size, size)\n\n\n# Create and inspect the full Transformer\nsrc_vocab_size = 50\ntgt_vocab_size = 50\nmodel = Transformer(\n    src_vocab_size=src_vocab_size,\n    tgt_vocab_size=tgt_vocab_size,\n    d_model=64,\n    n_heads=4,\n    n_layers=2,\n    d_ff=256,\n    dropout=0.1\n)\n\nprint(\"=\" * 60)\nprint(\"FULL TRANSFORMER ARCHITECTURE\")\nprint(\"=\" * 60)\nprint(f\"\\nModel configuration:\")\nprint(f\"  d_model:        64\")\nprint(f\"  n_heads:        4 (d_k = 16)\")\nprint(f\"  n_layers:       2 (encoder) + 2 (decoder)\")\nprint(f\"  d_ff:           256\")\nprint(f\"  src_vocab_size: {src_vocab_size}\")\nprint(f\"  tgt_vocab_size: {tgt_vocab_size}\")\n\n# Test forward pass\nsrc = torch.randint(0, src_vocab_size, (2, 10))  # batch=2, src_len=10\ntgt = torch.randint(0, tgt_vocab_size, (2, 8))   # batch=2, tgt_len=8\ntgt_mask = Transformer.generate_causal_mask(8)\n\nlogits = model(src, tgt, tgt_mask=tgt_mask)\nprint(f\"\\nForward pass:\")\nprint(f\"  Source shape:  {src.shape}\")\nprint(f\"  Target shape:  {tgt.shape}\")\nprint(f\"  Output logits: {logits.shape}\")\nprint(f\"  (batch=2, tgt_len=8, vocab={tgt_vocab_size})\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-26"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Parameter counting and analysis\nprint(\"=\" * 65)\nprint(\"PARAMETER ANALYSIS\")\nprint(\"=\" * 65)\n\ntotal = 0\nsections = {}\n\nfor name, param in model.named_parameters():\n    total += param.numel()\n    # Group by section\n    section = name.split('.')[0]\n    if section not in sections:\n        sections[section] = 0\n    sections[section] += param.numel()\n\nprint(f\"\\nParameter breakdown by component:\")\nprint(\"-\" * 45)\nfor section, count in sections.items():\n    pct = 100 * count / total\n    bar = '#' * int(pct / 2)\n    print(f\"  {section:25s}: {count:>8,} ({pct:5.1f}%) {bar}\")\nprint(\"-\" * 45)\nprint(f\"  {'TOTAL':25s}: {total:>8,}\")\n\nprint(f\"\\nDetailed parameter list:\")\nprint(\"-\" * 65)\nfor name, param in model.named_parameters():\n    print(f\"  {name:45s} {str(list(param.shape)):>18s} = {param.numel():>7,}\")\nprint(\"-\" * 65)\nprint(f\"  Total parameters: {total:,}\")\n\n# Compare to the original Transformer paper\nprint(f\"\\n--- For reference: Original 'Attention Is All You Need' ---\")\nprint(f\"  d_model=512, n_heads=8, n_layers=6, d_ff=2048\")\norig_params = (\n    2 * 37000 * 512 +                    # embeddings (src + tgt, ~37K vocab)\n    6 * (4 * 512 * 512 + 2 * 512) +      # encoder attention\n    6 * (512 * 2048 + 2048 + 2048 * 512 + 512) +  # encoder FFN\n    6 * (4 * 512 * 512 + 2 * 512) * 2 +  # decoder attention (self + cross)\n    6 * (512 * 2048 + 2048 + 2048 * 512 + 512) +  # decoder FFN\n    512 * 37000                           # output projection\n)\nprint(f\"  Approximate total: ~65M parameters\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 6. Training a Transformer\n\n### Teacher Forcing\n\nDuring training, the decoder does **not** generate tokens autoregressively (using its own predictions as input). Instead, we use **teacher forcing**: we feed the **ground truth** target sequence (shifted right by one position) as decoder input.\n\n```\nTarget:          [<sos>, \"le\", \"chat\", \"est\", \"assis\", <eos>]\nDecoder input:   [<sos>, \"le\", \"chat\", \"est\", \"assis\"]        ← shifted right\nExpected output: [\"le\", \"chat\", \"est\", \"assis\", <eos>]        ← what we train to predict\n```\n\n**Why?** If we used the decoder's own predictions during training, early in training (when predictions are random) the decoder would get garbage input and never learn. Teacher forcing provides a stable training signal.\n\n**F1 analogy:** Teacher forcing is like training a junior strategist by showing them the actual race outcomes. Instead of letting the trainee make calls and see them go wrong (which would compound errors -- a bad early call leads to worse later calls), you show them the correct sequence of decisions at each point: \"At lap 15, the right call was to pit; at lap 30, the right call was to stay out.\" The trainee learns from the correct history, not from their own early mistakes.\n\nThe causal mask ensures that even though all ground truth tokens are provided simultaneously, each position can only see previous positions -- maintaining the autoregressive property.",
   "id": "cell-28"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Learning Rate Warmup\n\nThe original Transformer paper uses a specific learning rate schedule that has become iconic in deep learning:\n\n$$lr = d_{model}^{-0.5} \\cdot \\min(step^{-0.5}, \\; step \\cdot warmup\\_steps^{-1.5})$$\n\n| Phase | What happens | Why | F1 Parallel |\n|-------|-------------|-----|-------------|\n| **Warmup** (first ~4000 steps) | LR increases linearly from 0 | Prevents early training instability; Adam needs good running estimates | Formation lap: building tire temperature gradually before racing |\n| **Decay** (after warmup) | LR decreases as $1/\\sqrt{step}$ | Gradually fine-tunes as model converges | Tire management: pushing hard early in a stint, then nursing tires as they degrade |\n\n**What this means:** Start gently (low learning rate), ramp up as the optimizer warms up, then gradually cool down. This schedule was crucial for training the original Transformer stably.",
   "id": "cell-29"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualization: Learning rate warmup schedule\ndef transformer_lr_schedule(step, d_model, warmup_steps):\n    \"\"\"The learning rate schedule from 'Attention Is All You Need'.\"\"\"\n    if step == 0:\n        step = 1\n    return d_model ** (-0.5) * min(step ** (-0.5), step * warmup_steps ** (-1.5))\n\n# Plot for different warmup values\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Different warmup steps\nax = axes[0]\nsteps = range(1, 20001)\nfor warmup in [500, 2000, 4000, 8000]:\n    lrs = [transformer_lr_schedule(s, 512, warmup) for s in steps]\n    ax.plot(steps, lrs, label=f'warmup={warmup}', linewidth=1.5)\n\nax.set_xlabel('Training Step', fontsize=11)\nax.set_ylabel('Learning Rate', fontsize=11)\nax.set_title('Transformer LR Schedule\\n(varying warmup steps)', fontsize=13, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Different model dimensions\nax = axes[1]\nfor d in [64, 128, 256, 512]:\n    lrs = [transformer_lr_schedule(s, d, 4000) for s in steps]\n    ax.plot(steps, lrs, label=f'd_model={d}', linewidth=1.5)\n\nax.set_xlabel('Training Step', fontsize=11)\nax.set_ylabel('Learning Rate', fontsize=11)\nax.set_title('Transformer LR Schedule\\n(varying d_model, warmup=4000)', fontsize=13, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('lr_warmup.png', dpi=100, bbox_inches='tight')\nplt.show()\nprint(\"Left: More warmup steps = slower ramp-up but higher peak learning rate.\")\nprint(\"Right: Larger d_model = lower learning rate (bigger models need gentler updates).\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Training Task: Sequence Reversal\n\nLet's train our Transformer on a simple but non-trivial task: **reversing a sequence of numbers**. For example:\n\n- Input: `[3, 7, 1, 4, 2]`\n- Output: `[2, 4, 1, 7, 3]`\n\nThis task requires the model to:\n1. Read and remember the entire input (encoder)\n2. Output tokens in reverse order (decoder with cross-attention)\n\n**F1 analogy:** Think of this as the Transformer learning to reverse-engineer a race: given the finishing order `[VER, NOR, LEC, HAM, PIA]`, reconstruct the starting grid `[PIA, HAM, LEC, NOR, VER]`. The encoder reads the finish, the decoder generates the start -- a non-trivial mapping that requires understanding the full sequence.\n\nWe use special tokens: `0 = PAD`, `1 = SOS` (start of sequence), `2 = EOS` (end of sequence), values `3+` are the actual numbers.",
   "id": "cell-31"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate training data for sequence reversal\ndef generate_reversal_data(n_samples, seq_len, vocab_size, pad_id=0, sos_id=1, eos_id=2):\n    \"\"\"\n    Generate source-target pairs for the reversal task.\n    \n    Source: [v1, v2, ..., vn, EOS, PAD, ...]\n    Target input:  [SOS, vn, vn-1, ..., v1]\n    Target output: [vn, vn-1, ..., v1, EOS]\n    \"\"\"\n    min_token = 3  # Tokens 0,1,2 are special\n    src_data = []\n    tgt_input_data = []\n    tgt_output_data = []\n    \n    for _ in range(n_samples):\n        # Random length between 3 and seq_len\n        length = np.random.randint(3, seq_len + 1)\n        \n        # Random sequence of tokens\n        seq = np.random.randint(min_token, vocab_size, size=length).tolist()\n        reversed_seq = seq[::-1]\n        \n        # Source: seq + EOS + padding\n        src = seq + [eos_id] + [pad_id] * (seq_len - length)\n        \n        # Target input: SOS + reversed_seq + padding\n        tgt_in = [sos_id] + reversed_seq + [pad_id] * (seq_len - length)\n        \n        # Target output: reversed_seq + EOS + padding  \n        tgt_out = reversed_seq + [eos_id] + [pad_id] * (seq_len - length)\n        \n        src_data.append(src)\n        tgt_input_data.append(tgt_in)\n        tgt_output_data.append(tgt_out)\n    \n    return (torch.tensor(src_data, dtype=torch.long),\n            torch.tensor(tgt_input_data, dtype=torch.long),\n            torch.tensor(tgt_output_data, dtype=torch.long))\n\n# Generate data\nVOCAB_SIZE = 20\nSEQ_LEN = 8\nN_TRAIN = 5000\nN_TEST = 500\n\nsrc_train, tgt_in_train, tgt_out_train = generate_reversal_data(N_TRAIN, SEQ_LEN, VOCAB_SIZE)\nsrc_test, tgt_in_test, tgt_out_test = generate_reversal_data(N_TEST, SEQ_LEN, VOCAB_SIZE)\n\nprint(f\"Training data: {N_TRAIN} samples\")\nprint(f\"Test data:     {N_TEST} samples\")\nprint(f\"Vocabulary:    {VOCAB_SIZE} tokens (0=PAD, 1=SOS, 2=EOS, 3-{VOCAB_SIZE-1}=values)\")\nprint(f\"Max seq length: {SEQ_LEN}\")\nprint(f\"\\nExample:\")\nprint(f\"  Source:        {src_train[0].tolist()}\")\nprint(f\"  Target input:  {tgt_in_train[0].tolist()}\")\nprint(f\"  Target output: {tgt_out_train[0].tolist()}\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-32"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train the Transformer on sequence reversal\ntorch.manual_seed(42)\n\n# Create model\nmodel = Transformer(\n    src_vocab_size=VOCAB_SIZE,\n    tgt_vocab_size=VOCAB_SIZE,\n    d_model=64,\n    n_heads=4,\n    n_layers=2,\n    d_ff=256,\n    max_len=SEQ_LEN + 5,\n    dropout=0.1\n)\n\n# Training setup\noptimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n\n# Training loop\nn_epochs = 30\nbatch_size = 128\ntrain_losses = []\ntest_accuracies = []\n\nmodel.train()\nfor epoch in range(n_epochs):\n    epoch_loss = 0\n    n_batches = 0\n    \n    # Shuffle training data\n    perm = torch.randperm(N_TRAIN)\n    src_shuffled = src_train[perm]\n    tgt_in_shuffled = tgt_in_train[perm]\n    tgt_out_shuffled = tgt_out_train[perm]\n    \n    for i in range(0, N_TRAIN, batch_size):\n        src_batch = src_shuffled[i:i+batch_size]\n        tgt_in_batch = tgt_in_shuffled[i:i+batch_size]\n        tgt_out_batch = tgt_out_shuffled[i:i+batch_size]\n        \n        # Create causal mask for decoder\n        tgt_len = tgt_in_batch.size(1)\n        tgt_mask = Transformer.generate_causal_mask(tgt_len)\n        \n        # Forward pass\n        logits = model(src_batch, tgt_in_batch, tgt_mask=tgt_mask)\n        \n        # Compute loss (reshape for CrossEntropyLoss)\n        loss = criterion(logits.view(-1, VOCAB_SIZE), tgt_out_batch.view(-1))\n        \n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        n_batches += 1\n    \n    avg_loss = epoch_loss / n_batches\n    train_losses.append(avg_loss)\n    \n    # Evaluate every 5 epochs\n    if (epoch + 1) % 5 == 0:\n        model.eval()\n        with torch.no_grad():\n            tgt_mask = Transformer.generate_causal_mask(tgt_in_test.size(1))\n            test_logits = model(src_test, tgt_in_test, tgt_mask=tgt_mask)\n            test_preds = test_logits.argmax(dim=-1)\n            \n            # Calculate accuracy (ignoring padding)\n            mask = tgt_out_test != 0\n            correct = (test_preds == tgt_out_test) & mask\n            accuracy = correct.sum().float() / mask.sum().float()\n            test_accuracies.append((epoch + 1, accuracy.item()))\n            \n            print(f\"Epoch {epoch+1:3d} | Loss: {avg_loss:.4f} | Test accuracy: {accuracy:.4f}\")\n        model.train()\n    else:\n        if (epoch + 1) % 10 == 0:\n            print(f\"Epoch {epoch+1:3d} | Loss: {avg_loss:.4f}\")\n\nprint(f\"\\nTraining complete!\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-33"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualization: Training progress\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Loss curve\nax = axes[0]\nax.plot(range(1, len(train_losses) + 1), train_losses, color='blue', linewidth=2)\nax.set_xlabel('Epoch', fontsize=11)\nax.set_ylabel('Cross-Entropy Loss', fontsize=11)\nax.set_title('Training Loss', fontsize=13, fontweight='bold')\nax.grid(True, alpha=0.3)\n\n# Accuracy\nax = axes[1]\nif test_accuracies:\n    epochs_acc, accs = zip(*test_accuracies)\n    ax.plot(epochs_acc, accs, 'o-', color='green', linewidth=2, markersize=8)\n    ax.set_xlabel('Epoch', fontsize=11)\n    ax.set_ylabel('Token Accuracy', fontsize=11)\n    ax.set_title('Test Accuracy (Sequence Reversal)', fontsize=13, fontweight='bold')\n    ax.set_ylim(0, 1.05)\n    ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5, label='Perfect')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('training_progress.png', dpi=100, bbox_inches='tight')\nplt.show()"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-34"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Show example predictions\nmodel.eval()\nwith torch.no_grad():\n    # Pick a few test examples\n    n_show = 8\n    tgt_mask = Transformer.generate_causal_mask(tgt_in_test.size(1))\n    test_logits = model(src_test[:n_show], tgt_in_test[:n_show], tgt_mask=tgt_mask)\n    test_preds = test_logits.argmax(dim=-1)\n\nprint(\"Example Predictions (Sequence Reversal)\")\nprint(\"=\" * 60)\nfor i in range(n_show):\n    src = src_test[i].tolist()\n    expected = tgt_out_test[i].tolist()\n    predicted = test_preds[i].tolist()\n    \n    # Remove padding and special tokens for display\n    src_clean = [t for t in src if t > 2]\n    exp_clean = [t for t in expected if t > 2]\n    pred_clean = [t for t in predicted if t > 2][:len(exp_clean)]\n    \n    match = \"OK\" if exp_clean == pred_clean else \"WRONG\"\n    print(f\"  Input:    {src_clean}\")\n    print(f\"  Expected: {exp_clean}\")\n    print(f\"  Got:      {pred_clean}  [{match}]\")\n    print()"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-35"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize attention patterns during inference\nmodel.eval()\n\n# Get a single example\nexample_idx = 0\nsrc_example = src_test[example_idx:example_idx+1]\ntgt_example = tgt_in_test[example_idx:example_idx+1]\n\n# Forward pass, collecting attention weights from all layers\nattention_maps = {'encoder_self': [], 'decoder_self': [], 'decoder_cross': []}\n\nwith torch.no_grad():\n    # Encoder pass\n    enc_x = model.src_embedding(src_example) * math.sqrt(model.d_model)\n    enc_x = model.positional_encoding(enc_x)\n    for block in model.encoder_blocks:\n        attn_out, attn_weights = block.self_attention(enc_x, enc_x, enc_x)\n        attention_maps['encoder_self'].append(attn_weights.squeeze(0).numpy())\n        enc_x = block.norm1(enc_x + attn_out)\n        ffn_out = block.ffn(enc_x)\n        enc_x = block.norm2(enc_x + ffn_out)\n    \n    encoder_output = enc_x\n    \n    # Decoder pass\n    tgt_mask = Transformer.generate_causal_mask(tgt_example.size(1))\n    dec_x = model.tgt_embedding(tgt_example) * math.sqrt(model.d_model)\n    dec_x = model.positional_encoding(dec_x)\n    for block in model.decoder_blocks:\n        self_attn_out, self_attn_w = block.self_attention(dec_x, dec_x, dec_x, tgt_mask)\n        attention_maps['decoder_self'].append(self_attn_w.squeeze(0).numpy())\n        dec_x = block.norm1(dec_x + self_attn_out)\n        \n        cross_attn_out, cross_attn_w = block.cross_attention(dec_x, encoder_output, encoder_output)\n        attention_maps['decoder_cross'].append(cross_attn_w.squeeze(0).numpy())\n        dec_x = block.norm2(dec_x + cross_attn_out)\n        \n        ffn_out = block.ffn(dec_x)\n        dec_x = block.norm3(dec_x + ffn_out)\n\n# Plot cross-attention from last decoder layer (most interpretable)\nsrc_tokens = [str(t) for t in src_test[example_idx].tolist() if t > 0]\ntgt_tokens = [str(t) for t in tgt_in_test[example_idx].tolist() if t > 0]\n\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\nn_heads = attention_maps['decoder_cross'][-1].shape[0]\nn_show_heads = min(4, n_heads)\n\nfor h in range(n_show_heads):\n    ax = axes[h]\n    attn = attention_maps['decoder_cross'][-1][h]\n    # Trim to non-padding tokens\n    n_tgt = len(tgt_tokens)\n    n_src = len(src_tokens)\n    attn_trimmed = attn[:n_tgt, :n_src]\n    \n    im = ax.imshow(attn_trimmed, cmap='Blues', aspect='auto', vmin=0, vmax=1)\n    ax.set_xticks(range(n_src))\n    ax.set_xticklabels(src_tokens, fontsize=8)\n    ax.set_yticks(range(n_tgt))\n    ax.set_yticklabels(tgt_tokens, fontsize=8)\n    ax.set_xlabel('Source (encoder)', fontsize=9)\n    if h == 0:\n        ax.set_ylabel('Target (decoder)', fontsize=9)\n    ax.set_title(f'Head {h+1}', fontsize=11, fontweight='bold')\n\nplt.suptitle('Cross-Attention Patterns (Last Decoder Layer)\\nFor sequence reversal task',\n             fontsize=13, fontweight='bold', y=1.05)\nplt.tight_layout()\nplt.savefig('attention_patterns.png', dpi=100, bbox_inches='tight')\nplt.show()\n\nprint(\"Each heatmap shows what source tokens the decoder attends to at each step.\")\nprint(\"For reversal, we expect the decoder to attend to source tokens in reverse order.\")\nprint(\"Different heads may specialize in different patterns.\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-36"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 7. Why Transformers Work So Well\n\n### Intuitive Explanation\n\nThe Transformer has become the dominant architecture across NLP, vision, speech, biology, and more. Here is why:\n\n**1. Parallel Computation**\nUnlike RNNs that process tokens sequentially, Transformers process all tokens in parallel during training. This means training time scales with model size, not sequence length. A 100-token sequence takes roughly the same time as a 10-token sequence (ignoring the O(n^2) attention cost).\n\n**2. Direct Connections Between All Positions**\nIn an RNN, information from position 1 must travel through positions 2, 3, 4, ... to reach position 100. Each step risks degrading the signal. In a Transformer, position 1 and position 100 are directly connected through attention -- information flows in a single step.\n\n**3. Multiple Representation Subspaces (Heads)**\nMulti-head attention creates multiple parallel \"views\" of the relationships between tokens. Different heads can specialize: one for syntax, one for coreference, one for semantic similarity, etc. This is like having multiple experts analyzing the same data simultaneously.\n\n**4. Depth + Residual Connections**\nStacking multiple layers with residual connections lets the model build increasingly abstract representations while maintaining stable gradient flow. Each layer can focus on refining the representation rather than learning everything at once.\n\n**5. Scalability**\nTransformers exhibit remarkable **scaling laws**: performance improves predictably as you increase model size, data, and compute. This predictability enables massive investments in training large models.\n\n**F1 analogy:** The Transformer's dominance mirrors why modern F1 strategy systems outperform human-only decision making:\n1. **Parallel computation** = processing all 20 cars' telemetry simultaneously, not one at a time\n2. **Direct connections** = any data point can inform any decision without going through intermediaries\n3. **Multiple heads** = like having separate specialists for tires, weather, competitors, and fuel -- all analyzing in parallel\n4. **Depth + residuals** = layered analysis that builds from raw data to strategy without losing the original signals\n5. **Scalability** = more compute and data predictably leads to better strategy calls",
   "id": "cell-37"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Deep Dive: Scaling Laws Preview\n\nResearch by Kaplan et al. (2020) and Hoffmann et al. (2022, \"Chinchilla\") revealed that Transformer performance follows **power law** relationships:\n\n$$L(N) \\approx \\left(\\frac{N_c}{N}\\right)^{\\alpha_N}$$\n\nwhere $L$ is the loss, $N$ is the number of parameters, and $\\alpha_N \\approx 0.076$.\n\n| Factor | Effect on Performance | Key Finding | F1 Parallel |\n|--------|----------------------|-------------|-------------|\n| **Parameters (N)** | Loss decreases as power law | Bigger models are more sample-efficient | More sensors = better data picture |\n| **Data (D)** | Loss decreases as power law | More data always helps | More laps of practice = better setup |\n| **Compute (C)** | Loss decreases as power law | Optimal N and D scale together | More simulation budget = better strategy |\n| **Architecture** | Relatively minor effect | Scaling matters more than architecture tweaks | Car regulations matter less than development budget at scale |\n\n#### Key Insight\n\nThe Transformer's real superpower is not any single clever trick -- it is that the architecture **scales**. As you add more parameters, more data, and more compute, performance improves predictably. This is why the field moved from \"design clever architectures\" to \"scale Transformers.\"\n\n**F1 analogy:** This mirrors F1's development philosophy: once the fundamental car concept is sound (like the Transformer architecture), the teams that win are the ones that can scale investment -- more wind tunnel time, more CFD simulation, more track testing data. The architecture is the baseline car design; scaling laws are the development curve.\n\n#### Common Misconceptions\n\n| Misconception | Reality |\n|---------------|---------|\n| \"Attention is computationally cheap\" | Attention is O(n^2) in sequence length -- this is actually a major bottleneck for long sequences |\n| \"Transformers understand language\" | They learn statistical patterns; whether this constitutes understanding is debated |\n| \"Bigger is always better\" | Chinchilla showed that many models were undertrained -- the optimal balance of size and data matters |\n| \"Transformers are only for NLP\" | Vision Transformers (ViT), protein folding (AlphaFold), music, robotics -- they work everywhere |",
   "id": "cell-38"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Interactive exploration: How architecture choices affect parameter count\ndef count_transformer_params(d_model, n_heads, n_layers, d_ff, vocab_size):\n    \"\"\"\n    Count parameters in a Transformer (encoder-decoder).\n    \n    Returns dict with parameter breakdown.\n    \"\"\"\n    params = {}\n    \n    # Embeddings (src + tgt, often shared)\n    params['src_embedding'] = vocab_size * d_model\n    params['tgt_embedding'] = vocab_size * d_model\n    \n    # Per encoder layer\n    enc_attn = 4 * d_model * d_model + 4 * d_model  # Q,K,V,O weights + biases\n    enc_ffn = d_model * d_ff + d_ff + d_ff * d_model + d_model  # Two linear layers\n    enc_norm = 2 * (2 * d_model)  # Two LayerNorms (gamma + beta each)\n    enc_per_layer = enc_attn + enc_ffn + enc_norm\n    params['encoder'] = n_layers * enc_per_layer\n    \n    # Per decoder layer (extra cross-attention)\n    dec_self_attn = 4 * d_model * d_model + 4 * d_model\n    dec_cross_attn = 4 * d_model * d_model + 4 * d_model\n    dec_ffn = d_model * d_ff + d_ff + d_ff * d_model + d_model\n    dec_norm = 3 * (2 * d_model)  # Three LayerNorms\n    dec_per_layer = dec_self_attn + dec_cross_attn + dec_ffn + dec_norm\n    params['decoder'] = n_layers * dec_per_layer\n    \n    # Output projection\n    params['output_proj'] = d_model * vocab_size + vocab_size\n    \n    params['total'] = sum(params.values())\n    return params\n\n# Compare different configurations\nconfigs = [\n    (\"Our model\", 64, 4, 2, 256, 20),\n    (\"Small\", 256, 4, 4, 1024, 32000),\n    (\"Base (paper)\", 512, 8, 6, 2048, 37000),\n    (\"Large (paper)\", 1024, 16, 6, 4096, 37000),\n    (\"GPT-2 Small*\", 768, 12, 12, 3072, 50257),\n    (\"GPT-3 175B*\", 12288, 96, 96, 49152, 50257),\n]\n\nprint(\"Transformer Parameter Counts Across Configurations\")\nprint(\"=\" * 80)\nprint(f\"{'Config':18s} {'d_model':>7s} {'heads':>5s} {'layers':>6s} {'d_ff':>6s} {'vocab':>7s} {'Total Params':>14s}\")\nprint(\"-\" * 80)\n\nfor name, d, h, l, ff, v in configs:\n    params = count_transformer_params(d, h, l, ff, v)\n    total = params['total']\n    if total > 1e9:\n        total_str = f\"{total/1e9:.1f}B\"\n    elif total > 1e6:\n        total_str = f\"{total/1e6:.1f}M\"\n    elif total > 1e3:\n        total_str = f\"{total/1e3:.1f}K\"\n    else:\n        total_str = str(total)\n    print(f\"  {name:18s} {d:>5d} {h:>5d} {l:>6d} {ff:>6d} {v:>7d} {total_str:>12s}\")\n\nprint(\"-\" * 80)\nprint(\"* GPT models are decoder-only; numbers are approximate\")\n\n# Show parameter distribution for the base model\nprint(f\"\\n--- Parameter Distribution for Base Model (d=512, L=6) ---\")\nparams = count_transformer_params(512, 8, 6, 2048, 37000)\ntotal = params['total']\nfor component, count in params.items():\n    if component != 'total':\n        pct = 100 * count / total\n        bar = '#' * int(pct / 2)\n        print(f\"  {component:18s}: {count:>12,} ({pct:5.1f}%) {bar}\")\nprint(f\"  {'TOTAL':18s}: {total:>12,}\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-39"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercises\n\n### Exercise 1: Implement Positional Encoding from Scratch\n\nImplement the sinusoidal positional encoding without looking at the code above. Verify that your implementation matches.\n\n**F1 framing:** You are building the lap/sector encoding system for the strategy computer. Each position (lap) needs a unique fingerprint composed of multiple frequency channels, just like how each moment in a race is uniquely identified by the combination of lap number, sector, tire age, and fuel load.",
   "id": "cell-40"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# EXERCISE 1: Implement sinusoidal positional encoding\ndef my_positional_encoding(max_len, d_model):\n    \"\"\"\n    Generate sinusoidal positional encoding.\n    \n    PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n    \n    Args:\n        max_len: Maximum sequence length\n        d_model: Model dimension (must be even)\n    \n    Returns:\n        numpy array of shape (max_len, d_model)\n    \"\"\"\n    # TODO: Implement this!\n    # Hint 1: Create position array [0, 1, ..., max_len-1] as column vector\n    # Hint 2: Create dimension array [0, 2, 4, ...] for the 2i values\n    # Hint 3: Compute 10000^(2i/d_model) as the denominator\n    # Hint 4: Fill even columns with sin, odd columns with cos\n    \n    pass\n\n# Test\nmy_pe = my_positional_encoding(50, 64)\nexpected_pe = get_positional_encoding(50, 64).numpy()\n\nif my_pe is not None:\n    print(f\"Your shape: {my_pe.shape}\")\n    print(f\"Expected shape: {expected_pe.shape}\")\n    print(f\"Match: {np.allclose(my_pe, expected_pe, atol=1e-6)}\")\n    print(f\"Max difference: {np.max(np.abs(my_pe - expected_pe)):.8f}\")\nelse:\n    print(\"Implement the function above!\")\n    print(f\"Expected shape: {expected_pe.shape}\")\n    print(f\"First row should start with: {expected_pe[0, :6].round(4)}\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2: Build a Decoder-Only Transformer (like GPT)\n\nMany modern language models (GPT, LLaMA) use only the decoder, with no encoder and no cross-attention. Implement a decoder-only Transformer that does autoregressive prediction.\n\n**F1 framing:** Build a race commentary generator -- a decoder-only model that, given the sequence of events so far (\"Safety car deployed, Verstappen pits, ...\"), predicts the next event. No encoder needed because there is no separate input to translate; just the running sequence of race events, generated one at a time.",
   "id": "cell-42"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# EXERCISE 2: Decoder-only Transformer\nclass DecoderOnlyBlock(nn.Module):\n    \"\"\"\n    A single block for a decoder-only Transformer.\n    Like an encoder block but with a causal mask on self-attention.\n    No cross-attention needed.\n    \"\"\"\n    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n        super().__init__()\n        # TODO: Implement this!\n        # Hint: Same structure as EncoderBlock, but you'll apply a causal mask\n        # in the forward pass\n        \n        pass\n    \n    def forward(self, x, mask=None):\n        # TODO: Implement this!\n        # Hint: Self-attention with causal mask + Add&Norm + FFN + Add&Norm\n        \n        pass\n\n\nclass DecoderOnlyTransformer(nn.Module):\n    \"\"\"\n    GPT-style decoder-only Transformer.\n    \n    Takes a sequence of token IDs and predicts the next token at each position.\n    \"\"\"\n    def __init__(self, vocab_size, d_model=64, n_heads=4, n_layers=2, \n                 d_ff=256, max_len=100, dropout=0.1):\n        super().__init__()\n        # TODO: Implement this!\n        # Components needed:\n        # 1. Token embedding\n        # 2. Positional encoding \n        # 3. Stack of DecoderOnlyBlocks\n        # 4. Output projection (Linear to vocab_size)\n        \n        pass\n    \n    def forward(self, x):\n        # TODO: Implement this!\n        # 1. Embed tokens and add positional encoding\n        # 2. Create causal mask\n        # 3. Pass through all blocks\n        # 4. Project to vocab size\n        # Return logits\n        \n        pass\n\n# Test (uncomment when implemented)\n# gpt = DecoderOnlyTransformer(vocab_size=100, d_model=64, n_heads=4, n_layers=2, d_ff=256)\n# x = torch.randint(0, 100, (2, 20))\n# logits = gpt(x)\n# print(f\"Input:  {x.shape}\")\n# print(f\"Output: {logits.shape}  (should be [2, 20, 100])\")\n# print(f\"Params: {sum(p.numel() for p in gpt.parameters()):,}\")\nprint(\"Implement DecoderOnlyBlock and DecoderOnlyTransformer, then uncomment the test!\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-43"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 3: Experiment with Architecture Hyperparameters\n\nTrain the reversal model with different hyperparameters and observe the effect on learning speed and final accuracy.\n\n**F1 framing:** Think of this as testing different strategy computer configurations. Does a \"wider\" system (larger d_model, like more sensor channels) learn faster than a \"deeper\" one (more layers, like more analysis stages)? How does the number of attention heads (specialist analysts) affect performance?",
   "id": "cell-44"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# EXERCISE 3: Hyperparameter exploration\n# Try training with different configurations and compare results.\n# Fill in the results table after running each experiment.\n\nconfigs_to_try = {\n    \"baseline\":    {\"d_model\": 64,  \"n_heads\": 4, \"n_layers\": 2, \"d_ff\": 256},\n    \"deeper\":      {\"d_model\": 64,  \"n_heads\": 4, \"n_layers\": 4, \"d_ff\": 256},\n    \"wider\":       {\"d_model\": 128, \"n_heads\": 4, \"n_layers\": 2, \"d_ff\": 512},\n    \"more_heads\":  {\"d_model\": 64,  \"n_heads\": 8, \"n_layers\": 2, \"d_ff\": 256},\n    \"smaller_ffn\": {\"d_model\": 64,  \"n_heads\": 4, \"n_layers\": 2, \"d_ff\": 128},\n}\n\n# TODO: Train each configuration for 30 epochs and record:\n# 1. Final training loss\n# 2. Test accuracy\n# 3. Total parameter count\n# 4. Observations about convergence speed\n\n# Example for one config:\n# config = configs_to_try[\"baseline\"]\n# model = Transformer(VOCAB_SIZE, VOCAB_SIZE, **config)\n# ... train ...\n# Record results\n\nprint(\"Experiment with the configurations above!\")\nprint(\"Key questions to answer:\")\nprint(\"  1. Does depth (more layers) or width (larger d_model) help more?\")\nprint(\"  2. Do more attention heads improve accuracy?\")\nprint(\"  3. What's the smallest model that can solve this task perfectly?\")\nprint(\"  4. Is there a point of diminishing returns?\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cell-45"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Summary\n\n### Key Concepts\n\n**The Transformer Architecture:**\n- Introduced in \"Attention Is All You Need\" (Vaswani et al., 2017)\n- Replaces recurrence with self-attention for full parallelization\n- Encoder-decoder structure, though many modern variants use only one side\n- Every sub-layer wrapped with residual connections and layer normalization\n\n**Positional Encoding:**\n- Self-attention is permutation invariant -- it has no sense of order\n- Sinusoidal encoding uses sin/cos waves at different frequencies\n- Creates unique, smooth position representations\n- Modern alternatives: learned embeddings, rotary positional encoding (RoPE)\n\n**Encoder Block (2 sub-layers):**\n1. Multi-head self-attention (bidirectional)\n2. Position-wise feed-forward network\n- Each with Add & Norm (residual + LayerNorm)\n\n**Decoder Block (3 sub-layers):**\n1. Masked multi-head self-attention (causal)\n2. Multi-head cross-attention (Q from decoder, K/V from encoder)\n3. Position-wise feed-forward network\n- Each with Add & Norm\n\n**Training:**\n- Teacher forcing: feed ground truth target during training\n- Learning rate warmup is critical for stability\n- Label smoothing and gradient clipping improve generalization\n\n### Connection to Deep Learning\n\n| Concept | Where it appears in modern AI | F1 Parallel |\n|---------|-------------------------------|-------------|\n| Self-attention | Core of all Transformer-based models (GPT, BERT, T5, ViT) | Every data stream cross-referencing every other in parallel |\n| Positional encoding | Every Transformer; RoPE in LLaMA, learned in GPT-2 | Lap number and track position tagging for telemetry |\n| Encoder-only | BERT, RoBERTa (bidirectional understanding) | Full session analysis -- sees everything at once |\n| Decoder-only | GPT, LLaMA, Claude (autoregressive generation) | Real-time strategy calls, one decision at a time |\n| Encoder-decoder | T5, BART, mBART (seq-to-seq tasks) | Telemetry (encoder) translated into strategy calls (decoder) |\n| Residual + LayerNorm | Universal in deep learning; enables very deep networks | Preserving baseline signal and normalizing across conditions |\n| FFN layers | Store factual knowledge; recent research on \"neurons as features\" | Per-channel signal processing and calibration |\n| Scaling laws | Drive modern AI investment decisions; compute-optimal training | More development budget predictably yields faster cars |",
   "id": "cell-46"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checklist\n\nBefore moving on, make sure you can:\n\n- [ ] Explain why Transformers replaced RNNs (parallelism, direct connections, scalability)\n- [ ] Implement sinusoidal positional encoding and explain why position information is needed\n- [ ] Draw the data flow through an encoder block (self-attention + Add&Norm + FFN + Add&Norm)\n- [ ] Draw the data flow through a decoder block (masked self-attn + cross-attn + FFN, each with Add&Norm)\n- [ ] Explain the difference between self-attention and cross-attention\n- [ ] Explain why the feed-forward network is needed (nonlinearity + per-position processing)\n- [ ] Assemble a complete Transformer and trace a forward pass\n- [ ] Count parameters in a Transformer given its hyperparameters\n- [ ] Explain teacher forcing and learning rate warmup\n- [ ] Describe the difference between encoder-only, decoder-only, and encoder-decoder models\n\n---"
   ],
   "id": "cell-47"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Next Steps\n\nYou have now built a Transformer **from scratch** -- the architecture that powers modern AI. You understand every component: embeddings, positional encoding, multi-head attention, feed-forward networks, residual connections, layer normalization, and how they all fit together. In F1 terms, you have built the complete strategy computer from individual components: sensor encoding, parallel data processing, cross-referencing, signal normalization, and the strategy-telemetry bridge.\n\nIn the next notebook, **Part 6.4: Language Models**, you will see how this architecture is used in practice:\n\n- **Autoregressive language modeling** (GPT-style): predicting the next token\n- **Masked language modeling** (BERT-style): predicting masked tokens\n- **Tokenization**: BPE, WordPiece, and how text becomes tokens\n- **Generation strategies**: greedy, beam search, top-k, top-p sampling\n- **The journey from Transformer to ChatGPT**\n\nYou now have all the architectural knowledge needed to understand how large language models work. The next notebook will show you how they are trained and used.",
   "id": "cell-48"
  }
 ]
}
