{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Part 6.2: Q-Learning and Deep Q-Networks — The Formula 1 Edition\n\nIn Notebook 21, we learned the RL framework and solved small MDPs with dynamic programming — but those methods required knowing the environment's transition dynamics. Real-world agents don't have that luxury.\n\n**Q-learning** changed everything in 1989: it learns optimal behavior purely from experience, without a model of the environment. Then in 2013, DeepMind's **DQN** scaled this idea with neural networks, learning to play Atari games from raw pixels — a landmark achievement that reignited interest in deep RL.\n\n**The F1 Connection:** Q-learning is how a race strategist would learn optimal pit stop timing *without* a lap-time simulator — purely from racing experience. Each race provides data: \"We pitted on lap 22 from P3 with worn mediums, and the outcome was a P2 finish.\" Over hundreds of races, the Q-table converges: Q(P3_worn_mediums_lap22, pit_now) = 0.85. DQN scales this to high-dimensional states — a deep network that takes in the full telemetry snapshot (position, gaps, tire temps, fuel load, weather forecast) and outputs the value of every possible strategic action. Modern F1 teams use exactly this kind of neural network-based strategy tool, trained on thousands of simulated races.\n\n## Learning Objectives\n\n- [ ] Implement tabular Q-learning from scratch and understand its convergence guarantees\n- [ ] Distinguish between on-policy (SARSA) and off-policy (Q-learning) methods\n- [ ] Understand why naive function approximation with neural networks fails in RL\n- [ ] Implement experience replay and understand why it's critical\n- [ ] Implement target networks and understand how they stabilize training\n- [ ] Build a complete DQN from scratch in PyTorch\n- [ ] Train a DQN agent on a control task\n- [ ] Understand Double DQN and why vanilla DQN overestimates Q-values\n- [ ] Recognize the limitations that motivate policy gradient methods"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from collections import defaultdict, deque, namedtuple\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Part 6.2: Q-Learning and Deep Q-Networks\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 1. From TD Learning to Q-Learning\n\nRecall from Notebook 21 that TD(0) learns the value function $V(s)$. But to act optimally, we need $Q(s,a)$ — the value of taking action $a$ in state $s$.\n\n### SARSA: On-Policy TD Control\n\n**SARSA** updates Q-values using the action the agent *actually takes* next:\n\n$$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)\\right]$$\n\nThe name comes from the quintuple: $(S_t, A_t, R_t, S_{t+1}, A_{t+1})$.\n\n### Q-Learning: Off-Policy TD Control\n\n**Q-learning** updates using the *best possible* next action, regardless of what the agent actually does:\n\n$$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[r_t + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)\\right]$$\n\nThis is **off-policy** because it learns about the optimal policy while following an exploratory policy.\n\n**F1 analogy:** SARSA is like a cautious strategist who evaluates strategies based on what the driver *actually does* — including mistakes. If the driver sometimes misses the pit entry (exploration), SARSA learns cautious values that account for that. Q-learning is like an idealist strategist who evaluates every decision assuming *perfect execution going forward*. \"If we pit now and the driver nails every remaining lap, we finish P2.\" Q-learning learns the optimal strategy regardless of how sloppy the current execution is.\n\n| Property | SARSA (On-Policy) | Q-Learning (Off-Policy) | F1 Analogy |\n|----------|------------------|------------------------|------------|\n| **Update target** | $r + \\gamma Q(s', a')$ | $r + \\gamma \\max_{a'} Q(s', a')$ | Actual outcome vs. best possible outcome |\n| **Learns about** | The policy being followed | The optimal policy | Strategy with driver errors vs. ideal strategy |\n| **Exploration impact** | Exploration affects learned values | Learns optimal regardless of exploration | Conservative near danger vs. optimistic everywhere |\n| **Safety** | More conservative (accounts for exploration) | Can be overoptimistic | SARSA avoids risky pit entries; Q-learning assumes perfect execution |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Gridworld (from Notebook 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"Gridworld environment from Notebook 21.\"\"\"\n",
    "    EMPTY, WALL, GOAL, TRAP = 0, 1, 2, 3\n",
    "    ACTIONS = ['up', 'down', 'left', 'right']\n",
    "    ACTION_DELTAS = {'up': (-1,0), 'down': (1,0), 'left': (0,-1), 'right': (0,1)}\n",
    "    \n",
    "    def __init__(self, grid_size=4, slip_prob=0.1):\n",
    "        self.grid_size = grid_size\n",
    "        self.slip_prob = slip_prob\n",
    "        self.grid = np.zeros((grid_size, grid_size), dtype=int)\n",
    "        self.grid[0, 3] = self.GOAL\n",
    "        self.grid[1, 1] = self.WALL\n",
    "        self.grid[2, 3] = self.TRAP\n",
    "        self.states = [(i,j) for i in range(grid_size) for j in range(grid_size)\n",
    "                       if self.grid[i,j] != self.WALL]\n",
    "        self.terminal_states = [(i,j) for i in range(grid_size) for j in range(grid_size)\n",
    "                                if self.grid[i,j] in [self.GOAL, self.TRAP]]\n",
    "        self.start = (3, 0)\n",
    "        self.agent_pos = self.start\n",
    "    \n",
    "    def reset(self):\n",
    "        self.agent_pos = self.start\n",
    "        return self.agent_pos\n",
    "    \n",
    "    def _is_valid(self, pos):\n",
    "        r, c = pos\n",
    "        return (0 <= r < self.grid_size and 0 <= c < self.grid_size\n",
    "                and self.grid[r,c] != self.WALL)\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.agent_pos in self.terminal_states:\n",
    "            return self.agent_pos, 0.0, True\n",
    "        \n",
    "        # Stochastic transitions\n",
    "        if np.random.random() < self.slip_prob:\n",
    "            perp = ['left','right'] if action in ['up','down'] else ['up','down']\n",
    "            action = np.random.choice(perp)\n",
    "        \n",
    "        dr, dc = self.ACTION_DELTAS[action]\n",
    "        new_pos = (self.agent_pos[0] + dr, self.agent_pos[1] + dc)\n",
    "        if self._is_valid(new_pos):\n",
    "            self.agent_pos = new_pos\n",
    "        \n",
    "        # Rewards\n",
    "        cell = self.grid[self.agent_pos[0], self.agent_pos[1]]\n",
    "        if cell == self.GOAL:\n",
    "            return self.agent_pos, 1.0, True\n",
    "        elif cell == self.TRAP:\n",
    "            return self.agent_pos, -1.0, True\n",
    "        else:\n",
    "            return self.agent_pos, -0.04, False\n",
    "\n",
    "\n",
    "env = GridWorld()\n",
    "print(f\"GridWorld: {env.grid_size}×{env.grid_size}, {len(env.states)} states, {len(env.ACTIONS)} actions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Tabular Q-Learning\n",
    "\n",
    "Let's implement Q-learning with a lookup table — one entry for every (state, action) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, n_episodes=10000, alpha=0.1, gamma=0.9, \n",
    "               epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
    "    \"\"\"Tabular Q-learning with epsilon-greedy exploration.\"\"\"\n",
    "    Q = defaultdict(float)  # Q[(state, action)] -> value\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    epsilon = epsilon_start\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for _ in range(200):  # Max steps per episode\n",
    "            # Epsilon-greedy action selection\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.choice(env.ACTIONS)\n",
    "            else:\n",
    "                q_values = [Q[(state, a)] for a in env.ACTIONS]\n",
    "                action = env.ACTIONS[np.argmax(q_values)]\n",
    "            \n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Q-learning update: use MAX over next actions (off-policy)\n",
    "            best_next_q = max(Q[(next_state, a)] for a in env.ACTIONS)\n",
    "            td_target = reward + gamma * best_next_q * (1 - done)\n",
    "            td_error = td_target - Q[(state, action)]\n",
    "            Q[(state, action)] += alpha * td_error\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "    \n",
    "    return dict(Q), episode_rewards, episode_lengths\n",
    "\n",
    "\n",
    "def sarsa(env, n_episodes=10000, alpha=0.1, gamma=0.9,\n",
    "          epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
    "    \"\"\"SARSA: on-policy TD control.\"\"\"\n",
    "    Q = defaultdict(float)\n",
    "    episode_rewards = []\n",
    "    epsilon = epsilon_start\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        # Choose initial action\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(env.ACTIONS)\n",
    "        else:\n",
    "            q_values = [Q[(state, a)] for a in env.ACTIONS]\n",
    "            action = env.ACTIONS[np.argmax(q_values)]\n",
    "        \n",
    "        total_reward = 0\n",
    "        \n",
    "        for _ in range(200):\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Choose next action (for SARSA update)\n",
    "            if np.random.random() < epsilon:\n",
    "                next_action = np.random.choice(env.ACTIONS)\n",
    "            else:\n",
    "                q_values = [Q[(next_state, a)] for a in env.ACTIONS]\n",
    "                next_action = env.ACTIONS[np.argmax(q_values)]\n",
    "            \n",
    "            # SARSA update: use the ACTUAL next action (on-policy)\n",
    "            td_target = reward + gamma * Q[(next_state, next_action)] * (1 - done)\n",
    "            td_error = td_target - Q[(state, action)]\n",
    "            Q[(state, action)] += alpha * td_error\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "    \n",
    "    return dict(Q), episode_rewards\n",
    "\n",
    "\n",
    "# Train both\n",
    "Q_ql, rewards_ql, lengths_ql = q_learning(env, n_episodes=10000)\n",
    "Q_sarsa, rewards_sarsa = sarsa(env, n_episodes=10000)\n",
    "\n",
    "print(\"Training complete!\")\n",
    "print(f\"Q-learning: avg reward (last 100) = {np.mean(rewards_ql[-100:]):.3f}\")\n",
    "print(f\"SARSA:      avg reward (last 100) = {np.mean(rewards_sarsa[-100:]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Q-Learning vs. SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Learning curves\n",
    "window = 100\n",
    "ql_smooth = np.convolve(rewards_ql, np.ones(window)/window, mode='valid')\n",
    "sarsa_smooth = np.convolve(rewards_sarsa, np.ones(window)/window, mode='valid')\n",
    "\n",
    "axes[0].plot(ql_smooth, label='Q-Learning (off-policy)', color='#3498db', linewidth=2)\n",
    "axes[0].plot(sarsa_smooth, label='SARSA (on-policy)', color='#e74c3c', linewidth=2)\n",
    "axes[0].set_xlabel('Episode', fontsize=12)\n",
    "axes[0].set_ylabel('Average Reward (100-ep window)', fontsize=12)\n",
    "axes[0].set_title('Q-Learning vs. SARSA', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Learned Q-values heatmap for Q-learning\n",
    "n = env.grid_size\n",
    "max_q_grid = np.full((n, n), np.nan)\n",
    "best_action_grid = {}\n",
    "\n",
    "for s in env.states:\n",
    "    if s not in env.terminal_states:\n",
    "        q_vals = [Q_ql.get((s, a), 0) for a in env.ACTIONS]\n",
    "        max_q_grid[s[0], s[1]] = max(q_vals)\n",
    "        best_action_grid[s] = env.ACTIONS[np.argmax(q_vals)]\n",
    "    else:\n",
    "        if env.grid[s[0], s[1]] == GridWorld.GOAL:\n",
    "            max_q_grid[s[0], s[1]] = 1.0\n",
    "        else:\n",
    "            max_q_grid[s[0], s[1]] = -1.0\n",
    "\n",
    "im = axes[1].imshow(max_q_grid, cmap='RdYlGn', vmin=-1, vmax=1)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if not np.isnan(max_q_grid[i, j]):\n",
    "            axes[1].text(j, i, f'{max_q_grid[i,j]:.2f}', ha='center', va='center',\n",
    "                        fontsize=10, fontweight='bold')\n",
    "            if (i, j) in best_action_grid:\n",
    "                arrow_map = {'up': '↑', 'down': '↓', 'left': '←', 'right': '→'}\n",
    "                axes[1].text(j, i + 0.3, arrow_map[best_action_grid[(i,j)]],\n",
    "                           ha='center', va='center', fontsize=14, color='#2c3e50')\n",
    "        elif env.grid[i,j] == GridWorld.WALL:\n",
    "            axes[1].text(j, i, 'W', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "axes[1].set_title('Learned Q-Values (max over actions)', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(im, ax=axes[1], shrink=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Deep Dive: On-Policy vs. Off-Policy\n\nThe difference matters in practice:\n\n- **SARSA** learns values that account for its own exploration. Near the trap, SARSA learns cautious values because it knows *it might accidentally step into the trap* during exploration.\n- **Q-learning** learns the value of the *optimal* policy, assuming perfect action selection afterward. It's more optimistic near dangerous states.\n\nThis is why SARSA is sometimes called \"safer\" — it produces policies that account for the agent's own imperfections.\n\n**F1 analogy:** Near the end of a wet race at Spa, SARSA is the strategist who says \"Stay on inters — our driver sometimes brakes too late on slicks in the wet.\" Q-learning is the strategist who says \"Switch to slicks — the optimal driver would gain 2 seconds per lap.\" Both are correct in their own frame. If your driver is Max Verstappen, Q-learning's optimism is justified. If it's a rookie, SARSA's caution keeps you in the points."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 3. The Challenge of Scaling: Why Tables Aren't Enough\n\nTabular Q-learning works beautifully for small problems. But consider:\n\n| Problem | State Space Size | F1 Equivalent |\n|---------|------------------|---------------|\n| 4x4 Gridworld | 15 states | 15 race scenarios on a napkin |\n| Tic-Tac-Toe | ~5,000 states | Simple pit window calculator |\n| Chess | ~10^47 states | — |\n| Atari (pixel input) | 256^(210x160x3) states | — |\n| F1 Race Strategy | Continuous: position x gaps x tire_age x compound x fuel x weather x ... | The real problem — infinite states |\n\nWe need **function approximation** — use a neural network to estimate $Q(s, a; \\theta)$ instead of storing a table. This is exactly what F1 teams do: they can't store a Q-value for every possible race situation, so they train neural networks on simulated race data to generalize across states.\n\n### The Deadly Triad\n\nSimply plugging a neural network into Q-learning doesn't work. Three factors combine to cause instability:\n\n1. **Function approximation**: Neural networks generalize across states (a change in one Q-value affects others)\n2. **Bootstrapping**: TD updates use the network's own predictions as targets\n3. **Off-policy learning**: Training on data from a different policy than we're evaluating\n\nAny two of these are fine. All three together cause divergence. DQN's key insight was solving this with two techniques: **experience replay** and **target networks**."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Why Naive Deep Q-Learning Fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4.5))\n",
    "\n",
    "# Problem 1: Correlated data\n",
    "ax = axes[0]\n",
    "ax.set_title('Problem 1: Correlated Data', fontsize=12, fontweight='bold')\n",
    "t = np.arange(100)\n",
    "# Simulate sequential experiences from one trajectory\n",
    "trajectory = np.cumsum(np.random.randn(100) * 0.3)\n",
    "ax.plot(t, trajectory, 'b-', linewidth=2, label='Sequential experience')\n",
    "# Random samples from different trajectories\n",
    "random_samples = np.random.randn(100) * 2\n",
    "ax.scatter(t[::5], random_samples[::5], c='red', s=30, zorder=5, label='Random replay')\n",
    "ax.set_xlabel('Time step')\n",
    "ax.set_ylabel('Experience')\n",
    "ax.legend(fontsize=9)\n",
    "ax.text(50, -4, 'Sequential data\\nis highly correlated', ha='center', fontsize=9,\n",
    "        style='italic', color='gray')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Problem 2: Moving target\n",
    "ax = axes[1]\n",
    "ax.set_title('Problem 2: Moving Target', fontsize=12, fontweight='bold')\n",
    "steps = np.arange(50)\n",
    "# Target keeps moving as network updates\n",
    "predictions = np.zeros(50)\n",
    "targets = np.zeros(50)\n",
    "pred, target = 0.0, 1.0\n",
    "for i in range(50):\n",
    "    predictions[i] = pred\n",
    "    targets[i] = target\n",
    "    pred += 0.15 * (target - pred)  # Chase the target\n",
    "    target += 0.1 * np.sin(i * 0.3)  # Target also moves\n",
    "\n",
    "ax.plot(steps, targets, 'r--', linewidth=2, label='Moving target')\n",
    "ax.plot(steps, predictions, 'b-', linewidth=2, label='Network prediction')\n",
    "ax.fill_between(steps, predictions, targets, alpha=0.15, color='purple')\n",
    "ax.set_xlabel('Training step')\n",
    "ax.set_ylabel('Q-value')\n",
    "ax.legend(fontsize=9)\n",
    "ax.text(25, 0.3, 'Target shifts as\\nnetwork updates', ha='center', fontsize=9,\n",
    "        style='italic', color='gray')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Problem 3: Catastrophic forgetting\n",
    "ax = axes[2]\n",
    "ax.set_title('Problem 3: Catastrophic Forgetting', fontsize=12, fontweight='bold')\n",
    "regions = ['Region A\\n(visited early)', 'Region B\\n(visited now)', 'Region C\\n(visited later)']\n",
    "accuracy_before = [0.9, 0.3, 0.1]\n",
    "accuracy_after = [0.3, 0.9, 0.2]\n",
    "x = np.arange(3)\n",
    "ax.bar(x - 0.15, accuracy_before, 0.3, label='Before training on B', color='#3498db', alpha=0.8)\n",
    "ax.bar(x + 0.15, accuracy_after, 0.3, label='After training on B', color='#e74c3c', alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(regions, fontsize=9)\n",
    "ax.set_ylabel('Q-value accuracy')\n",
    "ax.legend(fontsize=9)\n",
    "ax.text(1, 0.05, 'Learning B destroys A', ha='center', fontsize=9,\n",
    "        style='italic', color='gray')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Three Problems with Naive Deep Q-Learning', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 4. Experience Replay\n\n**Experience replay** stores transitions $(s, a, r, s', \\text{done})$ in a buffer and samples random mini-batches for training. This solves two problems:\n\n1. **Breaks correlations**: Random sampling from the buffer produces i.i.d.-like training data\n2. **Data efficiency**: Each experience can be reused many times\n3. **Prevents forgetting**: Old experiences are revisited during training\n\n**F1 analogy:** Without experience replay, the strategy model would only learn from the *most recent race* — and consecutive laps within that race are highly correlated (similar tire state, similar gaps). That's like training your strategy only on Monza data and forgetting everything about Monaco. Experience replay is like the team's historical race database: during training, you randomly sample from Silverstone 2019, Spa 2022, Suzuka 2023. The randomness breaks the correlations, and revisiting old races prevents catastrophic forgetting of track-specific knowledge."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a transition to the buffer.\"\"\"\n",
    "        self.buffer.append(Transition(state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a random batch of transitions.\"\"\"\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        return batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# Demonstrate replay buffer\n",
    "buffer = ReplayBuffer(capacity=1000)\n",
    "\n",
    "# Fill with some random experience\n",
    "state = env.reset()\n",
    "for _ in range(500):\n",
    "    action = np.random.choice(env.ACTIONS)\n",
    "    next_state, reward, done = env.step(action)\n",
    "    buffer.push(state, env.ACTIONS.index(action), reward, next_state, done)\n",
    "    state = next_state if not done else env.reset()\n",
    "\n",
    "# Sample a batch\n",
    "batch = buffer.sample(4)\n",
    "print(f\"Buffer size: {len(buffer)}\")\n",
    "print(f\"\\nSample batch of 4 transitions:\")\n",
    "for i in range(4):\n",
    "    print(f\"  s={batch.state[i]}, a={env.ACTIONS[batch.action[i]]}, \"\n",
    "          f\"r={batch.reward[i]:.2f}, s'={batch.next_state[i]}, done={batch.done[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 5. Target Networks\n\nThe second DQN innovation: use a **separate, slowly-updated copy** of the Q-network to compute TD targets.\n\nInstead of:\n$$\\text{target} = r + \\gamma \\max_{a'} Q(s', a'; \\theta) \\quad \\text{(same network — moving target!)}$$\n\nWe use:\n$$\\text{target} = r + \\gamma \\max_{a'} Q(s', a'; \\theta^{-}) \\quad \\text{(frozen target network)}$$\n\nThe target network $\\theta^{-}$ is updated periodically by copying the main network's weights. This stabilizes training by keeping the target fixed for multiple updates.\n\n**Intuition**: Imagine trying to hit a target that moves every time you adjust your aim. By freezing the target periodically, you can make progress before it shifts again.\n\n**F1 analogy:** Without a target network, it's like a strategist who changes their \"benchmark lap time\" every time they recalculate — the goal keeps moving. With a target network, the benchmark stays fixed for, say, 5 race weekends. The strategist optimizes pit stop timing against that stable benchmark, then updates the benchmark. It's the difference between chasing a moving goalposts and methodically improving against a fixed standard."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Building DQN from Scratch\n",
    "\n",
    "Now let's put it all together. We'll build a DQN to solve a custom control task — a cart that needs to balance a pole (a simplified version of the classic CartPole problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleSimple:\n",
    "    \"\"\"Simplified CartPole environment (no external dependencies needed).\n",
    "    \n",
    "    State: [cart_position, cart_velocity, pole_angle, pole_angular_velocity]\n",
    "    Actions: 0 (push left) or 1 (push right)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masscart + self.masspole\n",
    "        self.length = 0.5  # Half the pole length\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # Time step\n",
    "        \n",
    "        # Failure thresholds\n",
    "        self.x_threshold = 2.4\n",
    "        self.theta_threshold = 12 * np.pi / 180  # 12 degrees\n",
    "        \n",
    "        self.state_dim = 4\n",
    "        self.n_actions = 2\n",
    "        self.state = None\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset to random state near center.\"\"\"\n",
    "        self.state = np.random.uniform(-0.05, 0.05, size=4)\n",
    "        return self.state.copy()\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Simulate one step of physics.\"\"\"\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        \n",
    "        cos_theta = np.cos(theta)\n",
    "        sin_theta = np.sin(theta)\n",
    "        \n",
    "        # Physics equations\n",
    "        temp = (force + self.polemass_length * theta_dot**2 * sin_theta) / self.total_mass\n",
    "        theta_acc = (self.gravity * sin_theta - cos_theta * temp) / (\n",
    "            self.length * (4.0/3.0 - self.masspole * cos_theta**2 / self.total_mass))\n",
    "        x_acc = temp - self.polemass_length * theta_acc * cos_theta / self.total_mass\n",
    "        \n",
    "        # Euler integration\n",
    "        x += self.tau * x_dot\n",
    "        x_dot += self.tau * x_acc\n",
    "        theta += self.tau * theta_dot\n",
    "        theta_dot += self.tau * theta_acc\n",
    "        \n",
    "        self.state = np.array([x, x_dot, theta, theta_dot])\n",
    "        \n",
    "        # Check termination\n",
    "        done = (abs(x) > self.x_threshold or abs(theta) > self.theta_threshold)\n",
    "        reward = 1.0 if not done else 0.0\n",
    "        \n",
    "        return self.state.copy(), reward, done\n",
    "\n",
    "\n",
    "# Test the environment\n",
    "cart_env = CartPoleSimple()\n",
    "state = cart_env.reset()\n",
    "print(f\"CartPole state dim: {cart_env.state_dim}\")\n",
    "print(f\"Actions: {cart_env.n_actions} (left, right)\")\n",
    "print(f\"Initial state: {state}\")\n",
    "\n",
    "# Random agent baseline\n",
    "episode_lengths = []\n",
    "for _ in range(100):\n",
    "    state = cart_env.reset()\n",
    "    length = 0\n",
    "    for _ in range(500):\n",
    "        action = np.random.randint(2)\n",
    "        state, reward, done = cart_env.step(action)\n",
    "        length += 1\n",
    "        if done:\n",
    "            break\n",
    "    episode_lengths.append(length)\n",
    "\n",
    "print(f\"\\nRandom agent: avg episode length = {np.mean(episode_lengths):.1f} steps\")\n",
    "print(\"(Goal: balance for 200+ steps)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Neural network that estimates Q(s,a) for all actions.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, n_actions, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_actions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"Deep Q-Network agent with experience replay and target network.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, n_actions, hidden_dim=64, lr=1e-3,\n",
    "                 gamma=0.99, buffer_size=10000, batch_size=64,\n",
    "                 target_update_freq=100, epsilon_start=1.0,\n",
    "                 epsilon_end=0.01, epsilon_decay=0.995):\n",
    "        \n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        \n",
    "        # Exploration\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        # Networks\n",
    "        self.q_network = QNetwork(state_dim, n_actions, hidden_dim)\n",
    "        self.target_network = QNetwork(state_dim, n_actions, hidden_dim)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())  # Copy weights\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        self.steps_done = 0\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax(dim=1).item()\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Perform one step of DQN training.\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample batch from replay buffer\n",
    "        batch = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor(np.array(batch.state))\n",
    "        actions = torch.LongTensor(batch.action)\n",
    "        rewards = torch.FloatTensor(batch.reward)\n",
    "        next_states = torch.FloatTensor(np.array(batch.next_state))\n",
    "        dones = torch.FloatTensor(batch.done)\n",
    "        \n",
    "        # Current Q-values: Q(s, a) for the actions we actually took\n",
    "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Target Q-values: r + γ max_a' Q_target(s', a')\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_network(next_states).max(dim=1)[0]\n",
    "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
    "        \n",
    "        # Loss: MSE between current and target Q-values\n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.steps_done += 1\n",
    "        \n",
    "        # Periodically update target network\n",
    "        if self.steps_done % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "print(\"DQN agent architecture:\")\n",
    "agent = DQNAgent(state_dim=4, n_actions=2)\n",
    "print(agent.q_network)\n",
    "total_params = sum(p.numel() for p in agent.q_network.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(env, agent, n_episodes=500, max_steps=500):\n",
    "    \"\"\"Train DQN agent on the environment.\"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    losses = []\n",
    "    epsilons = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        episode_loss = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Store transition\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, float(done))\n",
    "            \n",
    "            # Train\n",
    "            loss = agent.update()\n",
    "            if loss is not None:\n",
    "                episode_loss.append(loss)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(step + 1)\n",
    "        epsilons.append(agent.epsilon)\n",
    "        if episode_loss:\n",
    "            losses.append(np.mean(episode_loss))\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-50:])\n",
    "            avg_length = np.mean(episode_lengths[-50:])\n",
    "            print(f\"Episode {episode+1:4d} | Avg Reward: {avg_reward:6.1f} | \"\n",
    "                  f\"Avg Length: {avg_length:5.1f} | ε: {agent.epsilon:.3f}\")\n",
    "    \n",
    "    return episode_rewards, episode_lengths, losses, epsilons\n",
    "\n",
    "\n",
    "# Train!\n",
    "cart_env = CartPoleSimple()\n",
    "agent = DQNAgent(state_dim=4, n_actions=2, lr=1e-3, gamma=0.99,\n",
    "                 buffer_size=10000, batch_size=64, target_update_freq=100)\n",
    "\n",
    "rewards, lengths, losses, epsilons = train_dqn(cart_env, agent, n_episodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: DQN Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Episode lengths (performance)\n",
    "ax = axes[0, 0]\n",
    "ax.plot(lengths, alpha=0.3, color='#3498db')\n",
    "window = 20\n",
    "if len(lengths) >= window:\n",
    "    smoothed = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(lengths)), smoothed, color='#2c3e50', linewidth=2,\n",
    "            label=f'{window}-ep moving avg')\n",
    "ax.axhline(y=200, color='red', linestyle='--', label='Goal (200 steps)')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Episode Length')\n",
    "ax.set_title('DQN Training: Episode Length', fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Episode rewards\n",
    "ax = axes[0, 1]\n",
    "ax.plot(rewards, alpha=0.3, color='#2ecc71')\n",
    "if len(rewards) >= window:\n",
    "    smoothed_r = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(rewards)), smoothed_r, color='#27ae60', linewidth=2,\n",
    "            label=f'{window}-ep moving avg')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Total Reward')\n",
    "ax.set_title('DQN Training: Reward', fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Training loss\n",
    "ax = axes[1, 0]\n",
    "if losses:\n",
    "    ax.plot(losses, alpha=0.5, color='#e74c3c')\n",
    "    if len(losses) >= window:\n",
    "        smoothed_l = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "        ax.plot(range(window-1, len(losses)), smoothed_l, color='#c0392b', linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('DQN Training: Loss', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Epsilon schedule\n",
    "ax = axes[1, 1]\n",
    "ax.plot(epsilons, color='#9b59b6', linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Epsilon')\n",
    "ax.set_title('Exploration Rate (ε) Decay', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('DQN Agent Training on CartPole', fontsize=15, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final evaluation\n",
    "agent.epsilon = 0  # No exploration\n",
    "eval_lengths = []\n",
    "for _ in range(20):\n",
    "    state = cart_env.reset()\n",
    "    length = 0\n",
    "    for _ in range(500):\n",
    "        action = agent.select_action(state)\n",
    "        state, _, done = cart_env.step(action)\n",
    "        length += 1\n",
    "        if done:\n",
    "            break\n",
    "    eval_lengths.append(length)\n",
    "\n",
    "print(f\"\\nFinal evaluation (20 episodes, no exploration):\")\n",
    "print(f\"  Average length: {np.mean(eval_lengths):.1f} ± {np.std(eval_lengths):.1f}\")\n",
    "print(f\"  Min: {min(eval_lengths)}, Max: {max(eval_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Ablation Study: Why Each Component Matters\n",
    "\n",
    "Let's prove that experience replay and target networks are both necessary by removing them one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNoReplay(DQNAgent):\n",
    "    \"\"\"DQN without experience replay — trains on most recent transition only.\"\"\"\n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < 1:\n",
    "            return None\n",
    "        \n",
    "        # Use only the LAST transition (no replay)\n",
    "        last = self.replay_buffer.buffer[-1]\n",
    "        \n",
    "        state = torch.FloatTensor(np.array(last.state)).unsqueeze(0)\n",
    "        action = torch.LongTensor([last.action])\n",
    "        reward = torch.FloatTensor([last.reward])\n",
    "        next_state = torch.FloatTensor(np.array(last.next_state)).unsqueeze(0)\n",
    "        done = torch.FloatTensor([last.done])\n",
    "        \n",
    "        current_q = self.q_network(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_network(next_state).max(dim=1)[0]\n",
    "            target_q = reward + self.gamma * next_q * (1 - done)\n",
    "        \n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.steps_done += 1\n",
    "        if self.steps_done % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "class DQNNoTarget(DQNAgent):\n",
    "    \"\"\"DQN without target network — uses same network for targets.\"\"\"\n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        batch = self.replay_buffer.sample(self.batch_size)\n",
    "        states = torch.FloatTensor(np.array(batch.state))\n",
    "        actions = torch.LongTensor(batch.action)\n",
    "        rewards = torch.FloatTensor(batch.reward)\n",
    "        next_states = torch.FloatTensor(np.array(batch.next_state))\n",
    "        dones = torch.FloatTensor(batch.done)\n",
    "        \n",
    "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        # Uses SAME network for target (no target network)\n",
    "        with torch.no_grad():\n",
    "            next_q = self.q_network(next_states).max(dim=1)[0]\n",
    "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
    "        \n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        self.steps_done += 1\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "# Run ablations (shorter training for speed)\n",
    "n_ep = 300\n",
    "print(\"Training full DQN...\")\n",
    "agent_full = DQNAgent(4, 2)\n",
    "r_full, l_full, _, _ = train_dqn(CartPoleSimple(), agent_full, n_ep)\n",
    "\n",
    "print(\"\\nTraining DQN without replay...\")\n",
    "agent_no_replay = DQNNoReplay(4, 2)\n",
    "r_no_replay, l_no_replay, _, _ = train_dqn(CartPoleSimple(), agent_no_replay, n_ep)\n",
    "\n",
    "print(\"\\nTraining DQN without target network...\")\n",
    "agent_no_target = DQNNoTarget(4, 2)\n",
    "r_no_target, l_no_target, _, _ = train_dqn(CartPoleSimple(), agent_no_target, n_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ablation results\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "window = 20\n",
    "configs = [\n",
    "    ('Full DQN', l_full, '#2ecc71'),\n",
    "    ('No Experience Replay', l_no_replay, '#e74c3c'),\n",
    "    ('No Target Network', l_no_target, '#f39c12'),\n",
    "]\n",
    "\n",
    "for name, lengths, color in configs:\n",
    "    if len(lengths) >= window:\n",
    "        smoothed = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
    "        ax.plot(smoothed, label=name, color=color, linewidth=2.5)\n",
    "\n",
    "ax.axhline(y=200, color='gray', linestyle='--', alpha=0.5, label='Goal')\n",
    "ax.set_xlabel('Episode', fontsize=12)\n",
    "ax.set_ylabel('Episode Length (smoothed)', fontsize=12)\n",
    "ax.set_title('DQN Ablation Study: Both Components Are Critical', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Ablation results (avg length, last 50 episodes):\")\n",
    "for name, lengths, _ in configs:\n",
    "    print(f\"  {name}: {np.mean(lengths[-50:]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 8. Double DQN: Fixing Overestimation\n\nVanilla DQN tends to **overestimate** Q-values. The max operator in the target:\n\n$$\\text{target} = r + \\gamma \\max_{a'} Q(s', a'; \\theta^{-})$$\n\nsystematically selects overestimated values (because noise in Q-estimates tends to be positive when you take the max).\n\n**Double DQN** decouples action selection from evaluation:\n\n$$\\text{target} = r + \\gamma Q\\left(s', \\arg\\max_{a'} Q(s', a'; \\theta); \\theta^{-}\\right)$$\n\n- Use the **online network** to select the best action\n- Use the **target network** to evaluate that action\n\nThis simple change significantly reduces overestimation.\n\n**F1 analogy:** Regular DQN is like an overconfident strategist who always assumes the *best possible* outcome for each action — \"If we pit now, the undercut will definitely work AND we'll get a perfect pit stop AND the safety car will come out.\" By always picking the most optimistic estimate, they systematically overvalue risky strategies. Double DQN separates the question: one model says \"pitting now looks best\" (action selection), and a different model says \"here's what pitting now is actually worth\" (evaluation). The decoupling grounds the optimism in more realistic assessments."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQNAgent(DQNAgent):\n",
    "    \"\"\"Double DQN: decouple action selection from evaluation.\"\"\"\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        batch = self.replay_buffer.sample(self.batch_size)\n",
    "        states = torch.FloatTensor(np.array(batch.state))\n",
    "        actions = torch.LongTensor(batch.action)\n",
    "        rewards = torch.FloatTensor(batch.reward)\n",
    "        next_states = torch.FloatTensor(np.array(batch.next_state))\n",
    "        dones = torch.FloatTensor(batch.done)\n",
    "        \n",
    "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Double DQN: select action with ONLINE network\n",
    "            best_actions = self.q_network(next_states).argmax(dim=1)\n",
    "            # Evaluate with TARGET network\n",
    "            next_q = self.target_network(next_states).gather(1, best_actions.unsqueeze(1)).squeeze(1)\n",
    "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
    "        \n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        self.steps_done += 1\n",
    "        if self.steps_done % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "# Demonstrate Q-value overestimation\n",
    "def measure_q_overestimation(AgentClass, n_episodes=300):\n",
    "    \"\"\"Track max Q-value predictions during training.\"\"\"\n",
    "    env = CartPoleSimple()\n",
    "    agent = AgentClass(4, 2)\n",
    "    \n",
    "    max_q_history = []\n",
    "    lengths = []\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        ep_max_q = []\n",
    "        length = 0\n",
    "        \n",
    "        for _ in range(500):\n",
    "            with torch.no_grad():\n",
    "                q_vals = agent.q_network(torch.FloatTensor(state).unsqueeze(0))\n",
    "                ep_max_q.append(q_vals.max().item())\n",
    "            \n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, float(done))\n",
    "            agent.update()\n",
    "            length += 1\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        max_q_history.append(np.mean(ep_max_q))\n",
    "        lengths.append(length)\n",
    "    \n",
    "    return max_q_history, lengths\n",
    "\n",
    "\n",
    "print(\"Measuring Q-value overestimation...\")\n",
    "q_dqn, l_dqn = measure_q_overestimation(DQNAgent)\n",
    "q_ddqn, l_ddqn = measure_q_overestimation(DoubleDQNAgent)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "window = 20\n",
    "\n",
    "# Q-value estimates\n",
    "ax = axes[0]\n",
    "for data, label, color in [(q_dqn, 'DQN', '#e74c3c'), (q_ddqn, 'Double DQN', '#3498db')]:\n",
    "    smoothed = np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(smoothed, label=label, color=color, linewidth=2)\n",
    "ax.set_xlabel('Episode', fontsize=12)\n",
    "ax.set_ylabel('Average Max Q-Value', fontsize=12)\n",
    "ax.set_title('Q-Value Estimates: DQN Overestimates', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Performance comparison\n",
    "ax = axes[1]\n",
    "for data, label, color in [(l_dqn, 'DQN', '#e74c3c'), (l_ddqn, 'Double DQN', '#3498db')]:\n",
    "    smoothed = np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(smoothed, label=label, color=color, linewidth=2)\n",
    "ax.axhline(y=200, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Episode', fontsize=12)\n",
    "ax.set_ylabel('Episode Length', fontsize=12)\n",
    "ax.set_title('Performance: DQN vs Double DQN', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 9. Limitations of Value-Based Methods\n\nDQN is powerful, but it has fundamental limitations:\n\n| Limitation | Why It Matters | F1 Parallel |\n|-----------|----------------|-------------|\n| **Discrete actions only** | Can't directly handle continuous control (robotics, steering angles) | Can't output \"pit on lap 22.7\" — only \"pit\" or \"don't pit\" |\n| **Deterministic policy** | Always outputs one action per state (can't learn stochastic strategies) | Can't say \"70% chance we should pit\" — only yes or no |\n| **Overestimation** | Even Double DQN doesn't fully solve this | Still too optimistic about undercut success rates |\n| **No policy gradient** | Can't optimize policy directly for objectives like RLHF | Can't directly optimize for \"smooth strategic decisions\" |\n\nThese limitations motivate **policy gradient methods** (Notebook 23), which:\n- Learn a policy $\\pi_\\theta(a|s)$ directly as a neural network\n- Handle continuous action spaces naturally — perfect for steering angle, throttle, brake pressure\n- Can optimize any differentiable objective\n- Are the backbone of PPO and RLHF"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "ax.set_title('DQN Architecture Summary', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Components\n",
    "components = [\n",
    "    (1, 6, 3, 1.2, 'Experience\\nReplay Buffer', '#3498db',\n",
    "     'Stores transitions\\nBreaks correlations'),\n",
    "    (1, 3.5, 3, 1.2, 'Q-Network\\n(Online)', '#2ecc71',\n",
    "     'Predicts Q(s,a)\\nUpdated every step'),\n",
    "    (6, 3.5, 3, 1.2, 'Target Network\\n(Frozen)', '#e74c3c',\n",
    "     'Computes TD targets\\nPeriodically synced'),\n",
    "    (3.5, 1, 3, 1.0, 'ε-Greedy\\nExploration', '#f39c12',\n",
    "     'Balances explore/exploit\\nDecays over training'),\n",
    "]\n",
    "\n",
    "for x, y, w, h, label, color, desc in components:\n",
    "    box = mpatches.FancyBboxPatch((x, y), w, h, boxstyle=\"round,pad=0.2\",\n",
    "                                   facecolor=color, edgecolor='black', linewidth=2, alpha=0.9)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x + w/2, y + h/2 + 0.15, label, ha='center', va='center',\n",
    "            fontsize=10, fontweight='bold', color='white')\n",
    "    ax.text(x + w/2, y - 0.3, desc, ha='center', va='center',\n",
    "            fontsize=8, color='gray', style='italic')\n",
    "\n",
    "# Arrows\n",
    "ax.annotate('', xy=(2.5, 6), xytext=(2.5, 4.7),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='gray'))\n",
    "ax.annotate('', xy=(6, 4.1), xytext=(4, 4.1),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='gray'))\n",
    "ax.text(5, 4.5, 'periodic\\ncopy', ha='center', fontsize=8, color='gray')\n",
    "ax.annotate('', xy=(3.5, 3.5), xytext=(3.5, 2),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='gray'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercises\n\n### Exercise 1: Dueling DQN Architecture — Separating Track Position Value from Action Advantage\n\nImplement the **Dueling DQN** architecture, which separates the Q-network into two streams:\n- A **value stream** $V(s)$: how good is this state? (\"Being P3 with fresh tires is inherently valuable\")\n- An **advantage stream** $A(s,a)$: how much better is this action than average? (\"Pitting now is 0.3 better than average from P3\")\n\n$$Q(s,a) = V(s) + A(s,a) - \\frac{1}{|\\mathcal{A}|}\\sum_{a'} A(s,a')$$\n\nIn F1 terms, some race positions are good regardless of what you do next (P2 with a 5-second gap). The value stream captures that. The advantage stream captures whether pitting NOW specifically is better than the alternatives. Separating these makes learning more efficient — the network can quickly learn which positions are valuable and independently learn which actions improve things."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Hint: Modify QNetwork to have shared layers, then split into\n",
    "# a value head (outputs 1 value) and advantage head (outputs n_actions values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2: Prioritized Experience Replay — Learning More from Surprising Races\n\nNot all experiences are equally useful. Implement **prioritized replay** where transitions with higher TD error are sampled more frequently. Use importance sampling weights to correct the bias.\n\nIn F1 terms, a race where your strategy prediction was wildly wrong (huge TD error) — like an unexpected safety car changing the pit window — is far more informative than a processional race where everything went as expected. Prioritized replay ensures the model trains more on those surprising, information-rich races."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Hint: Store |TD error| + small epsilon as priority for each transition\n",
    "# Sample proportional to priority, apply importance sampling weights to loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 3: Hyperparameter Sensitivity — Tuning the Strategy Engine\n\nDQN has many hyperparameters, much like an F1 car has many setup parameters. Run experiments varying:\n- Learning rate: [1e-4, 1e-3, 1e-2] — how fast the model adapts (like setup change aggressiveness)\n- Target update frequency: [10, 100, 500] — how often the benchmark refreshes (like strategy model update cadence)\n- Buffer size: [1000, 10000, 50000] — how much historical data to retain (like seasons of race data in memory)\n\nWhich hyperparameter has the biggest impact on performance? In F1, teams agonize over which setup parameters matter most for each track — this exercise gives you intuition for the same question in DQN."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "# Hint: Create a function that trains a DQN with given hyperparameters\n",
    "# and returns the average episode length over the last 50 episodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Summary\n\n### Key Concepts\n\n| Concept | What It Means | F1 Parallel |\n|---------|--------------|-------------|\n| **Q-learning** | Learn Q(s,a) from experience using off-policy TD updates | Learn pit stop value from race experience, assuming optimal future execution |\n| **SARSA** | On-policy variant — learns about the exploration policy | Learn strategy value accounting for driver imperfections |\n| **Deadly triad** | Function approx + bootstrapping + off-policy = instability | Strategy model that generalizes, self-references, and learns from old data = chaos |\n| **Experience replay** | Break data correlations, enable data reuse | Historical race database with random sampling |\n| **Target networks** | Fixed TD targets for stability | Stable benchmark lap time, updated periodically |\n| **Double DQN** | Decouple action selection from evaluation | One model picks strategy, another estimates its true value |\n| **DQN limitations** | Discrete actions, deterministic policy only | Can't do continuous throttle control or probabilistic strategies |\n\n### Fundamental Insight\n\nDQN showed that the combination of deep learning and RL can solve problems previously thought intractable. The key wasn't a new algorithm — it was engineering: experience replay and target networks turned an unstable process into a robust learning system. Much of deep RL research is about making the learning process stable enough for neural networks to work. In F1 terms, the raw strategy math was always there — the breakthrough was building the engineering infrastructure (data pipelines, simulation tools, real-time telemetry) that made it practical at race speed."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Next Steps\n\nDQN learns *which actions are good* (value function) and derives a policy from that. But what if we could learn the policy **directly**? In **Notebook 23: Policy Gradient Methods**, we'll:\n\n- Derive the policy gradient theorem — the mathematical foundation for directly optimizing policies\n- Implement REINFORCE, the simplest policy gradient algorithm — directly learning race strategy by reinforcing good decisions\n- Understand variance reduction with baselines\n- Build an actor-critic architecture that combines the best of both worlds — like having a driver (actor) and strategist (critic) working together\n- See why policy gradients are the path to PPO and RLHF"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}