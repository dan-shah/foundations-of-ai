{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2.2: NumPy Deep Dive\n",
    "\n",
    "NumPy is the foundation of scientific computing in Python. Understanding it deeply will help you:\n",
    "- Write faster, more efficient code\n",
    "- Understand how PyTorch tensors work (they're very similar!)\n",
    "- Debug shape mismatches in neural networks\n",
    "\n",
    "## Learning Objectives\n",
    "- [ ] Master NumPy broadcasting rules\n",
    "- [ ] Use advanced indexing effectively\n",
    "- [ ] Vectorize operations for performance\n",
    "- [ ] Understand memory layout and views\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Array Creation and Basics\n",
    "\n",
    "### Creating Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Python lists\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(f\"1D array: {a}, shape: {a.shape}\")\n",
    "print(f\"2D array:\\n{b}\\nshape: {b.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common creation functions\n",
    "print(\"np.zeros((2, 3)):\")\n",
    "print(np.zeros((2, 3)))\n",
    "\n",
    "print(\"\\nnp.ones((2, 3)):\")\n",
    "print(np.ones((2, 3)))\n",
    "\n",
    "print(\"\\nnp.eye(3) (identity matrix):\")\n",
    "print(np.eye(3))\n",
    "\n",
    "print(\"\\nnp.arange(0, 10, 2):\")\n",
    "print(np.arange(0, 10, 2))\n",
    "\n",
    "print(\"\\nnp.linspace(0, 1, 5):\")\n",
    "print(np.linspace(0, 1, 5))\n",
    "\n",
    "print(\"\\nnp.random.randn(2, 3) (standard normal):\")\n",
    "print(np.random.randn(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(3, 4, 5)\n",
    "\n",
    "print(f\"Shape: {x.shape}\")      # Dimensions\n",
    "print(f\"Ndim: {x.ndim}\")        # Number of dimensions\n",
    "print(f\"Size: {x.size}\")        # Total number of elements\n",
    "print(f\"Dtype: {x.dtype}\")      # Data type\n",
    "print(f\"Itemsize: {x.itemsize} bytes\")  # Bytes per element\n",
    "print(f\"Total bytes: {x.nbytes}\")       # Total memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Reshaping and Manipulating Arrays\n",
    "\n",
    "### Understanding Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape - VERY common in deep learning\n",
    "a = np.arange(12)\n",
    "print(f\"Original: {a}, shape: {a.shape}\")\n",
    "\n",
    "# Reshape to 2D\n",
    "b = a.reshape(3, 4)\n",
    "print(f\"\\nReshaped to (3, 4):\\n{b}\")\n",
    "\n",
    "c = a.reshape(4, 3)\n",
    "print(f\"\\nReshaped to (4, 3):\\n{c}\")\n",
    "\n",
    "# Use -1 to infer dimension\n",
    "d = a.reshape(2, -1)  # 2 rows, infer columns\n",
    "print(f\"\\nReshaped to (2, -1) -> {d.shape}:\\n{d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: Flatten vs Ravel vs Reshape(-1)\n",
    "\n",
    "| Method | Returns | Memory |\n",
    "|--------|---------|--------|\n",
    "| `flatten()` | Copy | Always new array |\n",
    "| `ravel()` | View if possible | Shares memory when possible |\n",
    "| `reshape(-1)` | View if possible | Same as ravel |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "flat = x.flatten()\n",
    "ravel = x.ravel()\n",
    "reshape = x.reshape(-1)\n",
    "\n",
    "print(f\"Original:\\n{x}\")\n",
    "print(f\"\\nflat: {flat}\")\n",
    "print(f\"ravel: {ravel}\")\n",
    "print(f\"reshape(-1): {reshape}\")\n",
    "\n",
    "# Modify original\n",
    "x[0, 0] = 999\n",
    "print(f\"\\nAfter modifying x[0,0] = 999:\")\n",
    "print(f\"flat: {flat}  (unchanged - it's a copy)\")\n",
    "print(f\"ravel: {ravel}  (changed - it's a view!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this means:** Computer memory is linear (1D), so 2D arrays must be \"flattened\" when stored. C-order stores row-by-row (natural for Python/C), while F-order stores column-by-column (natural for Fortran/MATLAB). This affects performance: accessing data along the \"fast\" axis is much quicker because it uses contiguous memory. In NumPy, iterating over rows is typically faster than columns.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# VISUALIZATION: Memory Layout - C-order vs F-order\nfig, axes = plt.subplots(1, 3, figsize=(14, 5))\n\n# Create a 2D array\narr_2d = np.array([[1, 2, 3], [4, 5, 6]])\n\n# Left: The conceptual 2D array\nax = axes[0]\nax.set_title('Conceptual 2D Array\\n(how we think about it)', fontsize=11)\nfor i in range(2):\n    for j in range(3):\n        color = plt.cm.viridis(arr_2d[i, j] / 7)\n        ax.add_patch(plt.Rectangle((j, 1-i), 0.9, 0.9, facecolor=color, edgecolor='black', lw=2))\n        ax.text(j + 0.45, 1.45 - i, str(arr_2d[i, j]), ha='center', va='center', fontsize=14, fontweight='bold', color='white')\nax.text(1.5, -0.5, 'rows', ha='center', fontsize=10)\nax.text(-0.5, 1, 'cols', ha='center', fontsize=10, rotation=90)\nax.set_xlim(-0.8, 3.5)\nax.set_ylim(-1, 2.5)\nax.axis('off')\n\n# Middle: C-order (row-major)\nax = axes[1]\nax.set_title('C-order (Row-major)\\nDefault in NumPy', fontsize=11)\nc_order = arr_2d.ravel(order='C')\nfor i, val in enumerate(c_order):\n    color = plt.cm.viridis(val / 7)\n    ax.add_patch(plt.Rectangle((i, 0), 0.9, 0.9, facecolor=color, edgecolor='black', lw=2))\n    ax.text(i + 0.45, 0.45, str(val), ha='center', va='center', fontsize=14, fontweight='bold', color='white')\nax.text(2.5, -0.5, 'Memory addresses →', ha='center', fontsize=10)\nax.annotate('Row 0', xy=(1, 1), xytext=(1, 1.5), fontsize=9, ha='center')\nax.annotate('Row 1', xy=(4, 1), xytext=(4, 1.5), fontsize=9, ha='center')\nax.plot([2.95, 2.95], [0, 0.9], 'r--', lw=2)\nax.set_xlim(-0.5, 6.5)\nax.set_ylim(-1, 2)\nax.axis('off')\n\n# Right: F-order (column-major)\nax = axes[2]\nax.set_title('F-order (Column-major)\\nUsed in Fortran, MATLAB', fontsize=11)\nf_order = arr_2d.ravel(order='F')\nfor i, val in enumerate(f_order):\n    color = plt.cm.viridis(val / 7)\n    ax.add_patch(plt.Rectangle((i, 0), 0.9, 0.9, facecolor=color, edgecolor='black', lw=2))\n    ax.text(i + 0.45, 0.45, str(val), ha='center', va='center', fontsize=14, fontweight='bold', color='white')\nax.text(2.5, -0.5, 'Memory addresses →', ha='center', fontsize=10)\nax.annotate('Col 0', xy=(0.5, 1), xytext=(0.5, 1.5), fontsize=9, ha='center')\nax.annotate('Col 1', xy=(2.5, 1), xytext=(2.5, 1.5), fontsize=9, ha='center')\nax.annotate('Col 2', xy=(4.5, 1), xytext=(4.5, 1.5), fontsize=9, ha='center')\nax.plot([1.95, 1.95], [0, 0.9], 'r--', lw=2)\nax.plot([3.95, 3.95], [0, 0.9], 'r--', lw=2)\nax.set_xlim(-0.5, 6.5)\nax.set_ylim(-1, 2)\nax.axis('off')\n\nplt.tight_layout()\nplt.suptitle('Memory Layout: How 2D Arrays are Stored in 1D Memory', y=1.02, fontsize=13, fontweight='bold')\nplt.show()\n\nprint(\"C-order traverses rows first: \", arr_2d.ravel(order='C'))\nprint(\"F-order traverses columns first:\", arr_2d.ravel(order='F'))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding and Removing Dimensions\n",
    "\n",
    "Common in deep learning when you need to:\n",
    "- Add batch dimension: `(H, W)` → `(1, H, W)`\n",
    "- Add channel dimension: `(B, H, W)` → `(B, 1, H, W)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.newaxis (same as None) adds a dimension\n",
    "x = np.array([1, 2, 3])  # Shape: (3,)\n",
    "print(f\"Original shape: {x.shape}\")\n",
    "\n",
    "# Add dimension at front (batch dimension)\n",
    "x_batch = x[np.newaxis, :]  # or x[None, :] or x.reshape(1, -1)\n",
    "print(f\"With batch dim: {x_batch.shape}\")\n",
    "\n",
    "# Add dimension at end\n",
    "x_col = x[:, np.newaxis]  # or x[:, None] or x.reshape(-1, 1)\n",
    "print(f\"As column: {x_col.shape}\")\n",
    "\n",
    "# np.expand_dims is more explicit\n",
    "print(f\"\\nnp.expand_dims(x, axis=0): {np.expand_dims(x, axis=0).shape}\")\n",
    "print(f\"np.expand_dims(x, axis=1): {np.expand_dims(x, axis=1).shape}\")\n",
    "\n",
    "# np.squeeze removes dimensions of size 1\n",
    "y = np.zeros((1, 3, 1, 4))\n",
    "print(f\"\\nOriginal: {y.shape}\")\n",
    "print(f\"Squeezed: {np.squeeze(y).shape}\")\n",
    "print(f\"Squeeze axis 0 only: {np.squeeze(y, axis=0).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose and Swapaxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D transpose\n",
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(f\"Original (2, 3):\\n{x}\")\n",
    "print(f\"\\nTransposed (3, 2):\\n{x.T}\")\n",
    "\n",
    "# For higher dimensions, use transpose with axis order\n",
    "# Example: Convert (batch, height, width, channels) to (batch, channels, height, width)\n",
    "img_nhwc = np.random.randn(32, 28, 28, 3)  # TensorFlow format\n",
    "img_nchw = img_nhwc.transpose(0, 3, 1, 2)  # PyTorch format\n",
    "\n",
    "print(f\"\\nNHWC (TensorFlow): {img_nhwc.shape}\")\n",
    "print(f\"NCHW (PyTorch): {img_nchw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Broadcasting\n",
    "\n",
    "**Broadcasting** allows NumPy to perform operations on arrays of different shapes. This is crucial for writing efficient, vectorized code.\n",
    "\n",
    "### Broadcasting Rules\n",
    "\n",
    "When operating on two arrays, NumPy compares shapes element-wise from the **trailing dimensions**:\n",
    "\n",
    "1. If dimensions are equal, they're compatible\n",
    "2. If one dimension is 1, it's \"stretched\" to match the other\n",
    "3. If neither condition is met, error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example: scalar + array\n",
    "a = np.array([1, 2, 3])\n",
    "print(f\"a + 10 = {a + 10}\")\n",
    "# 10 is \"broadcast\" to [10, 10, 10]\n",
    "\n",
    "# 2D + 1D\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "b = np.array([10, 20, 30])\n",
    "\n",
    "print(f\"\\nA (shape {A.shape}):\\n{A}\")\n",
    "print(f\"b (shape {b.shape}): {b}\")\n",
    "print(f\"\\nA + b (b broadcast across rows):\\n{A + b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: Visualizing Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_broadcast(a, b):\n",
    "    \"\"\"Visualize how two arrays are broadcast together.\"\"\"\n",
    "    print(f\"Array A shape: {a.shape}\")\n",
    "    print(f\"Array B shape: {b.shape}\")\n",
    "    \n",
    "    try:\n",
    "        result = a + b\n",
    "        print(f\"Result shape: {result.shape}\")\n",
    "        print(f\"\\nA:\\n{a}\")\n",
    "        print(f\"\\nB:\\n{b}\")\n",
    "        print(f\"\\nA + B:\\n{result}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "\n",
    "# Case 1: (3,) + (3,) - same shape\n",
    "print(\"=\" * 40)\n",
    "print(\"Case 1: Same shapes\")\n",
    "show_broadcast(np.array([1, 2, 3]), np.array([10, 20, 30]))\n",
    "\n",
    "# Case 2: (2, 3) + (3,) - trailing dimensions match\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"Case 2: Trailing dimensions match\")\n",
    "show_broadcast(\n",
    "    np.array([[1, 2, 3], [4, 5, 6]]),\n",
    "    np.array([10, 20, 30])\n",
    ")\n",
    "\n",
    "# Case 3: (2, 3) + (2, 1) - one dimension is 1\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"Case 3: Dimension of 1 gets stretched\")\n",
    "show_broadcast(\n",
    "    np.array([[1, 2, 3], [4, 5, 6]]),\n",
    "    np.array([[10], [20]])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this means:** Broadcasting is NumPy's way of \"stretching\" smaller arrays to match larger ones during arithmetic operations. Instead of manually copying data to match shapes, NumPy virtually expands the smaller array. This happens automatically and uses no extra memory - it's just a clever indexing trick under the hood.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# VISUALIZATION: Broadcasting - How shapes expand\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\n\n# Case 1: (3,) + scalar\nax = axes[0]\nax.set_title('Scalar + Array\\n(3,) + () → (3,)', fontsize=11)\n# Draw original array\nfor i, val in enumerate([1, 2, 3]):\n    ax.add_patch(plt.Rectangle((i, 1), 0.9, 0.9, facecolor='steelblue', edgecolor='black'))\n    ax.text(i + 0.45, 1.45, str(val), ha='center', va='center', fontsize=12, color='white', fontweight='bold')\n# Draw scalar being broadcast\nfor i in range(3):\n    ax.add_patch(plt.Rectangle((i, 0), 0.9, 0.9, facecolor='coral', edgecolor='black', alpha=0.7 if i > 0 else 1))\n    ax.text(i + 0.45, 0.45, '10', ha='center', va='center', fontsize=12)\nax.annotate('', xy=(1.5, 0.95), xytext=(1.5, 0.05), arrowprops=dict(arrowstyle='->', color='green', lw=2))\nax.text(2.2, 0.5, 'broadcast', fontsize=9, color='green')\nax.set_xlim(-0.5, 4)\nax.set_ylim(-0.5, 2.5)\nax.axis('off')\n\n# Case 2: (2, 3) + (3,)\nax = axes[1]\nax.set_title('2D + 1D\\n(2,3) + (3,) → (2,3)', fontsize=11)\n# Draw 2D array\nfor i in range(2):\n    for j in range(3):\n        ax.add_patch(plt.Rectangle((j, 1-i), 0.9, 0.9, facecolor='steelblue', edgecolor='black'))\n        ax.text(j + 0.45, 1.45 - i, f'{i*3+j+1}', ha='center', va='center', fontsize=12, color='white', fontweight='bold')\n# Draw 1D array being broadcast\nfor j in range(3):\n    ax.add_patch(plt.Rectangle((j, -1), 0.9, 0.9, facecolor='coral', edgecolor='black'))\n    ax.text(j + 0.45, -0.55, f'{(j+1)*10}', ha='center', va='center', fontsize=11)\n# Arrows showing broadcast\nfor i in range(2):\n    ax.annotate('', xy=(1.5, 1-i), xytext=(1.5, -0.5), arrowprops=dict(arrowstyle='->', color='green', lw=1.5, alpha=0.5))\nax.text(3.3, 0.2, 'broadcast\\nto rows', fontsize=9, color='green')\nax.set_xlim(-0.5, 4.5)\nax.set_ylim(-1.8, 2.5)\nax.axis('off')\n\n# Case 3: (2, 3) + (2, 1)\nax = axes[2]\nax.set_title('2D + Column\\n(2,3) + (2,1) → (2,3)', fontsize=11)\n# Draw 2D array\nfor i in range(2):\n    for j in range(3):\n        ax.add_patch(plt.Rectangle((j+1, 1-i), 0.9, 0.9, facecolor='steelblue', edgecolor='black'))\n        ax.text(j + 1.45, 1.45 - i, f'{i*3+j+1}', ha='center', va='center', fontsize=12, color='white', fontweight='bold')\n# Draw column array being broadcast\nfor i in range(2):\n    ax.add_patch(plt.Rectangle((0, 1-i), 0.9, 0.9, facecolor='coral', edgecolor='black'))\n    ax.text(0.45, 1.45 - i, f'{(i+1)*10}', ha='center', va='center', fontsize=11)\n# Arrows showing broadcast\nfor i in range(2):\n    ax.annotate('', xy=(1, 1.45-i), xytext=(0.95, 1.45-i), arrowprops=dict(arrowstyle='->', color='green', lw=1.5))\nax.text(0.2, -0.8, 'broadcast\\nto columns', fontsize=9, color='green')\nax.set_xlim(-0.5, 4.5)\nax.set_ylim(-1.2, 2.5)\nax.axis('off')\n\nplt.tight_layout()\nplt.suptitle('Broadcasting: How NumPy Expands Shapes', y=1.05, fontsize=13, fontweight='bold')\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# INTERACTIVE: Show effect of different broadcasting shapes\n# Experiment with broadcasting behavior\n\nprint(\"Broadcasting Shape Combinations\")\nprint(\"=\" * 60)\n\ntest_cases = [\n    ((3,), (3,), \"Same shapes - element-wise\"),\n    ((3, 4), (4,), \"2D + 1D - broadcast along rows\"),\n    ((3, 4), (3, 1), \"2D + column - broadcast along columns\"),\n    ((3, 1), (1, 4), \"Column + row - outer product pattern\"),\n    ((5, 3, 4), (4,), \"3D + 1D - broadcast to all batches\"),\n    ((5, 3, 4), (3, 1), \"3D + 2D - broadcast channel-wise\"),\n]\n\nfor shape_a, shape_b, description in test_cases:\n    a = np.ones(shape_a)\n    b = np.ones(shape_b)\n    try:\n        result = a + b\n        print(f\"{str(shape_a):>12} + {str(shape_b):<12} -> {str(result.shape):<12} | {description}\")\n    except ValueError as e:\n        print(f\"{str(shape_a):>12} + {str(shape_b):<12} -> ERROR | {e}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Broadcasting Failures (incompatible shapes):\")\nprint(\"=\" * 60)\n\nfailure_cases = [\n    ((3,), (4,), \"Different sizes, neither is 1\"),\n    ((3, 4), (3,), \"Trailing dims don't match\"),\n    ((2, 3, 4), (2, 4), \"Middle dimension mismatch\"),\n]\n\nfor shape_a, shape_b, description in failure_cases:\n    a = np.ones(shape_a)\n    b = np.ones(shape_b)\n    try:\n        result = a + b\n        print(f\"{str(shape_a):>12} + {str(shape_b):<12} -> {str(result.shape)}\")\n    except ValueError as e:\n        print(f\"{str(shape_a):>12} + {str(shape_b):<12} -> FAIL | {description}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic use case: outer product via broadcasting\n",
    "a = np.array([1, 2, 3, 4])  # Shape (4,)\n",
    "b = np.array([10, 20, 30])  # Shape (3,)\n",
    "\n",
    "# Make shapes compatible for broadcasting\n",
    "# (4, 1) * (3,) -> (4, 1) * (1, 3) -> (4, 3)\n",
    "outer = a[:, np.newaxis] * b[np.newaxis, :]\n",
    "\n",
    "print(f\"a (shape {a.shape}): {a}\")\n",
    "print(f\"b (shape {b.shape}): {b}\")\n",
    "print(f\"\\na[:, None] shape: {a[:, np.newaxis].shape}\")\n",
    "print(f\"b[None, :] shape: {b[np.newaxis, :].shape}\")\n",
    "print(f\"\\nOuter product (4, 3):\\n{outer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting in Deep Learning\n",
    "\n",
    "| Operation | Shapes | Use Case |\n",
    "|-----------|--------|----------|\n",
    "| Add bias | `(batch, features) + (features,)` | FC layer output + bias |\n",
    "| Normalize | `(B, C, H, W) - (C, 1, 1)` | Subtract channel means |\n",
    "| Scale | `(B, C, H, W) * (C, 1, 1)` | Batch normalization |\n",
    "| Attention mask | `(B, H, L, L) + (1, 1, L, L)` | Causal mask |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical example: Batch normalization-style operation\n",
    "# Input: (batch, channels, height, width)\n",
    "x = np.random.randn(32, 64, 8, 8)  # 32 images, 64 channels, 8x8\n",
    "\n",
    "# Compute per-channel mean and std\n",
    "mean = x.mean(axis=(0, 2, 3), keepdims=True)  # (1, 64, 1, 1)\n",
    "std = x.std(axis=(0, 2, 3), keepdims=True)    # (1, 64, 1, 1)\n",
    "\n",
    "# Normalize (broadcasts automatically!)\n",
    "x_normalized = (x - mean) / (std + 1e-5)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Mean shape: {mean.shape}\")\n",
    "print(f\"Normalized shape: {x_normalized.shape}\")\n",
    "print(f\"\\nPer-channel mean after normalization: {x_normalized.mean(axis=(0, 2, 3))[:5].round(6)}\")\n",
    "print(f\"Per-channel std after normalization: {x_normalized.std(axis=(0, 2, 3))[:5].round(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Advanced Indexing\n",
    "\n",
    "NumPy offers powerful ways to select elements from arrays.\n",
    "\n",
    "### Basic Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "print(f\"x: {x}\")\n",
    "print(f\"x[2:7]: {x[2:7]}\")\n",
    "print(f\"x[::2] (every 2nd): {x[::2]}\")\n",
    "print(f\"x[::-1] (reversed): {x[::-1]}\")\n",
    "\n",
    "# 2D slicing\n",
    "A = np.arange(20).reshape(4, 5)\n",
    "print(f\"\\nA:\\n{A}\")\n",
    "print(f\"\\nA[1:3, 2:4] (rows 1-2, cols 2-3):\\n{A[1:3, 2:4]}\")\n",
    "print(f\"\\nA[:, 0] (first column): {A[:, 0]}\")\n",
    "print(f\"A[0, :] (first row): {A[0, :]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Indexing\n",
    "\n",
    "Select elements based on conditions. Extremely useful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, -2, 3, -4, 5, -6])\n",
    "\n",
    "# Create boolean mask\n",
    "mask = x > 0\n",
    "print(f\"x: {x}\")\n",
    "print(f\"mask (x > 0): {mask}\")\n",
    "print(f\"x[mask]: {x[mask]}\")\n",
    "\n",
    "# Directly in one line\n",
    "print(f\"x[x > 0]: {x[x > 0]}\")\n",
    "\n",
    "# Combine conditions\n",
    "print(f\"x[(x > 0) & (x < 5)]: {x[(x > 0) & (x < 5)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical: Apply ReLU using boolean indexing\n",
    "def relu_boolean(x):\n",
    "    result = x.copy()\n",
    "    result[result < 0] = 0\n",
    "    return result\n",
    "\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "print(f\"x: {x}\")\n",
    "print(f\"ReLU(x): {relu_boolean(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer Array Indexing (Fancy Indexing)\n",
    "\n",
    "Use arrays of indices to select specific elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([10, 20, 30, 40, 50])\n",
    "indices = np.array([0, 2, 4])\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"indices: {indices}\")\n",
    "print(f\"x[indices]: {x[indices]}\")\n",
    "\n",
    "# Can repeat indices\n",
    "print(f\"x[[0, 0, 1, 1]]: {x[[0, 0, 1, 1]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D fancy indexing - select specific (row, col) pairs\n",
    "A = np.arange(12).reshape(3, 4)\n",
    "print(f\"A:\\n{A}\")\n",
    "\n",
    "rows = np.array([0, 1, 2])\n",
    "cols = np.array([0, 2, 3])\n",
    "\n",
    "# This selects A[0,0], A[1,2], A[2,3]\n",
    "print(f\"\\nrows: {rows}\")\n",
    "print(f\"cols: {cols}\")\n",
    "print(f\"A[rows, cols]: {A[rows, cols]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this means:** NumPy indexing lets you select subsets of arrays without copying data. Basic slicing creates \"views\" (same memory), while boolean and fancy indexing create copies. Understanding this distinction matters for both performance and avoiding bugs when modifying arrays.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# VISUALIZATION: Indexing Patterns - Highlight selected elements\nfig, axes = plt.subplots(2, 3, figsize=(14, 8))\n\n# Create a sample 4x5 array for visualization\nA = np.arange(20).reshape(4, 5)\n\ndef visualize_selection(ax, A, mask, title):\n    \"\"\"Visualize array with selected elements highlighted.\"\"\"\n    rows, cols = A.shape\n    for i in range(rows):\n        for j in range(cols):\n            selected = mask[i, j] if mask.ndim == 2 else False\n            color = 'coral' if selected else 'lightgray'\n            edgecolor = 'darkred' if selected else 'gray'\n            lw = 3 if selected else 1\n            ax.add_patch(plt.Rectangle((j, rows-1-i), 0.9, 0.9, \n                                        facecolor=color, edgecolor=edgecolor, lw=lw))\n            ax.text(j + 0.45, rows - 0.55 - i, str(A[i, j]), \n                   ha='center', va='center', fontsize=11, fontweight='bold')\n    ax.set_xlim(-0.2, cols + 0.2)\n    ax.set_ylim(-0.2, rows + 0.2)\n    ax.set_title(title, fontsize=11, fontweight='bold')\n    ax.axis('off')\n\n# 1. Basic slicing: A[1:3, 2:4]\nax = axes[0, 0]\nmask = np.zeros_like(A, dtype=bool)\nmask[1:3, 2:4] = True\nvisualize_selection(ax, A, mask, 'A[1:3, 2:4]\\n(rows 1-2, cols 2-3)')\n\n# 2. Row selection: A[2, :]\nax = axes[0, 1]\nmask = np.zeros_like(A, dtype=bool)\nmask[2, :] = True\nvisualize_selection(ax, A, mask, 'A[2, :]\\n(entire row 2)')\n\n# 3. Column selection: A[:, 1]\nax = axes[0, 2]\nmask = np.zeros_like(A, dtype=bool)\nmask[:, 1] = True\nvisualize_selection(ax, A, mask, 'A[:, 1]\\n(entire column 1)')\n\n# 4. Boolean indexing: A > 10\nax = axes[1, 0]\nmask = A > 10\nvisualize_selection(ax, A, mask, 'A[A > 10]\\n(boolean mask)')\n\n# 5. Fancy indexing: A[[0, 2, 3], [1, 3, 4]]\nax = axes[1, 1]\nmask = np.zeros_like(A, dtype=bool)\nmask[0, 1] = True\nmask[2, 3] = True\nmask[3, 4] = True\nvisualize_selection(ax, A, mask, 'A[[0,2,3], [1,3,4]]\\n(fancy indexing)')\n\n# 6. Step slicing: A[::2, ::2]\nax = axes[1, 2]\nmask = np.zeros_like(A, dtype=bool)\nmask[::2, ::2] = True\nvisualize_selection(ax, A, mask, 'A[::2, ::2]\\n(every other element)')\n\nplt.suptitle('NumPy Indexing Patterns: Selected Elements in Red', y=1.02, fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"Array A:\")\nprint(A)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical: Selecting Class Probabilities\n",
    "\n",
    "In classification, you often need to select the probability of the true class for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax output: (batch_size, num_classes)\n",
    "probs = np.array([\n",
    "    [0.1, 0.7, 0.2],  # Sample 0: class 1 most likely\n",
    "    [0.8, 0.1, 0.1],  # Sample 1: class 0 most likely\n",
    "    [0.3, 0.3, 0.4],  # Sample 2: class 2 most likely\n",
    "])\n",
    "\n",
    "# True labels\n",
    "labels = np.array([1, 0, 2])\n",
    "\n",
    "# Get probability of true class for each sample\n",
    "batch_indices = np.arange(len(labels))\n",
    "true_probs = probs[batch_indices, labels]\n",
    "\n",
    "print(f\"Probabilities:\\n{probs}\")\n",
    "print(f\"\\nTrue labels: {labels}\")\n",
    "print(f\"Batch indices: {batch_indices}\")\n",
    "print(f\"\\nProbability of true class: {true_probs}\")\n",
    "\n",
    "# Cross-entropy loss\n",
    "loss = -np.log(true_probs).mean()\n",
    "print(f\"Cross-entropy loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# INTERACTIVE: Vary array sizes and show timing differences\n# See how vectorization advantage scales with data size\n\nsizes = [100, 1000, 10000, 100000, 1000000]\nloop_times_by_size = []\nvec_times_by_size = []\n\nprint(\"Timing element-wise multiplication at different array sizes...\")\nprint(\"-\" * 60)\n\nfor size in sizes:\n    a_test = np.random.randn(size)\n    b_test = np.random.randn(size)\n    \n    # Only run loop version for smaller sizes (it's too slow otherwise)\n    if size <= 100000:\n        def loop_op(a, b):\n            result = np.empty(len(a))\n            for i in range(len(a)):\n                result[i] = a[i] * b[i]\n            return result\n        t_loop, _ = time_function(loop_op, a_test, b_test, n_runs=3)\n    else:\n        # Estimate based on linear scaling\n        t_loop = loop_times_by_size[-1] * (size / sizes[sizes.index(size)-1])\n    \n    t_vec, _ = time_function(lambda a, b: a * b, a_test, b_test, n_runs=10)\n    \n    loop_times_by_size.append(t_loop * 1000)\n    vec_times_by_size.append(t_vec * 1000)\n    \n    speedup = t_loop / t_vec if t_vec > 0 else float('inf')\n    print(f\"Size {size:>10,}: Loop={t_loop*1000:>10.3f}ms, Vec={t_vec*1000:>8.4f}ms, Speedup={speedup:>6.0f}x\")\n\n# Plot the scaling behavior\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\nax = axes[0]\nax.loglog(sizes, loop_times_by_size, 'o-', color='coral', label='Loop', linewidth=2, markersize=8)\nax.loglog(sizes, vec_times_by_size, 's-', color='steelblue', label='Vectorized', linewidth=2, markersize=8)\nax.set_xlabel('Array Size', fontsize=11)\nax.set_ylabel('Time (ms)', fontsize=11)\nax.set_title('Execution Time vs Array Size\\n(log-log scale)', fontsize=12, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nax = axes[1]\nspeedups_by_size = [l/v for l, v in zip(loop_times_by_size, vec_times_by_size)]\nax.semilogx(sizes, speedups_by_size, 'o-', color='green', linewidth=2, markersize=8)\nax.set_xlabel('Array Size', fontsize=11)\nax.set_ylabel('Speedup Factor (x)', fontsize=11)\nax.set_title('Vectorization Speedup vs Array Size', fontsize=12, fontweight='bold')\nax.grid(True, alpha=0.3)\nax.axhline(y=100, color='red', linestyle='--', alpha=0.5, label='100x reference')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nNote: Speedup tends to stabilize or increase with larger arrays due to better cache utilization.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**What this means:** Vectorization is the single most important optimization in NumPy. When you write `a * b` instead of a loop, NumPy executes optimized C code that processes data in chunks, uses CPU cache efficiently, and leverages SIMD (Single Instruction, Multiple Data) parallelism. This is why NumPy can be 100x faster than pure Python.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Vectorization: The Key to Fast NumPy\n",
    "\n",
    "**Vectorization** means replacing explicit loops with array operations. This is MUCH faster because:\n",
    "1. Operations are implemented in C\n",
    "2. Can use SIMD instructions\n",
    "3. Better memory access patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_function(func, *args, n_runs=10):\n",
    "    \"\"\"Time a function.\"\"\"\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        result = func(*args)\n",
    "        times.append(time.time() - start)\n",
    "    return np.mean(times), result\n",
    "\n",
    "\n",
    "# Compare: Element-wise multiplication\n",
    "n = 1000000\n",
    "a = np.random.randn(n)\n",
    "b = np.random.randn(n)\n",
    "\n",
    "def loop_multiply(a, b):\n",
    "    result = np.empty(len(a))\n",
    "    for i in range(len(a)):\n",
    "        result[i] = a[i] * b[i]\n",
    "    return result\n",
    "\n",
    "def vectorized_multiply(a, b):\n",
    "    return a * b\n",
    "\n",
    "loop_time, _ = time_function(loop_multiply, a, b, n_runs=3)\n",
    "vec_time, _ = time_function(vectorized_multiply, a, b, n_runs=3)\n",
    "\n",
    "print(f\"Array size: {n:,}\")\n",
    "print(f\"Loop time: {loop_time*1000:.2f} ms\")\n",
    "print(f\"Vectorized time: {vec_time*1000:.4f} ms\")\n",
    "print(f\"Speedup: {loop_time/vec_time:.0f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization Examples"
   ]
  },
  {
   "cell_type": "code",
   "source": "# VISUALIZATION: Performance Comparison Bar Charts\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Store results for visualization\noperations = ['Element-wise\\nMultiply', 'Pairwise\\nDistance', 'Softmax']\nloop_times = []\nvec_times = []\n\n# Re-run timing for visualization (smaller sizes for quick demo)\n# 1. Element-wise multiply\nn = 100000\na_small = np.random.randn(n)\nb_small = np.random.randn(n)\n\ndef loop_mult(a, b):\n    result = np.empty(len(a))\n    for i in range(len(a)):\n        result[i] = a[i] * b[i]\n    return result\n\nt1, _ = time_function(loop_mult, a_small, b_small, n_runs=3)\nt2, _ = time_function(lambda a, b: a * b, a_small, b_small, n_runs=3)\nloop_times.append(t1 * 1000)\nvec_times.append(t2 * 1000)\n\n# 2. Pairwise distance (smaller for speed)\nX_small = np.random.randn(50, 10)\nt1, _ = time_function(pairwise_distance_loops, X_small, n_runs=3)\nt2, _ = time_function(pairwise_distance_vectorized, X_small, n_runs=3)\nloop_times.append(t1 * 1000)\nvec_times.append(t2 * 1000)\n\n# 3. Softmax\nx_small = np.random.randn(500, 50)\nt1, _ = time_function(softmax_loops, x_small, n_runs=3)\nt2, _ = time_function(softmax_vectorized, x_small, n_runs=3)\nloop_times.append(t1 * 1000)\nvec_times.append(t2 * 1000)\n\n# Left plot: Absolute times (log scale)\nax = axes[0]\nx_pos = np.arange(len(operations))\nwidth = 0.35\nbars1 = ax.bar(x_pos - width/2, loop_times, width, label='Loop', color='coral', edgecolor='darkred')\nbars2 = ax.bar(x_pos + width/2, vec_times, width, label='Vectorized', color='steelblue', edgecolor='darkblue')\nax.set_ylabel('Time (ms)', fontsize=11)\nax.set_title('Execution Time Comparison\\n(log scale)', fontsize=12, fontweight='bold')\nax.set_xticks(x_pos)\nax.set_xticklabels(operations)\nax.legend()\nax.set_yscale('log')\nax.grid(axis='y', alpha=0.3)\n\n# Right plot: Speedup factors\nax = axes[1]\nspeedups = [l/v for l, v in zip(loop_times, vec_times)]\ncolors = ['green' if s > 10 else 'orange' for s in speedups]\nbars = ax.bar(operations, speedups, color=colors, edgecolor='black')\nax.set_ylabel('Speedup Factor (x)', fontsize=11)\nax.set_title('Vectorization Speedup\\n(higher is better)', fontsize=12, fontweight='bold')\nax.axhline(y=1, color='red', linestyle='--', alpha=0.5, label='Break-even')\n\n# Add speedup labels on bars\nfor bar, speedup in zip(bars, speedups):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n            f'{speedup:.0f}x', ha='center', va='bottom', fontweight='bold')\n\nax.grid(axis='y', alpha=0.3)\nax.set_ylim(0, max(speedups) * 1.2)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey Insight: Vectorization typically provides 10-100x+ speedup over Python loops!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Euclidean distance between all pairs of points\n",
    "\n",
    "def pairwise_distance_loops(X):\n",
    "    \"\"\"Compute pairwise distances using loops.\"\"\"\n",
    "    n = len(X)\n",
    "    D = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            D[i, j] = np.sqrt(np.sum((X[i] - X[j])**2))\n",
    "    return D\n",
    "\n",
    "def pairwise_distance_vectorized(X):\n",
    "    \"\"\"Compute pairwise distances using broadcasting.\"\"\"\n",
    "    # X: (n, d)\n",
    "    # X[:, None, :] - X[None, :, :] gives (n, n, d) differences\n",
    "    diff = X[:, np.newaxis, :] - X[np.newaxis, :, :]\n",
    "    return np.sqrt(np.sum(diff**2, axis=2))\n",
    "\n",
    "# Test\n",
    "X = np.random.randn(100, 10)  # 100 points in 10D\n",
    "\n",
    "loop_time, D_loop = time_function(pairwise_distance_loops, X, n_runs=3)\n",
    "vec_time, D_vec = time_function(pairwise_distance_vectorized, X, n_runs=3)\n",
    "\n",
    "print(f\"Results match: {np.allclose(D_loop, D_vec)}\")\n",
    "print(f\"Loop time: {loop_time*1000:.2f} ms\")\n",
    "print(f\"Vectorized time: {vec_time*1000:.4f} ms\")\n",
    "print(f\"Speedup: {loop_time/vec_time:.0f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Softmax\n",
    "\n",
    "def softmax_loops(x):\n",
    "    \"\"\"Softmax with loops.\"\"\"\n",
    "    result = np.empty_like(x)\n",
    "    for i in range(len(x)):\n",
    "        max_val = x[i].max()\n",
    "        exp_x = np.exp(x[i] - max_val)\n",
    "        result[i] = exp_x / exp_x.sum()\n",
    "    return result\n",
    "\n",
    "def softmax_vectorized(x):\n",
    "    \"\"\"Softmax vectorized.\"\"\"\n",
    "    max_val = x.max(axis=1, keepdims=True)\n",
    "    exp_x = np.exp(x - max_val)\n",
    "    return exp_x / exp_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Test\n",
    "x = np.random.randn(1000, 100)  # 1000 samples, 100 classes\n",
    "\n",
    "loop_time, s_loop = time_function(softmax_loops, x)\n",
    "vec_time, s_vec = time_function(softmax_vectorized, x)\n",
    "\n",
    "print(f\"Results match: {np.allclose(s_loop, s_vec)}\")\n",
    "print(f\"Loop time: {loop_time*1000:.2f} ms\")\n",
    "print(f\"Vectorized time: {vec_time*1000:.4f} ms\")\n",
    "print(f\"Speedup: {loop_time/vec_time:.0f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization Patterns\n",
    "\n",
    "| Loop Pattern | Vectorized Version |\n",
    "|--------------|--------------------|\n",
    "| `for i: result[i] = a[i] + b[i]` | `result = a + b` |\n",
    "| `for i: result[i] = f(a[i])` | `result = f(a)` (if f is ufunc) |\n",
    "| `for i: total += a[i]` | `total = a.sum()` |\n",
    "| `for i: if cond: result[i] = x` | `result[cond] = x` |\n",
    "| `for i,j: C[i,j] = A[i,:] @ B[:,j]` | `C = A @ B` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Useful NumPy Functions\n",
    "\n",
    "### Aggregation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(3, 4)\n",
    "print(f\"x:\\n{x.round(2)}\")\n",
    "\n",
    "print(f\"\\nsum: {x.sum():.2f}\")\n",
    "print(f\"sum(axis=0) (column sums): {x.sum(axis=0).round(2)}\")\n",
    "print(f\"sum(axis=1) (row sums): {x.sum(axis=1).round(2)}\")\n",
    "\n",
    "print(f\"\\nmean: {x.mean():.2f}\")\n",
    "print(f\"std: {x.std():.2f}\")\n",
    "print(f\"min: {x.min():.2f}\")\n",
    "print(f\"max: {x.max():.2f}\")\n",
    "\n",
    "print(f\"\\nargmax (index of max): {x.argmax()}\")\n",
    "print(f\"argmax(axis=1) (max index per row): {x.argmax(axis=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### np.where - Conditional Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "\n",
    "# np.where(condition, value_if_true, value_if_false)\n",
    "result = np.where(x > 0, x, 0)  # ReLU!\n",
    "print(f\"x: {x}\")\n",
    "print(f\"np.where(x > 0, x, 0): {result}\")\n",
    "\n",
    "# Just get indices where condition is true\n",
    "indices = np.where(x > 0)[0]\n",
    "print(f\"Indices where x > 0: {indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### np.clip - Limit Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([-5, -1, 0, 1, 5, 10])\n",
    "print(f\"x: {x}\")\n",
    "print(f\"np.clip(x, 0, 6): {np.clip(x, 0, 6)}\")\n",
    "\n",
    "# Useful for gradient clipping\n",
    "gradients = np.random.randn(5) * 10\n",
    "clipped = np.clip(gradients, -1, 1)\n",
    "print(f\"\\nGradients: {gradients.round(2)}\")\n",
    "print(f\"Clipped: {clipped.round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking and Concatenating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "# Concatenate - join along existing axis\n",
    "print(f\"np.concatenate([a, b]): {np.concatenate([a, b])}\")\n",
    "\n",
    "# Stack - create new axis\n",
    "print(f\"np.stack([a, b]): (new axis)\")\n",
    "print(np.stack([a, b]))\n",
    "\n",
    "print(f\"\\nnp.vstack([a, b]): (vertical stack)\")\n",
    "print(np.vstack([a, b]))\n",
    "\n",
    "print(f\"\\nnp.hstack([a, b]): (horizontal stack)\")\n",
    "print(np.hstack([a, b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Implement Batch Matrix Multiplication\n",
    "\n",
    "Given `A` of shape `(batch, m, n)` and `B` of shape `(batch, n, p)`, compute the batch matrix product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_matmul(A, B):\n",
    "    \"\"\"\n",
    "    Batch matrix multiplication.\n",
    "    A: (batch, m, n)\n",
    "    B: (batch, n, p)\n",
    "    Returns: (batch, m, p)\n",
    "    \"\"\"\n",
    "    # TODO: Implement (hint: use np.einsum or @ with proper broadcasting)\n",
    "    return np.einsum('bmn,bnp->bmp', A, B)\n",
    "    # Or: return A @ B  # NumPy handles batch dimension!\n",
    "\n",
    "# Test\n",
    "batch, m, n, p = 32, 64, 128, 32\n",
    "A = np.random.randn(batch, m, n)\n",
    "B = np.random.randn(batch, n, p)\n",
    "\n",
    "result = batch_matmul(A, B)\n",
    "print(f\"A shape: {A.shape}\")\n",
    "print(f\"B shape: {B.shape}\")\n",
    "print(f\"Result shape: {result.shape}\")\n",
    "\n",
    "# Verify with loop\n",
    "expected = np.stack([A[i] @ B[i] for i in range(batch)])\n",
    "print(f\"Correct: {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, num_classes):\n",
    "    \"\"\"\n",
    "    Convert labels to one-hot encoding.\n",
    "    labels: (n,) array of integers\n",
    "    num_classes: number of classes\n",
    "    Returns: (n, num_classes) one-hot encoded\n",
    "    \"\"\"\n",
    "    # TODO: Implement without loops!\n",
    "    n = len(labels)\n",
    "    result = np.zeros((n, num_classes))\n",
    "    result[np.arange(n), labels] = 1\n",
    "    return result\n",
    "\n",
    "# Test\n",
    "labels = np.array([0, 2, 1, 0, 3])\n",
    "one_hot_encoded = one_hot(labels, num_classes=4)\n",
    "print(f\"Labels: {labels}\")\n",
    "print(f\"One-hot:\\n{one_hot_encoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Implement Conv2D (Naive)\n",
    "\n",
    "Implement a simple 2D convolution using `np.lib.stride_tricks.as_strided` or loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_simple(image, kernel):\n",
    "    \"\"\"\n",
    "    Simple 2D convolution (no padding, stride=1).\n",
    "    image: (H, W)\n",
    "    kernel: (kH, kW)\n",
    "    Returns: (H-kH+1, W-kW+1)\n",
    "    \"\"\"\n",
    "    H, W = image.shape\n",
    "    kH, kW = kernel.shape\n",
    "    out_H = H - kH + 1\n",
    "    out_W = W - kW + 1\n",
    "    \n",
    "    # TODO: Implement convolution\n",
    "    output = np.zeros((out_H, out_W))\n",
    "    for i in range(out_H):\n",
    "        for j in range(out_W):\n",
    "            output[i, j] = np.sum(image[i:i+kH, j:j+kW] * kernel)\n",
    "    return output\n",
    "\n",
    "# Test with edge detection kernel\n",
    "image = np.random.randn(10, 10)\n",
    "sobel_x = np.array([[-1, 0, 1],\n",
    "                    [-2, 0, 2],\n",
    "                    [-1, 0, 1]])\n",
    "\n",
    "result = conv2d_simple(image, sobel_x)\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "print(f\"Kernel shape: {sobel_x.shape}\")\n",
    "print(f\"Output shape: {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description | Example |\n",
    "|---------|-------------|--------|\n",
    "| **Shape manipulation** | Reshape, transpose, add dims | `x.reshape(-1)`, `x.T` |\n",
    "| **Broadcasting** | Auto-expand dimensions | `(3,4) + (4,)` works |\n",
    "| **Boolean indexing** | Select by condition | `x[x > 0]` |\n",
    "| **Fancy indexing** | Select by indices | `x[[0, 2, 4]]` |\n",
    "| **Vectorization** | Replace loops with array ops | 100x+ speedup |\n",
    "| **keepdims** | Preserve dimensions for broadcasting | `x.sum(axis=1, keepdims=True)` |\n",
    "\n",
    "### Checklist\n",
    "- [ ] I can reshape and transpose arrays\n",
    "- [ ] I understand broadcasting rules\n",
    "- [ ] I can use boolean and fancy indexing\n",
    "- [ ] I can vectorize loop-based code"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Connection to Deep Learning\n\n| NumPy Concept | PyTorch Equivalent | ML Application |\n|---------------|-------------------|----------------|\n| `np.array()` | `torch.tensor()` | Creating weight matrices, input data |\n| `x.reshape()` | `x.view()` / `x.reshape()` | Flattening CNN output before FC layer |\n| `x.T` / `x.transpose()` | `x.T` / `x.transpose()` | Converting between NHWC and NCHW formats |\n| Broadcasting | Same rules | Adding biases, batch normalization |\n| `x[x > 0]` (boolean indexing) | `x[x > 0]` | ReLU activation, masking padded tokens |\n| `x[indices]` (fancy indexing) | `x[indices]` | Embedding lookup, selecting class probs |\n| `np.sum(x, axis=1, keepdims=True)` | `x.sum(dim=1, keepdim=True)` | Softmax normalization |\n| `np.matmul()` / `@` | `torch.matmul()` / `@` | Linear layers, attention scores |\n| `np.concatenate()` | `torch.cat()` | Skip connections, feature fusion |\n| `np.stack()` | `torch.stack()` | Batching sequences |\n| `np.where(cond, x, y)` | `torch.where(cond, x, y)` | Conditional operations, masking |\n| `np.clip()` | `torch.clamp()` | Gradient clipping |\n| `np.einsum()` | `torch.einsum()` | Attention mechanisms, tensor contractions |\n| `x.mean(axis=(0,2,3))` | `x.mean(dim=(0,2,3))` | Batch normalization statistics |\n\n**Key insight:** If you master NumPy, you already know 90% of PyTorch tensor operations. The main differences are: (1) PyTorch tracks gradients automatically, (2) PyTorch can run on GPU, and (3) some method names differ slightly (`axis` vs `dim`, `keepdims` vs `keepdim`).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "You've completed the Python Foundations! Next up: **Part 3: Neural Network Fundamentals**\n",
    "- Building perceptrons from scratch\n",
    "- Implementing backpropagation\n",
    "- Introduction to PyTorch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}