{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3.3: PyTorch Fundamentals\n",
    "\n",
    "You've built neural networks from scratch using NumPy. You understand backpropagation, gradient descent, and the chain rule at a fundamental level. Now it's time to use a framework that handles the tedious parts automatically while giving you full control over the model architecture.\n",
    "\n",
    "**PyTorch** is the framework of choice for research and increasingly for production. It feels like NumPy with superpowers: automatic differentiation, GPU acceleration, and a rich ecosystem of tools.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you should be able to:\n",
    "\n",
    "- [ ] Explain why PyTorch is preferred over raw NumPy for deep learning\n",
    "- [ ] Create and manipulate tensors (PyTorch's version of arrays)\n",
    "- [ ] Understand autograd and how it tracks gradients automatically\n",
    "- [ ] Build custom neural network modules using nn.Module\n",
    "- [ ] Implement a complete training loop from scratch\n",
    "- [ ] Save and load model weights\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Why PyTorch?\n",
    "\n",
    "### The Problem with From-Scratch Implementations\n",
    "\n",
    "Remember building backpropagation by hand? You had to:\n",
    "1. Manually compute gradients for every operation\n",
    "2. Carefully chain them together using the chain rule\n",
    "3. Keep track of intermediate values for the backward pass\n",
    "4. Run everything on CPU (slow for large models)\n",
    "\n",
    "This was valuable for understanding, but it doesn't scale. Modern networks have millions of parameters and complex architectures. PyTorch solves this.\n",
    "\n",
    "### NumPy vs PyTorch Comparison\n",
    "\n",
    "| Feature | NumPy | PyTorch |\n",
    "|---------|-------|--------|\n",
    "| **Array/Tensor creation** | `np.array([1, 2, 3])` | `torch.tensor([1, 2, 3])` |\n",
    "| **Random arrays** | `np.random.randn(3, 4)` | `torch.randn(3, 4)` |\n",
    "| **Matrix multiply** | `a @ b` or `np.dot(a, b)` | `a @ b` or `torch.mm(a, b)` |\n",
    "| **Element-wise ops** | `a + b`, `a * b` | `a + b`, `a * b` |\n",
    "| **Automatic gradients** | No | Yes (autograd) |\n",
    "| **GPU support** | No | Yes |\n",
    "| **Neural network layers** | No | Yes (nn.Module) |\n",
    "\n",
    "**Key insight:** PyTorch syntax is nearly identical to NumPy, but with superpowers.\n",
    "\n",
    "### The Three Superpowers of PyTorch\n",
    "\n",
    "| Superpower | What it does | Why it matters |\n",
    "|------------|--------------|----------------|\n",
    "| **Autograd** | Automatically computes gradients | No more manual chain rule! |\n",
    "| **GPU Acceleration** | Runs computations on graphics cards | 10-100x faster training |\n",
    "| **Ecosystem** | Pre-built layers, optimizers, datasets | Focus on research, not plumbing |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick comparison: NumPy vs PyTorch syntax\n",
    "print(\"=\" * 50)\n",
    "print(\"NumPy vs PyTorch: Nearly identical syntax!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# NumPy\n",
    "np_array = np.array([1.0, 2.0, 3.0])\n",
    "print(f\"\\nNumPy array:  {np_array}\")\n",
    "print(f\"NumPy type:   {type(np_array)}\")\n",
    "\n",
    "# PyTorch\n",
    "torch_tensor = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(f\"\\nPyTorch tensor: {torch_tensor}\")\n",
    "print(f\"PyTorch type:   {type(torch_tensor)}\")\n",
    "\n",
    "# Same operations!\n",
    "print(f\"\\nNumPy sum:    {np_array.sum()}\")\n",
    "print(f\"PyTorch sum:  {torch_tensor.sum()}\")\n",
    "\n",
    "print(f\"\\nNumPy mean:   {np_array.mean()}\")\n",
    "print(f\"PyTorch mean: {torch_tensor.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Tensors: The Foundation\n",
    "\n",
    "### What is a Tensor?\n",
    "\n",
    "A tensor is just a multi-dimensional array - PyTorch's equivalent of NumPy's `ndarray`. The name comes from physics/math, but for our purposes:\n",
    "\n",
    "- **0D tensor (scalar):** A single number\n",
    "- **1D tensor (vector):** A list of numbers\n",
    "- **2D tensor (matrix):** A table of numbers\n",
    "- **3D+ tensor:** Multiple matrices stacked (images, batches, etc.)\n",
    "\n",
    "### Creating Tensors\n",
    "\n",
    "| Method | Example | Use case |\n",
    "|--------|---------|----------|\n",
    "| From data | `torch.tensor([1, 2, 3])` | Convert existing data |\n",
    "| Zeros | `torch.zeros(3, 4)` | Initialize biases |\n",
    "| Ones | `torch.ones(3, 4)` | Create masks |\n",
    "| Random normal | `torch.randn(3, 4)` | Initialize weights |\n",
    "| Random uniform | `torch.rand(3, 4)` | Values in [0, 1) |\n",
    "| Range | `torch.arange(0, 10, 2)` | Create sequences |\n",
    "| Like another | `torch.zeros_like(x)` | Match shape/type |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors - many ways!\n",
    "print(\"=\" * 50)\n",
    "print(\"Creating Tensors\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# From Python data\n",
    "from_list = torch.tensor([1, 2, 3, 4])\n",
    "print(f\"\\nFrom list: {from_list}\")\n",
    "\n",
    "from_nested = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "print(f\"From nested list:\\n{from_nested}\")\n",
    "\n",
    "# Initialized tensors\n",
    "zeros = torch.zeros(2, 3)\n",
    "print(f\"\\nZeros (2x3):\\n{zeros}\")\n",
    "\n",
    "ones = torch.ones(3, 2)\n",
    "print(f\"\\nOnes (3x2):\\n{ones}\")\n",
    "\n",
    "# Random tensors (most common for weight initialization)\n",
    "random_normal = torch.randn(2, 3)  # Standard normal distribution\n",
    "print(f\"\\nRandom normal:\\n{random_normal}\")\n",
    "\n",
    "random_uniform = torch.rand(2, 3)  # Uniform [0, 1)\n",
    "print(f\"\\nRandom uniform:\\n{random_uniform}\")\n",
    "\n",
    "# Range-based\n",
    "sequence = torch.arange(0, 10, 2)\n",
    "print(f\"\\nSequence (0 to 10, step 2): {sequence}\")\n",
    "\n",
    "# Like another tensor\n",
    "like_zeros = torch.zeros_like(random_normal)\n",
    "print(f\"\\nZeros like random_normal:\\n{like_zeros}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Attributes\n",
    "\n",
    "Every tensor has three key attributes:\n",
    "\n",
    "| Attribute | What it tells you | Example |\n",
    "|-----------|-------------------|--------|\n",
    "| **shape** | Dimensions of the tensor | `torch.Size([3, 4])` |\n",
    "| **dtype** | Data type of elements | `torch.float32`, `torch.int64` |\n",
    "| **device** | Where the tensor lives | `cpu` or `cuda:0` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor attributes\n",
    "x = torch.randn(3, 4, 5)\n",
    "\n",
    "print(\"Tensor Attributes\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {x.shape}\")       # Dimensions\n",
    "print(f\"Size:  {x.size()}\")      # Same as shape (method form)\n",
    "print(f\"Dtype: {x.dtype}\")       # Data type\n",
    "print(f\"Device: {x.device}\")     # CPU or GPU\n",
    "print(f\"Requires grad: {x.requires_grad}\")  # Tracking gradients?\n",
    "\n",
    "# Common data types\n",
    "print(\"\\nCommon Data Types:\")\n",
    "print(f\"Float tensor:   {torch.tensor([1.0]).dtype}\")\n",
    "print(f\"Int tensor:     {torch.tensor([1]).dtype}\")\n",
    "print(f\"Explicit float: {torch.tensor([1], dtype=torch.float32).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Operations\n",
    "\n",
    "PyTorch operations mirror NumPy almost exactly. If you know NumPy, you know 90% of PyTorch tensor operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor operations - same as NumPy!\n",
    "a = torch.tensor([[1., 2.], [3., 4.]])\n",
    "b = torch.tensor([[5., 6.], [7., 8.]])\n",
    "\n",
    "print(\"Tensor Operations\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"a:\\n{a}\")\n",
    "print(f\"\\nb:\\n{b}\")\n",
    "\n",
    "# Element-wise operations\n",
    "print(f\"\\na + b (element-wise):\\n{a + b}\")\n",
    "print(f\"\\na * b (element-wise):\\n{a * b}\")\n",
    "print(f\"\\na ** 2 (element-wise square):\\n{a ** 2}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "print(f\"\\na @ b (matrix multiply):\\n{a @ b}\")\n",
    "\n",
    "# Reductions\n",
    "print(f\"\\na.sum(): {a.sum()}\")\n",
    "print(f\"a.mean(): {a.mean()}\")\n",
    "print(f\"a.sum(dim=0): {a.sum(dim=0)}\")  # Sum along rows\n",
    "print(f\"a.sum(dim=1): {a.sum(dim=1)}\")  # Sum along columns\n",
    "\n",
    "# Reshaping\n",
    "c = torch.arange(12)\n",
    "print(f\"\\nOriginal: {c}\")\n",
    "print(f\"Reshaped to 3x4:\\n{c.reshape(3, 4)}\")\n",
    "print(f\"View as 4x3:\\n{c.view(4, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Between NumPy and PyTorch\n",
    "\n",
    "You'll often need to move data between NumPy and PyTorch:\n",
    "\n",
    "| Direction | Method | Notes |\n",
    "|-----------|--------|-------|\n",
    "| NumPy -> PyTorch | `torch.from_numpy(arr)` | Shares memory! |\n",
    "| PyTorch -> NumPy | `tensor.numpy()` | Shares memory (CPU only) |\n",
    "| Safe copy | `torch.tensor(arr)` | Creates a copy |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy <-> PyTorch conversion\n",
    "print(\"NumPy <-> PyTorch Conversion\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# NumPy to PyTorch (shares memory!)\n",
    "np_arr = np.array([1., 2., 3.])\n",
    "torch_from_np = torch.from_numpy(np_arr)\n",
    "print(f\"NumPy array: {np_arr}\")\n",
    "print(f\"Torch from NumPy: {torch_from_np}\")\n",
    "\n",
    "# They share memory - modifying one affects the other!\n",
    "np_arr[0] = 999\n",
    "print(f\"\\nAfter modifying NumPy array:\")\n",
    "print(f\"NumPy array: {np_arr}\")\n",
    "print(f\"Torch tensor: {torch_from_np}\")  # Also changed!\n",
    "\n",
    "# PyTorch to NumPy\n",
    "torch_tensor = torch.tensor([4., 5., 6.])\n",
    "np_from_torch = torch_tensor.numpy()\n",
    "print(f\"\\nTorch tensor: {torch_tensor}\")\n",
    "print(f\"NumPy from Torch: {np_from_torch}\")\n",
    "\n",
    "# Safe copy (no shared memory)\n",
    "np_safe = np.array([7., 8., 9.])\n",
    "torch_copy = torch.tensor(np_safe)  # Creates a copy\n",
    "np_safe[0] = 0\n",
    "print(f\"\\nSafe copy (using torch.tensor):\")\n",
    "print(f\"NumPy (modified): {np_safe}\")\n",
    "print(f\"Torch (unchanged): {torch_copy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Tensors\n",
    "\n",
    "One of PyTorch's killer features is seamless GPU support. Moving tensors to GPU is simple:\n",
    "\n",
    "```python\n",
    "# Move to GPU\n",
    "x_gpu = x.to('cuda')        # Or x.cuda()\n",
    "\n",
    "# Move back to CPU\n",
    "x_cpu = x_gpu.to('cpu')     # Or x_gpu.cpu()\n",
    "\n",
    "# Device-agnostic code\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x = x.to(device)\n",
    "```\n",
    "\n",
    "**Note:** All tensors in an operation must be on the same device!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU tensors (works even without GPU - just stays on CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create tensor on the best available device\n",
    "x = torch.randn(3, 3, device=device)\n",
    "print(f\"\\nTensor on {x.device}:\\n{x}\")\n",
    "\n",
    "# Or move an existing tensor\n",
    "y = torch.randn(3, 3)\n",
    "y = y.to(device)\n",
    "print(f\"\\nMoved tensor to {y.device}\")\n",
    "\n",
    "# Operations happen on the same device\n",
    "z = x @ y\n",
    "print(f\"Result on {z.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Tensor Shapes\n",
    "\n",
    "Understanding tensor shapes is crucial. Let's visualize how different tensors are organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing tensor dimensions\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# 0D: Scalar\n",
    "ax = axes[0]\n",
    "ax.text(0.5, 0.5, '42', fontsize=40, ha='center', va='center',\n",
    "        bbox=dict(boxstyle='circle', facecolor='lightblue', edgecolor='blue', linewidth=2))\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('0D Tensor (Scalar)\\nshape: ()', fontsize=12)\n",
    "ax.axis('off')\n",
    "\n",
    "# 1D: Vector\n",
    "ax = axes[1]\n",
    "vector = [1, 2, 3, 4]\n",
    "for i, v in enumerate(vector):\n",
    "    ax.add_patch(plt.Rectangle((i, 0), 0.9, 0.9, facecolor='lightgreen', edgecolor='green', linewidth=2))\n",
    "    ax.text(i + 0.45, 0.45, str(v), ha='center', va='center', fontsize=14)\n",
    "ax.set_xlim(-0.2, 4.2)\n",
    "ax.set_ylim(-0.2, 1.2)\n",
    "ax.set_title('1D Tensor (Vector)\\nshape: (4,)', fontsize=12)\n",
    "ax.axis('off')\n",
    "\n",
    "# 2D: Matrix\n",
    "ax = axes[2]\n",
    "matrix = [[1, 2, 3], [4, 5, 6]]\n",
    "for i, row in enumerate(matrix):\n",
    "    for j, v in enumerate(row):\n",
    "        ax.add_patch(plt.Rectangle((j, 1-i), 0.9, 0.9, facecolor='lightyellow', edgecolor='orange', linewidth=2))\n",
    "        ax.text(j + 0.45, 1.45 - i, str(v), ha='center', va='center', fontsize=14)\n",
    "ax.set_xlim(-0.2, 3.2)\n",
    "ax.set_ylim(-0.2, 2.2)\n",
    "ax.set_title('2D Tensor (Matrix)\\nshape: (2, 3)', fontsize=12)\n",
    "ax.axis('off')\n",
    "\n",
    "# 3D: Batch of matrices\n",
    "ax = axes[3]\n",
    "colors = ['lightcoral', 'lightblue']\n",
    "offsets = [(0, 0), (0.4, 0.4)]\n",
    "for batch, (ox, oy) in enumerate(offsets):\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.add_patch(plt.Rectangle((j + ox, 1 - i + oy), 0.9, 0.9, \n",
    "                                       facecolor=colors[batch], edgecolor='gray', linewidth=1, alpha=0.8))\n",
    "ax.set_xlim(-0.2, 3)\n",
    "ax.set_ylim(-0.2, 3)\n",
    "ax.set_title('3D Tensor\\nshape: (2, 2, 2)', fontsize=12)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tensor_shapes.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Common tensor shapes in deep learning:\")\n",
    "print(\"  (batch, features)          - Tabular data\")\n",
    "print(\"  (batch, channels, H, W)    - Images\")\n",
    "print(\"  (batch, seq_len, features) - Sequences/text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Autograd: Automatic Differentiation\n",
    "\n",
    "### The Magic of Automatic Gradients\n",
    "\n",
    "Remember computing gradients by hand in your from-scratch implementations? You had to:\n",
    "1. Derive the gradient formula for each operation\n",
    "2. Implement the backward pass manually\n",
    "3. Chain rule everything together\n",
    "\n",
    "**Autograd does all of this automatically!**\n",
    "\n",
    "### How It Works (Intuition)\n",
    "\n",
    "When you set `requires_grad=True` on a tensor, PyTorch:\n",
    "1. **Records** every operation you perform on that tensor\n",
    "2. **Builds** a computation graph in the background\n",
    "3. **Computes** gradients automatically when you call `.backward()`\n",
    "\n",
    "It's like having a perfect calculus student watching everything you do and computing derivatives on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic autograd example\n",
    "print(\"Basic Autograd Example\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a tensor with gradient tracking\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "print(f\"x = {x.item()}, requires_grad = {x.requires_grad}\")\n",
    "\n",
    "# Perform some operations: y = x^2 + 3x + 1\n",
    "y = x**2 + 3*x + 1\n",
    "print(f\"y = x^2 + 3x + 1 = {y.item()}\")\n",
    "\n",
    "# Compute gradients: dy/dx = 2x + 3 = 2(2) + 3 = 7\n",
    "y.backward()\n",
    "\n",
    "print(f\"\\nGradient dy/dx at x=2: {x.grad.item()}\")\n",
    "print(f\"Manual calculation: 2*x + 3 = 2*2 + 3 = 7 âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Three Key Concepts\n",
    "\n",
    "| Concept | Code | What it does |\n",
    "|---------|------|-------------|\n",
    "| **Enable tracking** | `requires_grad=True` | Tells PyTorch to record operations |\n",
    "| **Compute gradients** | `.backward()` | Runs backpropagation |\n",
    "| **Access gradients** | `.grad` | Gets the computed gradient |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex example with multiple variables\n",
    "print(\"Autograd with Multiple Variables\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Two input variables\n",
    "a = torch.tensor([3.0], requires_grad=True)\n",
    "b = torch.tensor([4.0], requires_grad=True)\n",
    "\n",
    "# Complex expression: c = a^2 * b + sin(b)\n",
    "c = a**2 * b + torch.sin(b)\n",
    "print(f\"a = {a.item()}, b = {b.item()}\")\n",
    "print(f\"c = a^2 * b + sin(b) = {c.item():.4f}\")\n",
    "\n",
    "# Compute gradients\n",
    "c.backward()\n",
    "\n",
    "# dc/da = 2*a*b = 2*3*4 = 24\n",
    "print(f\"\\ndc/da = {a.grad.item():.4f}\")\n",
    "print(f\"Manual: 2*a*b = 2*3*4 = 24.0 âœ“\")\n",
    "\n",
    "# dc/db = a^2 + cos(b) = 9 + cos(4)\n",
    "print(f\"\\ndc/db = {b.grad.item():.4f}\")\n",
    "print(f\"Manual: a^2 + cos(b) = 9 + cos(4) = {9 + np.cos(4):.4f} âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: How Autograd Works Under the Hood\n",
    "\n",
    "PyTorch builds a **dynamic computation graph** as you perform operations. Each node in the graph knows:\n",
    "1. What operation created it\n",
    "2. What its inputs were\n",
    "3. How to compute its local gradient\n",
    "\n",
    "When you call `.backward()`, PyTorch walks backward through this graph, applying the chain rule automatically.\n",
    "\n",
    "#### Key Insight\n",
    "\n",
    "The graph is **dynamic** - it's rebuilt fresh each forward pass. This is different from TensorFlow 1.x where graphs were static. Dynamic graphs are:\n",
    "- Easier to debug (use regular Python debugger)\n",
    "- More flexible (control flow just works)\n",
    "- More intuitive (code runs line by line)\n",
    "\n",
    "#### Common Misconceptions\n",
    "\n",
    "| Misconception | Reality |\n",
    "|---------------|--------|\n",
    "| \"Autograd is magic\" | It's just the chain rule applied systematically |\n",
    "| \"Gradients accumulate by default\" | Yes! Call `.zero_()` before each backward pass |\n",
    "| \"I need to track all tensors\" | Only track tensors you want to differentiate w.r.t. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the computation graph\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Draw computation graph for y = (a*x + b)^2\n",
    "nodes = {\n",
    "    'x': (1, 4),\n",
    "    'a': (1, 2),\n",
    "    'b': (3, 2),\n",
    "    'ax': (2, 3),\n",
    "    'ax+b': (3, 4),\n",
    "    'y=(ax+b)^2': (4, 3)\n",
    "}\n",
    "\n",
    "edges = [\n",
    "    ('x', 'ax', 'x'),\n",
    "    ('a', 'ax', 'a'),\n",
    "    ('ax', 'ax+b', ''),\n",
    "    ('b', 'ax+b', 'b'),\n",
    "    ('ax+b', 'y=(ax+b)^2', '')\n",
    "]\n",
    "\n",
    "# Draw nodes\n",
    "for name, (x, y) in nodes.items():\n",
    "    color = 'lightgreen' if name in ['x', 'a', 'b'] else 'lightblue'\n",
    "    ax.add_patch(plt.Circle((x, y), 0.3, facecolor=color, edgecolor='black', linewidth=2))\n",
    "    ax.text(x, y, name, ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Draw edges with gradient labels\n",
    "gradient_labels = {\n",
    "    ('x', 'ax'): 'dy/dx = a*2(ax+b)',\n",
    "    ('a', 'ax'): 'dy/da = x*2(ax+b)',\n",
    "    ('b', 'ax+b'): 'dy/db = 2(ax+b)'\n",
    "}\n",
    "\n",
    "for start, end, _ in edges:\n",
    "    x1, y1 = nodes[start]\n",
    "    x2, y2 = nodes[end]\n",
    "    ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n",
    "                arrowprops=dict(arrowstyle='->', color='gray', lw=2))\n",
    "\n",
    "# Add gradient flow arrows (backward direction)\n",
    "ax.annotate('', xy=(3.3, 3.8), xytext=(3.7, 3.3),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "ax.text(3.7, 3.5, 'gradient\\nflows back', color='red', fontsize=10, ha='left')\n",
    "\n",
    "ax.set_xlim(0, 5.5)\n",
    "ax.set_ylim(1, 5)\n",
    "ax.set_title('Computation Graph: y = (ax + b)^2\\n(Gradients flow backward through the graph)', fontsize=14)\n",
    "ax.axis('off')\n",
    "\n",
    "# Add legend\n",
    "ax.add_patch(plt.Circle((0.5, 1.5), 0.15, facecolor='lightgreen', edgecolor='black'))\n",
    "ax.text(0.8, 1.5, 'Input (leaf) tensors', va='center', fontsize=10)\n",
    "ax.add_patch(plt.Circle((0.5, 1.2), 0.15, facecolor='lightblue', edgecolor='black'))\n",
    "ax.text(0.8, 1.2, 'Intermediate values', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('computation_graph.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient accumulation demo - a common gotcha!\n",
    "print(\"Gradient Accumulation (Important!)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "w = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# First backward pass\n",
    "y1 = w * 2\n",
    "y1.backward()\n",
    "print(f\"After first backward: w.grad = {w.grad.item()}\")\n",
    "\n",
    "# Second backward pass WITHOUT zeroing gradients\n",
    "y2 = w * 2\n",
    "y2.backward()\n",
    "print(f\"After second backward (accumulated!): w.grad = {w.grad.item()}\")\n",
    "\n",
    "# The right way: zero gradients first\n",
    "w.grad.zero_()  # Reset to zero\n",
    "y3 = w * 2\n",
    "y3.backward()\n",
    "print(f\"After zeroing and backward: w.grad = {w.grad.item()}\")\n",
    "\n",
    "print(\"\\nâš ï¸ Always zero gradients before each training step!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disabling Gradient Tracking\n",
    "\n",
    "During inference (testing), you don't need gradients. Disabling them:\n",
    "- Saves memory\n",
    "- Speeds up computation\n",
    "\n",
    "Two ways to disable:\n",
    "\n",
    "| Method | Use case |\n",
    "|--------|----------|\n",
    "| `with torch.no_grad():` | Temporarily disable (inference) |\n",
    "| `.detach()` | Remove tensor from graph permanently |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabling gradients\n",
    "print(\"Disabling Gradient Tracking\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "# Normal: gradients tracked\n",
    "y = x * 2\n",
    "print(f\"Normal: y.requires_grad = {y.requires_grad}\")\n",
    "\n",
    "# With no_grad context: no tracking\n",
    "with torch.no_grad():\n",
    "    y_no_grad = x * 2\n",
    "    print(f\"In no_grad: y.requires_grad = {y_no_grad.requires_grad}\")\n",
    "\n",
    "# Back to normal after the context\n",
    "y_after = x * 2\n",
    "print(f\"After no_grad: y.requires_grad = {y_after.requires_grad}\")\n",
    "\n",
    "# Detach: permanently removes from graph\n",
    "y_detached = y.detach()\n",
    "print(f\"\\nDetached: y_detached.requires_grad = {y_detached.requires_grad}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Use torch.no_grad() during inference to save memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd Example: Gradient Descent from Scratch\n",
    "\n",
    "Let's use autograd to minimize a function, just like we did by hand before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent using autograd\n",
    "print(\"Gradient Descent with Autograd\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Minimize f(x) = (x - 3)^2 + 1\n",
    "# Minimum at x = 3\n",
    "\n",
    "x = torch.tensor([0.0], requires_grad=True)\n",
    "learning_rate = 0.1\n",
    "history = {'x': [], 'loss': []}\n",
    "\n",
    "for step in range(50):\n",
    "    # Forward pass: compute loss\n",
    "    loss = (x - 3)**2 + 1\n",
    "    \n",
    "    # Record history\n",
    "    history['x'].append(x.item())\n",
    "    history['loss'].append(loss.item())\n",
    "    \n",
    "    # Backward pass: compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update x using gradient descent\n",
    "    with torch.no_grad():  # Don't track this operation\n",
    "        x -= learning_rate * x.grad\n",
    "    \n",
    "    # Zero gradient for next iteration\n",
    "    x.grad.zero_()\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step:2d}: x = {history['x'][-1]:.4f}, loss = {history['loss'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal x = {x.item():.4f} (should be close to 3.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the gradient descent\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Function and optimization path\n",
    "ax = axes[0]\n",
    "x_range = np.linspace(-1, 5, 100)\n",
    "y_range = (x_range - 3)**2 + 1\n",
    "ax.plot(x_range, y_range, 'b-', linewidth=2, label='f(x) = (x-3)^2 + 1')\n",
    "ax.scatter(history['x'], history['loss'], c=range(len(history['x'])), \n",
    "           cmap='Reds', s=50, zorder=5, label='Gradient descent path')\n",
    "ax.scatter([3], [1], color='green', s=200, marker='*', zorder=6, label='Minimum')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('Gradient Descent Finding the Minimum')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Loss over time\n",
    "ax = axes[1]\n",
    "ax.plot(history['loss'], 'b-', linewidth=2)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Loss Decreasing Over Time')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('autograd_gd.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. nn.Module: Building Blocks\n",
    "\n",
    "### Why Use nn.Module?\n",
    "\n",
    "You could build neural networks using raw tensors and autograd. But `nn.Module` provides:\n",
    "\n",
    "| Feature | Benefit |\n",
    "|---------|--------|\n",
    "| **Parameter management** | Automatically tracks all learnable weights |\n",
    "| **GPU movement** | `.to(device)` moves all parameters at once |\n",
    "| **Training/eval modes** | `.train()` / `.eval()` for dropout, batchnorm |\n",
    "| **Hooks** | Inspect activations, gradients during forward/backward |\n",
    "| **Serialization** | Easy save/load with state_dict |\n",
    "\n",
    "### The Basic Pattern\n",
    "\n",
    "Every neural network module follows the same pattern:\n",
    "\n",
    "```python\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define layers here\n",
    "        self.layer1 = nn.Linear(10, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define computation here\n",
    "        return self.layer1(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a simple custom module\n",
    "print(\"Building Custom Modules\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class SimpleLinear(nn.Module):\n",
    "    \"\"\"A simple linear layer: y = Wx + b (what you built from scratch!)\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        # nn.Parameter tells PyTorch these are learnable\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Same computation as your from-scratch implementation!\n",
    "        return x @ self.weight.T + self.bias\n",
    "\n",
    "# Create and test\n",
    "layer = SimpleLinear(3, 2)\n",
    "x = torch.randn(5, 3)  # 5 samples, 3 features\n",
    "output = layer(x)      # Calls forward() automatically\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nWeight shape: {layer.weight.shape}\")\n",
    "print(f\"Bias shape:   {layer.bias.shape}\")\n",
    "\n",
    "# List all parameters\n",
    "print(\"\\nAll parameters:\")\n",
    "for name, param in layer.named_parameters():\n",
    "    print(f\"  {name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Built-in Layers\n",
    "\n",
    "PyTorch provides optimized versions of common layers:\n",
    "\n",
    "| Layer | Purpose | Parameters |\n",
    "|-------|---------|------------|\n",
    "| `nn.Linear(in, out)` | Fully connected | W: (out, in), b: (out,) |\n",
    "| `nn.Conv2d(in_ch, out_ch, kernel)` | 2D convolution | W: (out_ch, in_ch, k, k) |\n",
    "| `nn.ReLU()` | Activation | None |\n",
    "| `nn.Sigmoid()` | Activation | None |\n",
    "| `nn.Dropout(p)` | Regularization | None |\n",
    "| `nn.BatchNorm1d(features)` | Normalization | gamma, beta |\n",
    "| `nn.Embedding(vocab, dim)` | Word embeddings | (vocab, dim) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring common layers\n",
    "print(\"Common PyTorch Layers\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Linear layer\n",
    "linear = nn.Linear(10, 5)\n",
    "x = torch.randn(32, 10)  # Batch of 32\n",
    "print(f\"nn.Linear(10, 5):\")\n",
    "print(f\"  Input:  {x.shape}\")\n",
    "print(f\"  Output: {linear(x).shape}\")\n",
    "print(f\"  Params: weight {linear.weight.shape}, bias {linear.bias.shape}\")\n",
    "\n",
    "# ReLU activation\n",
    "relu = nn.ReLU()\n",
    "x = torch.tensor([-2., -1., 0., 1., 2.])\n",
    "print(f\"\\nnn.ReLU():\")\n",
    "print(f\"  Input:  {x}\")\n",
    "print(f\"  Output: {relu(x)}\")\n",
    "\n",
    "# Dropout (training mode)\n",
    "dropout = nn.Dropout(p=0.5)  # 50% dropout\n",
    "x = torch.ones(10)\n",
    "print(f\"\\nnn.Dropout(0.5) in training mode:\")\n",
    "dropout.train()  # Enable dropout\n",
    "print(f\"  Input:  {x}\")\n",
    "print(f\"  Output: {dropout(x)}\")  # Some values zeroed and others scaled\n",
    "dropout.eval()  # Disable dropout\n",
    "print(f\"  In eval mode: {dropout(x)}\")  # No dropout applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequential: Quick Model Building\n",
    "\n",
    "For simple feed-forward networks, `nn.Sequential` lets you stack layers without writing a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential for quick models\n",
    "print(\"nn.Sequential Example\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# A simple MLP using Sequential\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(784, 256),   # Input layer\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(256, 128),   # Hidden layer\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(128, 10)     # Output layer\n",
    ")\n",
    "\n",
    "print(mlp)\n",
    "\n",
    "# Test it\n",
    "x = torch.randn(32, 784)  # Batch of 32 flattened MNIST images\n",
    "output = mlp(x)\n",
    "print(f\"\\nInput shape:  {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in mlp.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Building a Complete Model\n",
    "\n",
    "Let's build a proper neural network class that combines everything:\n",
    "- Multiple layers\n",
    "- Activations\n",
    "- Clear forward method\n",
    "\n",
    "This is equivalent to what you built from scratch, but now PyTorch handles all the gradient computation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A complete neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A flexible feed-forward neural network.\n",
    "    \n",
    "    This is the PyTorch equivalent of the network you built from scratch!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Number of input features\n",
    "            hidden_dims: List of hidden layer sizes\n",
    "            output_dim: Number of output classes/values\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Build layers dynamically\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        return self.network(x)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count total trainable parameters.\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = NeuralNetwork(\n",
    "    input_dim=784,      # 28x28 flattened image\n",
    "    hidden_dims=[256, 128, 64],  # Three hidden layers\n",
    "    output_dim=10       # 10 digit classes\n",
    ")\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {model.count_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "print(\"\\nTesting the model:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create dummy input\n",
    "batch_size = 32\n",
    "x = torch.randn(batch_size, 784)\n",
    "\n",
    "# Forward pass\n",
    "model.eval()  # Set to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output = model(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Get predictions\n",
    "probabilities = F.softmax(output, dim=1)\n",
    "predictions = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "print(f\"\\nSample output (raw logits): {output[0][:5]}...\")  # First 5 logits\n",
    "print(f\"Sample probabilities: {probabilities[0][:5]}...\")\n",
    "print(f\"Predicted class: {predictions[0].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the network architecture\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Layer sizes for our model\n",
    "layer_sizes = [784, 256, 128, 64, 10]\n",
    "layer_names = ['Input\\n(784)', 'Hidden 1\\n(256)', 'Hidden 2\\n(128)', 'Hidden 3\\n(64)', 'Output\\n(10)']\n",
    "\n",
    "# Normalize for visualization (show fewer neurons)\n",
    "max_neurons_display = 8\n",
    "display_sizes = [min(s, max_neurons_display) for s in layer_sizes]\n",
    "\n",
    "# Draw layers\n",
    "x_positions = np.linspace(1, 9, len(layer_sizes))\n",
    "colors = ['lightgreen', 'lightblue', 'lightblue', 'lightblue', 'lightyellow']\n",
    "\n",
    "for layer_idx, (x_pos, n_neurons, color, name) in enumerate(zip(x_positions, display_sizes, colors, layer_names)):\n",
    "    y_positions = np.linspace(1, 7, n_neurons)\n",
    "    \n",
    "    for y_pos in y_positions:\n",
    "        circle = plt.Circle((x_pos, y_pos), 0.2, facecolor=color, edgecolor='black', linewidth=1.5)\n",
    "        ax.add_patch(circle)\n",
    "    \n",
    "    # Add \"...\" if we're not showing all neurons\n",
    "    if layer_sizes[layer_idx] > max_neurons_display:\n",
    "        ax.text(x_pos, 0.4, '...', ha='center', va='center', fontsize=16)\n",
    "    \n",
    "    # Layer name\n",
    "    ax.text(x_pos, 8, name, ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Draw connections to next layer (simplified)\n",
    "    if layer_idx < len(layer_sizes) - 1:\n",
    "        next_x = x_positions[layer_idx + 1]\n",
    "        next_neurons = display_sizes[layer_idx + 1]\n",
    "        next_y_positions = np.linspace(1, 7, next_neurons)\n",
    "        \n",
    "        # Draw a subset of connections\n",
    "        for i, y1 in enumerate(y_positions[::2]):  # Every other neuron\n",
    "            for j, y2 in enumerate(next_y_positions[::2]):\n",
    "                ax.plot([x_pos + 0.2, next_x - 0.2], [y1, y2], 'gray', alpha=0.2, linewidth=0.5)\n",
    "\n",
    "# Add ReLU labels\n",
    "for i in range(3):\n",
    "    x_mid = (x_positions[i] + x_positions[i+1]) / 2\n",
    "    ax.text(x_mid, 0, 'ReLU', ha='center', va='center', fontsize=9,\n",
    "            bbox=dict(boxstyle='round', facecolor='white', edgecolor='gray'))\n",
    "\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(-0.5, 9)\n",
    "ax.set_title('Neural Network Architecture\\n(784 -> 256 -> 128 -> 64 -> 10)', fontsize=14)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('network_architecture.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Training Loop Anatomy\n",
    "\n",
    "### The Core Training Loop\n",
    "\n",
    "Every PyTorch training loop follows the same pattern:\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        # 1. Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 2. Backward pass\n",
    "        optimizer.zero_grad()  # Clear old gradients\n",
    "        loss.backward()        # Compute new gradients\n",
    "        \n",
    "        # 3. Update weights\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "### Breaking Down Each Step\n",
    "\n",
    "| Step | What happens | PyTorch code |\n",
    "|------|--------------|-------------|\n",
    "| **Forward** | Input flows through network | `outputs = model(inputs)` |\n",
    "| **Loss** | Compare predictions to targets | `loss = criterion(outputs, targets)` |\n",
    "| **Zero grad** | Clear accumulated gradients | `optimizer.zero_grad()` |\n",
    "| **Backward** | Compute gradients via backprop | `loss.backward()` |\n",
    "| **Step** | Update weights using gradients | `optimizer.step()` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training loop\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Draw training loop steps\n",
    "steps = [\n",
    "    ('1. Forward Pass', 'Input -> Model -> Predictions', 'lightgreen'),\n",
    "    ('2. Compute Loss', 'Compare predictions to targets', 'lightyellow'),\n",
    "    ('3. Zero Gradients', 'Clear old gradients', 'lightgray'),\n",
    "    ('4. Backward Pass', 'Compute gradients (backprop)', 'lightcoral'),\n",
    "    ('5. Optimizer Step', 'Update weights', 'lightblue'),\n",
    "]\n",
    "\n",
    "y_positions = np.linspace(7, 1, len(steps))\n",
    "box_width = 4\n",
    "box_height = 0.8\n",
    "\n",
    "for i, (title, desc, color) in enumerate(steps):\n",
    "    y = y_positions[i]\n",
    "    \n",
    "    # Main box\n",
    "    rect = plt.Rectangle((1, y - box_height/2), box_width, box_height,\n",
    "                         facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(3, y, title, ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Description\n",
    "    ax.text(5.5, y, desc, ha='left', va='center', fontsize=11)\n",
    "    \n",
    "    # Arrow to next step\n",
    "    if i < len(steps) - 1:\n",
    "        ax.annotate('', xy=(3, y_positions[i+1] + box_height/2 + 0.1),\n",
    "                   xytext=(3, y - box_height/2 - 0.1),\n",
    "                   arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "\n",
    "# Add loop arrow\n",
    "ax.annotate('', xy=(0.5, y_positions[0]),\n",
    "           xytext=(0.5, y_positions[-1]),\n",
    "           arrowprops=dict(arrowstyle='->', color='blue', lw=2,\n",
    "                          connectionstyle='arc3,rad=0.3'))\n",
    "ax.text(0.2, 4, 'Repeat\\nfor each\\nbatch', ha='center', va='center', fontsize=10, color='blue')\n",
    "\n",
    "ax.set_xlim(-0.5, 10)\n",
    "ax.set_ylim(0, 8.5)\n",
    "ax.set_title('The PyTorch Training Loop', fontsize=14, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_loop.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Training Example on Synthetic Data\n",
    "\n",
    "Let's train a model on synthetic data to see the full loop in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "print(\"Creating Synthetic Dataset\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Binary classification: predict if x is inside or outside a circle\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate random points\n",
    "X = np.random.randn(n_samples, 2) * 2\n",
    "\n",
    "# Labels: 1 if inside circle of radius 1.5, else 0\n",
    "distances = np.sqrt(X[:, 0]**2 + X[:, 1]**2)\n",
    "y = (distances < 1.5).astype(np.float32)\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# Split into train/test\n",
    "train_size = int(0.8 * n_samples)\n",
    "X_train, X_test = X_tensor[:train_size], X_tensor[train_size:]\n",
    "y_train, y_test = y_tensor[:train_size], y_tensor[train_size:]\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples:     {len(X_test)}\")\n",
    "print(f\"Input features:   {X_train.shape[1]}\")\n",
    "print(f\"Class balance:    {y_train.mean().item():.2%} positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "colors = ['red' if label == 0 else 'blue' for label in y]\n",
    "ax.scatter(X[:, 0], X[:, 1], c=colors, alpha=0.5, s=20)\n",
    "\n",
    "# Draw the decision boundary (circle)\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "ax.plot(1.5 * np.cos(theta), 1.5 * np.sin(theta), 'g--', linewidth=2, label='True boundary')\n",
    "\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_title('Synthetic Dataset: Points Inside (blue) vs Outside (red) Circle')\n",
    "ax.legend()\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('synthetic_data.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model for our binary classification task\n",
    "class BinaryClassifier(nn.Module):\n",
    "    \"\"\"Simple binary classifier for our circle dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()  # Output probability\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Create model, loss function, and optimizer\n",
    "model = BinaryClassifier()\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Model:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE TRAINING LOOP\n",
    "print(\"\\nTraining Loop\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "n_batches = len(X_train) // batch_size\n",
    "\n",
    "# History for plotting\n",
    "history = {'train_loss': [], 'test_loss': [], 'test_acc': []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set to training mode\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Shuffle data\n",
    "    perm = torch.randperm(len(X_train))\n",
    "    X_shuffled = X_train[perm]\n",
    "    y_shuffled = y_train[perm]\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        # Get batch\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        X_batch = X_shuffled[start:end]\n",
    "        y_batch = y_shuffled[start:end]\n",
    "        \n",
    "        # ====== THE FIVE KEY STEPS ======\n",
    "        \n",
    "        # 1. Forward pass\n",
    "        predictions = model(X_batch)\n",
    "        \n",
    "        # 2. Compute loss\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        \n",
    "        # 3. Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 4. Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # ================================\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        test_pred = model(X_test)\n",
    "        test_loss = criterion(test_pred, y_test).item()\n",
    "        test_acc = ((test_pred > 0.5) == y_test).float().mean().item()\n",
    "    \n",
    "    # Record history\n",
    "    history['train_loss'].append(epoch_loss / n_batches)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    if epoch % 20 == 0 or epoch == num_epochs - 1:\n",
    "        print(f\"Epoch {epoch:3d}: Train Loss = {epoch_loss/n_batches:.4f}, \"\n",
    "              f\"Test Loss = {test_loss:.4f}, Test Acc = {test_acc:.2%}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss curves\n",
    "ax = axes[0]\n",
    "ax.plot(history['train_loss'], label='Train Loss', color='blue')\n",
    "ax.plot(history['test_loss'], label='Test Loss', color='red')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training and Test Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax = axes[1]\n",
    "ax.plot(history['test_acc'], color='green')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Test Accuracy')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Decision boundary\n",
    "ax = axes[2]\n",
    "\n",
    "# Create a mesh grid\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                      np.linspace(y_min, y_max, 100))\n",
    "\n",
    "# Get predictions on the grid\n",
    "grid_tensor = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    Z = model(grid_tensor).numpy().reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "ax.contourf(xx, yy, Z, levels=50, cmap='RdBu', alpha=0.6)\n",
    "ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "\n",
    "# Plot test points\n",
    "colors_test = ['red' if label == 0 else 'blue' for label in y_test.numpy()]\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=colors_test, alpha=0.7, s=30, edgecolors='white', linewidth=0.5)\n",
    "\n",
    "# True boundary\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "ax.plot(1.5 * np.cos(theta), 1.5 * np.sin(theta), 'g--', linewidth=2, label='True boundary')\n",
    "\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_title('Learned Decision Boundary')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to Your From-Scratch Implementation\n",
    "\n",
    "Compare what you did manually vs what PyTorch does:\n",
    "\n",
    "| From Scratch | PyTorch | Notes |\n",
    "|--------------|---------|-------|\n",
    "| Manually derived gradients | `loss.backward()` | Autograd computes automatically |\n",
    "| Stored intermediate values | Built into computation graph | PyTorch handles this |\n",
    "| `weights -= lr * gradients` | `optimizer.step()` | Optimizer handles updates |\n",
    "| Implemented ReLU, Sigmoid | `nn.ReLU()`, `nn.Sigmoid()` | Pre-built, optimized |\n",
    "| Tracked loss manually | Same | You still do this |\n",
    "\n",
    "**Key insight:** The conceptual steps are identical! PyTorch just automates the tedious parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Saving and Loading Models\n",
    "\n",
    "After training, you'll want to save your model for later use. PyTorch offers two approaches:\n",
    "\n",
    "| Method | What it saves | Use case |\n",
    "|--------|--------------|----------|\n",
    "| `state_dict()` | Just the weights | Recommended for most cases |\n",
    "| `torch.save(model)` | Entire model | Convenient but less flexible |\n",
    "\n",
    "### Best Practice: Save state_dict()\n",
    "\n",
    "The `state_dict()` contains all learnable parameters as a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining state_dict\n",
    "print(\"Model State Dict\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "for name, tensor in state_dict.items():\n",
    "    print(f\"{name}: {tensor.shape}\")\n",
    "\n",
    "print(f\"\\nTotal parameters: {sum(t.numel() for t in state_dict.values()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and loading\n",
    "print(\"\\nSaving and Loading Models\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Save\n",
    "save_path = 'circle_classifier.pth'\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "\n",
    "# Load into a new model\n",
    "new_model = BinaryClassifier()  # Create same architecture\n",
    "new_model.load_state_dict(torch.load(save_path, weights_only=True))\n",
    "new_model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Verify it works\n",
    "with torch.no_grad():\n",
    "    original_pred = model(X_test[:5])\n",
    "    loaded_pred = new_model(X_test[:5])\n",
    "\n",
    "print(f\"\\nVerification (predictions should match):\")\n",
    "print(f\"Original: {original_pred.flatten().numpy()}\")\n",
    "print(f\"Loaded:   {loaded_pred.flatten().numpy()}\")\n",
    "print(f\"Match: {torch.allclose(original_pred, loaded_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving checkpoints with more information\n",
    "print(\"\\nSaving Complete Checkpoint\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "checkpoint = {\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_loss': history['train_loss'][-1],\n",
    "    'test_acc': history['test_acc'][-1],\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')\n",
    "print(\"Checkpoint saved with:\")\n",
    "for key in checkpoint.keys():\n",
    "    print(f\"  - {key}\")\n",
    "\n",
    "# Loading checkpoint\n",
    "print(\"\\nLoading checkpoint:\")\n",
    "loaded_checkpoint = torch.load('checkpoint.pth', weights_only=False)\n",
    "print(f\"  Trained for {loaded_checkpoint['epoch']} epochs\")\n",
    "print(f\"  Final test accuracy: {loaded_checkpoint['test_acc']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: state_dict vs Pickling the Model\n",
    "\n",
    "You might be tempted to just save the entire model:\n",
    "\n",
    "```python\n",
    "# This works but has drawbacks\n",
    "torch.save(model, 'model.pth')\n",
    "model = torch.load('model.pth')\n",
    "```\n",
    "\n",
    "Why `state_dict()` is better:\n",
    "\n",
    "| Issue | Saving entire model | Saving state_dict |\n",
    "|-------|---------------------|------------------|\n",
    "| **File includes** | Code + weights | Just weights |\n",
    "| **Code changes** | Breaks loading | No problem |\n",
    "| **Different PyTorch version** | May break | Usually works |\n",
    "| **Flexibility** | Fixed architecture | Load into any compatible architecture |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Tensor Warmup\n",
    "\n",
    "Create and manipulate tensors to get comfortable with the PyTorch API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 1: Tensor Warmup\n",
    "\n",
    "def tensor_exercises():\n",
    "    \"\"\"\n",
    "    Complete the following tensor operations:\n",
    "    \n",
    "    1. Create a 3x4 tensor of ones\n",
    "    2. Create a 4x5 tensor of random values (normal distribution)\n",
    "    3. Multiply them together (matrix multiplication)\n",
    "    4. Take the sum along dimension 1\n",
    "    5. Return the result\n",
    "    \"\"\"\n",
    "    # TODO: Implement this!\n",
    "    # Hint: Use torch.ones(), torch.randn(), @, and .sum()\n",
    "    \n",
    "    # Step 1: Create ones tensor\n",
    "    a = None  # Your code here\n",
    "    \n",
    "    # Step 2: Create random tensor\n",
    "    b = None  # Your code here\n",
    "    \n",
    "    # Step 3: Matrix multiplication\n",
    "    c = None  # Your code here\n",
    "    \n",
    "    # Step 4: Sum along dimension 1\n",
    "    result = None  # Your code here\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test\n",
    "torch.manual_seed(42)\n",
    "result = tensor_exercises()\n",
    "print(f\"Your result shape: {result.shape if result is not None else 'None'}\")\n",
    "print(f\"Expected shape: torch.Size([3])\")\n",
    "print(f\"Your result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Manual Gradient Verification\n",
    "\n",
    "Use autograd to verify your calculus skills!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 2: Gradient Verification\n",
    "\n",
    "def verify_gradient(x_value, function_str):\n",
    "    \"\"\"\n",
    "    Given a function string, compute both:\n",
    "    1. The autograd gradient\n",
    "    2. Your manual calculation\n",
    "    \n",
    "    Function: f(x) = x^3 - 2x^2 + 3x - 1\n",
    "    Derivative: f'(x) = 3x^2 - 4x + 3\n",
    "    \n",
    "    Return: (autograd_gradient, manual_gradient)\n",
    "    \"\"\"\n",
    "    # Autograd computation\n",
    "    x = torch.tensor([x_value], requires_grad=True, dtype=torch.float32)\n",
    "    \n",
    "    # f(x) = x^3 - 2x^2 + 3x - 1\n",
    "    f = x**3 - 2*x**2 + 3*x - 1\n",
    "    f.backward()\n",
    "    autograd_gradient = x.grad.item()\n",
    "    \n",
    "    # TODO: Calculate the derivative manually\n",
    "    # f'(x) = 3x^2 - 4x + 3\n",
    "    manual_gradient = None  # Your code here\n",
    "    \n",
    "    return autograd_gradient, manual_gradient\n",
    "\n",
    "# Test at x = 2\n",
    "auto_grad, manual_grad = verify_gradient(2.0, \"x^3 - 2x^2 + 3x - 1\")\n",
    "print(f\"At x = 2:\")\n",
    "print(f\"  Autograd: {auto_grad}\")\n",
    "print(f\"  Manual:   {manual_grad}\")\n",
    "print(f\"  Match:    {np.isclose(auto_grad, manual_grad) if manual_grad else 'Not implemented'}\")\n",
    "\n",
    "# Test at x = -1\n",
    "auto_grad, manual_grad = verify_gradient(-1.0, \"x^3 - 2x^2 + 3x - 1\")\n",
    "print(f\"\\nAt x = -1:\")\n",
    "print(f\"  Autograd: {auto_grad}\")\n",
    "print(f\"  Manual:   {manual_grad}\")\n",
    "print(f\"  Match:    {np.isclose(auto_grad, manual_grad) if manual_grad else 'Not implemented'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Build a Custom Module\n",
    "\n",
    "Implement a module that applies a custom transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 3: Custom Module\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement the Swish activation function:\n",
    "    swish(x) = x * sigmoid(x)\n",
    "    \n",
    "    This is a smooth approximation to ReLU that often performs better!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # No learnable parameters needed\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement swish(x) = x * sigmoid(x)\n",
    "        # Hint: Use torch.sigmoid()\n",
    "        pass\n",
    "\n",
    "# Test\n",
    "swish = Swish()\n",
    "x = torch.tensor([-2., -1., 0., 1., 2.])\n",
    "output = swish(x)\n",
    "\n",
    "print(f\"Input:  {x}\")\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "# Expected: approximately tensor([-0.2384, -0.2689, 0.0000, 0.7311, 1.7616])\n",
    "expected = x * torch.sigmoid(x)\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct: {torch.allclose(output, expected) if output is not None else 'Not implemented'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Complete Training Loop\n",
    "\n",
    "Build and train a model for a regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 4: Regression Training Loop\n",
    "\n",
    "# Generate regression data: y = 2*x + 1 + noise\n",
    "torch.manual_seed(42)\n",
    "X_reg = torch.randn(200, 1) * 2\n",
    "y_reg = 2 * X_reg + 1 + torch.randn(200, 1) * 0.5\n",
    "\n",
    "# TODO: Complete this training loop\n",
    "\n",
    "# 1. Create a simple linear model\n",
    "# Hint: nn.Linear(1, 1) is enough for this!\n",
    "reg_model = None  # Your code here\n",
    "\n",
    "# 2. Create MSE loss function\n",
    "# Hint: nn.MSELoss()\n",
    "criterion = None  # Your code here\n",
    "\n",
    "# 3. Create SGD optimizer with lr=0.01\n",
    "# Hint: torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "optimizer = None  # Your code here\n",
    "\n",
    "# 4. Training loop (fill in the 5 key steps)\n",
    "if reg_model and criterion and optimizer:\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        # Forward pass\n",
    "        predictions = None  # Your code\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = None  # Your code\n",
    "        \n",
    "        # Zero gradients\n",
    "        # Your code\n",
    "        \n",
    "        # Backward pass\n",
    "        # Your code\n",
    "        \n",
    "        # Update weights\n",
    "        # Your code\n",
    "        \n",
    "        if loss:\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        if epoch % 20 == 0 and loss:\n",
    "            print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    # Check learned parameters\n",
    "    with torch.no_grad():\n",
    "        learned_weight = reg_model.weight.item()\n",
    "        learned_bias = reg_model.bias.item()\n",
    "        print(f\"\\nLearned: y = {learned_weight:.2f}*x + {learned_bias:.2f}\")\n",
    "        print(f\"True:    y = 2.00*x + 1.00\")\n",
    "else:\n",
    "    print(\"Please implement the model, criterion, and optimizer first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Tensors** are PyTorch's version of NumPy arrays, with GPU support and automatic differentiation\n",
    "- **Autograd** automatically computes gradients by building a computation graph during the forward pass\n",
    "- **nn.Module** provides structure for organizing neural network components with automatic parameter tracking\n",
    "- **The training loop** has five key steps: forward, loss, zero_grad, backward, step\n",
    "- **state_dict()** is the recommended way to save and load model weights\n",
    "\n",
    "### Connection to Deep Learning\n",
    "\n",
    "| Concept | What you did from scratch | What PyTorch does |\n",
    "|---------|--------------------------|-------------------|\n",
    "| Forward pass | Implemented layer-by-layer | Same, but in `forward()` method |\n",
    "| Backpropagation | Derived and coded all gradients | `loss.backward()` does it automatically |\n",
    "| Weight updates | `w -= lr * grad` | `optimizer.step()` |\n",
    "| Layer operations | Wrote matrix multiplications | `nn.Linear`, `nn.Conv2d`, etc. |\n",
    "| Gradient tracking | Stored activations manually | Autograd tracks everything |\n",
    "\n",
    "### Checklist\n",
    "\n",
    "After completing this notebook, you should be able to:\n",
    "\n",
    "- [ ] Create tensors using various methods (zeros, ones, randn, from data)\n",
    "- [ ] Move tensors between CPU and GPU (conceptually)\n",
    "- [ ] Convert between NumPy arrays and PyTorch tensors\n",
    "- [ ] Use `requires_grad=True` to enable gradient tracking\n",
    "- [ ] Call `.backward()` to compute gradients\n",
    "- [ ] Access gradients via `.grad` attribute\n",
    "- [ ] Use `torch.no_grad()` for inference\n",
    "- [ ] Build custom modules by subclassing `nn.Module`\n",
    "- [ ] Use `nn.Sequential` for simple architectures\n",
    "- [ ] Write a complete training loop with the five key steps\n",
    "- [ ] Save and load model weights using `state_dict()`\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand PyTorch fundamentals, you're ready to:\n",
    "\n",
    "1. **Part 3.4: Training on Real Data** - Use DataLoader, work with real datasets, and implement proper training/validation splits\n",
    "\n",
    "2. **Part 4: Convolutional Neural Networks** - Apply what you've learned to image data with specialized architectures\n",
    "\n",
    "3. **Part 5: Recurrent Neural Networks** - Process sequential data like text and time series\n",
    "\n",
    "The foundation you've built - both from-scratch understanding and PyTorch skills - will serve you well as you tackle increasingly complex architectures!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up saved files\n",
    "import os\n",
    "for f in ['circle_classifier.pth', 'checkpoint.pth']:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "        print(f\"Removed {f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
