{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": "# Part 4.3: Recurrent Neural Networks (RNNs) â€” The Formula 1 Edition\n\nSequential data is everywhere: language, music, stock prices, weather, DNA. Unlike images where pixels exist in a grid, sequential data has an inherent **order** and **temporal context**. A word's meaning depends on the words before it. A stock price depends on its history. RNNs are the first architecture designed to handle this challenge by maintaining a **memory** of what came before.\n\n**F1 analogy:** A Grand Prix is the ultimate sequential data problem. Every lap depends on what happened in previous laps -- tire degradation accumulates, fuel burns off making the car lighter, track evolution (rubber laid down) changes grip levels. You cannot understand lap 45 without knowing the history of laps 1-44. An RNN is like the car's accumulated state: the hidden state carries forward everything the network \"remembers\" about the race so far -- tire wear, fuel load, track conditions -- and updates it with each new lap of data.\n\n---\n\n## Learning Objectives\n\nBy the end of this notebook, you should be able to:\n\n- [ ] Explain why feedforward networks fail on sequential data\n- [ ] Implement a vanilla RNN from scratch in NumPy\n- [ ] Describe backpropagation through time and the vanishing gradient problem\n- [ ] Explain the LSTM architecture and the purpose of each gate\n- [ ] Compare GRU and LSTM and know when to use each\n- [ ] Use PyTorch's nn.RNN, nn.LSTM, and nn.GRU modules correctly\n- [ ] Build a character-level language model for text generation"
  },
  {
   "cell_type": "code",
   "id": "vi2uyii0i9g",
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n%matplotlib inline\nplt.style.use('seaborn-v0_8-whitegrid')\ntorch.manual_seed(42)\nnp.random.seed(42)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fourw9perj",
   "source": "---\n\n## 1. Why Sequences Need Special Treatment\n\n### Intuitive Explanation\n\nConsider predicting the next word in a sentence:\n\n- \"The clouds are dark, it will probably **___**\" --> \"rain\"\n- \"She picked up the phone and **___**\" --> \"called\"\n\nA feedforward network takes a **fixed-size input** and produces a **fixed-size output**. But sequences break this assumption in two fundamental ways:\n\n1. **Variable length**: Sentences can be 5 words or 500 words\n2. **Order matters**: \"dog bites man\" vs \"man bites dog\" have the same words but opposite meanings\n3. **Context accumulates**: Understanding word 50 may require remembering word 3\n\n| Problem | Feedforward Approach | Why It Fails | F1 Parallel |\n|---------|---------------------|--------------|-------------|\n| Text classification | Flatten all words into one vector | Loses word order | Averaging all lap times ignores the race trajectory |\n| Time series prediction | Fixed window of past values | Can't adapt window size | Looking at only the last 5 laps misses a tire change 10 laps ago |\n| Speech recognition | Fixed audio chunk | Utterances vary in length | Races vary from 44 to 78 laps depending on the circuit |\n| Machine translation | Fixed input/output size | Languages have different sentence lengths | Predicting stint length varies by compound, fuel, and strategy |\n\n**The key insight:** We need an architecture that can process inputs **one step at a time** while maintaining a **running summary** (memory) of everything it has seen so far.\n\n**F1 analogy:** Think of a race engineer monitoring a Grand Prix. They do not wait until the race is over to analyze it -- they process each lap as it happens, updating their running understanding of tire degradation, fuel burn, and relative pace. Their mental model after lap 30 incorporates everything from laps 1-30. That running mental model is the hidden state of an RNN.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "qz8mb2cjd7g",
   "source": "### Visualization: Feedforward vs Sequential Processing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "pm4d0et3r1",
   "source": "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: Feedforward approach (all at once)\nax = axes[0]\nwords = ['The', 'cat', 'sat', 'on', 'mat']\n# Draw input words\nfor i, word in enumerate(words):\n    ax.text(i * 0.8 + 0.2, 0.0, word, ha='center', va='center',\n            fontsize=12, bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', edgecolor='blue'))\n    # Arrow from word to single box\n    ax.annotate('', xy=(2.0, 0.4), xytext=(i * 0.8 + 0.2, 0.15),\n                arrowprops=dict(arrowstyle='->', color='gray', lw=1.5))\n\n# Single processing box\nax.add_patch(plt.Rectangle((1.0, 0.4), 2.0, 0.3, facecolor='lightyellow', edgecolor='orange', lw=2))\nax.text(2.0, 0.55, 'Feedforward\\nNetwork', ha='center', va='center', fontsize=11, fontweight='bold')\n\n# Output\nax.annotate('', xy=(2.0, 0.9), xytext=(2.0, 0.7),\n            arrowprops=dict(arrowstyle='->', color='orange', lw=2))\nax.text(2.0, 1.0, 'Output', ha='center', va='center', fontsize=12,\n        bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow', edgecolor='orange'))\n\nax.set_xlim(-0.3, 4.0)\nax.set_ylim(-0.3, 1.2)\nax.set_title('Feedforward: All inputs at once\\n(loses order information)', fontsize=13)\nax.axis('off')\n\n# Right: Sequential approach (one at a time with memory)\nax = axes[1]\ncolors_seq = ['#a8d8ea', '#aa96da', '#fcbad3', '#ffffd2', '#a8e6cf']\nfor i, word in enumerate(words):\n    x_pos = i * 0.8 + 0.1\n    # Word input\n    ax.text(x_pos, 0.0, word, ha='center', va='center', fontsize=12,\n            bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', edgecolor='blue'))\n    # Arrow up to hidden state\n    ax.annotate('', xy=(x_pos, 0.35), xytext=(x_pos, 0.15),\n                arrowprops=dict(arrowstyle='->', color='gray', lw=1.5))\n    # Hidden state box\n    ax.add_patch(plt.Rectangle((x_pos - 0.25, 0.35), 0.5, 0.25,\n                                facecolor=colors_seq[i], edgecolor='purple', lw=2, alpha=0.8))\n    ax.text(x_pos, 0.475, f'h{i}', ha='center', va='center', fontsize=11, fontweight='bold')\n    # Arrow between hidden states\n    if i < len(words) - 1:\n        ax.annotate('', xy=((i + 1) * 0.8 + 0.1 - 0.25, 0.475),\n                    xytext=(x_pos + 0.25, 0.475),\n                    arrowprops=dict(arrowstyle='->', color='purple', lw=2))\n\n# Final output\nlast_x = (len(words) - 1) * 0.8 + 0.1\nax.annotate('', xy=(last_x, 0.8), xytext=(last_x, 0.6),\n            arrowprops=dict(arrowstyle='->', color='green', lw=2))\nax.text(last_x, 0.9, 'Output', ha='center', va='center', fontsize=12,\n        bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgreen', edgecolor='green'))\n\nax.text(1.6, 0.75, 'Memory flows forward\\nthrough hidden states', ha='center',\n        fontsize=10, fontstyle='italic', color='purple')\n\nax.set_xlim(-0.3, 4.0)\nax.set_ylim(-0.3, 1.15)\nax.set_title('Sequential (RNN): One input at a time\\n(preserves order via hidden state)', fontsize=13)\nax.axis('off')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "wq0vj1sp19a",
   "source": "---\n\n## 2. Vanilla RNN\n\n### Intuitive Explanation\n\nAn RNN processes a sequence one element at a time, maintaining a **hidden state** that acts as its memory. At each time step, the RNN:\n\n1. Looks at the current input $x_t$\n2. Looks at its previous memory $h_{t-1}$\n3. Combines them to create a new memory $h_t$\n\n**Analogy:** Imagine reading a book one word at a time. After each word, your understanding of the story (hidden state) updates. You don't re-read the entire book at every word -- you carry a summary in your head and update it.\n\n**F1 analogy:** The hidden state is the car's accumulated wear state. At each lap $t$, the RNN receives new data (lap time, tire temps, fuel load) and combines it with the car's accumulated state from all previous laps. After lap 30, $h_{30}$ encodes everything the network knows about the car's condition -- how much rubber is left, how the balance has shifted, how the fuel burn is tracking. The same update rule applies at every lap, just as the same physics governs tire degradation whether it is lap 5 or lap 50.\n\n### The Recurrence Relation\n\n$$h_t = \\tanh(W_{hh} \\cdot h_{t-1} + W_{xh} \\cdot x_t + b_h)$$\n$$y_t = W_{hy} \\cdot h_t + b_y$$\n\n#### Breaking down the formula:\n\n| Component | Shape | Meaning | F1 Parallel |\n|-----------|-------|---------|-------------|\n| $x_t$ | (input_size,) | Input at time step t | Lap t data: [lap_time, tire_temp, fuel_load, gap_to_leader] |\n| $h_{t-1}$ | (hidden_size,) | Previous hidden state (memory) | Car's accumulated state after lap t-1 |\n| $W_{xh}$ | (hidden_size, input_size) | How to process new input | How new lap data updates the car state |\n| $W_{hh}$ | (hidden_size, hidden_size) | How to process previous memory | How the car's existing state carries forward |\n| $b_h$ | (hidden_size,) | Bias term | -- |\n| $\\tanh$ | -- | Squash to [-1, 1] to prevent explosion | -- |\n| $h_t$ | (hidden_size,) | New hidden state (updated memory) | Car's accumulated state after lap t |\n| $W_{hy}$ | (output_size, hidden_size) | Transform memory to output | Predict next-lap performance from current state |\n\n**What this means:** The hidden state $h_t$ is a compressed summary of everything the network has seen from time step 0 to t. The same weights $W_{hh}$ and $W_{xh}$ are shared across all time steps -- the RNN applies the same transformation at every step.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "akcgue8jioo",
   "source": "### Visualization: Unrolling Through Time",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "iyzbd0z51fe",
   "source": "fig, ax = plt.subplots(figsize=(14, 6))\n\nn_steps = 5\nlabels = ['x0', 'x1', 'x2', 'x3', 'x4']\nspacing = 2.5\n\nfor t in range(n_steps):\n    x = t * spacing\n    \n    # Input\n    ax.text(x, 0, labels[t], ha='center', va='center', fontsize=13,\n            bbox=dict(boxstyle='round,pad=0.4', facecolor='lightblue', edgecolor='blue', lw=2))\n    \n    # Arrow from input to hidden\n    ax.annotate('', xy=(x, 1.2), xytext=(x, 0.35),\n                arrowprops=dict(arrowstyle='->', color='blue', lw=1.5))\n    ax.text(x + 0.3, 0.75, '$W_{xh}$', fontsize=10, color='blue')\n    \n    # Hidden state\n    ax.add_patch(plt.Rectangle((x - 0.6, 1.2), 1.2, 0.8,\n                                facecolor='#e8daef', edgecolor='purple', lw=2.5, zorder=3))\n    ax.text(x, 1.6, f'$h_{t}$', ha='center', va='center', fontsize=14, fontweight='bold', zorder=4)\n    ax.text(x, 1.3, 'tanh', ha='center', va='center', fontsize=9, color='gray', zorder=4)\n    \n    # Arrow from hidden to output\n    ax.annotate('', xy=(x, 2.8), xytext=(x, 2.0),\n                arrowprops=dict(arrowstyle='->', color='green', lw=1.5))\n    ax.text(x + 0.3, 2.4, '$W_{hy}$', fontsize=10, color='green')\n    \n    # Output\n    ax.text(x, 3.0, f'$y_{t}$', ha='center', va='center', fontsize=13,\n            bbox=dict(boxstyle='round,pad=0.4', facecolor='lightgreen', edgecolor='green', lw=2))\n    \n    # Arrow between hidden states\n    if t < n_steps - 1:\n        ax.annotate('', xy=((t + 1) * spacing - 0.6, 1.6),\n                    xytext=(x + 0.6, 1.6),\n                    arrowprops=dict(arrowstyle='->', color='purple', lw=2.5))\n        ax.text(x + spacing / 2, 1.85, '$W_{hh}$', ha='center', fontsize=10, color='purple')\n\n# Initial hidden state\nax.annotate('', xy=(-0.6, 1.6), xytext=(-1.5, 1.6),\n            arrowprops=dict(arrowstyle='->', color='purple', lw=2))\nax.text(-2.0, 1.6, '$h_0$\\n(zeros)', ha='center', va='center', fontsize=11,\n        bbox=dict(boxstyle='round,pad=0.3', facecolor='lavender', edgecolor='purple'))\n\nax.text(n_steps * spacing / 2 - 0.5, 3.7,\n        'Same weights (W_xh, W_hh, W_hy) shared across ALL time steps',\n        ha='center', fontsize=12, fontstyle='italic', color='darkred',\n        bbox=dict(boxstyle='round,pad=0.4', facecolor='#fff3e0', edgecolor='darkred', alpha=0.8))\n\nax.set_xlim(-2.5, n_steps * spacing)\nax.set_ylim(-0.5, 4.2)\nax.set_title('RNN Unrolled Through Time', fontsize=15, fontweight='bold')\nax.axis('off')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8op6ip4o5z9",
   "source": "### Implementing a Vanilla RNN from Scratch",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "wrbbxrhfboc",
   "source": "class VanillaRNN:\n    \"\"\"\n    Vanilla RNN implemented from scratch in NumPy.\n    \n    Args:\n        input_size: Dimension of input at each time step\n        hidden_size: Dimension of hidden state\n        output_size: Dimension of output at each time step\n    \"\"\"\n    def __init__(self, input_size, hidden_size, output_size):\n        # Xavier initialization for weights\n        scale_xh = np.sqrt(2.0 / (input_size + hidden_size))\n        scale_hh = np.sqrt(2.0 / (hidden_size + hidden_size))\n        scale_hy = np.sqrt(2.0 / (hidden_size + output_size))\n        \n        self.W_xh = np.random.randn(hidden_size, input_size) * scale_xh\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * scale_hh\n        self.b_h = np.zeros(hidden_size)\n        \n        self.W_hy = np.random.randn(output_size, hidden_size) * scale_hy\n        self.b_y = np.zeros(output_size)\n        \n        self.hidden_size = hidden_size\n    \n    def forward(self, inputs, h_prev=None):\n        \"\"\"\n        Process a sequence of inputs.\n        \n        Args:\n            inputs: List of input vectors, each shape (input_size,)\n            h_prev: Initial hidden state, shape (hidden_size,)\n        \n        Returns:\n            outputs: List of output vectors\n            hidden_states: List of hidden states (including initial)\n        \"\"\"\n        if h_prev is None:\n            h_prev = np.zeros(self.hidden_size)\n        \n        hidden_states = [h_prev]\n        outputs = []\n        \n        for x_t in inputs:\n            # Core RNN computation\n            h_t = np.tanh(self.W_hh @ hidden_states[-1] + self.W_xh @ x_t + self.b_h)\n            y_t = self.W_hy @ h_t + self.b_y\n            \n            hidden_states.append(h_t)\n            outputs.append(y_t)\n        \n        return outputs, hidden_states\n\n# Create an RNN and process a simple sequence\nnp.random.seed(42)\nrnn = VanillaRNN(input_size=3, hidden_size=4, output_size=2)\n\n# Create a sequence of 5 time steps, each with 3 features\nsequence = [np.random.randn(3) for _ in range(5)]\n\noutputs, hidden_states = rnn.forward(sequence)\n\nprint(\"Vanilla RNN from scratch:\")\nprint(f\"  Input size: 3, Hidden size: 4, Output size: 2\")\nprint(f\"  Sequence length: {len(sequence)}\")\nprint(f\"\\nHidden states evolve over time:\")\nfor t, h in enumerate(hidden_states):\n    print(f\"  h_{t}: {h.round(3)}\")\nprint(f\"\\nOutputs at each step:\")\nfor t, y in enumerate(outputs):\n    print(f\"  y_{t}: {y.round(3)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2sy78cose9f",
   "source": "### Visualization: Hidden State Evolution\n\nLet's see how the hidden state evolves as the RNN reads a sequence. We will feed in a sine wave and watch each hidden unit track different aspects of the signal.\n\n**F1 analogy:** Think of each hidden unit as a different aspect of the car's state. One might track tire degradation, another fuel burn, another track evolution. As the RNN reads lap-by-lap data, each hidden unit updates independently, capturing a different facet of the car's evolving condition.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "y2xszjxv8fs",
   "source": "# Feed a sine wave through our RNN and watch hidden states\nnp.random.seed(42)\nrnn_vis = VanillaRNN(input_size=1, hidden_size=8, output_size=1)\n\n# Create sine wave input\nt = np.linspace(0, 4 * np.pi, 50)\nsine_input = [np.array([np.sin(ti)]) for ti in t]\n\noutputs, hidden_states = rnn_vis.forward(sine_input)\n\n# Convert to arrays for plotting\nh_array = np.array(hidden_states[1:])  # Skip initial zero state\nout_array = np.array(outputs).flatten()\n\nfig, axes = plt.subplots(3, 1, figsize=(12, 10))\n\n# Plot 1: Input signal\nax = axes[0]\nax.plot(t, [s[0] for s in sine_input], 'b-', linewidth=2, label='Input: sin(t)')\nax.set_xlabel('Time')\nax.set_ylabel('Input Value')\nax.set_title('Input Sequence (Sine Wave)')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Plot 2: Hidden state evolution (each unit as a different color)\nax = axes[1]\ncolors = plt.cm.tab10(np.linspace(0, 1, 8))\nfor i in range(8):\n    ax.plot(t, h_array[:, i], linewidth=1.5, alpha=0.8, color=colors[i],\n            label=f'h[{i}]')\nax.set_xlabel('Time')\nax.set_ylabel('Hidden Unit Value')\nax.set_title('Hidden State Evolution (Each Unit Tracks Different Features)')\nax.legend(ncol=4, fontsize=9, loc='upper right')\nax.grid(True, alpha=0.3)\n\n# Plot 3: Hidden state as heatmap\nax = axes[2]\nim = ax.imshow(h_array.T, aspect='auto', cmap='RdBu', vmin=-1, vmax=1,\n               extent=[t[0], t[-1], 7.5, -0.5])\nax.set_xlabel('Time')\nax.set_ylabel('Hidden Unit')\nax.set_title('Hidden State Heatmap (Blue = -1, Red = +1)')\nplt.colorbar(im, ax=ax, label='Activation')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Each hidden unit learns to respond differently to the input pattern.\")\nprint(\"Some track the input directly, others track derivatives or longer patterns.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0kkvtc0u80n",
   "source": "---\n\n## 3. Backpropagation Through Time (BPTT)\n\n### Intuitive Explanation\n\nTo train an RNN, we need to compute gradients. Since the RNN is unrolled through time, backpropagation must flow **backward through every time step**. This is called **Backpropagation Through Time (BPTT)**.\n\nThe gradient of the loss at time step $T$ with respect to earlier hidden states involves a **chain of multiplications** through $W_{hh}$:\n\n$$\\frac{\\partial L_T}{\\partial h_t} = \\frac{\\partial L_T}{\\partial h_T} \\cdot \\prod_{k=t+1}^{T} \\frac{\\partial h_k}{\\partial h_{k-1}}$$\n\nEach factor $\\frac{\\partial h_k}{\\partial h_{k-1}}$ involves multiplying by $W_{hh}$ and the derivative of tanh.\n\n**The problem:** Multiplying many numbers together causes exponential behavior:\n- If each factor is < 1: the product **vanishes** (goes to 0)\n- If each factor is > 1: the product **explodes** (goes to infinity)\n\nThis is the **vanishing/exploding gradient problem** -- the fundamental limitation of vanilla RNNs.\n\n**F1 analogy:** This is like forgetting what happened in lap 1 by the time you reach lap 50. The vanilla RNN's memory degrades exponentially with distance. If you are trying to predict tire performance on lap 50 and the key information (tire compound choice, formation lap conditions) was set on lap 1, the gradient signal from lap 50 back to lap 1 has to survive 49 multiplications. With vanishing gradients, that signal arrives as essentially zero -- the network cannot learn that lap 1 conditions matter for lap 50 performance.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "yim490ultfl",
   "source": "### Visualization: Gradient Magnitude vs Sequence Length",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "uej0koeid6l",
   "source": "# Demonstrate vanishing/exploding gradients\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nseq_lengths = np.arange(1, 51)\n\n# Case 1: Vanishing (factor < 1)\nax = axes[0]\nfactors = [0.9, 0.7, 0.5]\nfor f in factors:\n    grad_magnitudes = f ** seq_lengths\n    ax.plot(seq_lengths, grad_magnitudes, linewidth=2, label=f'factor = {f}')\nax.set_xlabel('Steps Back in Time')\nax.set_ylabel('Gradient Magnitude')\nax.set_title('Vanishing Gradients\\n(factor < 1)')\nax.set_yscale('log')\nax.legend()\nax.grid(True, alpha=0.3)\nax.axhline(y=1e-6, color='red', linestyle='--', alpha=0.5, label='Effective zero')\n\n# Case 2: Exploding (factor > 1)\nax = axes[1]\nfactors = [1.1, 1.3, 1.5]\nfor f in factors:\n    grad_magnitudes = f ** seq_lengths\n    ax.plot(seq_lengths, grad_magnitudes, linewidth=2, label=f'factor = {f}')\nax.set_xlabel('Steps Back in Time')\nax.set_ylabel('Gradient Magnitude')\nax.set_title('Exploding Gradients\\n(factor > 1)')\nax.set_yscale('log')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Case 3: Simulate actual RNN gradient flow\nax = axes[2]\n\ndef simulate_gradient_flow(hidden_size, seq_len, n_trials=50):\n    \"\"\"Simulate gradient magnitude through an RNN.\"\"\"\n    magnitudes = []\n    for _ in range(n_trials):\n        W_hh = np.random.randn(hidden_size, hidden_size) * (1.0 / np.sqrt(hidden_size))\n        grad = np.eye(hidden_size)\n        mags = [1.0]\n        for t in range(seq_len):\n            # Gradient through tanh: diag(1 - tanh^2(h)) * W_hh\n            # Approximate tanh derivative as random diagonal in (0, 1)\n            tanh_deriv = np.diag(np.random.uniform(0.1, 1.0, hidden_size))\n            grad = tanh_deriv @ W_hh @ grad\n            mags.append(np.linalg.norm(grad))\n        magnitudes.append(mags)\n    return np.mean(magnitudes, axis=0)\n\nfor hs in [16, 32, 64]:\n    mags = simulate_gradient_flow(hs, 50)\n    ax.plot(mags, linewidth=2, label=f'hidden_size={hs}')\n\nax.set_xlabel('Steps Back in Time')\nax.set_ylabel('Gradient Magnitude')\nax.set_title('Simulated RNN Gradient Flow\\n(averaged over 50 trials)')\nax.set_yscale('log')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key takeaway: Vanilla RNN gradients vanish exponentially with sequence length.\")\nprint(\"After ~20 steps, the gradient is effectively zero -- the RNN 'forgets' early inputs.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "vm6t5ctwchf",
   "source": "### Deep Dive: Vanishing vs Exploding Gradients in RNNs\n\nThe vanishing gradient problem is not unique to RNNs -- deep feedforward networks suffer from it too. But RNNs make it **much worse** because the same weight matrix $W_{hh}$ is multiplied at every step. In a 100-layer feedforward network, you have 100 different weight matrices. In an RNN processing a 100-step sequence, you multiply by the **same** $W_{hh}$ 100 times.\n\n#### Key Insight\n\nThe eigenvalues of $W_{hh}$ determine what happens:\n- If the largest eigenvalue < 1: gradients vanish\n- If the largest eigenvalue > 1: gradients explode\n- We need eigenvalues close to 1 -- but that is a razor's edge to balance on\n\n**F1 analogy:** Imagine a relay of information along the pit wall. If each station attenuates the signal by 5% (eigenvalue = 0.95), after 50 stations the signal is at $0.95^{50} \\approx 0.08$ -- barely audible. If each station amplifies by 5% (eigenvalue = 1.05), after 50 stations it is at $1.05^{50} \\approx 11.5$ -- deafening feedback. This is exactly the vanishing/exploding gradient dilemma.\n\n#### Practical Consequences\n\n| Problem | Effect | Solution |\n|---------|--------|----------|\n| Vanishing gradients | RNN cannot learn long-range dependencies | Use LSTM/GRU |\n| Exploding gradients | Training diverges, loss becomes NaN | Gradient clipping |\n\n#### Gradient Clipping\n\nExploding gradients have a simple fix -- **clip** the gradient norm:\n\n$$\\text{if } \\|\\nabla\\| > \\text{threshold}: \\quad \\nabla \\leftarrow \\frac{\\text{threshold}}{\\|\\nabla\\|} \\cdot \\nabla$$\n\nThis rescales the gradient to have a maximum norm, preventing explosions while preserving direction. In PyTorch: `torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)`\n\nVanishing gradients are harder to fix. We need a fundamentally different architecture -- which leads us to LSTM.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "httwuzjscta",
   "source": "---\n\n## 4. LSTM (Long Short-Term Memory)\n\n### Intuitive Explanation\n\nThe LSTM (Hochreiter & Schmidhuber, 1997) solves the vanishing gradient problem with one key idea: **add a separate memory highway**.\n\n**Analogy:** Think of the vanilla RNN as passing a message through a long chain of people (telephone game). By the end, the message is garbled. The LSTM adds a **conveyor belt** running alongside the chain. Important information can be placed on the belt and travel long distances without being distorted.\n\nThis conveyor belt is called the **cell state** $C_t$. It runs through the entire sequence with only minor linear interactions, allowing gradients to flow unchanged over many steps.\n\n**F1 analogy:** The LSTM gates are like a race engineer deciding what information to remember and what to forget lap by lap. The **forget gate** says: \"the pit crew fumble 20 laps ago? Irrelevant now -- forget it.\" The **input gate** says: \"tire age just crossed 15 laps -- that is critical, store it.\" The **cell state** is like the strategy whiteboard that carries forward only the information the engineer has decided is important. And the **output gate** decides what to broadcast to the driver right now: \"Focus on tire management, ignore the rest.\"\n\nThe LSTM uses **four gates** to control what goes on and off the conveyor belt:\n\n| Gate | Symbol | Purpose | Analogy | F1 Parallel |\n|------|--------|---------|---------|-------------|\n| **Forget gate** | $f_t$ | What old info to discard | Removing items from the belt | \"Forget the formation lap rain -- the track is dry now\" |\n| **Input gate** | $i_t$ | What new info to add | Placing items on the belt | \"Record that we just switched to hard tires\" |\n| **Cell update** | $\\tilde{C}_t$ | Candidate new info | The actual items to place | The specific tire compound and age data |\n| **Output gate** | $o_t$ | What to read from memory | Peeking at the belt | \"Report current tire life and fuel delta to the driver\" |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "m5zu682gao",
   "source": "### LSTM Equations\n\n#### Step 1: Forget Gate -- What to throw away\n\n$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n\n| Component | Meaning | Range | F1 Example |\n|-----------|---------|-------|------------|\n| $\\sigma$ | Sigmoid function | 0 to 1 | -- |\n| $f_t = 0$ | Completely forget | -- | \"Discard last stint's tire data after a pit stop\" |\n| $f_t = 1$ | Completely remember | -- | \"Keep tracking cumulative fuel burn\" |\n\n#### Step 2: Input Gate -- What new info to store\n\n$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n$$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n\nThe input gate $i_t$ decides **how much** to store, and $\\tilde{C}_t$ creates **candidate values** to store.\n\n#### Step 3: Cell State Update\n\n$$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$\n\n**What this means:** Forget some of the old cell state ($f_t \\odot C_{t-1}$), then add some new info ($i_t \\odot \\tilde{C}_t$). The $\\odot$ symbol means element-wise multiplication.\n\n**F1 analogy:** After a pit stop, the forget gate might zero out tire degradation data (new tires = fresh state), while the input gate stores the new compound type. Meanwhile, fuel data and track evolution data carry forward unchanged -- those are not \"reset\" by a pit stop.\n\n#### Step 4: Output Gate -- What to output\n\n$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n$$h_t = o_t \\odot \\tanh(C_t)$$\n\nThe hidden state $h_t$ is a filtered version of the cell state -- only outputting what is relevant right now.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "mdbgiu6tan",
   "source": "### Visualization: LSTM Data Flow",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ylle22o2dj9",
   "source": "fig, ax = plt.subplots(figsize=(14, 8))\n\n# Main cell body\nax.add_patch(plt.Rectangle((1, 1), 10, 6, facecolor='#f5f5f5', edgecolor='black', lw=2, zorder=0))\n\n# === Cell state highway (top) ===\nax.annotate('', xy=(11.5, 6.2), xytext=(-0.5, 6.2),\n            arrowprops=dict(arrowstyle='->', color='green', lw=4))\nax.text(6, 6.7, 'Cell State $C_t$ (the conveyor belt)', ha='center', fontsize=13,\n        fontweight='bold', color='green')\n\n# === Forget Gate ===\n# Gate circle\ncircle_f = plt.Circle((3, 6.2), 0.4, facecolor='#ffcdd2', edgecolor='red', lw=2, zorder=5)\nax.add_patch(circle_f)\nax.text(3, 6.2, 'x', ha='center', va='center', fontsize=16, fontweight='bold', color='red', zorder=6)\n\n# Forget gate sigmoid\nax.add_patch(plt.Rectangle((2.3, 3.5), 1.4, 0.8, facecolor='#ffcdd2', edgecolor='red',\n                            lw=2, zorder=3, alpha=0.9))\nax.text(3, 3.9, '$f_t$\\n$\\\\sigma$', ha='center', va='center', fontsize=11, fontweight='bold', zorder=4)\n\n# Arrow from forget gate to multiply\nax.annotate('', xy=(3, 5.8), xytext=(3, 4.3),\n            arrowprops=dict(arrowstyle='->', color='red', lw=2))\nax.text(2.2, 5.0, 'forget', fontsize=9, color='red', fontstyle='italic')\n\n# === Input Gate ===\n# Gate circle (add)\ncircle_i = plt.Circle((6, 6.2), 0.4, facecolor='#c8e6c9', edgecolor='green', lw=2, zorder=5)\nax.add_patch(circle_i)\nax.text(6, 6.2, '+', ha='center', va='center', fontsize=18, fontweight='bold', color='green', zorder=6)\n\n# Input gate sigmoid\nax.add_patch(plt.Rectangle((5.0, 3.5), 1.2, 0.8, facecolor='#c8e6c9', edgecolor='green',\n                            lw=2, zorder=3, alpha=0.9))\nax.text(5.6, 3.9, '$i_t$\\n$\\\\sigma$', ha='center', va='center', fontsize=11, fontweight='bold', zorder=4)\n\n# Candidate values\nax.add_patch(plt.Rectangle((6.5, 3.5), 1.2, 0.8, facecolor='#fff9c4', edgecolor='orange',\n                            lw=2, zorder=3, alpha=0.9))\nax.text(7.1, 3.9, '$\\\\tilde{C}_t$\\ntanh', ha='center', va='center', fontsize=11, fontweight='bold', zorder=4)\n\n# Multiply circle between input gate and candidate\ncircle_ic = plt.Circle((6, 5.0), 0.3, facecolor='#c8e6c9', edgecolor='green', lw=2, zorder=5)\nax.add_patch(circle_ic)\nax.text(6, 5.0, 'x', ha='center', va='center', fontsize=14, fontweight='bold', color='green', zorder=6)\n\n# Arrows for input path\nax.annotate('', xy=(5.75, 4.7), xytext=(5.6, 4.3),\n            arrowprops=dict(arrowstyle='->', color='green', lw=1.5))\nax.annotate('', xy=(6.25, 4.7), xytext=(7.1, 4.3),\n            arrowprops=dict(arrowstyle='->', color='orange', lw=1.5))\nax.annotate('', xy=(6, 5.8), xytext=(6, 5.3),\n            arrowprops=dict(arrowstyle='->', color='green', lw=2))\n\n# === Output Gate ===\n# tanh on cell state\ncircle_tanh = plt.Circle((9, 5.2), 0.35, facecolor='#e1bee7', edgecolor='purple', lw=2, zorder=5)\nax.add_patch(circle_tanh)\nax.text(9, 5.2, 'tanh', ha='center', va='center', fontsize=8, fontweight='bold', color='purple', zorder=6)\n\n# Arrow from cell state down to tanh\nax.annotate('', xy=(9, 5.55), xytext=(9, 6.0),\n            arrowprops=dict(arrowstyle='->', color='purple', lw=1.5))\n\n# Output gate sigmoid\nax.add_patch(plt.Rectangle((8.4, 3.5), 1.2, 0.8, facecolor='#bbdefb', edgecolor='blue',\n                            lw=2, zorder=3, alpha=0.9))\nax.text(9.0, 3.9, '$o_t$\\n$\\\\sigma$', ha='center', va='center', fontsize=11, fontweight='bold', zorder=4)\n\n# Multiply circle for output\ncircle_o = plt.Circle((9, 4.6), 0.3, facecolor='#bbdefb', edgecolor='blue', lw=2, zorder=5)\nax.add_patch(circle_o)\nax.text(9, 4.6, 'x', ha='center', va='center', fontsize=14, fontweight='bold', color='blue', zorder=6)\n\n# Arrows for output path\nax.annotate('', xy=(9, 4.9), xytext=(9, 4.3),\n            arrowprops=dict(arrowstyle='->', color='blue', lw=1.5))\nax.annotate('', xy=(8.75, 4.6), xytext=(9, 4.85),\n            arrowprops=dict(arrowstyle='->', color='purple', lw=1.5))\n\n# === Inputs at bottom ===\nax.annotate('', xy=(6, 3.5), xytext=(6, 1.5),\n            arrowprops=dict(arrowstyle='->', color='gray', lw=2))\nax.text(6, 1.0, '$[h_{t-1}, x_t]$', ha='center', va='center', fontsize=14,\n        bbox=dict(boxstyle='round,pad=0.4', facecolor='lightyellow', edgecolor='gray', lw=2))\n\n# Connect input to all gates\nfor gate_x in [3.0, 5.6, 7.1, 9.0]:\n    ax.plot([6, gate_x], [2.5, 3.5], 'gray', lw=1, alpha=0.5, linestyle='--')\n\n# === Output ===\nax.annotate('', xy=(9, 0.5), xytext=(9, 4.3),\n            arrowprops=dict(arrowstyle='->', color='blue', lw=2))\nax.text(9, 0.2, '$h_t$ (output)', ha='center', va='center', fontsize=13,\n        bbox=dict(boxstyle='round,pad=0.4', facecolor='lightblue', edgecolor='blue', lw=2))\n\n# === Hidden state arrow (bottom) ===\nax.annotate('', xy=(11.5, 1.6), xytext=(9.5, 1.6),\n            arrowprops=dict(arrowstyle='->', color='blue', lw=3))\nax.text(11.5, 1.2, '$h_t$ to\\nnext step', fontsize=10, ha='center', color='blue')\n\n# Legend\nlegend_y = 7.8\nlegend_items = [\n    (1.5, '#ffcdd2', 'red', 'Forget Gate: what to discard from cell state'),\n    (4.5, '#c8e6c9', 'green', 'Input Gate: what new info to store'),\n    (8.0, '#bbdefb', 'blue', 'Output Gate: what to output as hidden state'),\n]\nfor lx, fc, ec, text in legend_items:\n    ax.add_patch(plt.Rectangle((lx, legend_y), 0.4, 0.3, facecolor=fc, edgecolor=ec, lw=2))\n    ax.text(lx + 0.6, legend_y + 0.15, text, va='center', fontsize=9)\n\nax.set_xlim(-1, 13)\nax.set_ylim(-0.3, 8.5)\nax.set_title('LSTM Cell Architecture', fontsize=16, fontweight='bold')\nax.axis('off')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "wegi2cvjsqc",
   "source": "### Why LSTM Solves Vanishing Gradients\n\nThe cell state update is:\n\n$$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$\n\nThe gradient of $C_t$ with respect to $C_{t-1}$ is simply $f_t$ (element-wise). When the forget gate is close to 1, the gradient flows through **unchanged**. Compare this to the vanilla RNN where the gradient must pass through $W_{hh}$ and tanh at every step.\n\n**F1 analogy:** The cell state is a \"gradient highway\" -- like the DRS straight of information flow. Information about tire compound choice from lap 1 can travel all the way to lap 50 without being distorted, because the forget gate keeps it at 1.0 (remember everything) for the dimensions that store that information. The vanilla RNN is like taking the twisty sector -- the signal degrades at every corner.\n\n### Visualization: RNN vs LSTM Gradient Flow",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "a36f0umlzai",
   "source": "# Compare gradient flow: RNN vs LSTM\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nseq_lengths = np.arange(1, 101)\nn_trials = 100\n\n# Simulate vanilla RNN gradient flow\nrnn_grads = []\nfor _ in range(n_trials):\n    grad = 1.0\n    grads_trial = [1.0]\n    for t in range(100):\n        # Each step: multiply by W_hh eigenvalue * tanh_deriv\n        factor = np.random.uniform(0.3, 0.95)  # tanh derivative shrinks things\n        grad *= factor\n        grads_trial.append(grad)\n    rnn_grads.append(grads_trial)\nrnn_mean = np.mean(rnn_grads, axis=0)\n\n# Simulate LSTM gradient flow (through cell state)\nlstm_grads = []\nfor _ in range(n_trials):\n    grad = 1.0\n    grads_trial = [1.0]\n    for t in range(100):\n        # Cell state gradient: just multiply by forget gate (close to 1)\n        forget_gate = np.random.uniform(0.85, 1.0)  # Forget gate typically near 1\n        grad *= forget_gate\n        grads_trial.append(grad)\n    lstm_grads.append(grads_trial)\nlstm_mean = np.mean(lstm_grads, axis=0)\n\n# Plot comparison\nax = axes[0]\nax.plot(rnn_mean, 'r-', linewidth=2, label='Vanilla RNN')\nax.plot(lstm_mean, 'b-', linewidth=2, label='LSTM (cell state path)')\nax.set_xlabel('Steps Back in Time', fontsize=12)\nax.set_ylabel('Gradient Magnitude', fontsize=12)\nax.set_title('Gradient Flow: RNN vs LSTM', fontsize=14)\nax.set_yscale('log')\nax.legend(fontsize=12)\nax.grid(True, alpha=0.3)\nax.axhline(y=1e-6, color='gray', linestyle='--', alpha=0.5)\nax.text(50, 2e-6, 'Effectively zero', fontsize=9, color='gray')\n\n# Show the key difference visually\nax = axes[1]\nsteps = np.arange(20)\n\n# RNN: repeated matrix multiply\nrnn_path = 0.7 ** steps  # typical shrinkage factor per step\n# LSTM: forget gate close to 1\nlstm_path = 0.95 ** steps  # forget gate near 1\n\nax.bar(steps - 0.15, rnn_path, width=0.3, color='red', alpha=0.7, label='RNN gradient')\nax.bar(steps + 0.15, lstm_path, width=0.3, color='blue', alpha=0.7, label='LSTM gradient')\nax.set_xlabel('Steps Back in Time', fontsize=12)\nax.set_ylabel('Gradient Magnitude', fontsize=12)\nax.set_title('First 20 Steps: Why LSTM Remembers Better', fontsize=14)\nax.legend(fontsize=12)\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"After 20 steps:\")\nprint(f\"  RNN gradient:  {0.7**20:.6f} (effectively zero)\")\nprint(f\"  LSTM gradient:  {0.95**20:.6f} (still meaningful!)\")\nprint(f\"\\nAfter 100 steps:\")\nprint(f\"  RNN gradient:  {0.7**100:.2e}\")\nprint(f\"  LSTM gradient:  {0.95**100:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "qiltknvrbxf",
   "source": "---\n\n## 5. GRU (Gated Recurrent Unit)\n\n### Intuitive Explanation\n\nThe GRU (Cho et al., 2014) is a simplified version of the LSTM. Instead of separate cell state and hidden state with four gates, the GRU:\n\n- Merges the cell state and hidden state into one\n- Uses only **two gates** instead of four\n- Has fewer parameters, so it trains faster\n\n**Analogy:** If the LSTM is a full-featured word processor, the GRU is a streamlined text editor. It handles most tasks just as well with less complexity.\n\n**F1 analogy:** The LSTM is like a full pit wall setup with separate displays for every metric and dedicated engineers for tires, fuel, aero, and strategy. The GRU is like a single combined dashboard that merges everything into fewer, more efficient displays. For most race scenarios, the streamlined setup works just as well -- and the team can react faster with less information overhead.\n\n### GRU Equations\n\n$$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z) \\quad \\text{(update gate)}$$\n$$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r) \\quad \\text{(reset gate)}$$\n$$\\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\odot h_{t-1}, x_t] + b_h) \\quad \\text{(candidate)}$$\n$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t \\quad \\text{(final state)}$$\n\n#### Breaking down the gates:\n\n| Gate | Purpose | When value is 0 | When value is 1 | F1 Parallel |\n|------|---------|-----------------|-----------------|-------------|\n| **Update** $z_t$ | How much to update | Keep old state entirely | Replace with new candidate | \"Nothing changed this lap\" vs \"pit stop -- reset everything\" |\n| **Reset** $r_t$ | How much past to forget | Ignore previous hidden state | Use full previous state | \"Start fresh (new stint)\" vs \"carry forward all history\" |\n\n**What this means:** The update gate $z_t$ plays the role of both the forget and input gates in LSTM. When $z_t = 0$, the hidden state is copied forward unchanged (like LSTM's forget gate = 1). When $z_t = 1$, the state is completely replaced.\n\n### GRU vs LSTM Comparison\n\n| Feature | LSTM | GRU |\n|---------|------|-----|\n| Number of gates | 4 (forget, input, cell, output) | 2 (update, reset) |\n| Separate cell state | Yes ($C_t$ and $h_t$) | No (only $h_t$) |\n| Parameters | More (~4x hidden_size^2) | Fewer (~3x hidden_size^2) |\n| Training speed | Slower | Faster |\n| Performance on long sequences | Often better | Comparable |\n| When to use | Long sequences, complex dependencies | Smaller datasets, faster training |\n\n#### Key Insight\n\nIn practice, GRU and LSTM perform similarly on most tasks. The GRU is often preferred when computational resources are limited or the dataset is small. Try both and pick what works best for your specific problem.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "eqehzasvg3i",
   "source": "### Visualization: RNN vs LSTM vs GRU Architecture Comparison",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "m38mbregpl",
   "source": "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Helper function to draw a simplified cell\ndef draw_cell(ax, title, gates, state_lines, color_scheme):\n    \"\"\"Draw a simplified RNN cell diagram.\"\"\"\n    ax.set_xlim(-0.5, 4.5)\n    ax.set_ylim(-0.5, 4.5)\n    \n    # Cell body\n    ax.add_patch(plt.Rectangle((0.5, 0.5), 3.5, 3.5, facecolor='#f9f9f9',\n                                edgecolor='black', lw=2))\n    \n    # Input\n    ax.text(2.25, -0.2, '$x_t, h_{t-1}$', ha='center', fontsize=10,\n            bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='gray'))\n    ax.annotate('', xy=(2.25, 0.5), xytext=(2.25, 0.1),\n                arrowprops=dict(arrowstyle='->', color='gray', lw=2))\n    \n    # Gates\n    gate_y = 1.5\n    gate_width = 0.8\n    for i, (name, gcolor) in enumerate(gates):\n        gx = 1.0 + i * 1.2\n        ax.add_patch(plt.Rectangle((gx - gate_width/2, gate_y - 0.3),\n                                    gate_width, 0.6, facecolor=gcolor, edgecolor='black', lw=1.5))\n        ax.text(gx, gate_y, name, ha='center', va='center', fontsize=8, fontweight='bold')\n    \n    # State lines\n    for line_y, label, lcolor in state_lines:\n        ax.annotate('', xy=(4.3, line_y), xytext=(0.2, line_y),\n                    arrowprops=dict(arrowstyle='->', color=lcolor, lw=3))\n        ax.text(2.25, line_y + 0.25, label, ha='center', fontsize=9,\n                color=lcolor, fontweight='bold')\n    \n    # Output\n    ax.text(3.5, 4.3, '$h_t$', ha='center', fontsize=11,\n            bbox=dict(boxstyle='round', facecolor='lightblue', edgecolor='blue'))\n    \n    ax.set_title(title, fontsize=13, fontweight='bold', color=color_scheme)\n    ax.axis('off')\n\n# Vanilla RNN\ndraw_cell(axes[0], 'Vanilla RNN\\n(1 operation)', \n          [('tanh', '#e8daef')],\n          [(3.0, '$h_t$', 'purple')],\n          'purple')\naxes[0].text(2.25, 2.5, 'Simple but\\nforgets quickly', ha='center', fontsize=10,\n             fontstyle='italic', color='gray')\n\n# LSTM\ndraw_cell(axes[1], 'LSTM\\n(4 gates)',\n          [('$f_t$', '#ffcdd2'), ('$i_t$', '#c8e6c9'), ('$o_t$', '#bbdefb')],\n          [(3.5, '$C_t$ (cell state)', 'green'), (2.8, '$h_t$ (hidden)', 'blue')],\n          'darkblue')\naxes[1].text(2.25, 2.3, '+ candidate $\\\\tilde{C}_t$', ha='center', fontsize=9, color='gray')\n\n# GRU\ndraw_cell(axes[2], 'GRU\\n(2 gates)',\n          [('$z_t$', '#c8e6c9'), ('$r_t$', '#ffcdd2')],\n          [(3.0, '$h_t$', 'purple')],\n          'darkgreen')\naxes[2].text(2.25, 2.5, 'Simpler than LSTM\\nsimilar performance', ha='center', fontsize=10,\n             fontstyle='italic', color='gray')\n\nplt.suptitle('Architecture Comparison', fontsize=15, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n\n# Parameter count comparison\nhidden_size = 256\ninput_size = 100\nprint(f\"\\nParameter count comparison (input={input_size}, hidden={hidden_size}):\")\nprint(f\"  Vanilla RNN: {(input_size * hidden_size + hidden_size * hidden_size + hidden_size):,} params\")\nprint(f\"  LSTM:         {4 * (input_size * hidden_size + hidden_size * hidden_size + hidden_size):,} params\")\nprint(f\"  GRU:          {3 * (input_size * hidden_size + hidden_size * hidden_size + hidden_size):,} params\")\nprint(f\"\\n  LSTM has {4/1:.0f}x more params than RNN, GRU has {3/1:.0f}x more\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8aqvuvi075c",
   "source": "---\n\n## 6. RNNs in PyTorch\n\n### Intuitive Explanation\n\nPyTorch provides optimized implementations of all three architectures: `nn.RNN`, `nn.LSTM`, and `nn.GRU`. The API is consistent across all three, so once you learn one, you know them all.\n\nThe key challenge is understanding the **shape conventions**:\n\n| Dimension | Meaning | Example | F1 Parallel |\n|-----------|---------|---------|-------------|\n| `seq_len` | Length of the sequence | 20 words in a sentence | 56 laps in a race |\n| `batch` | Number of sequences in parallel | 32 sentences at once | 20 cars' race data simultaneously |\n| `input_size` | Features per time step | 100-dim word embedding | [speed, throttle, brake, tire_temp, fuel] per lap |\n| `hidden_size` | Size of hidden state | 256 units | How much \"memory\" the model carries |\n| `num_layers` | Stacked RNN layers | 2 layers deep | Hierarchical analysis (raw -> events -> strategy) |\n| `num_directions` | 1 (forward) or 2 (bidirectional) | -- | -- |\n\n**Input shape:** `(seq_len, batch, input_size)` -- note that sequence length comes FIRST by default.\n\n**Output shape:** `(seq_len, batch, hidden_size * num_directions)` -- hidden state at every time step.\n\n**Hidden state shape:** `(num_layers * num_directions, batch, hidden_size)` -- final hidden state.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "juw5a835in",
   "source": "# Basic PyTorch RNN usage\nprint(\"=\" * 60)\nprint(\"nn.RNN - Basic Usage\")\nprint(\"=\" * 60)\n\n# Create an RNN\nrnn = nn.RNN(input_size=10, hidden_size=20, num_layers=1, batch_first=False)\n\n# Input: (seq_len=5, batch=3, input_size=10)\nx = torch.randn(5, 3, 10)\n\n# Forward pass\noutput, h_n = rnn(x)\n\nprint(f\"Input shape:       {x.shape}  (seq_len, batch, input_size)\")\nprint(f\"Output shape:      {output.shape}  (seq_len, batch, hidden_size)\")\nprint(f\"Final hidden shape: {h_n.shape}  (num_layers, batch, hidden_size)\")\nprint()\n\n# Output contains hidden states at ALL time steps\n# h_n contains ONLY the final hidden state\nprint(\"Verify: output[-1] == h_n[0] (last output equals final hidden state)\")\nprint(f\"  Match: {torch.allclose(output[-1], h_n[0])}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"nn.LSTM - Returns (output, (h_n, c_n))\")\nprint(\"=\" * 60)\n\nlstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=1)\noutput, (h_n, c_n) = lstm(x)\n\nprint(f\"Input shape:       {x.shape}\")\nprint(f\"Output shape:      {output.shape}\")\nprint(f\"Final hidden (h):  {h_n.shape}\")\nprint(f\"Final cell (c):    {c_n.shape}  <-- LSTM also returns cell state!\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"nn.GRU - Same API as RNN\")\nprint(\"=\" * 60)\n\ngru = nn.GRU(input_size=10, hidden_size=20, num_layers=1)\noutput, h_n = gru(x)\n\nprint(f\"Input shape:       {x.shape}\")\nprint(f\"Output shape:      {output.shape}\")\nprint(f\"Final hidden (h):  {h_n.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "zj51tpsr5x",
   "source": "### Bidirectional RNNs\n\nA standard RNN only reads the sequence left-to-right. But sometimes context from the **future** matters too. For example, in \"I saw the bank by the river,\" the word \"river\" helps disambiguate \"bank.\"\n\nA **bidirectional** RNN runs two separate RNNs: one forward and one backward. Their outputs are concatenated.\n\n**F1 analogy:** A forward-only RNN is like watching the race in real time -- you can only see what has already happened. A bidirectional RNN is like analyzing the race after it is over, where you can look both backward (what led to this lap) and forward (what happened next). Post-race analysis is naturally bidirectional: knowing that a driver pitted on lap 35 helps you understand why their pace dropped on lap 33.\n\n### Stacking RNN Layers\n\nLike feedforward networks, we can stack multiple RNN layers. The output of one layer becomes the input of the next. This lets the network learn hierarchical representations of sequences.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7quq80hrgtv",
   "source": "# Bidirectional LSTM\nprint(\"=\" * 60)\nprint(\"Bidirectional LSTM\")\nprint(\"=\" * 60)\n\nbi_lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=1, bidirectional=True)\nx = torch.randn(5, 3, 10)  # (seq_len=5, batch=3, features=10)\noutput, (h_n, c_n) = bi_lstm(x)\n\nprint(f\"Input:           {x.shape}\")\nprint(f\"Output:          {output.shape}  <-- hidden_size * 2 = {20*2} (forward + backward concatenated)\")\nprint(f\"Hidden states:   {h_n.shape}  <-- 2 directions * 1 layer = 2\")\nprint(f\"  h_n[0] = final forward hidden state\")\nprint(f\"  h_n[1] = final backward hidden state\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Stacked (Multi-layer) LSTM\")\nprint(\"=\" * 60)\n\nstacked_lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=3, dropout=0.2)\noutput, (h_n, c_n) = stacked_lstm(x)\n\nprint(f\"Input:           {x.shape}\")\nprint(f\"Output:          {output.shape}  <-- output from LAST layer only\")\nprint(f\"Hidden states:   {h_n.shape}  <-- 3 layers, each with its own hidden state\")\nprint(f\"  h_n[0] = layer 1 final hidden state\")\nprint(f\"  h_n[1] = layer 2 final hidden state\")\nprint(f\"  h_n[2] = layer 3 final hidden state\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Stacked + Bidirectional\")\nprint(\"=\" * 60)\n\ncombo = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, bidirectional=True)\noutput, (h_n, c_n) = combo(x)\n\nprint(f\"Input:           {x.shape}\")\nprint(f\"Output:          {output.shape}  <-- hidden * 2 directions = {20*2}\")\nprint(f\"Hidden states:   {h_n.shape}  <-- 2 layers * 2 directions = 4\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"batch_first=True (more intuitive ordering)\")\nprint(\"=\" * 60)\n\nlstm_bf = nn.LSTM(input_size=10, hidden_size=20, batch_first=True)\nx_bf = torch.randn(3, 5, 10)  # (batch=3, seq_len=5, features=10)\noutput, (h_n, c_n) = lstm_bf(x_bf)\n\nprint(f\"Input:           {x_bf.shape}  (batch, seq_len, features)\")\nprint(f\"Output:          {output.shape}  (batch, seq_len, hidden_size)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ei9ulzoifcu",
   "source": "### Interactive Exploration: How Hidden Size Affects Capacity\n\nLet's see how different hidden sizes affect an RNN's ability to learn a simple sequence pattern.\n\n**F1 analogy:** The hidden size is like the bandwidth of the race engineer's mental model. A small hidden size (say, 8) might only track a couple of metrics -- tire age and fuel. A large hidden size (256) can simultaneously track degradation curves, gaps to every competitor, weather forecasts, and optimal pit windows. More capacity means richer race understanding, but also more data needed to train.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "4g7e7mt0887",
   "source": "# Train simple RNNs with different hidden sizes to predict a sine wave\ndef train_sine_predictor(hidden_size, rnn_type='LSTM', epochs=200):\n    \"\"\"\n    Train an RNN to predict the next value in a sine wave.\n    \n    Args:\n        hidden_size: Number of hidden units\n        rnn_type: 'RNN', 'LSTM', or 'GRU'\n        epochs: Number of training epochs\n    \n    Returns:\n        List of losses per epoch\n    \"\"\"\n    torch.manual_seed(42)\n    \n    # Generate sine wave data\n    t = torch.linspace(0, 8 * np.pi, 200)\n    data = torch.sin(t)\n    \n    # Create sequences: use 20 steps to predict the next\n    seq_len = 20\n    X, y = [], []\n    for i in range(len(data) - seq_len):\n        X.append(data[i:i+seq_len])\n        y.append(data[i+seq_len])\n    X = torch.stack(X).unsqueeze(-1)  # (samples, seq_len, 1)\n    y = torch.stack(y).unsqueeze(-1)  # (samples, 1)\n    \n    # Build model\n    RNNClass = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[rnn_type]\n    rnn = RNNClass(input_size=1, hidden_size=hidden_size, batch_first=True)\n    fc = nn.Linear(hidden_size, 1)\n    \n    params = list(rnn.parameters()) + list(fc.parameters())\n    optimizer = optim.Adam(params, lr=0.01)\n    criterion = nn.MSELoss()\n    \n    losses = []\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n        output, _ = rnn(X)\n        pred = fc(output[:, -1, :])  # Use last hidden state\n        loss = criterion(pred, y)\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n    \n    return losses\n\n# Compare different hidden sizes\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Vary hidden size\nax = axes[0]\nfor hs in [2, 8, 32, 128]:\n    losses = train_sine_predictor(hs, 'LSTM', epochs=200)\n    ax.plot(losses, linewidth=2, label=f'hidden_size={hs}')\n\nax.set_xlabel('Epoch')\nax.set_ylabel('MSE Loss')\nax.set_title('Effect of Hidden Size (LSTM)')\nax.set_yscale('log')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Compare architectures\nax = axes[1]\nfor rnn_type, color in [('RNN', 'red'), ('LSTM', 'blue'), ('GRU', 'green')]:\n    losses = train_sine_predictor(32, rnn_type, epochs=200)\n    ax.plot(losses, linewidth=2, color=color, label=rnn_type)\n\nax.set_xlabel('Epoch')\nax.set_ylabel('MSE Loss')\nax.set_title('RNN vs LSTM vs GRU (hidden_size=32)')\nax.set_yscale('log')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1o2thnik17",
   "source": "### Why This Matters in Machine Learning\n\n| Application | Architecture | Key Details | F1 Parallel |\n|-------------|-------------|-------------|-------------|\n| Sentiment analysis | Bidirectional LSTM | Use final hidden state for classification | Post-race analysis of radio messages for driver mood |\n| Machine translation | Encoder-decoder LSTM | Encoder summarizes input, decoder generates output | Translating one team's strategy into predicted race outcome |\n| Speech recognition | Stacked bidirectional LSTM | Multiple layers capture different time scales | Decoding team radio through engine noise at multiple frequencies |\n| Time series forecasting | LSTM or GRU | Use last output to predict next value | Predicting next-lap tire degradation from race history |\n| Music generation | LSTM with sampling | Generate one note at a time | Generating synthetic telemetry for simulation testing |\n| Named entity recognition | Bidirectional LSTM | Output at every time step | Classifying each lap as \"push,\" \"conserve,\" or \"pit window\" |\n\n### Packed Sequences (Brief Note)\n\nWhen processing batches of sequences with **different lengths**, PyTorch provides `pack_padded_sequence` and `pad_packed_sequence`. These ensure the RNN does not waste computation on padding tokens:\n\n```python\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\n# lengths = actual length of each sequence in the batch\npacked = pack_padded_sequence(padded_input, lengths, batch_first=True, enforce_sorted=False)\noutput, hidden = lstm(packed)\noutput, lengths = pad_packed_sequence(output, batch_first=True)\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "3pe2u3asskt",
   "source": "---\n\n## 7. Text Generation Project\n\n### Character-Level Language Model\n\nNow let's put everything together and build a **character-level language model**. This model:\n\n1. Reads text one character at a time\n2. Learns patterns in the text (spelling, common words, structure)\n3. Generates new text by predicting one character at a time\n\nWe will train on a small text corpus and watch the generated text improve from random gibberish to recognizable patterns.\n\n**F1 analogy:** This is analogous to a model that reads race telemetry one sample at a time and learns to generate synthetic telemetry. After training on enough real laps, the model would produce realistic-looking speed traces, brake pressure curves, and steering inputs -- capturing the statistical patterns of actual driving without ever having been on track.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "tuznusvx9cb",
   "source": "# Our training text -- a collection of short passages\ntext = \"\"\"To be or not to be that is the question\nWhether tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune\nOr to take arms against a sea of troubles\nAnd by opposing end them To die to sleep\nNo more and by a sleep to say we end\nThe heartache and the thousand natural shocks\nThat flesh is heir to Tis a consummation\nDevoutly to be wished To die to sleep\nTo sleep perchance to dream ay there is the rub\nFor in that sleep of death what dreams may come\nWhen we have shuffled off this mortal coil\nMust give us pause There is the respect\nThat makes calamity of so long life\nFor who would bear the whips and scorns of time\nThe oppressor wrong the proud man contumely\nThe pangs of despised love the law delay\nThe insolence of office and the spurns\nThat patient merit of the unworthy takes\"\"\"\n\n# Build character vocabulary\nchars = sorted(list(set(text)))\nchar_to_idx = {ch: i for i, ch in enumerate(chars)}\nidx_to_char = {i: ch for ch, i in char_to_idx.items()}\nvocab_size = len(chars)\n\nprint(f\"Text length: {len(text)} characters\")\nprint(f\"Vocabulary size: {vocab_size} unique characters\")\nprint(f\"Characters: {''.join(chars)}\")\n\n# Encode the text as integers\nencoded = torch.tensor([char_to_idx[ch] for ch in text], dtype=torch.long)\nprint(f\"\\nFirst 50 chars: '{text[:50]}'\")\nprint(f\"Encoded:        {encoded[:50].tolist()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "v9rmbwev7sm",
   "source": "# Create training sequences\ndef create_sequences(encoded_text, seq_len=40):\n    \"\"\"\n    Create input-target pairs for training.\n    \n    For each position, the input is a sequence of characters and\n    the target is the same sequence shifted by one character.\n    \"\"\"\n    inputs, targets = [], []\n    for i in range(0, len(encoded_text) - seq_len):\n        inputs.append(encoded_text[i:i+seq_len])\n        targets.append(encoded_text[i+1:i+seq_len+1])\n    return torch.stack(inputs), torch.stack(targets)\n\nseq_len = 40\nX, y = create_sequences(encoded, seq_len)\nprint(f\"Training sequences: {X.shape[0]}\")\nprint(f\"Sequence length: {seq_len}\")\nprint(f\"\\nExample input:  '{''.join([idx_to_char[i.item()] for i in X[0]])}'\")\nprint(f\"Example target: '{''.join([idx_to_char[i.item()] for i in y[0]])}'\")\nprint(\"\\nNotice: target is shifted by 1 character (predict the next character)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "6559u897fzg",
   "source": "class CharLSTM(nn.Module):\n    \"\"\"\n    Character-level language model using LSTM.\n    \n    Architecture:\n        Embedding -> LSTM -> Linear -> Softmax (over characters)\n    \n    Args:\n        vocab_size: Number of unique characters\n        embed_size: Dimension of character embeddings\n        hidden_size: LSTM hidden state size\n        num_layers: Number of stacked LSTM layers\n    \"\"\"\n    def __init__(self, vocab_size, embed_size=32, hidden_size=128, num_layers=2):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        # Character embedding: maps integer indices to dense vectors\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        \n        # LSTM layers\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers,\n                           batch_first=True, dropout=0.2 if num_layers > 1 else 0)\n        \n        # Output projection: hidden state -> character probabilities\n        self.fc = nn.Linear(hidden_size, vocab_size)\n    \n    def forward(self, x, hidden=None):\n        \"\"\"\n        Forward pass.\n        \n        Args:\n            x: Input character indices, shape (batch, seq_len)\n            hidden: Optional initial hidden state\n        \n        Returns:\n            output: Character logits at each position, shape (batch, seq_len, vocab_size)\n            hidden: Final hidden state tuple (h_n, c_n)\n        \"\"\"\n        # Embed characters: (batch, seq_len) -> (batch, seq_len, embed_size)\n        embedded = self.embed(x)\n        \n        # LSTM: (batch, seq_len, embed_size) -> (batch, seq_len, hidden_size)\n        lstm_out, hidden = self.lstm(embedded, hidden)\n        \n        # Project to vocabulary: (batch, seq_len, hidden_size) -> (batch, seq_len, vocab_size)\n        output = self.fc(lstm_out)\n        \n        return output, hidden\n    \n    def generate(self, start_char, char_to_idx, idx_to_char, length=200, temperature=1.0):\n        \"\"\"\n        Generate text one character at a time.\n        \n        Args:\n            start_char: Starting character\n            char_to_idx: Character to index mapping\n            idx_to_char: Index to character mapping\n            length: Number of characters to generate\n            temperature: Controls randomness (lower = more deterministic)\n        \n        Returns:\n            Generated text string\n        \"\"\"\n        self.eval()\n        generated = [start_char]\n        x = torch.tensor([[char_to_idx[start_char]]])\n        hidden = None\n        \n        with torch.no_grad():\n            for _ in range(length - 1):\n                output, hidden = self(x, hidden)\n                \n                # Apply temperature scaling\n                logits = output[0, -1, :] / temperature\n                probs = torch.softmax(logits, dim=0)\n                \n                # Sample from the distribution\n                next_idx = torch.multinomial(probs, 1).item()\n                generated.append(idx_to_char[next_idx])\n                \n                x = torch.tensor([[next_idx]])\n        \n        self.train()\n        return ''.join(generated)\n\nmodel = CharLSTM(vocab_size, embed_size=32, hidden_size=128, num_layers=2)\nprint(model)\nprint(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "vvh9attt6z9",
   "source": "# Training loop\ntorch.manual_seed(42)\nmodel = CharLSTM(vocab_size, embed_size=32, hidden_size=128, num_layers=2)\noptimizer = optim.Adam(model.parameters(), lr=0.003)\ncriterion = nn.CrossEntropyLoss()\n\n# Training\nepochs = 300\nlosses = []\nsamples = {}\n\nprint(\"Training character-level language model...\")\nprint(\"=\" * 60)\n\n# Generate sample before training\nsample = model.generate('T', char_to_idx, idx_to_char, length=100, temperature=0.8)\nsamples[0] = sample\nprint(f\"\\nEpoch 0 (before training):\")\nprint(f\"  '{sample[:80]}...'\")\n\nfor epoch in range(1, epochs + 1):\n    model.train()\n    \n    # Shuffle training data\n    perm = torch.randperm(X.shape[0])\n    X_shuffled = X[perm]\n    y_shuffled = y[perm]\n    \n    epoch_loss = 0\n    batch_size = 64\n    n_batches = 0\n    \n    for i in range(0, X.shape[0], batch_size):\n        X_batch = X_shuffled[i:i+batch_size]\n        y_batch = y_shuffled[i:i+batch_size]\n        \n        optimizer.zero_grad()\n        output, _ = model(X_batch)\n        \n        # Reshape for cross entropy: (batch * seq_len, vocab_size) vs (batch * seq_len,)\n        loss = criterion(output.reshape(-1, vocab_size), y_batch.reshape(-1))\n        loss.backward()\n        \n        # Gradient clipping to prevent exploding gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n        \n        optimizer.step()\n        epoch_loss += loss.item()\n        n_batches += 1\n    \n    avg_loss = epoch_loss / n_batches\n    losses.append(avg_loss)\n    \n    # Print progress and generate samples at milestones\n    if epoch in [50, 100, 150, 200, 300]:\n        sample = model.generate('T', char_to_idx, idx_to_char, length=100, temperature=0.8)\n        samples[epoch] = sample\n        print(f\"\\nEpoch {epoch} (loss={avg_loss:.4f}):\")\n        print(f\"  '{sample[:80]}...'\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Training complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ui92p6c7aoe",
   "source": "# Plot training loss\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(losses, 'b-', linewidth=2)\nax.set_xlabel('Epoch', fontsize=12)\nax.set_ylabel('Cross-Entropy Loss', fontsize=12)\nax.set_title('Character Language Model Training Loss', fontsize=14)\nax.grid(True, alpha=0.3)\n\n# Mark sample points\nfor epoch in samples:\n    if epoch > 0:\n        ax.axvline(x=epoch-1, color='red', linestyle='--', alpha=0.3)\n        ax.text(epoch-1, ax.get_ylim()[1] * 0.95, f'Epoch {epoch}',\n                fontsize=8, rotation=90, va='top', color='red')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "relqaqfp38",
   "source": "### Temperature Sampling\n\nThe **temperature** parameter controls how \"creative\" vs \"conservative\" the model is when generating text:\n\n| Temperature | Effect | Use Case | F1 Parallel |\n|-------------|--------|----------|-------------|\n| 0.2 (low) | Very predictable, repetitive | When accuracy matters | Conservative strategy: follow the data, minimize risk |\n| 0.8 (medium) | Balanced creativity | General text generation | Balanced strategy: react to conditions but stay within model |\n| 1.0 (default) | True model distribution | Standard sampling | Neutral: let the probabilities speak for themselves |\n| 1.5+ (high) | Very random, creative | Brainstorming, art | Aggressive strategy: gamble on an unusual call for a podium |\n\nMathematically, temperature divides the logits before softmax:\n\n$$P(c) = \\frac{e^{z_c / T}}{\\sum_j e^{z_j / T}}$$\n\nLow temperature makes the distribution **sharper** (peak character dominates). High temperature makes it **flatter** (more uniform).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2zyqtk211rf",
   "source": "# Demonstrate temperature effect\nprint(\"Generated text at different temperatures:\")\nprint(\"=\" * 60)\n\nfor temp in [0.2, 0.5, 0.8, 1.0, 1.5]:\n    sample = model.generate('T', char_to_idx, idx_to_char, length=120, temperature=temp)\n    print(f\"\\nTemperature = {temp}:\")\n    print(f\"  '{sample[:100]}...'\")\n\n# Visualize temperature effect on probability distribution\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Create sample logits\nlogits = torch.tensor([2.0, 1.5, 0.5, 0.2, -0.5, -1.0, -1.5, -2.0])\nlabels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n\nfor ax, temp in zip(axes, [0.2, 1.0, 2.0]):\n    probs = torch.softmax(logits / temp, dim=0).numpy()\n    bars = ax.bar(labels, probs, color='steelblue', edgecolor='black', alpha=0.8)\n    bars[0].set_color('orange')  # Highlight most likely\n    ax.set_xlabel('Character')\n    ax.set_ylabel('Probability')\n    ax.set_title(f'Temperature = {temp}')\n    ax.set_ylim(0, 1.0)\n    ax.grid(True, alpha=0.3)\n\nplt.suptitle('How Temperature Affects the Probability Distribution', fontsize=14)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c6smmv475ec",
   "source": "---\n\n## Exercises\n\n### Exercise 1: Implement a GRU Cell from Scratch\n\nImplement the GRU equations in NumPy, following the same pattern as the VanillaRNN class.\n\n**F1 scenario:** You are building a lightweight race-state tracker that updates the car's condition lap by lap using only two gates (update and reset) instead of four. This is the GRU approach -- simpler than the full LSTM pit-wall setup, but often just as effective for predicting tire degradation curves.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "6uk1jr4kz6v",
   "source": "# EXERCISE 1: Implement GRU from scratch\ndef sigmoid(x):\n    \"\"\"Numerically stable sigmoid.\"\"\"\n    return 1.0 / (1.0 + np.exp(-np.clip(x, -500, 500)))\n\nclass GRUFromScratch:\n    \"\"\"\n    GRU implemented from scratch in NumPy.\n    \n    Equations:\n        z_t = sigmoid(W_z @ [h_{t-1}, x_t] + b_z)    # update gate\n        r_t = sigmoid(W_r @ [h_{t-1}, x_t] + b_r)    # reset gate\n        h_tilde = tanh(W_h @ [r_t * h_{t-1}, x_t] + b_h)  # candidate\n        h_t = (1 - z_t) * h_{t-1} + z_t * h_tilde    # final state\n    \"\"\"\n    def __init__(self, input_size, hidden_size):\n        scale = np.sqrt(2.0 / (input_size + hidden_size))\n        combined = input_size + hidden_size\n        \n        # TODO: Initialize weights for update gate (W_z, b_z)\n        # TODO: Initialize weights for reset gate (W_r, b_r)\n        # TODO: Initialize weights for candidate (W_h, b_h)\n        # Hint: Each W should be shape (hidden_size, input_size + hidden_size)\n        # Hint: Each b should be shape (hidden_size,)\n        \n        self.W_z = np.random.randn(hidden_size, combined) * scale\n        self.b_z = np.zeros(hidden_size)\n        self.W_r = np.random.randn(hidden_size, combined) * scale\n        self.b_r = np.zeros(hidden_size)\n        self.W_h = np.random.randn(hidden_size, combined) * scale\n        self.b_h = np.zeros(hidden_size)\n        \n        self.hidden_size = hidden_size\n    \n    def forward(self, inputs, h_prev=None):\n        \"\"\"\n        Process a sequence through the GRU.\n        \n        Args:\n            inputs: List of input vectors\n            h_prev: Initial hidden state\n        \n        Returns:\n            hidden_states: List of hidden states\n        \"\"\"\n        if h_prev is None:\n            h_prev = np.zeros(self.hidden_size)\n        \n        hidden_states = [h_prev]\n        \n        for x_t in inputs:\n            h = hidden_states[-1]\n            \n            # TODO: Implement the GRU equations\n            # Step 1: Concatenate h and x_t\n            # Step 2: Compute update gate z_t\n            # Step 3: Compute reset gate r_t\n            # Step 4: Compute candidate h_tilde (using r_t * h concatenated with x_t)\n            # Step 5: Compute final state h_t\n            \n            # Hint: np.concatenate([h, x_t])\n            \n            pass  # Replace with your implementation\n        \n        return hidden_states\n\n# Test your implementation\nnp.random.seed(42)\ngru_scratch = GRUFromScratch(input_size=3, hidden_size=4)\ntest_seq = [np.random.randn(3) for _ in range(5)]\n\n# If implemented correctly, this should produce 6 hidden states (initial + 5 steps)\n# hidden_states = gru_scratch.forward(test_seq)\n# print(f\"Number of hidden states: {len(hidden_states)}\")\n# for t, h in enumerate(hidden_states):\n#     print(f\"  h_{t}: {h.round(3)}\")\n\n# Verify against PyTorch GRU\nprint(\"Verify: PyTorch GRU output for reference:\")\ntorch.manual_seed(42)\ngru_pt = nn.GRU(input_size=3, hidden_size=4, batch_first=True)\nx_pt = torch.tensor(np.stack(test_seq)).unsqueeze(0).float()\nout_pt, h_pt = gru_pt(x_pt)\nprint(f\"  PyTorch final hidden: {h_pt.squeeze().detach().numpy().round(3)}\")\nprint(f\"  (Your implementation should produce similar-magnitude values)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1mkl1j4i5p7",
   "source": "### Exercise 2: Sequence Classification with LSTM\n\nBuild an LSTM that classifies whether a sequence of numbers is trending up or down.\n\n**F1 scenario:** Think of this as classifying whether a driver's pace over a stint is improving (trending down in lap times as they warm up the tires and the car gets lighter) or degrading (trending up as tires wear out). The LSTM reads the sequence of lap times and outputs a single classification: \"improving\" or \"degrading.\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "qhgtxksex8r",
   "source": "# EXERCISE 2: Sequence Classification\n\n# Generate data: sequences that trend up (label=1) or down (label=0)\ndef generate_trend_data(n_samples=500, seq_len=20):\n    \"\"\"Generate sequences with upward or downward trends.\"\"\"\n    X = []\n    y = []\n    for _ in range(n_samples):\n        if np.random.random() > 0.5:\n            # Upward trend\n            slope = np.random.uniform(0.05, 0.2)\n            seq = slope * np.arange(seq_len) + np.random.randn(seq_len) * 0.3\n            label = 1\n        else:\n            # Downward trend\n            slope = np.random.uniform(-0.2, -0.05)\n            seq = slope * np.arange(seq_len) + np.random.randn(seq_len) * 0.3\n            label = 0\n        X.append(seq)\n        y.append(label)\n    \n    X = torch.tensor(np.array(X), dtype=torch.float32).unsqueeze(-1)  # (N, seq_len, 1)\n    y = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)  # (N, 1)\n    return X, y\n\nX_train, y_train = generate_trend_data(500, 20)\nX_test, y_test = generate_trend_data(100, 20)\n\nprint(f\"Training data: {X_train.shape}, Labels: {y_train.shape}\")\nprint(f\"Test data: {X_test.shape}, Labels: {y_test.shape}\")\n\nclass TrendClassifier(nn.Module):\n    \"\"\"\n    LSTM-based sequence classifier.\n    \n    TODO: Implement this model with:\n    1. An LSTM layer (input_size=1, hidden_size=32)\n    2. A Linear layer that maps hidden_size -> 1\n    \n    The forward method should:\n    1. Pass input through LSTM\n    2. Take the LAST hidden state output[:, -1, :]\n    3. Pass through linear layer\n    4. Return logits (no sigmoid -- use BCEWithLogitsLoss)\n    \"\"\"\n    def __init__(self, hidden_size=32):\n        super().__init__()\n        # TODO: Define layers\n        # Hint: self.lstm = nn.LSTM(input_size=1, hidden_size=hidden_size, batch_first=True)\n        # Hint: self.fc = nn.Linear(hidden_size, 1)\n        pass\n    \n    def forward(self, x):\n        # TODO: Implement forward pass\n        # Hint: output, (h_n, c_n) = self.lstm(x)\n        # Hint: return self.fc(output[:, -1, :])\n        pass\n\n# Train and evaluate\n# model = TrendClassifier(hidden_size=32)\n# optimizer = optim.Adam(model.parameters(), lr=0.01)\n# criterion = nn.BCEWithLogitsLoss()\n# \n# for epoch in range(50):\n#     optimizer.zero_grad()\n#     output = model(X_train)\n#     loss = criterion(output, y_train)\n#     loss.backward()\n#     optimizer.step()\n#     \n#     if (epoch + 1) % 10 == 0:\n#         with torch.no_grad():\n#             pred = (torch.sigmoid(model(X_test)) > 0.5).float()\n#             acc = (pred == y_test).float().mean()\n#             print(f\"Epoch {epoch+1}: loss={loss.item():.4f}, test_acc={acc.item():.4f}\")\n\n# Expected: Test accuracy > 0.90 after 50 epochs\nprint(\"\\n(Uncomment the training code above after implementing TrendClassifier)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ja8tn7hvok",
   "source": "### Exercise 3: Exploring LSTM Gates\n\nUse PyTorch hooks to visualize what the LSTM gates are doing during text generation. This exercise explores how the forget and input gates behave on real data.\n\n**F1 scenario:** Imagine peering inside the LSTM during a race simulation. When does the forget gate fire (what information is being discarded)? When does the input gate spike (what new information is being stored)? Visualizing gate activations is like watching the strategy engineer's decision-making in real time -- seeing exactly when they decide to forget old tire data and store new pit stop information.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "iyiw71zffb",
   "source": "# EXERCISE 3: Visualize LSTM gate activations\n\ndef visualize_lstm_gates(model, text_input, char_to_idx, idx_to_char):\n    \"\"\"\n    Feed a string through the trained CharLSTM and visualize gate activations.\n    \n    TODO: \n    1. Encode the text as indices\n    2. Pass through the model's embedding layer\n    3. Manually step through the LSTM to record gate activations\n    \n    Hint: PyTorch LSTM stores weights as [W_ii, W_if, W_ig, W_io] concatenated\n    where i=input, f=forget, g=cell candidate, o=output\n    \n    For a simpler approach, just feed the text through and visualize\n    the hidden state heatmap (similar to what we did with the vanilla RNN).\n    \"\"\"\n    model.eval()\n    \n    # Encode text\n    indices = torch.tensor([[char_to_idx[ch] for ch in text_input]])\n    \n    # Get hidden states at each position\n    with torch.no_grad():\n        embedded = model.embed(indices)\n        output, _ = model.lstm(embedded)\n        # output shape: (1, seq_len, hidden_size)\n        hidden_states = output.squeeze(0).numpy()\n    \n    # Visualize\n    fig, axes = plt.subplots(2, 1, figsize=(14, 6))\n    \n    # Hidden state heatmap\n    ax = axes[0]\n    im = ax.imshow(hidden_states.T[:32], aspect='auto', cmap='RdBu', vmin=-1, vmax=1)\n    ax.set_ylabel('Hidden Unit (first 32)')\n    ax.set_title('LSTM Hidden State Activations')\n    plt.colorbar(im, ax=ax)\n    \n    # Set x-axis to show characters\n    ax.set_xticks(range(len(text_input)))\n    ax.set_xticklabels(list(text_input), fontsize=8)\n    \n    # Mean activation magnitude per character\n    ax = axes[1]\n    mean_act = np.abs(hidden_states).mean(axis=1)\n    ax.bar(range(len(text_input)), mean_act, color='steelblue', alpha=0.8)\n    ax.set_xticks(range(len(text_input)))\n    ax.set_xticklabels(list(text_input), fontsize=8)\n    ax.set_ylabel('Mean |Activation|')\n    ax.set_title('Activation Magnitude Per Character')\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Visualize with a sample from the training text\nsample_text = \"To be or not to be\"\nvisualize_lstm_gates(model, sample_text, char_to_idx, idx_to_char)\nprint(\"Notice how the hidden state changes as the model processes each character.\")\nprint(\"Spaces and word boundaries often cause distinct activation patterns.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8kz2l6j4gv9",
   "source": "---\n\n## Summary\n\n### Key Concepts\n\n**Why Sequences Need RNNs:**\n- Feedforward networks require fixed-size inputs and ignore order\n- Sequential data (text, audio, time series) needs memory of past context\n- RNNs process one step at a time, maintaining a hidden state as memory\n- **F1 parallel:** A Grand Prix is inherently sequential -- each lap depends on cumulative tire wear, fuel burn, and track evolution from all previous laps\n\n**Vanilla RNN:**\n- Core equation: $h_t = \\tanh(W_{hh} \\cdot h_{t-1} + W_{xh} \\cdot x_t + b)$\n- Same weights shared across all time steps (parameter efficiency)\n- Hidden state is a compressed summary of the entire sequence so far\n- Limited by vanishing gradients for long sequences\n- **F1 parallel:** The hidden state is the car's accumulated wear state -- but vanilla RNNs \"forget\" what happened on lap 1 by the time they reach lap 50\n\n**Backpropagation Through Time (BPTT):**\n- Gradients flow backward through unrolled time steps\n- Repeated multiplication causes exponential vanishing or exploding\n- Gradient clipping fixes explosions; gated architectures fix vanishing\n- **F1 parallel:** Like a radio relay chain -- the signal degrades at every station unless you build a direct link\n\n**LSTM:**\n- Cell state provides a \"gradient highway\" for long-range dependencies\n- Four gates: forget (what to discard), input (what to store), cell update (new candidates), output (what to reveal)\n- Key: cell state gradient is just $f_t$ -- near 1 means gradient flows unchanged\n- **F1 parallel:** Gates decide what race info to remember vs forget. Tire age matters; a pit crew fumble 20 laps ago does not. The cell state is the strategy whiteboard.\n\n**GRU:**\n- Simplified LSTM with two gates: update and reset\n- Fewer parameters, faster training, similar performance\n- Merges cell state and hidden state into one\n- **F1 parallel:** A streamlined dashboard vs the full pit wall setup -- less overhead, similar results\n\n**PyTorch RNN Modules:**\n- Consistent API: `nn.RNN`, `nn.LSTM`, `nn.GRU`\n- Shape convention: `(seq_len, batch, features)` or `batch_first=True`\n- Support for bidirectional and stacked layers\n\n### Connection to Deep Learning\n\n| Concept | Application | F1 Parallel |\n|---------|------------|-------------|\n| Vanilla RNN | Simple sequence tasks, educational baseline | Basic lap-by-lap state tracking |\n| LSTM | Machine translation, speech recognition, text generation | Full race-state modeling with selective memory |\n| GRU | Smaller models, mobile deployment, quick prototyping | Lightweight real-time tire degradation predictor |\n| Bidirectional RNN | NER, sentiment analysis, any task needing full context | Post-race analysis looking forward and backward |\n| Character-level LM | Text generation, spelling correction, data augmentation | Synthetic telemetry generation for simulation |\n| Temperature sampling | Controlling creativity in generation | Conservative vs aggressive strategy selection |\n| Gradient clipping | Essential for stable RNN training | Keeping the signal from exploding in long races |\n\n### Checklist\n\n- [ ] I can explain why feedforward networks fail on sequences\n- [ ] I can implement a vanilla RNN from scratch and describe each component\n- [ ] I understand why gradients vanish/explode and how BPTT causes it\n- [ ] I can draw the LSTM architecture and explain each gate's purpose\n- [ ] I can compare GRU and LSTM and choose appropriately\n- [ ] I can use PyTorch RNN modules with correct input/output shapes\n- [ ] I can build and train a character-level language model\n- [ ] I understand temperature sampling and its effect on generation",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "qy8o706o3i",
   "source": "---\n\n## Next Steps\n\nNow that you understand recurrent architectures, you are ready for the next major breakthrough in sequence modeling:\n\n1. **Attention Mechanisms**: Instead of compressing the entire sequence into a fixed-size hidden state, attention lets the model \"look back\" at any position. This solves the information bottleneck of RNNs. In F1 terms: instead of relying on a compressed race summary, the model can look back at any specific lap and ask \"what happened there?\"\n\n2. **Transformers**: Built entirely on attention (no recurrence), transformers process all positions in parallel. They power GPT, BERT, and virtually all modern language models.\n\n3. **Sequence-to-Sequence Models**: Encoder-decoder architectures for translation, summarization, and other sequence transformation tasks.\n\n**Historical context:** RNNs dominated NLP from 2013-2017. The Transformer (Vaswani et al., 2017) largely replaced them for most tasks. However, RNNs remain valuable for:\n- Understanding the foundations of sequence modeling\n- Resource-constrained environments\n- Streaming data where you process one element at a time\n- State-space models (Mamba, etc.) which borrow RNN-like ideas\n\n**Practical next steps:**\n- Try training the character-level model on a larger text corpus\n- Build a sentiment classifier using bidirectional LSTM\n- Experiment with different sequence lengths to observe vanishing gradients firsthand\n- Compare RNN and Transformer performance on the same sequence task",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}