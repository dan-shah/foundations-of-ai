{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6.4: PPO and Modern RL\n",
    "\n",
    "This is the capstone of our RL journey — and it connects directly to how modern AI assistants like ChatGPT and Claude are trained. **Proximal Policy Optimization (PPO)** is the workhorse algorithm behind RLHF, the technique that transforms a raw language model into a helpful, harmless, and honest assistant.\n",
    "\n",
    "PPO solves the trust region problem from Notebook 19 with a beautifully simple clipped objective. Combined with **Generalized Advantage Estimation (GAE)**, it provides stable, efficient policy optimization that scales from simple control tasks to aligning billion-parameter language models.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- [ ] Derive and implement PPO's clipped surrogate objective\n",
    "- [ ] Understand why clipping prevents destructively large policy updates\n",
    "- [ ] Implement Generalized Advantage Estimation (GAE)\n",
    "- [ ] Build a complete PPO agent from scratch in PyTorch\n",
    "- [ ] Train PPO on a control task and analyze its behavior\n",
    "- [ ] Implement a reward model trained on preference data\n",
    "- [ ] Build the complete RLHF pipeline: SFT → Reward Model → PPO\n",
    "- [ ] Understand the KL penalty and why it's critical for RLHF\n",
    "- [ ] Connect the full curriculum: from linear algebra to language model alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Part 6.4: PPO and Modern RL\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The PPO Objective\n",
    "\n",
    "Recall the problem from Notebook 19: vanilla policy gradients can take destructively large steps, causing the policy to collapse. TRPO solved this with constrained optimization, but it's complex and expensive.\n",
    "\n",
    "### PPO's Key Insight: Clipping\n",
    "\n",
    "PPO uses the **probability ratio** between the new and old policies:\n",
    "\n",
    "$$r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$$\n",
    "\n",
    "- $r_t = 1$: New policy same as old\n",
    "- $r_t > 1$: New policy makes this action *more* likely\n",
    "- $r_t < 1$: New policy makes this action *less* likely\n",
    "\n",
    "The **clipped surrogate objective** is:\n",
    "\n",
    "$$L^{CLIP}(\\theta) = \\mathbb{E}_t\\left[\\min\\left(r_t(\\theta) \\hat{A}_t, \\; \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t\\right)\\right]$$\n",
    "\n",
    "where $\\epsilon$ (typically 0.2) is the clipping parameter.\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "The min and clip create a \"trust region\" around the old policy:\n",
    "\n",
    "- If $\\hat{A}_t > 0$ (good action): We want to increase $r_t$, but the clip at $1+\\epsilon$ prevents going too far\n",
    "- If $\\hat{A}_t < 0$ (bad action): We want to decrease $r_t$, but the clip at $1-\\epsilon$ prevents going too far\n",
    "\n",
    "The result: the policy improves monotonically, without the instability of unconstrained policy gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: The Clipped Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "epsilon = 0.2\n",
    "r = np.linspace(0.2, 2.0, 500)\n",
    "\n",
    "# Case 1: Positive advantage\n",
    "ax = axes[0]\n",
    "A = 1.0  # Positive advantage\n",
    "unclipped = r * A\n",
    "clipped = np.clip(r, 1 - epsilon, 1 + epsilon) * A\n",
    "objective = np.minimum(unclipped, clipped)\n",
    "\n",
    "ax.plot(r, unclipped, 'b--', linewidth=2, label='Unclipped: r·A', alpha=0.6)\n",
    "ax.plot(r, clipped, 'r--', linewidth=2, label='Clipped: clip(r)·A', alpha=0.6)\n",
    "ax.plot(r, objective, 'g-', linewidth=3, label='PPO objective: min(·,·)')\n",
    "ax.axvline(x=1.0, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.axvline(x=1-epsilon, color='orange', linestyle='--', alpha=0.5, label=f'1-ε = {1-epsilon}')\n",
    "ax.axvline(x=1+epsilon, color='orange', linestyle='--', alpha=0.5, label=f'1+ε = {1+epsilon}')\n",
    "ax.fill_between(r, objective, alpha=0.1, color='green')\n",
    "ax.set_xlabel('Probability ratio r(θ)', fontsize=12)\n",
    "ax.set_ylabel('Objective', fontsize=12)\n",
    "ax.set_title('Positive Advantage (A > 0)\\n\"Good action — increase probability, but not too much\"',\n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Case 2: Negative advantage\n",
    "ax = axes[1]\n",
    "A = -1.0  # Negative advantage\n",
    "unclipped = r * A\n",
    "clipped = np.clip(r, 1 - epsilon, 1 + epsilon) * A\n",
    "objective = np.minimum(unclipped, clipped)\n",
    "\n",
    "ax.plot(r, unclipped, 'b--', linewidth=2, label='Unclipped: r·A', alpha=0.6)\n",
    "ax.plot(r, clipped, 'r--', linewidth=2, label='Clipped: clip(r)·A', alpha=0.6)\n",
    "ax.plot(r, objective, 'g-', linewidth=3, label='PPO objective: min(·,·)')\n",
    "ax.axvline(x=1.0, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.axvline(x=1-epsilon, color='orange', linestyle='--', alpha=0.5)\n",
    "ax.axvline(x=1+epsilon, color='orange', linestyle='--', alpha=0.5)\n",
    "ax.fill_between(r, objective, alpha=0.1, color='green')\n",
    "ax.set_xlabel('Probability ratio r(θ)', fontsize=12)\n",
    "ax.set_ylabel('Objective', fontsize=12)\n",
    "ax.set_title('Negative Advantage (A < 0)\\n\"Bad action — decrease probability, but not too much\"',\n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('PPO Clipped Surrogate Objective (ε = 0.2)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key: The green line (PPO objective) is flat outside [1-ε, 1+ε].\")\n",
    "print(\"This means there's NO gradient incentive to change the policy too much.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Generalized Advantage Estimation (GAE)\n",
    "\n",
    "To compute advantages, we need a good estimate. **GAE** provides a tunable tradeoff between bias and variance:\n",
    "\n",
    "$$\\hat{A}_t^{GAE} = \\sum_{l=0}^{T-t} (\\gamma \\lambda)^l \\delta_{t+l}$$\n",
    "\n",
    "where $\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$ is the TD error.\n",
    "\n",
    "The parameter $\\lambda \\in [0, 1]$ controls the tradeoff:\n",
    "\n",
    "| λ | Estimate | Bias | Variance |\n",
    "|---|---------|------|----------|\n",
    "| 0 | One-step TD: $\\delta_t$ | High bias | Low variance |\n",
    "| 1 | Full Monte Carlo return | No bias | High variance |\n",
    "| 0.95 | Typical PPO setting | Good balance | Good balance |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, next_value, gamma=0.99, lam=0.95, dones=None):\n",
    "    \"\"\"Compute Generalized Advantage Estimation.\n",
    "    \n",
    "    Args:\n",
    "        rewards: list of rewards [r_0, r_1, ..., r_T]\n",
    "        values: list of value estimates [V(s_0), V(s_1), ..., V(s_T)]\n",
    "        next_value: V(s_{T+1}) (bootstrap value)\n",
    "        gamma: discount factor\n",
    "        lam: GAE parameter (0 = TD, 1 = Monte Carlo)\n",
    "        dones: list of done flags\n",
    "    \n",
    "    Returns:\n",
    "        advantages: GAE advantage estimates\n",
    "        returns: advantages + values (targets for value function)\n",
    "    \"\"\"\n",
    "    T = len(rewards)\n",
    "    advantages = np.zeros(T)\n",
    "    \n",
    "    if dones is None:\n",
    "        dones = [False] * T\n",
    "    \n",
    "    # Work backwards\n",
    "    gae = 0\n",
    "    for t in reversed(range(T)):\n",
    "        if t == T - 1:\n",
    "            next_val = next_value * (1 - dones[t])\n",
    "        else:\n",
    "            next_val = values[t + 1] * (1 - dones[t])\n",
    "        \n",
    "        # TD error\n",
    "        delta = rewards[t] + gamma * next_val - values[t]\n",
    "        \n",
    "        # GAE: exponentially-weighted sum of TD errors\n",
    "        gae = delta + gamma * lam * (1 - dones[t]) * gae\n",
    "        advantages[t] = gae\n",
    "    \n",
    "    returns = advantages + np.array(values)\n",
    "    return advantages, returns\n",
    "\n",
    "\n",
    "# Demonstrate GAE with different lambda values\n",
    "np.random.seed(42)\n",
    "T = 20\n",
    "rewards = np.random.randn(T) * 0.5 + 0.5\n",
    "values = np.cumsum(np.random.randn(T) * 0.3) + 5\n",
    "next_value = values[-1] + 0.1\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "lambdas = [0.0, 0.5, 0.95, 1.0]\n",
    "colors = ['#e74c3c', '#f39c12', '#2ecc71', '#3498db']\n",
    "\n",
    "for lam, color in zip(lambdas, colors):\n",
    "    advantages, _ = compute_gae(rewards, values, next_value, gamma=0.99, lam=lam)\n",
    "    ax.plot(advantages, 'o-', color=color, linewidth=2, markersize=4,\n",
    "            label=f'λ = {lam} (var = {np.var(advantages):.2f})')\n",
    "\n",
    "ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "ax.set_xlabel('Time step', fontsize=12)\n",
    "ax.set_ylabel('Advantage estimate', fontsize=12)\n",
    "ax.set_title('GAE: Bias-Variance Tradeoff with λ', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"λ=0: Low variance but biased (only uses one-step TD error)\")\n",
    "print(\"λ=1: Unbiased but high variance (uses full returns)\")\n",
    "print(\"λ=0.95: Sweet spot — PPO's default, used in practice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Building PPO from Scratch\n",
    "\n",
    "Let's build a complete PPO implementation. The algorithm:\n",
    "\n",
    "1. **Collect** a batch of trajectories using the current policy\n",
    "2. **Compute** advantages using GAE\n",
    "3. **Optimize** the clipped surrogate objective for multiple epochs on the same batch\n",
    "4. **Repeat**\n",
    "\n",
    "The key innovation: PPO reuses the same batch for **multiple gradient steps** (unlike vanilla policy gradients which use each sample once). The clipping ensures these multiple steps don't move too far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleSimple:\n",
    "    \"\"\"CartPole environment (reused from Notebooks 18-19).\"\"\"\n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masscart + self.masspole\n",
    "        self.length = 0.5\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02\n",
    "        self.x_threshold = 2.4\n",
    "        self.theta_threshold = 12 * np.pi / 180\n",
    "        self.state_dim = 4\n",
    "        self.n_actions = 2\n",
    "        self.state = None\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.random.uniform(-0.05, 0.05, size=4)\n",
    "        return self.state.copy()\n",
    "    \n",
    "    def step(self, action):\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        cos_theta, sin_theta = np.cos(theta), np.sin(theta)\n",
    "        temp = (force + self.polemass_length * theta_dot**2 * sin_theta) / self.total_mass\n",
    "        theta_acc = (self.gravity * sin_theta - cos_theta * temp) / (\n",
    "            self.length * (4.0/3.0 - self.masspole * cos_theta**2 / self.total_mass))\n",
    "        x_acc = temp - self.polemass_length * theta_acc * cos_theta / self.total_mass\n",
    "        x += self.tau * x_dot; x_dot += self.tau * x_acc\n",
    "        theta += self.tau * theta_dot; theta_dot += self.tau * theta_acc\n",
    "        self.state = np.array([x, x_dot, theta, theta_dot])\n",
    "        done = (abs(x) > self.x_threshold or abs(theta) > self.theta_threshold)\n",
    "        return self.state.copy(), 1.0 if not done else 0.0, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOActorCritic(nn.Module):\n",
    "    \"\"\"Shared actor-critic network for PPO.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, n_actions, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.actor = nn.Linear(hidden_dim, n_actions)\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.shared(x)\n",
    "        action_logits = self.actor(features)\n",
    "        value = self.critic(features).squeeze(-1)\n",
    "        return action_logits, value\n",
    "    \n",
    "    def get_action_and_value(self, state):\n",
    "        logits, value = self.forward(state)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        return action, dist.log_prob(action), dist.entropy(), value\n",
    "    \n",
    "    def evaluate_action(self, state, action):\n",
    "        \"\"\"Evaluate a previously taken action (for PPO update).\"\"\"\n",
    "        logits, value = self.forward(state)\n",
    "        dist = Categorical(logits=logits)\n",
    "        return dist.log_prob(action), dist.entropy(), value\n",
    "\n",
    "\n",
    "class PPOAgent:\n",
    "    \"\"\"Proximal Policy Optimization agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, n_actions, lr=3e-4, gamma=0.99, lam=0.95,\n",
    "                 clip_epsilon=0.2, value_coef=0.5, entropy_coef=0.01,\n",
    "                 ppo_epochs=4, mini_batch_size=64, max_grad_norm=0.5):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        \n",
    "        self.network = PPOActorCritic(state_dim, n_actions)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "    \n",
    "    def collect_rollout(self, env, n_steps=2048):\n",
    "        \"\"\"Collect a batch of experience from the environment.\"\"\"\n",
    "        states, actions, rewards, dones = [], [], [], []\n",
    "        log_probs, values = [], []\n",
    "        \n",
    "        state = env.reset()\n",
    "        episode_rewards = []\n",
    "        episode_lengths = []\n",
    "        current_ep_reward = 0\n",
    "        current_ep_length = 0\n",
    "        \n",
    "        for _ in range(n_steps):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                action, log_prob, _, value = self.network.get_action_and_value(state_tensor)\n",
    "            \n",
    "            next_state, reward, done = env.step(action.item())\n",
    "            \n",
    "            states.append(state)\n",
    "            actions.append(action.item())\n",
    "            rewards.append(reward)\n",
    "            dones.append(float(done))\n",
    "            log_probs.append(log_prob.item())\n",
    "            values.append(value.item())\n",
    "            \n",
    "            current_ep_reward += reward\n",
    "            current_ep_length += 1\n",
    "            \n",
    "            if done:\n",
    "                episode_rewards.append(current_ep_reward)\n",
    "                episode_lengths.append(current_ep_length)\n",
    "                current_ep_reward = 0\n",
    "                current_ep_length = 0\n",
    "                state = env.reset()\n",
    "            else:\n",
    "                state = next_state\n",
    "        \n",
    "        # Bootstrap value for last state\n",
    "        with torch.no_grad():\n",
    "            _, next_value = self.network(torch.FloatTensor(state).unsqueeze(0))\n",
    "            next_value = next_value.item()\n",
    "        \n",
    "        # Compute GAE advantages\n",
    "        advantages, returns = compute_gae(\n",
    "            rewards, values, next_value, self.gamma, self.lam, dones\n",
    "        )\n",
    "        \n",
    "        rollout = {\n",
    "            'states': np.array(states),\n",
    "            'actions': np.array(actions),\n",
    "            'log_probs': np.array(log_probs),\n",
    "            'returns': returns,\n",
    "            'advantages': advantages,\n",
    "            'values': np.array(values),\n",
    "        }\n",
    "        \n",
    "        return rollout, episode_rewards, episode_lengths\n",
    "    \n",
    "    def update(self, rollout):\n",
    "        \"\"\"Perform PPO update on collected rollout.\"\"\"\n",
    "        states = torch.FloatTensor(rollout['states'])\n",
    "        actions = torch.LongTensor(rollout['actions'])\n",
    "        old_log_probs = torch.FloatTensor(rollout['log_probs'])\n",
    "        returns = torch.FloatTensor(rollout['returns'])\n",
    "        advantages = torch.FloatTensor(rollout['advantages'])\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        n_samples = len(states)\n",
    "        metrics = {'policy_loss': [], 'value_loss': [], 'entropy': [],\n",
    "                   'approx_kl': [], 'clip_fraction': []}\n",
    "        \n",
    "        # Multiple epochs over the same data (the PPO innovation!)\n",
    "        for epoch in range(self.ppo_epochs):\n",
    "            # Random mini-batch indices\n",
    "            indices = np.arange(n_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            for start in range(0, n_samples, self.mini_batch_size):\n",
    "                end = start + self.mini_batch_size\n",
    "                batch_idx = indices[start:end]\n",
    "                \n",
    "                # Get current policy's evaluation of old actions\n",
    "                new_log_probs, entropy, new_values = self.network.evaluate_action(\n",
    "                    states[batch_idx], actions[batch_idx]\n",
    "                )\n",
    "                \n",
    "                # Probability ratio\n",
    "                ratio = torch.exp(new_log_probs - old_log_probs[batch_idx])\n",
    "                \n",
    "                # Clipped surrogate objective\n",
    "                batch_advantages = advantages[batch_idx]\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon,\n",
    "                                    1 + self.clip_epsilon) * batch_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                # Value loss\n",
    "                value_loss = F.mse_loss(new_values, returns[batch_idx])\n",
    "                \n",
    "                # Entropy bonus\n",
    "                entropy_loss = -entropy.mean()\n",
    "                \n",
    "                # Total loss\n",
    "                total_loss = (policy_loss + \n",
    "                             self.value_coef * value_loss + \n",
    "                             self.entropy_coef * entropy_loss)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Track metrics\n",
    "                with torch.no_grad():\n",
    "                    approx_kl = (old_log_probs[batch_idx] - new_log_probs).mean().item()\n",
    "                    clip_frac = ((ratio - 1.0).abs() > self.clip_epsilon).float().mean().item()\n",
    "                \n",
    "                metrics['policy_loss'].append(policy_loss.item())\n",
    "                metrics['value_loss'].append(value_loss.item())\n",
    "                metrics['entropy'].append(-entropy_loss.item())\n",
    "                metrics['approx_kl'].append(approx_kl)\n",
    "                metrics['clip_fraction'].append(clip_frac)\n",
    "        \n",
    "        return {k: np.mean(v) for k, v in metrics.items()}\n",
    "\n",
    "\n",
    "print(\"PPO Agent architecture:\")\n",
    "agent = PPOAgent(state_dim=4, n_actions=2)\n",
    "print(agent.network)\n",
    "total_params = sum(p.numel() for p in agent.network.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"PPO epochs per update: {agent.ppo_epochs}\")\n",
    "print(f\"Clip epsilon: {agent.clip_epsilon}\")\n",
    "print(f\"GAE lambda: {agent.lam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(env, agent, n_iterations=50, n_steps_per_rollout=2048):\n",
    "    \"\"\"Train PPO agent.\"\"\"\n",
    "    all_rewards = []\n",
    "    all_lengths = []\n",
    "    all_metrics = []\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        # Collect experience\n",
    "        rollout, ep_rewards, ep_lengths = agent.collect_rollout(env, n_steps_per_rollout)\n",
    "        \n",
    "        # PPO update\n",
    "        metrics = agent.update(rollout)\n",
    "        \n",
    "        all_rewards.extend(ep_rewards)\n",
    "        all_lengths.extend(ep_lengths)\n",
    "        all_metrics.append(metrics)\n",
    "        \n",
    "        if (iteration + 1) % 5 == 0:\n",
    "            avg_reward = np.mean(ep_rewards) if ep_rewards else 0\n",
    "            avg_length = np.mean(ep_lengths) if ep_lengths else 0\n",
    "            print(f\"Iter {iteration+1:3d} | Avg Reward: {avg_reward:6.1f} | \"\n",
    "                  f\"Avg Length: {avg_length:5.1f} | \"\n",
    "                  f\"KL: {metrics['approx_kl']:.4f} | \"\n",
    "                  f\"Clip%: {metrics['clip_fraction']:.2%}\")\n",
    "    \n",
    "    return all_rewards, all_lengths, all_metrics\n",
    "\n",
    "\n",
    "# Train PPO\n",
    "env = CartPoleSimple()\n",
    "ppo_agent = PPOAgent(state_dim=4, n_actions=2, lr=3e-4)\n",
    "\n",
    "rewards_ppo, lengths_ppo, metrics_ppo = train_ppo(env, ppo_agent, n_iterations=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: PPO Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Episode lengths\n",
    "ax = axes[0, 0]\n",
    "window = 20\n",
    "ax.plot(lengths_ppo, alpha=0.3, color='#3498db')\n",
    "if len(lengths_ppo) >= window:\n",
    "    smoothed = np.convolve(lengths_ppo, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(lengths_ppo)), smoothed, color='#2c3e50', linewidth=2)\n",
    "ax.axhline(y=200, color='red', linestyle='--', label='Goal (200 steps)')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Episode Length')\n",
    "ax.set_title('PPO: Episode Length', fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Approximate KL divergence\n",
    "ax = axes[0, 1]\n",
    "kls = [m['approx_kl'] for m in metrics_ppo]\n",
    "ax.plot(kls, 'o-', color='#9b59b6', markersize=4)\n",
    "ax.set_xlabel('PPO Iteration')\n",
    "ax.set_ylabel('Approx KL Divergence')\n",
    "ax.set_title('KL Divergence (Policy Change per Update)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Clip fraction\n",
    "ax = axes[1, 0]\n",
    "clips = [m['clip_fraction'] for m in metrics_ppo]\n",
    "ax.plot(clips, 'o-', color='#e74c3c', markersize=4)\n",
    "ax.set_xlabel('PPO Iteration')\n",
    "ax.set_ylabel('Fraction of Clipped Updates')\n",
    "ax.set_title('Clip Fraction (How Often Clipping Activates)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Entropy\n",
    "ax = axes[1, 1]\n",
    "entropies = [m['entropy'] for m in metrics_ppo]\n",
    "ax.plot(entropies, 'o-', color='#2ecc71', markersize=4)\n",
    "ax.set_xlabel('PPO Iteration')\n",
    "ax.set_ylabel('Policy Entropy')\n",
    "ax.set_title('Entropy (Exploration Level)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('PPO Training Dashboard', fontsize=15, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observations:\")\n",
    "print(\"  - KL divergence stays small → policy changes are controlled\")\n",
    "print(\"  - Clip fraction shows how often the trust region is active\")\n",
    "print(\"  - Entropy gradually decreases as policy becomes more confident\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. PPO vs. Previous Methods\n",
    "\n",
    "Let's compare PPO against A2C and REINFORCE on the same task, using the same total number of environment steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2C agent (from Notebook 19) for comparison\n",
    "class A2CAgent:\n",
    "    def __init__(self, state_dim, n_actions, lr=3e-4, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        self.network = PPOActorCritic(state_dim, n_actions)  # Same architecture\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "    \n",
    "    def train_episode(self, env, max_steps=500):\n",
    "        state = env.reset()\n",
    "        log_probs, values, rewards, entropies = [], [], [], []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            state_t = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action, lp, ent, val = self.network.get_action_and_value(state_t)\n",
    "            next_state, reward, done = env.step(action.item())\n",
    "            \n",
    "            log_probs.append(lp)\n",
    "            values.append(val)\n",
    "            rewards.append(reward)\n",
    "            entropies.append(ent)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Compute returns\n",
    "        returns_list = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns_list.insert(0, G)\n",
    "        returns_t = torch.FloatTensor(returns_list)\n",
    "        \n",
    "        values_t = torch.cat(values)\n",
    "        log_probs_t = torch.cat(log_probs)\n",
    "        entropies_t = torch.cat(entropies)\n",
    "        advantages = returns_t - values_t.detach()\n",
    "        if len(advantages) > 1:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        loss = (-(log_probs_t * advantages).mean() + \n",
    "                0.5 * F.mse_loss(values_t, returns_t) - \n",
    "                0.01 * entropies_t.mean())\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return sum(rewards), step + 1\n",
    "\n",
    "\n",
    "# Run comparison\n",
    "print(\"Training A2C for comparison...\")\n",
    "a2c_agent = A2CAgent(4, 2)\n",
    "l_a2c = []\n",
    "for ep in range(500):\n",
    "    _, length = a2c_agent.train_episode(CartPoleSimple())\n",
    "    l_a2c.append(length)\n",
    "    if (ep + 1) % 100 == 0:\n",
    "        print(f\"  Episode {ep+1}: avg length = {np.mean(l_a2c[-100:]):.1f}\")\n",
    "\n",
    "# Compare\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "window = 30\n",
    "\n",
    "for data, label, color in [(lengths_ppo, 'PPO', '#2ecc71'),\n",
    "                            (l_a2c, 'A2C', '#3498db')]:\n",
    "    if len(data) >= window:\n",
    "        smoothed = np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "        ax.plot(smoothed, label=label, color=color, linewidth=2.5)\n",
    "\n",
    "ax.axhline(y=200, color='gray', linestyle='--', alpha=0.5, label='Goal')\n",
    "ax.set_xlabel('Episode', fontsize=12)\n",
    "ax.set_ylabel('Episode Length (smoothed)', fontsize=12)\n",
    "ax.set_title('PPO vs A2C: Stability and Performance', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. From RL to RLHF: The Complete Pipeline\n",
    "\n",
    "Now we connect everything to **language model alignment**. The RLHF pipeline has three stages:\n",
    "\n",
    "### Stage 1: Supervised Fine-Tuning (SFT)\n",
    "Train the LM on high-quality demonstrations (human-written responses).\n",
    "\n",
    "### Stage 2: Reward Model Training\n",
    "Train a model to predict human preferences. Given two responses, it learns which one humans prefer.\n",
    "\n",
    "### Stage 3: PPO Optimization\n",
    "Use PPO to optimize the LM's policy to maximize the reward model's scores, with a KL penalty to stay close to the SFT model.\n",
    "\n",
    "$$\\text{objective} = \\mathbb{E}_{x \\sim D, y \\sim \\pi_\\theta}\\left[R_\\phi(x, y) - \\beta \\cdot D_{KL}(\\pi_\\theta \\| \\pi_{SFT})\\right]$$\n",
    "\n",
    "The KL penalty is **critical** — without it, the model would find degenerate ways to maximize the reward model (reward hacking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: RLHF Pipeline\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_title('The RLHF Pipeline', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Stage boxes\n",
    "stages = [\n",
    "    (0.5, 7, 3.5, 2, 'Stage 1: SFT', '#3498db',\n",
    "     'Fine-tune LM on\\nhuman demonstrations'),\n",
    "    (5, 7, 3.5, 2, 'Stage 2: Reward\\nModel', '#e74c3c',\n",
    "     'Learn human\\npreferences'),\n",
    "    (9.5, 7, 3.5, 2, 'Stage 3: PPO', '#2ecc71',\n",
    "     'Optimize policy with\\nreward + KL penalty'),\n",
    "]\n",
    "\n",
    "for x, y, w, h, title, color, desc in stages:\n",
    "    box = mpatches.FancyBboxPatch((x, y), w, h, boxstyle=\"round,pad=0.3\",\n",
    "                                   facecolor=color, edgecolor='black', linewidth=2, alpha=0.9)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x + w/2, y + h/2 + 0.2, title, ha='center', va='center',\n",
    "            fontsize=11, fontweight='bold', color='white')\n",
    "    ax.text(x + w/2, y - 0.5, desc, ha='center', va='center',\n",
    "            fontsize=9, color='gray', style='italic')\n",
    "\n",
    "# Arrows between stages\n",
    "ax.annotate('', xy=(5, 8), xytext=(4, 8),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2.5, color='gray'))\n",
    "ax.annotate('', xy=(9.5, 8), xytext=(8.5, 8),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2.5, color='gray'))\n",
    "\n",
    "# Data flows\n",
    "data_items = [\n",
    "    (2.25, 5.8, 'Human\\nDemonstrations', '#3498db'),\n",
    "    (6.75, 5.8, 'Comparison\\nData (A vs B)', '#e74c3c'),\n",
    "    (11.25, 5.8, 'PPO + KL Penalty\\n+ Reward Signal', '#2ecc71'),\n",
    "]\n",
    "\n",
    "for x, y, text, color in data_items:\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=9,\n",
    "            color=color, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', \n",
    "                     edgecolor=color, alpha=0.8))\n",
    "\n",
    "# Result\n",
    "result_box = mpatches.FancyBboxPatch((4, 2), 6, 1.5, boxstyle=\"round,pad=0.3\",\n",
    "                                      facecolor='#f39c12', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(result_box)\n",
    "ax.text(7, 2.75, 'Aligned Language Model\\nHelpful, Harmless, Honest',\n",
    "        ha='center', va='center', fontsize=12, fontweight='bold', color='white')\n",
    "\n",
    "ax.annotate('', xy=(7, 3.5), xytext=(7, 5.3),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2.5, color='#f39c12'))\n",
    "\n",
    "# RL mapping\n",
    "mapping = [\n",
    "    (1, 1, 'Agent: LM'),\n",
    "    (4, 1, 'State: Prompt + context'),\n",
    "    (7.5, 1, 'Action: Next token'),\n",
    "    (11, 1, 'Reward: R(x,y) - β·KL'),\n",
    "]\n",
    "\n",
    "for x, y, text in mapping:\n",
    "    ax.text(x, y, text, fontsize=9, color='#2c3e50',\n",
    "            bbox=dict(boxstyle='round,pad=0.2', facecolor='#ecf0f1', alpha=0.8))\n",
    "\n",
    "ax.text(7, 0.3, 'RL Mapping', ha='center', fontsize=10, fontweight='bold', color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Building a Reward Model\n",
    "\n",
    "The reward model is trained on **preference data**: pairs of responses where humans indicated which they prefer. Let's build one from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "    \"\"\"Reward model trained on preference data.\n",
    "    \n",
    "    Takes a (prompt, response) embedding and outputs a scalar reward score.\n",
    "    Trained with the Bradley-Terry preference model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # Scalar reward\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "\n",
    "def generate_preference_data(n_pairs=1000, input_dim=16):\n",
    "    \"\"\"Simulate preference data.\n",
    "    \n",
    "    We simulate a hidden 'quality' function that humans implicitly use to\n",
    "    rank responses. The reward model must learn to approximate this function.\n",
    "    \"\"\"\n",
    "    # Hidden quality function: true reward = linear combination + noise\n",
    "    true_weights = np.random.randn(input_dim)\n",
    "    \n",
    "    pairs_chosen = []\n",
    "    pairs_rejected = []\n",
    "    \n",
    "    for _ in range(n_pairs):\n",
    "        # Generate two candidate responses (as embeddings)\n",
    "        response_a = np.random.randn(input_dim) * 0.5\n",
    "        response_b = np.random.randn(input_dim) * 0.5\n",
    "        \n",
    "        # True quality scores\n",
    "        quality_a = np.dot(response_a, true_weights) + np.random.randn() * 0.3\n",
    "        quality_b = np.dot(response_b, true_weights) + np.random.randn() * 0.3\n",
    "        \n",
    "        # Human \"chooses\" the higher quality response\n",
    "        if quality_a > quality_b:\n",
    "            pairs_chosen.append(response_a)\n",
    "            pairs_rejected.append(response_b)\n",
    "        else:\n",
    "            pairs_chosen.append(response_b)\n",
    "            pairs_rejected.append(response_a)\n",
    "    \n",
    "    return (torch.FloatTensor(np.array(pairs_chosen)),\n",
    "            torch.FloatTensor(np.array(pairs_rejected)),\n",
    "            true_weights)\n",
    "\n",
    "\n",
    "def train_reward_model(model, chosen, rejected, n_epochs=50, lr=1e-3):\n",
    "    \"\"\"Train reward model using Bradley-Terry preference loss.\n",
    "    \n",
    "    Loss = -log(sigmoid(r_chosen - r_rejected))\n",
    "    This is equivalent to: chosen should score higher than rejected.\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        r_chosen = model(chosen)\n",
    "        r_rejected = model(rejected)\n",
    "        \n",
    "        # Bradley-Terry loss: -log P(chosen > rejected)\n",
    "        loss = -F.logsigmoid(r_chosen - r_rejected).mean()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accuracy: how often does the model rank chosen > rejected?\n",
    "        with torch.no_grad():\n",
    "            accuracy = (r_chosen > r_rejected).float().mean().item()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1:3d} | Loss: {loss.item():.4f} | Accuracy: {accuracy:.3f}\")\n",
    "    \n",
    "    return losses, accuracies\n",
    "\n",
    "\n",
    "# Generate data and train\n",
    "input_dim = 16\n",
    "chosen, rejected, true_weights = generate_preference_data(n_pairs=2000, input_dim=input_dim)\n",
    "\n",
    "reward_model = RewardModel(input_dim)\n",
    "rm_losses, rm_accuracies = train_reward_model(reward_model, chosen, rejected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(rm_losses, color='#e74c3c', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Bradley-Terry Loss', fontsize=12)\n",
    "axes[0].set_title('Reward Model: Training Loss', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(rm_accuracies, color='#2ecc71', linewidth=2)\n",
    "axes[1].axhline(y=0.5, color='gray', linestyle='--', label='Random baseline')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Preference Accuracy', fontsize=12)\n",
    "axes[1].set_title('Reward Model: Agreement with Human Preferences', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(0.4, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verify: reward model scores align with true quality\n",
    "with torch.no_grad():\n",
    "    test_responses = torch.randn(200, input_dim) * 0.5\n",
    "    predicted_rewards = reward_model(test_responses).numpy()\n",
    "    true_rewards = test_responses.numpy() @ true_weights\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 6))\n",
    "ax.scatter(true_rewards, predicted_rewards, alpha=0.5, s=30, color='#3498db')\n",
    "# Fit line\n",
    "z = np.polyfit(true_rewards, predicted_rewards, 1)\n",
    "p = np.poly1d(z)\n",
    "ax.plot(sorted(true_rewards), p(sorted(true_rewards)), 'r-', linewidth=2, label=f'Correlation')\n",
    "ax.set_xlabel('True Quality Score', fontsize=12)\n",
    "ax.set_ylabel('Reward Model Prediction', fontsize=12)\n",
    "ax.set_title('Reward Model Captures True Preferences', fontsize=13, fontweight='bold')\n",
    "correlation = np.corrcoef(true_rewards, predicted_rewards)[0, 1]\n",
    "ax.text(0.05, 0.95, f'Correlation: {correlation:.3f}', transform=ax.transAxes,\n",
    "        fontsize=12, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. The KL Penalty: Preventing Reward Hacking\n",
    "\n",
    "Without constraints, RL optimization will find **degenerate solutions** that maximize the reward model's score without actually being helpful. This is called **reward hacking**.\n",
    "\n",
    "Example: A reward model might give high scores to long, confident-sounding responses. Without a KL penalty, the LM would learn to generate extremely long, repetitive text that sounds confident but says nothing useful.\n",
    "\n",
    "The **KL divergence penalty** keeps the RL-trained policy close to the SFT model:\n",
    "\n",
    "$$\\text{reward}_{\\text{total}} = R_\\phi(x, y) - \\beta \\cdot D_{KL}(\\pi_\\theta(\\cdot|x) \\| \\pi_{SFT}(\\cdot|x))$$\n",
    "\n",
    "- $\\beta$ small: More freedom to optimize reward → risk of reward hacking\n",
    "- $\\beta$ large: Stay close to SFT model → limited improvement from RL\n",
    "- $\\beta$ just right: Meaningful improvement while maintaining quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the KL penalty effect\n",
    "def simulate_rlhf_with_kl(beta, n_steps=100):\n",
    "    \"\"\"Simulate RLHF optimization with different KL penalty strengths.\"\"\"\n",
    "    # Simulate a policy as a distribution over 5 \"response styles\"\n",
    "    sft_policy = np.array([0.2, 0.3, 0.15, 0.25, 0.1])  # SFT baseline\n",
    "    reward_scores = np.array([0.3, 0.8, -0.2, 0.9, -0.5])  # Reward model scores\n",
    "    \n",
    "    policy = sft_policy.copy()\n",
    "    policy_history = [policy.copy()]\n",
    "    reward_history = []\n",
    "    kl_history = []\n",
    "    \n",
    "    lr = 0.1\n",
    "    for step in range(n_steps):\n",
    "        # Sample from current policy\n",
    "        action = np.random.choice(5, p=policy)\n",
    "        reward = reward_scores[action]\n",
    "        \n",
    "        # KL divergence from SFT policy\n",
    "        kl = np.sum(policy * np.log(policy / (sft_policy + 1e-8) + 1e-8))\n",
    "        \n",
    "        # Total reward with KL penalty\n",
    "        total_reward = reward - beta * kl\n",
    "        \n",
    "        # Update (simplified policy gradient)\n",
    "        gradient = np.zeros(5)\n",
    "        gradient[action] = total_reward\n",
    "        logits = np.log(policy + 1e-8) + lr * gradient\n",
    "        policy = np.exp(logits) / np.exp(logits).sum()\n",
    "        \n",
    "        policy_history.append(policy.copy())\n",
    "        reward_history.append(reward)\n",
    "        kl_history.append(kl)\n",
    "    \n",
    "    return np.array(policy_history), reward_history, kl_history\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "betas = [0.0, 0.1, 1.0]\n",
    "titles = ['β=0 (No KL penalty)', 'β=0.1 (Moderate)', 'β=1.0 (Strong)']\n",
    "style_names = ['Verbose', 'Concise', 'Rude', 'Helpful', 'Off-topic']\n",
    "style_colors = ['#e74c3c', '#2ecc71', '#95a5a6', '#3498db', '#f39c12']\n",
    "\n",
    "for ax, beta, title in zip(axes, betas, titles):\n",
    "    policy_hist, rewards, kls = simulate_rlhf_with_kl(beta, n_steps=200)\n",
    "    \n",
    "    for i, (name, color) in enumerate(zip(style_names, style_colors)):\n",
    "        ax.plot(policy_hist[:, i], color=color, linewidth=2, label=name)\n",
    "    \n",
    "    ax.set_xlabel('Step', fontsize=11)\n",
    "    ax.set_ylabel('Policy probability', fontsize=11)\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if ax == axes[2]:\n",
    "        ax.legend(fontsize=8, loc='center right')\n",
    "\n",
    "plt.suptitle('KL Penalty Controls Policy Drift from SFT', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"β=0: Policy collapses to one style (reward hacking)\")\n",
    "print(\"β=0.1: Policy shifts toward high-reward styles while maintaining diversity\")\n",
    "print(\"β=1.0: Policy barely moves from SFT (too conservative)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Simplified RLHF Pipeline\n",
    "\n",
    "Let's build a complete (simplified) RLHF pipeline that trains a small \"language model\" using PPO with a reward model and KL penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLanguageModel(nn.Module):\n",
    "    \"\"\"Simplified 'language model' that generates responses as continuous vectors.\n",
    "    \n",
    "    In reality, LLMs output token probabilities. We simplify by having the\n",
    "    model output a response embedding directly.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, prompt_dim=8, response_dim=16, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(prompt_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.mean_head = nn.Linear(hidden_dim, response_dim)\n",
    "        self.log_std = nn.Parameter(torch.zeros(response_dim) - 1.0)\n",
    "    \n",
    "    def forward(self, prompt):\n",
    "        features = self.net(prompt)\n",
    "        mean = self.mean_head(features)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return mean, std\n",
    "    \n",
    "    def generate(self, prompt):\n",
    "        \"\"\"Generate a response (sample from policy).\"\"\"\n",
    "        mean, std = self.forward(prompt)\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        response = dist.rsample()\n",
    "        log_prob = dist.log_prob(response).sum(dim=-1)\n",
    "        return response, log_prob\n",
    "    \n",
    "    def log_prob(self, prompt, response):\n",
    "        \"\"\"Compute log probability of a response under this model.\"\"\"\n",
    "        mean, std = self.forward(prompt)\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        return dist.log_prob(response).sum(dim=-1)\n",
    "\n",
    "\n",
    "class RLHFTrainer:\n",
    "    \"\"\"Complete RLHF training pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, prompt_dim=8, response_dim=16, kl_coef=0.1,\n",
    "                 clip_epsilon=0.2, lr=1e-4):\n",
    "        self.kl_coef = kl_coef\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        \n",
    "        # The policy being trained\n",
    "        self.policy = SimpleLanguageModel(prompt_dim, response_dim)\n",
    "        \n",
    "        # SFT reference model (frozen)\n",
    "        self.ref_model = SimpleLanguageModel(prompt_dim, response_dim)\n",
    "        self.ref_model.load_state_dict(self.policy.state_dict())\n",
    "        for param in self.ref_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Pre-trained reward model\n",
    "        self.reward_model = reward_model  # From section 6\n",
    "        for param in self.reward_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "    \n",
    "    def compute_reward(self, prompts, responses):\n",
    "        \"\"\"Compute reward = R(x,y) - β * KL(π || π_ref).\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Reward model score\n",
    "            rm_reward = self.reward_model(responses)\n",
    "            \n",
    "            # KL divergence between policy and reference\n",
    "            policy_logprob = self.policy.log_prob(prompts, responses)\n",
    "            ref_logprob = self.ref_model.log_prob(prompts, responses)\n",
    "            kl_div = policy_logprob - ref_logprob\n",
    "            \n",
    "            # Total reward\n",
    "            total_reward = rm_reward - self.kl_coef * kl_div\n",
    "        \n",
    "        return total_reward, rm_reward, kl_div\n",
    "    \n",
    "    def train_step(self, prompts, n_ppo_epochs=4):\n",
    "        \"\"\"One PPO training step.\"\"\"\n",
    "        # Generate responses from current policy\n",
    "        with torch.no_grad():\n",
    "            responses, old_log_probs = self.policy.generate(prompts)\n",
    "        \n",
    "        # Compute rewards\n",
    "        total_rewards, rm_rewards, kl_divs = self.compute_reward(prompts, responses)\n",
    "        \n",
    "        # Normalize rewards (advantage estimate)\n",
    "        advantages = (total_rewards - total_rewards.mean()) / (total_rewards.std() + 1e-8)\n",
    "        \n",
    "        # PPO update\n",
    "        for _ in range(n_ppo_epochs):\n",
    "            new_log_probs = self.policy.log_prob(prompts, responses)\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs.detach())\n",
    "            \n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon,\n",
    "                               1 + self.clip_epsilon) * advantages\n",
    "            loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            'loss': loss.item(),\n",
    "            'rm_reward': rm_rewards.mean().item(),\n",
    "            'kl_div': kl_divs.mean().item(),\n",
    "            'total_reward': total_rewards.mean().item(),\n",
    "        }\n",
    "\n",
    "\n",
    "# Run RLHF training\n",
    "trainer = RLHFTrainer(prompt_dim=8, response_dim=16, kl_coef=0.1)\n",
    "\n",
    "history = {'rm_reward': [], 'kl_div': [], 'total_reward': []}\n",
    "\n",
    "print(\"Training RLHF pipeline...\")\n",
    "for step in range(200):\n",
    "    # Generate random prompts\n",
    "    prompts = torch.randn(32, 8) * 0.5\n",
    "    \n",
    "    metrics = trainer.train_step(prompts)\n",
    "    \n",
    "    for key in history:\n",
    "        history[key].append(metrics[key])\n",
    "    \n",
    "    if (step + 1) % 50 == 0:\n",
    "        print(f\"Step {step+1:3d} | RM Reward: {metrics['rm_reward']:7.3f} | \"\n",
    "              f\"KL: {metrics['kl_div']:6.3f} | Total: {metrics['total_reward']:7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RLHF training\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Reward model score\n",
    "ax = axes[0]\n",
    "ax.plot(history['rm_reward'], color='#2ecc71', linewidth=2)\n",
    "ax.set_xlabel('Step', fontsize=12)\n",
    "ax.set_ylabel('Reward Model Score', fontsize=12)\n",
    "ax.set_title('RM Reward Increases\\n(Model learns to please RM)', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# KL divergence\n",
    "ax = axes[1]\n",
    "ax.plot(history['kl_div'], color='#e74c3c', linewidth=2)\n",
    "ax.set_xlabel('Step', fontsize=12)\n",
    "ax.set_ylabel('KL Divergence', fontsize=12)\n",
    "ax.set_title('KL Divergence from SFT\\n(Policy drift, controlled by β)', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Total reward\n",
    "ax = axes[2]\n",
    "ax.plot(history['total_reward'], color='#3498db', linewidth=2)\n",
    "ax.set_xlabel('Step', fontsize=12)\n",
    "ax.set_ylabel('Total Reward (RM - β·KL)', fontsize=12)\n",
    "ax.set_title('Total RLHF Objective\\n(Balances quality and safety)', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('RLHF Training Progress', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The RLHF pipeline successfully:\")\n",
    "print(\"  1. Increases reward model score (learns better responses)\")\n",
    "print(\"  2. Controls KL divergence (doesn't drift too far from SFT)\")\n",
    "print(\"  3. Maximizes the total objective (quality + safety balance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. The Full Picture: From Linear Algebra to RLHF\n",
    "\n",
    "Let's trace how every notebook in this curriculum connects to RLHF:\n",
    "\n",
    "| Notebook | Topic | How It's Used in RLHF |\n",
    "|----------|-------|----------------------|\n",
    "| **01** | Linear Algebra | Embeddings, weight matrices, attention |\n",
    "| **02** | Calculus | Gradients, backpropagation, optimization |\n",
    "| **03** | Probability | KL divergence, policy distributions, Bradley-Terry model |\n",
    "| **04** | Python OOP | Model architectures, training loops |\n",
    "| **05** | NumPy | Efficient tensor operations |\n",
    "| **06** | Perceptrons | Foundation of neural networks |\n",
    "| **07** | Backpropagation | How all networks learn |\n",
    "| **08** | PyTorch | Framework for building everything |\n",
    "| **09** | Training Deep Networks | Optimization, regularization, stability |\n",
    "| **10** | CNNs | Feature extraction (vision RL uses CNNs) |\n",
    "| **11** | RNNs | Sequential processing, precursor to transformers |\n",
    "| **12** | Attention | The core mechanism of transformers |\n",
    "| **13** | Transformers | Architecture of the language model being aligned |\n",
    "| **14** | Language Models | The base model that RLHF fine-tunes |\n",
    "| **15** | Embeddings | How text is represented as vectors |\n",
    "| **16** | Fine-tuning & PEFT | SFT stage of RLHF, LoRA for efficient training |\n",
    "| **17** | RL Fundamentals | MDPs, value functions, Bellman equations |\n",
    "| **18** | Q-Learning & DQN | Value-based RL, foundation for understanding |\n",
    "| **19** | Policy Gradients | Policy optimization, the mechanism PPO uses |\n",
    "| **20** | PPO & Modern RL | **The algorithm that aligns language models** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The journey visualization\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "ax.set_xlim(0, 20)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "\n",
    "parts = [\n",
    "    (1, 'Math\\nFoundations', '#3498db', '01-03', 10),\n",
    "    (4.5, 'Python\\nFoundations', '#2ecc71', '04-05', 8.5),\n",
    "    (7.5, 'Neural Network\\nFundamentals', '#e74c3c', '06-09', 7),\n",
    "    (10.5, 'Neural Network\\nArchitectures', '#9b59b6', '10-12', 5.5),\n",
    "    (13.5, 'Transformers\\n& LLMs', '#f39c12', '13-16', 4),\n",
    "    (16.5, 'Reinforcement\\nLearning', '#1abc9c', '17-20', 2.5),\n",
    "]\n",
    "\n",
    "for x, label, color, nbs, y in parts:\n",
    "    box = mpatches.FancyBboxPatch((x, y), 2.5, 1.5, boxstyle=\"round,pad=0.2\",\n",
    "                                   facecolor=color, edgecolor='black', linewidth=2, alpha=0.9)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x + 1.25, y + 0.75, label, ha='center', va='center',\n",
    "            fontsize=9, fontweight='bold', color='white')\n",
    "    ax.text(x + 1.25, y - 0.3, f'NB {nbs}', ha='center', fontsize=8, color='gray')\n",
    "\n",
    "# Arrows connecting them\n",
    "for i in range(len(parts) - 1):\n",
    "    x1 = parts[i][0] + 2.5\n",
    "    y1 = parts[i][4] + 0.75\n",
    "    x2 = parts[i+1][0]\n",
    "    y2 = parts[i+1][4] + 0.75\n",
    "    ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n",
    "               arrowprops=dict(arrowstyle='->', lw=2, color='gray',\n",
    "                              connectionstyle='arc3,rad=0.15'))\n",
    "\n",
    "# Final destination\n",
    "box = mpatches.FancyBboxPatch((6, 0.2), 8, 1.2, boxstyle=\"round,pad=0.3\",\n",
    "                               facecolor='#2c3e50', edgecolor='gold', linewidth=3)\n",
    "ax.add_patch(box)\n",
    "ax.text(10, 0.8, 'RLHF: Aligned Language Models', ha='center', va='center',\n",
    "        fontsize=14, fontweight='bold', color='gold')\n",
    "\n",
    "ax.annotate('', xy=(10, 1.4), xytext=(17.75, 2.5),\n",
    "           arrowprops=dict(arrowstyle='->', lw=3, color='gold'))\n",
    "\n",
    "ax.set_title('The Complete Learning Journey: 20 Notebooks', fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: PPO Hyperparameter Study\n",
    "\n",
    "The clipping parameter ε is crucial. Train PPO on CartPole with ε = {0.05, 0.1, 0.2, 0.3, 0.5}. Plot learning curves and clip fractions. What happens when ε is too small or too large?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Hint: Create PPOAgent instances with different clip_epsilon values\n",
    "# and compare their training curves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Reward Hacking Demonstration\n",
    "\n",
    "Train the RLHF pipeline with β=0 (no KL penalty) and β=0.5 (strong penalty). Show that without the KL penalty, the model finds degenerate solutions that maximize the reward model but produce low-quality outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Hint: Run RLHFTrainer with different kl_coef values\n",
    "# Monitor both rm_reward and kl_div over training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: GAE Lambda Ablation\n",
    "\n",
    "Train PPO with λ = {0, 0.5, 0.9, 0.95, 1.0} and compare learning stability and final performance. Verify that λ=0 (pure TD) has lower variance but higher bias, while λ=1 (Monte Carlo) has higher variance but lower bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "# Hint: Modify the lam parameter in PPOAgent and compare training curves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **PPO** uses a clipped surrogate objective to prevent destructively large policy updates: $L^{CLIP} = \\min(r_t \\hat{A}_t, \\text{clip}(r_t, 1\\pm\\epsilon) \\hat{A}_t)$\n",
    "- **GAE** ($\\lambda$) provides a tunable bias-variance tradeoff for advantage estimation\n",
    "- PPO reuses data for **multiple gradient steps** per batch (unlike vanilla policy gradients)\n",
    "- The **reward model** learns human preferences from comparison data using the Bradley-Terry model\n",
    "- The **KL penalty** prevents reward hacking by keeping the policy close to the SFT reference\n",
    "- The RLHF pipeline: **SFT → Reward Model → PPO** transforms a base LM into an aligned assistant\n",
    "\n",
    "### Fundamental Insight\n",
    "\n",
    "PPO's genius is simplicity: a single clipping operation replaces TRPO's complex constrained optimization while achieving similar results. This simplicity is what made it practical enough to scale to RLHF with billion-parameter language models. The algorithm that makes AI assistants helpful, harmless, and honest is, at its core, just the policy gradient theorem + a clipped ratio + a KL penalty.\n",
    "\n",
    "### The Complete Journey\n",
    "\n",
    "From matrix multiplication to RLHF, we've traced the complete path from mathematical foundations through neural network architectures, language models, and reinforcement learning. Every concept built on the last — linear algebra enabled neural networks, which enabled transformers, which enabled language models, which are aligned using RL. You now have the conceptual and implementation foundation to understand how modern AI systems work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "Congratulations on completing the full curriculum! Here are paths for continued learning:\n",
    "\n",
    "- **Scaling**: How do these techniques work at the scale of GPT-4 / Claude? Study distributed training, mixed precision, model parallelism\n",
    "- **DPO**: Direct Preference Optimization — an alternative to RLHF that skips the reward model entirely\n",
    "- **Constitutional AI**: Anthropic's approach to alignment using AI-generated feedback\n",
    "- **Multi-modal models**: Extending transformers to vision, audio, and beyond\n",
    "- **Agents**: Using LLMs as reasoning engines that take actions in the world\n",
    "- **Safety & Alignment**: The broader challenge of ensuring AI systems remain beneficial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
