{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.6: Optimization Theory for Machine Learning\n",
    "\n",
    "Every machine learning model learns by optimizing an objective function. Understanding optimization theory\n",
    "is what separates practitioners who *use* ML from those who *understand* it. When your model fails to\n",
    "converge, when training is painfully slow, or when your network mysteriously generalizes despite having\n",
    "millions of parameters \u2014 optimization theory tells you why.\n",
    "\n",
    "This notebook covers the theoretical foundations: convergence guarantees, the bias-variance tradeoff,\n",
    "generalization theory, and regularization. These ideas form the bridge between the math foundations\n",
    "of Part 1 and the practical deep learning of Parts 3+."
   ],
   "id": "cell-0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- [ ] Analyze gradient descent convergence rates for convex and strongly convex functions\n",
    "- [ ] Explain why convexity guarantees global optima and how condition number affects convergence\n",
    "- [ ] Compare gradient descent vs. stochastic gradient descent and understand mini-batch tradeoffs\n",
    "- [ ] Derive the bias-variance decomposition and connect it to model selection\n",
    "- [ ] Understand VC dimension as a measure of model capacity\n",
    "- [ ] Explain the PAC learning framework and sample complexity\n",
    "- [ ] Visualize why L1 regularization produces sparsity and L2 produces small weights\n",
    "- [ ] Discuss why overparameterized networks generalize (double descent, implicit regularization)\n",
    "\n",
    "---"
   ],
   "id": "cell-1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle, FancyArrowPatch, Ellipse\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)"
   ],
   "id": "cell-2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient Descent Convergence\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "Gradient descent is the workhorse of machine learning optimization. At each step, we move in the\n",
    "direction of steepest descent:\n",
    "\n",
    "$$x_{t+1} = x_t - \\eta \\nabla f(x_t)$$\n",
    "\n",
    "But how fast does it converge? The answer depends on two things:\n",
    "1. **The step size** $\\eta$ (learning rate) \u2014 too large and we overshoot, too small and we crawl\n",
    "2. **The function's properties** \u2014 convex functions converge faster than non-convex ones\n",
    "\n",
    "**Key convergence rates:**\n",
    "\n",
    "| Function Type | Rate | What It Means |\n",
    "|--------------|------|---------------|\n",
    "| Convex | $O(1/t)$ | Error halves every time you double iterations |\n",
    "| Strongly convex | $O(e^{-ct})$ | Error decreases exponentially (linear convergence) |\n",
    "| Non-convex | No guarantee | May get stuck at local minima or saddle points |\n",
    "\n",
    "The difference is dramatic: for a strongly convex function, 100 iterations might suffice where a merely\n",
    "convex function needs 10,000."
   ],
   "id": "cell-3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(grad_f, x0, lr, n_steps, f=None):\n",
    "    \"\"\"\n",
    "    Gradient descent with convergence tracking.\n",
    "\n",
    "    Args:\n",
    "        grad_f: Function that returns the gradient at x\n",
    "        x0: Initial point (numpy array)\n",
    "        lr: Learning rate (step size)\n",
    "        n_steps: Number of iterations\n",
    "        f: Optional objective function for tracking loss\n",
    "\n",
    "    Returns:\n",
    "        history: dict with 'x' (trajectory), 'f' (function values), 'grad_norm' (gradient norms)\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float)\n",
    "    history = {'x': [x.copy()], 'f': [], 'grad_norm': []}\n",
    "\n",
    "    if f is not None:\n",
    "        history['f'].append(f(x))\n",
    "\n",
    "    for t in range(n_steps):\n",
    "        g = grad_f(x)\n",
    "        history['grad_norm'].append(np.linalg.norm(g))\n",
    "        x = x - lr * g\n",
    "        history['x'].append(x.copy())\n",
    "        if f is not None:\n",
    "            history['f'].append(f(x))\n",
    "\n",
    "    history['x'] = np.array(history['x'])\n",
    "    return history\n",
    "\n",
    "# Example: quadratic function f(x) = 0.5 * x^T A x\n",
    "# Strongly convex when all eigenvalues of A are positive\n",
    "A_well = np.array([[2.0, 0.0], [0.0, 2.0]])   # condition number = 1\n",
    "A_ill  = np.array([[10.0, 0.0], [0.0, 1.0]])   # condition number = 10\n",
    "\n",
    "def make_quadratic(A):\n",
    "    f = lambda x: 0.5 * x @ A @ x\n",
    "    grad_f = lambda x: A @ x\n",
    "    return f, grad_f\n",
    "\n",
    "x0 = np.array([5.0, 5.0])\n",
    "print(\"Well-conditioned (kappa=1): eigenvalues =\", np.linalg.eigvalsh(A_well))\n",
    "print(\"Ill-conditioned (kappa=10): eigenvalues =\", np.linalg.eigvalsh(A_ill))"
   ],
   "id": "cell-4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convergence for different step sizes\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "f_well, grad_well = make_quadratic(A_well)\n",
    "\n",
    "step_sizes = [0.01, 0.1, 0.45]\n",
    "titles = ['Too Small (lr=0.01)', 'Just Right (lr=0.1)', 'Too Large (lr=0.45)']\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "for ax, lr, title, color in zip(axes, step_sizes, titles, colors):\n",
    "    hist = gradient_descent(grad_well, x0, lr, 50, f=f_well)\n",
    "\n",
    "    # Plot contours\n",
    "    xx, yy = np.meshgrid(np.linspace(-6, 6, 100), np.linspace(-6, 6, 100))\n",
    "    zz = np.array([f_well(np.array([xi, yi])) for xi, yi in zip(xx.ravel(), yy.ravel())]).reshape(xx.shape)\n",
    "    ax.contour(xx, yy, zz, levels=15, alpha=0.5, cmap='coolwarm')\n",
    "\n",
    "    # Plot trajectory\n",
    "    traj = hist['x']\n",
    "    ax.plot(traj[:, 0], traj[:, 1], 'o-', color=color, markersize=3, linewidth=1, alpha=0.7)\n",
    "    ax.plot(traj[0, 0], traj[0, 1], 'ko', markersize=8, label='Start')\n",
    "    ax.plot(0, 0, 'r*', markersize=15, label='Optimum')\n",
    "\n",
    "    ax.set_title(title, fontsize=13)\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.set_xlim(-6, 6)\n",
    "    ax.set_ylim(-6, 6)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "plt.suptitle('Effect of Step Size on Gradient Descent', fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell-5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare convergence rates: convex vs strongly convex\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Strongly convex: f(x) = 0.5 * x^T A x (quadratic)\n",
    "f_strong, grad_strong = make_quadratic(A_well)\n",
    "hist_strong = gradient_descent(grad_strong, x0, 0.3, 100, f=f_strong)\n",
    "\n",
    "# Convex but not strongly convex: f(x) = |x1| + 0.5*x2^2 (approximated with smooth version)\n",
    "# Use Huber-like: f(x) = sqrt(x1^2 + 0.01) + 0.5*x2^2\n",
    "def f_convex(x):\n",
    "    return np.sqrt(x[0]**2 + 0.01) + 0.5 * x[1]**2\n",
    "\n",
    "def grad_convex(x):\n",
    "    g1 = x[0] / np.sqrt(x[0]**2 + 0.01)\n",
    "    g2 = x[1]\n",
    "    return np.array([g1, g2])\n",
    "\n",
    "hist_convex = gradient_descent(grad_convex, x0, 0.3, 100, f=f_convex)\n",
    "\n",
    "# Plot function value convergence\n",
    "f_star_strong = 0.0\n",
    "f_star_convex = np.sqrt(0.01)  # minimum value\n",
    "\n",
    "ax = axes[0]\n",
    "errors_strong = np.array(hist_strong['f']) - f_star_strong\n",
    "errors_convex = np.array(hist_convex['f']) - f_star_convex\n",
    "ax.semilogy(errors_strong + 1e-16, 'b-', linewidth=2, label='Strongly Convex ($O(e^{-ct})$)')\n",
    "ax.semilogy(errors_convex + 1e-16, 'r-', linewidth=2, label='Convex ($O(1/t)$)')\n",
    "ax.set_xlabel('Iteration', fontsize=12)\n",
    "ax.set_ylabel('$f(x_t) - f^*$ (log scale)', fontsize=12)\n",
    "ax.set_title('Convergence Rate Comparison', fontsize=13)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot theoretical rates\n",
    "ax = axes[1]\n",
    "t = np.arange(1, 101)\n",
    "ax.semilogy(1.0 / t, 'r--', linewidth=2, label='$O(1/t)$ \u2014 Convex')\n",
    "ax.semilogy(np.exp(-0.1 * t), 'b--', linewidth=2, label='$O(e^{-ct})$ \u2014 Strongly Convex')\n",
    "ax.set_xlabel('Iteration', fontsize=12)\n",
    "ax.set_ylabel('Error (log scale)', fontsize=12)\n",
    "ax.set_title('Theoretical Convergence Rates', fontsize=13)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"After 100 iterations:\")\n",
    "print(f\"  Strongly convex error: {errors_strong[-1]:.2e}\")\n",
    "print(f\"  Convex error:          {errors_convex[-1]:.2e}\")"
   ],
   "id": "cell-6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: Choosing the Right Step Size\n",
    "\n",
    "The optimal step size depends on the function's **Lipschitz constant** $L$ \u2014 the maximum curvature of the gradient:\n",
    "\n",
    "$$\\|\\nabla f(x) - \\nabla f(y)\\| \\leq L \\|x - y\\|$$\n",
    "\n",
    "**The rule:** Set $\\eta \\leq 1/L$ for guaranteed convergence.\n",
    "\n",
    "For a quadratic $f(x) = \\frac{1}{2}x^T A x$, the Lipschitz constant is $L = \\lambda_{\\max}(A)$ (the largest eigenvalue).\n",
    "\n",
    "| Step Size | Behavior | Convergence |\n",
    "|-----------|----------|-------------|\n",
    "| $\\eta \\ll 1/L$ | Very slow, safe | Guaranteed but wastes compute |\n",
    "| $\\eta = 1/L$ | Optimal for GD | Fastest guaranteed convergence |\n",
    "| $\\eta > 2/L$ | Oscillates then diverges | No convergence |\n",
    "\n",
    "**What this means:** In practice, learning rate is the most important hyperparameter. Too large = training explodes. Too small = training takes forever. Learning rate schedulers (starting large, decreasing over time) try to get the best of both worlds."
   ],
   "id": "cell-7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Convexity and Guarantees\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "A function is **convex** if every chord lies above the curve \u2014 there are no \"valleys\" to get trapped in. This is the single most important property in optimization because it guarantees that any local minimum is also the *global* minimum.\n",
    "\n",
    "**Three levels of \"niceness\":**\n",
    "\n",
    "1. **Non-convex**: Multiple local minima, saddle points. No guarantees. (Most neural network losses)\n",
    "2. **Convex**: One global minimum (or a flat region of minima). GD converges at $O(1/t)$.\n",
    "3. **Strongly convex**: A unique global minimum with curvature bounded away from zero. GD converges exponentially.\n",
    "\n",
    "**What this means:** Strongly convex is the \"easy mode\" of optimization. Convex is \"medium.\" Non-convex is \"hard mode\" \u2014 and it's where deep learning lives."
   ],
   "id": "cell-8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize three types of functions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "x = np.linspace(-3, 3, 300)\n",
    "\n",
    "# Strongly convex: f(x) = x^2\n",
    "ax = axes[0]\n",
    "y = x**2\n",
    "ax.plot(x, y, 'b-', linewidth=2.5)\n",
    "ax.fill_between(x, y, alpha=0.1, color='blue')\n",
    "# Show chord above curve\n",
    "x1, x2 = -2, 1.5\n",
    "ax.plot([x1, x2], [x1**2, x2**2], 'r--', linewidth=2, label='Chord (always above)')\n",
    "ax.plot(0, 0, 'g*', markersize=15, label='Global minimum')\n",
    "ax.set_title('Strongly Convex: $f(x) = x^2$', fontsize=13)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Convex but not strongly: f(x) = |x|\n",
    "ax = axes[1]\n",
    "y = np.abs(x)\n",
    "ax.plot(x, y, 'b-', linewidth=2.5)\n",
    "ax.fill_between(x, y, alpha=0.1, color='blue')\n",
    "x1, x2 = -2, 1.5\n",
    "ax.plot([x1, x2], [abs(x1), abs(x2)], 'r--', linewidth=2, label='Chord (always above)')\n",
    "ax.plot(0, 0, 'g*', markersize=15, label='Global minimum')\n",
    "ax.set_title('Convex: $f(x) = |x|$', fontsize=13)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Non-convex: f(x) = x^4 - 3x^2 + x\n",
    "ax = axes[2]\n",
    "y = x**4 - 3*x**2 + 0.5*x\n",
    "ax.plot(x, y, 'b-', linewidth=2.5)\n",
    "ax.fill_between(x, y, y.min()-1, alpha=0.1, color='blue')\n",
    "# Find and mark local minima\n",
    "from scipy.optimize import minimize_scalar\n",
    "res1 = minimize_scalar(lambda t: t**4 - 3*t**2 + 0.5*t, bounds=(-2, 0), method='bounded')\n",
    "res2 = minimize_scalar(lambda t: t**4 - 3*t**2 + 0.5*t, bounds=(0, 2), method='bounded')\n",
    "ax.plot(res1.x, res1.fun, 'g*', markersize=15, label=f'Global min')\n",
    "ax.plot(res2.x, res2.fun, 'ro', markersize=10, label=f'Local min')\n",
    "# Show chord below curve (non-convex!)\n",
    "x1, x2 = -1.5, 1.5\n",
    "fx1 = x1**4 - 3*x1**2 + 0.5*x1\n",
    "fx2 = x2**4 - 3*x2**2 + 0.5*x2\n",
    "ax.plot([x1, x2], [fx1, fx2], 'r--', linewidth=2, label='Chord (goes below!)')\n",
    "ax.set_title('Non-convex: $f(x) = x^4 - 3x^2 + 0.5x$', fontsize=13)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.legend(fontsize=9, loc='upper center')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Convexity: Why It Matters for Optimization', fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell-9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: Condition Number\n",
    "\n",
    "The **condition number** $\\kappa = \\lambda_{\\max} / \\lambda_{\\min}$ measures how \"stretched\" a function's contours are.\n",
    "\n",
    "- $\\kappa = 1$: Perfect circles \u2014 GD goes straight to the minimum\n",
    "- $\\kappa \\gg 1$: Elongated ellipses \u2014 GD zigzags painfully\n",
    "\n",
    "**Why it matters in ML:**\n",
    "\n",
    "| Situation | Effect |\n",
    "|-----------|--------|\n",
    "| Poorly scaled features | High condition number, slow training |\n",
    "| Batch normalization | Reduces effective condition number |\n",
    "| Adam optimizer | Adapts per-parameter, handles ill-conditioning |\n",
    "| Feature normalization | Reduces condition number before training |\n",
    "\n",
    "**Key insight:** The convergence rate of GD on strongly convex functions is $O\\left(\\left(\\frac{\\kappa - 1}{\\kappa + 1}\\right)^t\\right)$. When $\\kappa = 1$, this is $0^t$ \u2014 instant convergence. When $\\kappa = 100$, convergence is painfully slow."
   ],
   "id": "cell-10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize effect of condition number on GD paths\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "condition_numbers = [1, 5, 20]\n",
    "titles = ['$\\kappa = 1$ (circular)', '$\\kappa = 5$ (moderate)', '$\\kappa = 20$ (ill-conditioned)']\n",
    "\n",
    "for ax, kappa, title in zip(axes, condition_numbers, titles):\n",
    "    A = np.array([[kappa, 0.0], [0.0, 1.0]])\n",
    "    f, grad_f = make_quadratic(A)\n",
    "\n",
    "    # Optimal step size: 2 / (lambda_max + lambda_min)\n",
    "    lr = 2.0 / (kappa + 1.0)\n",
    "    hist = gradient_descent(grad_f, np.array([4.0, 4.0]), lr, 50, f=f)\n",
    "\n",
    "    # Contours\n",
    "    xx, yy = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))\n",
    "    zz = np.array([f(np.array([xi, yi])) for xi, yi in zip(xx.ravel(), yy.ravel())]).reshape(xx.shape)\n",
    "    ax.contour(xx, yy, zz, levels=15, alpha=0.5, cmap='coolwarm')\n",
    "\n",
    "    # Trajectory\n",
    "    traj = hist['x']\n",
    "    ax.plot(traj[:, 0], traj[:, 1], 'b.-', markersize=5, linewidth=1, alpha=0.8)\n",
    "    ax.plot(traj[0, 0], traj[0, 1], 'ko', markersize=8)\n",
    "    ax.plot(0, 0, 'r*', markersize=15)\n",
    "\n",
    "    ax.set_title(title, fontsize=13)\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.set_ylim(-5, 5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Condition Number: The Hidden Cost of Ill-Conditioning', fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Steps to reach f(x) < 0.01:\")\n",
    "for kappa in condition_numbers:\n",
    "    A = np.array([[kappa, 0.0], [0.0, 1.0]])\n",
    "    f, grad_f = make_quadratic(A)\n",
    "    lr = 2.0 / (kappa + 1.0)\n",
    "    hist = gradient_descent(grad_f, np.array([4.0, 4.0]), lr, 500, f=f)\n",
    "    for i, fval in enumerate(hist['f']):\n",
    "        if fval < 0.01:\n",
    "            print(f\"  kappa={kappa:2d}: {i} steps\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"  kappa={kappa:2d}: >500 steps\")"
   ],
   "id": "cell-11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Stochastic Gradient Descent\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "In machine learning, the objective is usually a sum over data points:\n",
    "\n",
    "$$f(w) = \\frac{1}{N}\\sum_{i=1}^{N} \\ell(w; x_i, y_i)$$\n",
    "\n",
    "**Full gradient descent** computes the gradient using *all* $N$ data points \u2014 expensive when $N$ is millions.\n",
    "\n",
    "**Stochastic gradient descent (SGD)** approximates the gradient using a single random data point (or a small mini-batch). The gradient estimate is noisy but *unbiased*:\n",
    "\n",
    "$$\\mathbb{E}[\\nabla \\ell(w; x_i, y_i)] = \\nabla f(w)$$\n",
    "\n",
    "**Why stochasticity helps:**\n",
    "1. **Computational efficiency**: Each step is $O(1)$ instead of $O(N)$\n",
    "2. **Escaping saddle points**: Noise helps SGD escape flat regions where GD gets stuck\n",
    "3. **Implicit regularization**: The noise acts as a regularizer, improving generalization\n",
    "\n",
    "**The tradeoff:** SGD steps are cheaper but noisier. Mini-batches balance this \u2014 larger batches reduce variance but cost more compute."
   ],
   "id": "cell-12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(grad_fi, x0, lr, n_steps, n_data, batch_size=1, f=None):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent with mini-batches.\n",
    "\n",
    "    Args:\n",
    "        grad_fi: Function(x, indices) returning gradient estimate from data subset\n",
    "        x0: Initial point\n",
    "        lr: Learning rate (can be a function of step t)\n",
    "        n_steps: Number of iterations\n",
    "        n_data: Total number of data points\n",
    "        batch_size: Mini-batch size\n",
    "        f: Optional full objective function for tracking\n",
    "\n",
    "    Returns:\n",
    "        history dict with trajectory and function values\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float)\n",
    "    history = {'x': [x.copy()], 'f': []}\n",
    "\n",
    "    if f is not None:\n",
    "        history['f'].append(f(x))\n",
    "\n",
    "    for t in range(n_steps):\n",
    "        # Sample mini-batch\n",
    "        indices = np.random.choice(n_data, size=batch_size, replace=False)\n",
    "\n",
    "        # Learning rate schedule\n",
    "        current_lr = lr(t) if callable(lr) else lr\n",
    "\n",
    "        g = grad_fi(x, indices)\n",
    "        x = x - current_lr * g\n",
    "        history['x'].append(x.copy())\n",
    "        if f is not None:\n",
    "            history['f'].append(f(x))\n",
    "\n",
    "    history['x'] = np.array(history['x'])\n",
    "    return history\n",
    "\n",
    "# Create a synthetic least-squares problem: f(w) = (1/N) sum_i (w^T x_i - y_i)^2\n",
    "np.random.seed(42)\n",
    "N = 200\n",
    "d = 2\n",
    "X_data = np.random.randn(N, d)\n",
    "w_true = np.array([3.0, -1.0])\n",
    "y_data = X_data @ w_true + 0.3 * np.random.randn(N)\n",
    "\n",
    "def full_loss(w):\n",
    "    residuals = X_data @ w - y_data\n",
    "    return np.mean(residuals**2)\n",
    "\n",
    "def full_grad(w):\n",
    "    residuals = X_data @ w - y_data\n",
    "    return 2.0 * X_data.T @ residuals / N\n",
    "\n",
    "def stochastic_grad(w, indices):\n",
    "    X_batch = X_data[indices]\n",
    "    y_batch = y_data[indices]\n",
    "    residuals = X_batch @ w - y_batch\n",
    "    return 2.0 * X_batch.T @ residuals / len(indices)\n",
    "\n",
    "w0 = np.array([0.0, 0.0])\n",
    "print(f\"True weights: {w_true}\")\n",
    "print(f\"Initial loss: {full_loss(w0):.4f}\")"
   ],
   "id": "cell-13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare GD vs SGD vs mini-batch SGD\n",
    "np.random.seed(42)\n",
    "n_steps = 200\n",
    "\n",
    "# Full GD\n",
    "hist_gd = gradient_descent(full_grad, w0, 0.05, n_steps, f=full_loss)\n",
    "\n",
    "# SGD (batch_size=1)\n",
    "np.random.seed(42)\n",
    "hist_sgd1 = sgd(stochastic_grad, w0, 0.01, n_steps, N, batch_size=1, f=full_loss)\n",
    "\n",
    "# Mini-batch SGD (batch_size=32)\n",
    "np.random.seed(42)\n",
    "hist_sgd32 = sgd(stochastic_grad, w0, 0.03, n_steps, N, batch_size=32, f=full_loss)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax = axes[0]\n",
    "ax.semilogy(hist_gd['f'], 'b-', linewidth=2, label='Full GD', alpha=0.9)\n",
    "ax.semilogy(hist_sgd1['f'], 'r-', linewidth=1, label='SGD (batch=1)', alpha=0.6)\n",
    "ax.semilogy(hist_sgd32['f'], 'g-', linewidth=1.5, label='Mini-batch (batch=32)', alpha=0.7)\n",
    "ax.set_xlabel('Iteration', fontsize=12)\n",
    "ax.set_ylabel('Loss (log scale)', fontsize=12)\n",
    "ax.set_title('Convergence: GD vs SGD', fontsize=13)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Trajectories in weight space\n",
    "ax = axes[1]\n",
    "ax.plot(hist_gd['x'][:, 0], hist_gd['x'][:, 1], 'b-', linewidth=2, label='Full GD', alpha=0.8)\n",
    "ax.plot(hist_sgd1['x'][:, 0], hist_sgd1['x'][:, 1], 'r-', linewidth=0.5, label='SGD (batch=1)', alpha=0.4)\n",
    "ax.plot(hist_sgd32['x'][:, 0], hist_sgd32['x'][:, 1], 'g-', linewidth=1, label='Mini-batch (batch=32)', alpha=0.6)\n",
    "ax.plot(w_true[0], w_true[1], 'k*', markersize=15, label='True weights', zorder=5)\n",
    "ax.plot(w0[0], w0[1], 'ko', markersize=8, label='Start')\n",
    "ax.set_xlabel('$w_1$', fontsize=12)\n",
    "ax.set_ylabel('$w_2$', fontsize=12)\n",
    "ax.set_title('Trajectories in Weight Space', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal weights:\")\n",
    "print(f\"  Full GD:      [{hist_gd['x'][-1][0]:.4f}, {hist_gd['x'][-1][1]:.4f}]\")\n",
    "print(f\"  SGD (B=1):    [{hist_sgd1['x'][-1][0]:.4f}, {hist_sgd1['x'][-1][1]:.4f}]\")\n",
    "print(f\"  SGD (B=32):   [{hist_sgd32['x'][-1][0]:.4f}, {hist_sgd32['x'][-1][1]:.4f}]\")\n",
    "print(f\"  True:         [{w_true[0]:.4f}, {w_true[1]:.4f}]\")"
   ],
   "id": "cell-14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch size tradeoff\n",
    "np.random.seed(42)\n",
    "batch_sizes = [1, 4, 16, 64, N]\n",
    "n_steps_per = 300\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Adjust learning rates for each batch size\n",
    "lr_map = {1: 0.005, 4: 0.01, 16: 0.02, 64: 0.04, N: 0.05}\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    np.random.seed(42)\n",
    "    lr = lr_map[bs]\n",
    "    label = f'B={bs}' if bs < N else f'B={N} (Full GD)'\n",
    "    if bs == N:\n",
    "        hist = gradient_descent(full_grad, w0, lr, n_steps_per, f=full_loss)\n",
    "    else:\n",
    "        hist = sgd(stochastic_grad, w0, lr, n_steps_per, N, batch_size=bs, f=full_loss)\n",
    "\n",
    "    # Smooth the SGD curves for visibility\n",
    "    fvals = np.array(hist['f'])\n",
    "    if bs < N:\n",
    "        # Moving average\n",
    "        window = max(1, 10)\n",
    "        fvals_smooth = np.convolve(fvals, np.ones(window)/window, mode='valid')\n",
    "        axes[0].semilogy(fvals_smooth, linewidth=1.5, label=label, alpha=0.8)\n",
    "    else:\n",
    "        axes[0].semilogy(fvals, linewidth=2.5, label=label, alpha=0.9)\n",
    "\n",
    "axes[0].set_xlabel('Iteration', fontsize=12)\n",
    "axes[0].set_ylabel('Loss (log scale, smoothed)', fontsize=12)\n",
    "axes[0].set_title('Convergence vs Batch Size', fontsize=13)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Variance of gradient estimates\n",
    "ax = axes[1]\n",
    "batch_range = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "grad_variances = []\n",
    "w_test = np.array([1.0, 1.0])\n",
    "\n",
    "for bs in batch_range:\n",
    "    grads = []\n",
    "    for _ in range(200):\n",
    "        idx = np.random.choice(N, size=bs, replace=False)\n",
    "        g = stochastic_grad(w_test, idx)\n",
    "        grads.append(g)\n",
    "    grads = np.array(grads)\n",
    "    grad_variances.append(np.mean(np.var(grads, axis=0)))\n",
    "\n",
    "ax.loglog(batch_range, grad_variances, 'bo-', linewidth=2, markersize=8)\n",
    "ax.loglog(batch_range, grad_variances[0] / np.array(batch_range), 'r--', linewidth=1.5, label='$O(1/B)$ scaling')\n",
    "ax.set_xlabel('Batch Size', fontsize=12)\n",
    "ax.set_ylabel('Gradient Variance', fontsize=12)\n",
    "ax.set_title('Gradient Variance vs Batch Size', fontsize=13)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: Gradient variance scales as O(1/B)\")\n",
    "print(\"Doubling batch size halves variance but doubles compute per step\")"
   ],
   "id": "cell-15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: Why SGD Works So Well in Practice\n",
    "\n",
    "SGD's noise is not just a computational shortcut \u2014 it's a *feature*:\n",
    "\n",
    "1. **Escaping sharp minima**: SGD's noise bounces it out of sharp, narrow minima and into flatter ones. Flat minima tend to generalize better (the loss doesn't change much if weights shift slightly).\n",
    "\n",
    "2. **Implicit regularization**: SGD with a finite learning rate implicitly prefers solutions with smaller norms. This acts like free regularization.\n",
    "\n",
    "3. **Exploration**: Early in training, large noise helps explore the loss landscape. Late in training, we want less noise \u2014 hence learning rate decay.\n",
    "\n",
    "| Technique | Purpose | Effect |\n",
    "|-----------|---------|--------|\n",
    "| Learning rate warmup | Avoid early instability | Start small, increase to target LR |\n",
    "| Learning rate decay | Converge precisely | Reduce noise late in training |\n",
    "| Momentum | Smooth out noise | Average gradients over time |\n",
    "| Adam | Adapt per-parameter | Handle different scales automatically |\n",
    "\n",
    "**Common misconception:** \"SGD is just a noisy version of GD.\" *Reality:* SGD finds qualitatively different (often better) solutions than GD because the noise structure matters."
   ],
   "id": "cell-16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. The Bias-Variance Tradeoff\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "When we train a model, we want it to generalize \u2014 to perform well on *new* data, not just the training data. The **bias-variance decomposition** tells us exactly what can go wrong:\n",
    "\n",
    "$$\\text{Expected Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Noise}$$\n",
    "\n",
    "#### Breaking down the formula:\n",
    "\n",
    "| Component | Meaning | Caused By |\n",
    "|-----------|---------|-----------|\n",
    "| **Bias**$^2$ | How far off the average prediction is from truth | Model too simple (underfitting) |\n",
    "| **Variance** | How much predictions change across different training sets | Model too complex (overfitting) |\n",
    "| **Irreducible noise** | Inherent randomness in the data | Nothing \u2014 this is the floor |\n",
    "\n",
    "**What this means:** You can't minimize both bias and variance simultaneously. Simple models have high bias but low variance. Complex models have low bias but high variance. The sweet spot is in the middle."
   ],
   "id": "cell-17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate bias-variance tradeoff with polynomial regression\n",
    "np.random.seed(42)\n",
    "\n",
    "# True function\n",
    "def true_function(x):\n",
    "    return np.sin(2 * x) + 0.5 * np.cos(3 * x)\n",
    "\n",
    "# Generate multiple training sets and fit models of different complexity\n",
    "n_train = 25\n",
    "n_datasets = 100\n",
    "noise_std = 0.4\n",
    "x_test = np.linspace(0, 2*np.pi, 200)\n",
    "y_true = true_function(x_test)\n",
    "\n",
    "degrees = [1, 3, 5, 9, 15]\n",
    "predictions = {d: [] for d in degrees}\n",
    "\n",
    "for _ in range(n_datasets):\n",
    "    x_train = np.random.uniform(0, 2*np.pi, n_train)\n",
    "    y_train = true_function(x_train) + noise_std * np.random.randn(n_train)\n",
    "\n",
    "    for d in degrees:\n",
    "        poly = PolynomialFeatures(d)\n",
    "        X_train_poly = poly.fit_transform(x_train.reshape(-1, 1))\n",
    "        X_test_poly = poly.transform(x_test.reshape(-1, 1))\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train_poly, y_train)\n",
    "        y_pred = model.predict(X_test_poly)\n",
    "        predictions[d].append(y_pred)\n",
    "\n",
    "# Compute bias^2 and variance\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for idx, d in enumerate(degrees[:5]):\n",
    "    row, col = divmod(idx, 3)\n",
    "    ax = axes[row][col]\n",
    "\n",
    "    preds = np.array(predictions[d])\n",
    "    mean_pred = preds.mean(axis=0)\n",
    "\n",
    "    # Plot individual predictions (sample of 20)\n",
    "    for i in range(min(20, n_datasets)):\n",
    "        ax.plot(x_test, preds[i], 'b-', alpha=0.1, linewidth=0.5)\n",
    "\n",
    "    ax.plot(x_test, y_true, 'r-', linewidth=2.5, label='True function')\n",
    "    ax.plot(x_test, mean_pred, 'g--', linewidth=2, label='Mean prediction')\n",
    "\n",
    "    bias_sq = np.mean((mean_pred - y_true)**2)\n",
    "    variance = np.mean(np.var(preds, axis=0))\n",
    "\n",
    "    ax.set_title(f'Degree {d}: Bias\u00b2={bias_sq:.3f}, Var={variance:.3f}', fontsize=11)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend(fontsize=8, loc='upper right')\n",
    "    ax.set_ylim(-3, 3)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Use last subplot for the decomposition summary\n",
    "ax = axes[1][2]\n",
    "biases_sq = []\n",
    "variances = []\n",
    "for d in degrees:\n",
    "    preds = np.array(predictions[d])\n",
    "    mean_pred = preds.mean(axis=0)\n",
    "    biases_sq.append(np.mean((mean_pred - y_true)**2))\n",
    "    variances.append(np.mean(np.var(preds, axis=0)))\n",
    "\n",
    "total = np.array(biases_sq) + np.array(variances) + noise_std**2\n",
    "ax.plot(degrees, biases_sq, 'bo-', linewidth=2, label='Bias\u00b2', markersize=8)\n",
    "ax.plot(degrees, variances, 'rs-', linewidth=2, label='Variance', markersize=8)\n",
    "ax.plot(degrees, total, 'g^-', linewidth=2, label='Total Error', markersize=8)\n",
    "ax.axhline(y=noise_std**2, color='gray', linestyle=':', linewidth=1.5, label=f'Noise floor ({noise_std**2:.2f})')\n",
    "ax.set_xlabel('Polynomial Degree', fontsize=12)\n",
    "ax.set_ylabel('Error', fontsize=12)\n",
    "ax.set_title('Bias-Variance Decomposition', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Bias-Variance Tradeoff: Model Complexity vs Generalization', fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell-18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classic U-shaped test error curve\n",
    "np.random.seed(42)\n",
    "n_train = 30\n",
    "n_test = 200\n",
    "\n",
    "x_train = np.random.uniform(0, 2*np.pi, n_train)\n",
    "y_train = true_function(x_train) + noise_std * np.random.randn(n_train)\n",
    "x_test_pts = np.random.uniform(0, 2*np.pi, n_test)\n",
    "y_test = true_function(x_test_pts) + noise_std * np.random.randn(n_test)\n",
    "\n",
    "degrees_range = range(1, 20)\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for d in degrees_range:\n",
    "    poly = PolynomialFeatures(d)\n",
    "    X_tr = poly.fit_transform(x_train.reshape(-1, 1))\n",
    "    X_te = poly.transform(x_test_pts.reshape(-1, 1))\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_tr, y_train)\n",
    "\n",
    "    train_errors.append(np.mean((model.predict(X_tr) - y_train)**2))\n",
    "    test_errors.append(np.mean((model.predict(X_te) - y_test)**2))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(list(degrees_range), train_errors, 'b-o', linewidth=2, markersize=6, label='Training Error')\n",
    "ax.plot(list(degrees_range), test_errors, 'r-s', linewidth=2, markersize=6, label='Test Error')\n",
    "ax.axhline(y=noise_std**2, color='gray', linestyle=':', linewidth=1.5, label=f'Irreducible noise ({noise_std**2:.2f})')\n",
    "\n",
    "# Mark the sweet spot\n",
    "best_deg = list(degrees_range)[np.argmin(test_errors)]\n",
    "ax.axvline(x=best_deg, color='green', linestyle='--', linewidth=1.5, alpha=0.7, label=f'Best complexity (degree={best_deg})')\n",
    "\n",
    "# Annotations\n",
    "ax.annotate('Underfitting\\n(high bias)', xy=(2, test_errors[1]), fontsize=11, color='orange',\n",
    "            xytext=(3.5, test_errors[1]+0.3), arrowprops=dict(arrowstyle='->', color='orange'))\n",
    "ax.annotate('Overfitting\\n(high variance)', xy=(15, min(test_errors[-5:])+0.5), fontsize=11, color='orange',\n",
    "            xytext=(13, min(test_errors[-5:])+1.0), arrowprops=dict(arrowstyle='->', color='orange'))\n",
    "\n",
    "ax.set_xlabel('Model Complexity (Polynomial Degree)', fontsize=12)\n",
    "ax.set_ylabel('Mean Squared Error', fontsize=12)\n",
    "ax.set_title('The U-Shaped Test Error Curve', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim(0, max(test_errors[0], 3))\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best polynomial degree: {best_deg}\")\n",
    "print(f\"Train error at best: {train_errors[best_deg-1]:.4f}\")\n",
    "print(f\"Test error at best:  {test_errors[best_deg-1]:.4f}\")"
   ],
   "id": "cell-19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. VC Dimension and Generalization\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "How do we measure a model's **capacity** \u2014 its ability to fit arbitrary patterns? The **Vapnik-Chervonenkis (VC) dimension** provides a formal answer.\n",
    "\n",
    "**Definition:** The VC dimension of a model class is the largest number of points it can **shatter** \u2014 i.e., classify correctly for *every possible* labeling of those points.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "| Model | VC Dimension | Intuition |\n",
    "|-------|-------------|-----------|\n",
    "| Linear classifier in $\\mathbb{R}^d$ | $d + 1$ | Can shatter $d+1$ points in general position |\n",
    "| Linear classifier in $\\mathbb{R}^2$ | 3 | Can shatter any 3 non-collinear points |\n",
    "| Polynomial of degree $k$ | $k + 1$ | Can memorize $k+1$ points exactly |\n",
    "| Neural net with $W$ weights | $O(W \\log W)$ | Roughly proportional to parameter count |\n",
    "| $k$-nearest neighbors | $\\infty$ | Can memorize any dataset (but doesn't generalize!) |\n",
    "\n",
    "**What this means:** Higher VC dimension = more expressive model = needs more data to generalize reliably."
   ],
   "id": "cell-20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize shattering: a linear classifier in R^2 can shatter 3 points but not 4\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# 3 points: show all 8 labelings can be separated\n",
    "points_3 = np.array([[0, 0], [2, 0], [1, 2]])\n",
    "labelings_3 = [\n",
    "    [0, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1],\n",
    "    [1, 1, 0], [1, 0, 1], [0, 1, 1], [1, 1, 1]\n",
    "]\n",
    "\n",
    "for i, (ax, labels) in enumerate(zip(axes[0], labelings_3[:4])):\n",
    "    for j, (pt, lab) in enumerate(zip(points_3, labels)):\n",
    "        color = 'blue' if lab == 1 else 'red'\n",
    "        marker = 'o' if lab == 1 else 'x'\n",
    "        ax.scatter(pt[0], pt[1], c=color, marker=marker, s=150, zorder=5, edgecolors='black', linewidths=1)\n",
    "\n",
    "    ax.set_xlim(-0.5, 3)\n",
    "    ax.set_ylim(-0.5, 3)\n",
    "    ax.set_title(f'Labeling {labels}', fontsize=10)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('VC dim = 3\\n(3 points shattered)', fontsize=11)\n",
    "\n",
    "for i, (ax, labels) in enumerate(zip(axes[1][:4], labelings_3[4:])):\n",
    "    for j, (pt, lab) in enumerate(zip(points_3, labels)):\n",
    "        color = 'blue' if lab == 1 else 'red'\n",
    "        marker = 'o' if lab == 1 else 'x'\n",
    "        ax.scatter(pt[0], pt[1], c=color, marker=marker, s=150, zorder=5, edgecolors='black', linewidths=1)\n",
    "\n",
    "    ax.set_xlim(-0.5, 3)\n",
    "    ax.set_ylim(-0.5, 3)\n",
    "    ax.set_title(f'Labeling {labels}', fontsize=10)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0][0].annotate('All 8 labelings\\nseparable by a line!',\n",
    "                      xy=(1.5, 2.5), fontsize=10, color='green',\n",
    "                      ha='center', fontweight='bold')\n",
    "\n",
    "plt.suptitle('VC Dimension: Shattering 3 Points with a Linear Classifier', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"A linear classifier in R^2 has VC dimension = 3\")\n",
    "print(\"It can shatter (perfectly classify) any 3 points in general position\")\n",
    "print(\"But there exist 4 points (e.g., XOR pattern) that NO line can shatter\")"
   ],
   "id": "cell-21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: The VC Generalization Bound\n",
    "\n",
    "The VC dimension connects model complexity to generalization through a fundamental bound:\n",
    "\n",
    "$$\\text{Test Error} \\leq \\text{Training Error} + O\\left(\\sqrt{\\frac{d_{VC} \\log(N/d_{VC})}{N}}\\right)$$\n",
    "\n",
    "where $d_{VC}$ is the VC dimension and $N$ is the number of training samples.\n",
    "\n",
    "**What this tells us:**\n",
    "- The gap between training and test error grows with $d_{VC}$ (model complexity)\n",
    "- The gap shrinks with $N$ (more data helps)\n",
    "- To keep the gap small: need $N \\gg d_{VC}$\n",
    "\n",
    "**Key insight:** This is the theoretical justification for the rule of thumb \"you need at least 10x more data points than parameters.\"\n",
    "\n",
    "#### Common Misconceptions\n",
    "\n",
    "| Misconception | Reality |\n",
    "|---------------|---------|\n",
    "| \"Low VC dim = good model\" | Low VC dim means simple model \u2014 might underfit |\n",
    "| \"VC bounds are tight\" | VC bounds are usually very loose; they give qualitative, not quantitative guidance |\n",
    "| \"More parameters = higher VC dim always\" | Regularization can effectively reduce VC dimension |"
   ],
   "id": "cell-22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. PAC Learning\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "**Probably Approximately Correct (PAC)** learning asks: \"How much data do we need to *probably* find a model that is *approximately* correct?\"\n",
    "\n",
    "More precisely, for given:\n",
    "- $\\epsilon$ = accuracy tolerance (\"approximately correct\" \u2014 error < $\\epsilon$)\n",
    "- $\\delta$ = confidence tolerance (\"probably\" \u2014 succeed with probability $\\geq 1 - \\delta$)\n",
    "\n",
    "PAC learning tells us the **sample complexity**: the minimum number of training examples $N$ needed.\n",
    "\n",
    "**For a finite hypothesis class** $\\mathcal{H}$ with $|\\mathcal{H}|$ hypotheses:\n",
    "\n",
    "$$N \\geq \\frac{1}{\\epsilon}\\left(\\ln |\\mathcal{H}| + \\ln \\frac{1}{\\delta}\\right)$$\n",
    "\n",
    "**What this means in plain English:**\n",
    "- Want more accuracy (smaller $\\epsilon$)? Need more data.\n",
    "- Want more confidence (smaller $\\delta$)? Need more data (but only logarithmically).\n",
    "- More complex model (larger $|\\mathcal{H}|$)? Need more data (also logarithmically)."
   ],
   "id": "cell-23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PAC sample complexity\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Sample complexity vs epsilon\n",
    "ax = axes[0]\n",
    "epsilon = np.linspace(0.01, 0.5, 100)\n",
    "for H_size, color, label in [(10, 'blue', '|H|=10'), (100, 'red', '|H|=100'), (1000, 'green', '|H|=1000')]:\n",
    "    delta = 0.05\n",
    "    N_pac = (1/epsilon) * (np.log(H_size) + np.log(1/delta))\n",
    "    ax.plot(epsilon, N_pac, color=color, linewidth=2, label=label)\n",
    "ax.set_xlabel('$\\epsilon$ (error tolerance)', fontsize=12)\n",
    "ax.set_ylabel('Sample complexity $N$', fontsize=12)\n",
    "ax.set_title('More accuracy \u2192 more data', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Sample complexity vs delta\n",
    "ax = axes[1]\n",
    "delta = np.linspace(0.001, 0.5, 100)\n",
    "for eps, color, label in [(0.1, 'blue', '\u03b5=0.1'), (0.05, 'red', '\u03b5=0.05'), (0.01, 'green', '\u03b5=0.01')]:\n",
    "    H_size = 100\n",
    "    N_pac = (1/eps) * (np.log(H_size) + np.log(1/delta))\n",
    "    ax.plot(delta, N_pac, color=color, linewidth=2, label=label)\n",
    "ax.set_xlabel('$\\delta$ (failure probability)', fontsize=12)\n",
    "ax.set_ylabel('Sample complexity $N$', fontsize=12)\n",
    "ax.set_title('More confidence \u2192 more data (log)', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Sample complexity vs hypothesis class size\n",
    "ax = axes[2]\n",
    "H_sizes = np.logspace(1, 6, 100)\n",
    "for eps, color, label in [(0.1, 'blue', '\u03b5=0.1'), (0.05, 'red', '\u03b5=0.05'), (0.01, 'green', '\u03b5=0.01')]:\n",
    "    delta = 0.05\n",
    "    N_pac = (1/eps) * (np.log(H_sizes) + np.log(1/delta))\n",
    "    ax.semilogx(H_sizes, N_pac, color=color, linewidth=2, label=label)\n",
    "ax.set_xlabel('$|\\mathcal{H}|$ (hypothesis class size)', fontsize=12)\n",
    "ax.set_ylabel('Sample complexity $N$', fontsize=12)\n",
    "ax.set_title('More complex model \u2192 more data (log)', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('PAC Learning: How Much Data Do You Need?', fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Concrete example\n",
    "epsilon = 0.05\n",
    "delta = 0.05\n",
    "H_size = 1000\n",
    "N_required = int(np.ceil((1/epsilon) * (np.log(H_size) + np.log(1/delta))))\n",
    "print(f\"Example: To achieve {epsilon:.0%} error with {1-delta:.0%} confidence\")\n",
    "print(f\"  with |H| = {H_size} hypotheses, you need N >= {N_required} samples\")"
   ],
   "id": "cell-24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: What PAC Learning Tells Us About ML\n",
    "\n",
    "PAC learning provides the theoretical foundation for understanding *learnability*:\n",
    "\n",
    "**A concept class is PAC-learnable** if there exists an algorithm that, for any $\\epsilon > 0$ and $\\delta > 0$, can learn a hypothesis with error $\\leq \\epsilon$ with probability $\\geq 1 - \\delta$, using a number of samples polynomial in $1/\\epsilon$, $1/\\delta$, and the model size.\n",
    "\n",
    "**Key takeaways for practitioners:**\n",
    "\n",
    "1. **Logarithmic dependence on $|\\mathcal{H}|$ and $1/\\delta$** \u2014 making your model 10x more complex only increases data needs by $\\ln(10) \\approx 2.3$. That's surprisingly cheap!\n",
    "\n",
    "2. **Linear dependence on $1/\\epsilon$** \u2014 going from 90% to 99% accuracy is much harder than 50% to 90%.\n",
    "\n",
    "3. **Connection to VC dimension**: For infinite hypothesis classes (like neural networks), replace $\\ln|\\mathcal{H}|$ with $d_{VC}$ to get similar bounds.\n",
    "\n",
    "**Why this matters:** PAC theory tells us that learning *is* possible with finite data \u2014 not something obvious from first principles. It also tells us the fundamental resource tradeoffs."
   ],
   "id": "cell-25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Regularization Theory\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "Regularization adds a penalty to the loss function to prevent overfitting:\n",
    "\n",
    "$$\\text{Total Loss} = \\text{Data Loss} + \\lambda \\cdot \\text{Penalty}(w)$$\n",
    "\n",
    "The two most common penalties:\n",
    "- **L1 (Lasso):** $\\|w\\|_1 = \\sum |w_i|$ \u2014 produces **sparse** solutions (many weights exactly zero)\n",
    "- **L2 (Ridge):** $\\|w\\|_2^2 = \\sum w_i^2$ \u2014 produces **small** weights (but rarely exactly zero)\n",
    "\n",
    "**Why does L1 give sparsity?** It's all about geometry. The L1 constraint region is a *diamond*, and the loss contours typically hit the diamond at a corner \u2014 where some coordinates are exactly zero. The L2 constraint region is a *sphere*, which has no corners, so the intersection point rarely has exact zeros."
   ],
   "id": "cell-26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The geometry of L1 vs L2 regularization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Create elliptical contours (represent loss function)\n",
    "theta_range = np.linspace(0, 2*np.pi, 200)\n",
    "\n",
    "# Shifted center for the loss function\n",
    "center = np.array([2.0, 1.5])\n",
    "\n",
    "# L1 constraint: |w1| + |w2| <= t\n",
    "def l1_boundary(t):\n",
    "    pts = []\n",
    "    for th in np.linspace(0, 2*np.pi, 200):\n",
    "        x = t * np.cos(th)\n",
    "        y = t * np.sin(th)\n",
    "        # Project onto L1 ball\n",
    "        scale = (np.abs(x) + np.abs(y))\n",
    "        if scale > 0:\n",
    "            x, y = x * t / scale, y * t / scale\n",
    "        pts.append([x, y])\n",
    "    return np.array(pts)\n",
    "\n",
    "# L2 constraint: w1^2 + w2^2 <= t^2\n",
    "def l2_boundary(t):\n",
    "    return np.column_stack([t * np.cos(theta_range), t * np.sin(theta_range)])\n",
    "\n",
    "# Plot L1\n",
    "ax = axes[0]\n",
    "A_loss = np.array([[1.0, 0.3], [0.3, 2.0]])\n",
    "xx, yy = np.meshgrid(np.linspace(-3, 4, 200), np.linspace(-3, 4, 200))\n",
    "zz = np.zeros_like(xx)\n",
    "for i in range(xx.shape[0]):\n",
    "    for j in range(xx.shape[1]):\n",
    "        d = np.array([xx[i,j], yy[i,j]]) - center\n",
    "        zz[i,j] = 0.5 * d @ A_loss @ d\n",
    "\n",
    "ax.contour(xx, yy, zz, levels=10, cmap='RdYlBu_r', alpha=0.6)\n",
    "\n",
    "# Diamond (L1 ball)\n",
    "t = 1.5\n",
    "diamond = np.array([[t, 0], [0, t], [-t, 0], [0, -t], [t, 0]])\n",
    "ax.fill(diamond[:, 0], diamond[:, 1], alpha=0.2, color='blue')\n",
    "ax.plot(diamond[:, 0], diamond[:, 1], 'b-', linewidth=2)\n",
    "\n",
    "# Mark the solution (corner of diamond)\n",
    "ax.plot(0, t, 'g*', markersize=20, zorder=5, label='L1 solution (sparse!)')\n",
    "ax.plot(center[0], center[1], 'r*', markersize=15, label='Unconstrained optimum')\n",
    "\n",
    "ax.set_title('L1 (Lasso): Diamond Constraint', fontsize=13)\n",
    "ax.set_xlabel('$w_1$', fontsize=12)\n",
    "ax.set_ylabel('$w_2$', fontsize=12)\n",
    "ax.set_xlim(-3, 4)\n",
    "ax.set_ylim(-3, 4)\n",
    "ax.set_aspect('equal')\n",
    "ax.legend(fontsize=9, loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot L2\n",
    "ax = axes[1]\n",
    "ax.contour(xx, yy, zz, levels=10, cmap='RdYlBu_r', alpha=0.6)\n",
    "\n",
    "# Circle (L2 ball)\n",
    "circle_pts = l2_boundary(t)\n",
    "ax.fill(circle_pts[:, 0], circle_pts[:, 1], alpha=0.2, color='red')\n",
    "ax.plot(circle_pts[:, 0], circle_pts[:, 1], 'r-', linewidth=2)\n",
    "\n",
    "# Mark the solution (on circle, not at corner)\n",
    "# Approximate L2-constrained solution\n",
    "w_l2 = center / np.linalg.norm(center) * t\n",
    "ax.plot(w_l2[0], w_l2[1], 'g*', markersize=20, zorder=5, label='L2 solution (small)')\n",
    "ax.plot(center[0], center[1], 'r*', markersize=15, label='Unconstrained optimum')\n",
    "\n",
    "ax.set_title('L2 (Ridge): Circle Constraint', fontsize=13)\n",
    "ax.set_xlabel('$w_1$', fontsize=12)\n",
    "ax.set_ylabel('$w_2$', fontsize=12)\n",
    "ax.set_xlim(-3, 4)\n",
    "ax.set_ylim(-3, 4)\n",
    "ax.set_aspect('equal')\n",
    "ax.legend(fontsize=9, loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot comparison of weight distributions\n",
    "ax = axes[2]\n",
    "np.random.seed(42)\n",
    "n_features = 20\n",
    "X_reg = np.random.randn(100, n_features)\n",
    "w_true_reg = np.zeros(n_features)\n",
    "w_true_reg[:5] = [3, -2, 1.5, -1, 0.5]  # Only 5 features matter\n",
    "y_reg = X_reg @ w_true_reg + 0.3 * np.random.randn(100)\n",
    "\n",
    "# Fit models\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "lasso = Lasso(alpha=0.1).fit(X_reg, y_reg)\n",
    "ridge = Ridge(alpha=1.0).fit(X_reg, y_reg)\n",
    "\n",
    "x_pos = np.arange(n_features)\n",
    "width = 0.35\n",
    "ax.bar(x_pos - width/2, np.abs(lasso.coef_), width, color='blue', alpha=0.7, label='L1 (Lasso)')\n",
    "ax.bar(x_pos + width/2, np.abs(ridge.coef_), width, color='red', alpha=0.7, label='L2 (Ridge)')\n",
    "ax.axvline(x=4.5, color='green', linestyle='--', linewidth=1.5, alpha=0.5, label='True sparse boundary')\n",
    "ax.set_xlabel('Feature Index', fontsize=12)\n",
    "ax.set_ylabel('|Weight|', fontsize=12)\n",
    "ax.set_title('Weight Magnitudes: L1 vs L2', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Why L1 Gives Sparsity and L2 Gives Small Weights', fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"L1 (Lasso) non-zero weights: {np.sum(np.abs(lasso.coef_) > 0.01)}/{n_features}\")\n",
    "print(f\"L2 (Ridge) non-zero weights: {np.sum(np.abs(ridge.coef_) > 0.01)}/{n_features}\")"
   ],
   "id": "cell-27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization paths: how weights change with lambda\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "alphas = np.logspace(-3, 2, 100)\n",
    "\n",
    "# Lasso path\n",
    "lasso_coefs = []\n",
    "for alpha in alphas:\n",
    "    model = Lasso(alpha=alpha, max_iter=10000)\n",
    "    model.fit(X_reg, y_reg)\n",
    "    lasso_coefs.append(model.coef_.copy())\n",
    "lasso_coefs = np.array(lasso_coefs)\n",
    "\n",
    "ax = axes[0]\n",
    "for j in range(n_features):\n",
    "    color = 'blue' if j < 5 else 'gray'\n",
    "    alpha_val = 0.9 if j < 5 else 0.3\n",
    "    linewidth = 2 if j < 5 else 0.5\n",
    "    label = f'Feature {j}' if j < 5 else (None if j > 5 else 'Noise features')\n",
    "    ax.semilogx(alphas, lasso_coefs[:, j], color=color, alpha=alpha_val, linewidth=linewidth, label=label)\n",
    "ax.set_xlabel('$\\lambda$ (regularization strength)', fontsize=12)\n",
    "ax.set_ylabel('Weight value', fontsize=12)\n",
    "ax.set_title('L1 Regularization Path (Lasso)', fontsize=13)\n",
    "ax.legend(fontsize=8, loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Ridge path\n",
    "ridge_coefs = []\n",
    "for alpha in alphas:\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(X_reg, y_reg)\n",
    "    ridge_coefs.append(model.coef_.copy())\n",
    "ridge_coefs = np.array(ridge_coefs)\n",
    "\n",
    "ax = axes[1]\n",
    "for j in range(n_features):\n",
    "    color = 'red' if j < 5 else 'gray'\n",
    "    alpha_val = 0.9 if j < 5 else 0.3\n",
    "    linewidth = 2 if j < 5 else 0.5\n",
    "    label = f'Feature {j}' if j < 5 else (None if j > 5 else 'Noise features')\n",
    "    ax.semilogx(alphas, ridge_coefs[:, j], color=color, alpha=alpha_val, linewidth=linewidth, label=label)\n",
    "ax.set_xlabel('$\\lambda$ (regularization strength)', fontsize=12)\n",
    "ax.set_ylabel('Weight value', fontsize=12)\n",
    "ax.set_title('L2 Regularization Path (Ridge)', fontsize=13)\n",
    "ax.legend(fontsize=8, loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observation:\")\n",
    "print(\"  L1: Weights drop to EXACTLY zero as lambda increases (feature selection!)\")\n",
    "print(\"  L2: Weights shrink smoothly toward zero but never reach it\")"
   ],
   "id": "cell-28"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: Regularization as Bayesian Priors\n",
    "\n",
    "There's a beautiful connection between regularization and Bayesian statistics:\n",
    "\n",
    "| Regularizer | Equivalent Prior | Distribution |\n",
    "|------------|-----------------|--------------|\n",
    "| L2 (Ridge) | Gaussian prior: $w_i \\sim \\mathcal{N}(0, 1/\\lambda)$ | Weights cluster near zero |\n",
    "| L1 (Lasso) | Laplace prior: $w_i \\sim \\text{Laplace}(0, 1/\\lambda)$ | Weights sparse (peak at zero) |\n",
    "| Elastic Net | Mix of Gaussian + Laplace | Best of both worlds |\n",
    "\n",
    "**What this means:** When you add L2 regularization, you're saying \"I believe the weights are probably small.\" When you add L1, you're saying \"I believe most weights are probably zero.\" The strength $\\lambda$ controls how strongly you hold this belief.\n",
    "\n",
    "**Elastic Net** combines both:\n",
    "$$\\text{Penalty} = \\alpha \\|w\\|_1 + (1-\\alpha) \\|w\\|_2^2$$\n",
    "\n",
    "This gives sparsity (from L1) while handling correlated features better (from L2).\n",
    "\n",
    "#### Key Insight\n",
    "\n",
    "Regularization is not a \"trick\" \u2014 it's a principled way of encoding prior knowledge about what good solutions look like. Every regularizer implicitly answers the question: \"What kind of models do I expect to work well?\" "
   ],
   "id": "cell-29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Why Overparameterized Networks Generalize\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "Classical learning theory says: more parameters \u2192 more overfitting. But modern neural networks have *millions* of parameters (far more than training examples) and still generalize beautifully. This contradicts classical theory and is one of the deepest open questions in ML.\n",
    "\n",
    "**The double descent phenomenon:** As model complexity increases, test error follows a U-shape (classical regime), but then *decreases again* after the interpolation threshold (where the model can perfectly fit the training data).\n",
    "\n",
    "Three key ideas explain why overparameterization works:\n",
    "\n",
    "1. **Double descent**: The classical bias-variance tradeoff is incomplete \u2014 it misses the \"modern\" regime\n",
    "2. **Implicit regularization**: SGD, by its nature, finds \"simple\" solutions among the many that fit the data\n",
    "3. **Lottery ticket hypothesis**: Large networks contain small subnetworks that do most of the work"
   ],
   "id": "cell-30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the double descent curve\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data from a moderately complex function\n",
    "n_total = 40\n",
    "x_all = np.linspace(0, 1, n_total)\n",
    "y_true_fn = lambda x: np.sin(4 * np.pi * x) + 0.5 * np.cos(6 * np.pi * x)\n",
    "noise_level = 0.5\n",
    "\n",
    "n_train = 25\n",
    "indices = np.random.choice(n_total, n_train, replace=False)\n",
    "x_train_dd = x_all[indices]\n",
    "y_train_dd = y_true_fn(x_train_dd) + noise_level * np.random.randn(n_train)\n",
    "mask = np.ones(n_total, dtype=bool)\n",
    "mask[indices] = False\n",
    "x_test_dd = x_all[mask]\n",
    "y_test_dd = y_true_fn(x_test_dd) + noise_level * np.random.randn(n_total - n_train)\n",
    "\n",
    "# Fit polynomials of increasing degree, including overparameterized\n",
    "degrees_dd = list(range(1, 35))\n",
    "train_errors_dd = []\n",
    "test_errors_dd = []\n",
    "\n",
    "for d in degrees_dd:\n",
    "    poly = PolynomialFeatures(d)\n",
    "    X_tr = poly.fit_transform(x_train_dd.reshape(-1, 1))\n",
    "    X_te = poly.transform(x_test_dd.reshape(-1, 1))\n",
    "\n",
    "    # Use Ridge with tiny regularization for numerical stability past interpolation\n",
    "    from sklearn.linear_model import Ridge\n",
    "    model = Ridge(alpha=1e-10)\n",
    "    model.fit(X_tr, y_train_dd)\n",
    "\n",
    "    y_pred_tr = model.predict(X_tr)\n",
    "    y_pred_te = model.predict(X_te)\n",
    "\n",
    "    train_errors_dd.append(np.mean((y_pred_tr - y_train_dd)**2))\n",
    "    test_errors_dd.append(np.clip(np.mean((y_pred_te - y_test_dd)**2), 0, 15))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(degrees_dd, train_errors_dd, 'b-o', linewidth=2, markersize=4, label='Training Error', alpha=0.8)\n",
    "ax.plot(degrees_dd, test_errors_dd, 'r-s', linewidth=2, markersize=4, label='Test Error', alpha=0.8)\n",
    "ax.axhline(y=noise_level**2, color='gray', linestyle=':', linewidth=1.5, label=f'Noise floor ({noise_level**2:.2f})')\n",
    "ax.axvline(x=n_train, color='green', linestyle='--', linewidth=2, alpha=0.6,\n",
    "           label=f'Interpolation threshold (d={n_train})')\n",
    "\n",
    "# Annotate regions\n",
    "ax.annotate('Classical\\nregime', xy=(8, 1), fontsize=12, color='blue',\n",
    "            ha='center', fontweight='bold')\n",
    "ax.annotate('Modern\\n(overparameterized)\\nregime', xy=(30, 1), fontsize=12, color='red',\n",
    "            ha='center', fontweight='bold')\n",
    "ax.annotate('Interpolation\\npeak', xy=(n_train, max(test_errors_dd)), fontsize=10, color='green',\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "ax.set_xlabel('Model Complexity (Polynomial Degree)', fontsize=13)\n",
    "ax.set_ylabel('Mean Squared Error', fontsize=13)\n",
    "ax.set_title('Double Descent: Beyond the Classical Bias-Variance Tradeoff', fontsize=14)\n",
    "ax.legend(fontsize=11, loc='upper right')\n",
    "ax.set_ylim(0, min(15, max(test_errors_dd) * 1.1))\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Double descent: test error peaks at the interpolation threshold,\")\n",
    "print(\"then DECREASES as model becomes even more overparameterized!\")"
   ],
   "id": "cell-31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate implicit regularization: SGD finds minimum-norm solutions\n",
    "np.random.seed(42)\n",
    "\n",
    "# Underdetermined system: more parameters than data points\n",
    "n_data_imp = 20\n",
    "n_params = 100  # Way more parameters than data\n",
    "\n",
    "X_imp = np.random.randn(n_data_imp, n_params)\n",
    "w_target = np.zeros(n_params)\n",
    "w_target[:5] = np.array([2, -1, 0.5, 1.5, -0.8])  # Sparse true weights\n",
    "y_imp = X_imp @ w_target + 0.1 * np.random.randn(n_data_imp)\n",
    "\n",
    "# Many solutions exist! Let's see what different methods find.\n",
    "\n",
    "# Method 1: Minimum L2-norm solution (pseudoinverse)\n",
    "w_minnorm = X_imp.T @ np.linalg.solve(X_imp @ X_imp.T, y_imp)\n",
    "\n",
    "# Method 2: SGD from zero initialization (implicit regularization)\n",
    "w_sgd = np.zeros(n_params)\n",
    "lr_imp = 0.001\n",
    "for epoch in range(1000):\n",
    "    for i in np.random.permutation(n_data_imp):\n",
    "        xi = X_imp[i:i+1]\n",
    "        yi = y_imp[i:i+1]\n",
    "        grad = 2 * xi.T @ (xi @ w_sgd - yi)\n",
    "        w_sgd = w_sgd - lr_imp * grad.flatten()\n",
    "\n",
    "# Method 3: Random solution (also fits data but has large norm)\n",
    "# Find a particular solution + null space component\n",
    "w_particular = w_minnorm\n",
    "null_component = np.random.randn(n_params)\n",
    "# Project onto null space of X\n",
    "proj = X_imp.T @ np.linalg.solve(X_imp @ X_imp.T, X_imp)\n",
    "null_component = null_component - proj @ null_component\n",
    "w_random = w_particular + 3 * null_component  # Add large null space component\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "methods = [\n",
    "    ('Min-norm (Pseudoinverse)', w_minnorm, 'blue'),\n",
    "    ('SGD from Zero Init', w_sgd, 'green'),\n",
    "    ('Random Solution', w_random, 'red')\n",
    "]\n",
    "\n",
    "for ax, (name, w, color) in zip(axes, methods):\n",
    "    ax.bar(range(n_params), w, color=color, alpha=0.7, width=1.0)\n",
    "    ax.set_xlabel('Parameter Index', fontsize=11)\n",
    "    ax.set_ylabel('Weight Value', fontsize=11)\n",
    "    train_err = np.mean((X_imp @ w - y_imp)**2)\n",
    "    w_norm = np.linalg.norm(w)\n",
    "    ax.set_title(f'{name}\\nTrain MSE={train_err:.4f}, ||w||={w_norm:.1f}', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim(-4, 4)\n",
    "\n",
    "plt.suptitle('Implicit Regularization: All Solutions Fit Data, But SGD Finds the Simple One',\n",
    "             fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Solution norms:\")\n",
    "print(f\"  Min-norm:  ||w|| = {np.linalg.norm(w_minnorm):.4f}\")\n",
    "print(f\"  SGD:       ||w|| = {np.linalg.norm(w_sgd):.4f}\")\n",
    "print(f\"  Random:    ||w|| = {np.linalg.norm(w_random):.4f}\")\n",
    "print(f\"\\nAll have near-zero training error \u2014 but the norms are very different!\")\n",
    "print(f\"SGD implicitly finds a solution close to the minimum-norm solution.\")"
   ],
   "id": "cell-32"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: The Lottery Ticket Hypothesis\n",
    "\n",
    "The **Lottery Ticket Hypothesis** (Frankle & Carlin, 2019) proposes:\n",
    "\n",
    "> A randomly initialized, dense neural network contains a subnetwork (a \"winning ticket\") that \u2014 when trained in isolation \u2014 reaches comparable accuracy to the full network in a similar number of training steps.\n",
    "\n",
    "**What this means:**\n",
    "- Large networks work well not because all parameters are needed, but because having many parameters increases the chance of containing a good subnetwork\n",
    "- You can prune 90%+ of weights after training with minimal accuracy loss\n",
    "- The initial random values of the winning ticket matter \u2014 it's the specific initialization that makes it trainable\n",
    "\n",
    "**Why this is profound:**\n",
    "1. It suggests most parameters in large networks are \"wasted\" at inference time\n",
    "2. It explains why overparameterization helps *training* even if not needed for *representation*\n",
    "3. It motivates network pruning and efficient inference\n",
    "\n",
    "#### Open Questions (Honest Assessment)\n",
    "\n",
    "| Question | Current Understanding |\n",
    "|----------|---------------------|\n",
    "| Why does SGD find flat minima? | Partially understood \u2014 noise scale matters |\n",
    "| Is double descent universal? | Observed broadly, but theory is incomplete |\n",
    "| Can we find winning tickets cheaply? | Active research; training is still needed to identify them |\n",
    "| Why do large models generalize at all? | Multiple competing theories; no consensus yet |\n",
    "\n",
    "**The honest truth:** We don't fully understand why deep learning works as well as it does. The theory is catching up to the practice, and that's what makes this field exciting."
   ],
   "id": "cell-33"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Implement Gradient Descent with Momentum\n",
    "\n",
    "Momentum accelerates GD by accumulating a velocity vector:\n",
    "\n",
    "$$v_{t+1} = \\beta v_t + \\nabla f(x_t)$$\n",
    "$$x_{t+1} = x_t - \\eta v_{t+1}$$\n",
    "\n",
    "Implement this and compare convergence with vanilla GD on an ill-conditioned quadratic."
   ],
   "id": "cell-34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 1: Implement gradient descent with momentum\n",
    "def gradient_descent_momentum(grad_f, x0, lr, n_steps, beta=0.9, f=None):\n",
    "    \"\"\"\n",
    "    Gradient descent with momentum.\n",
    "\n",
    "    Args:\n",
    "        grad_f: Gradient function\n",
    "        x0: Initial point\n",
    "        lr: Learning rate\n",
    "        n_steps: Number of iterations\n",
    "        beta: Momentum coefficient (default 0.9)\n",
    "        f: Optional objective function for tracking\n",
    "\n",
    "    Returns:\n",
    "        history dict with trajectory and function values\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float)\n",
    "    v = np.zeros_like(x)  # Initialize velocity\n",
    "    history = {'x': [x.copy()], 'f': []}\n",
    "\n",
    "    if f is not None:\n",
    "        history['f'].append(f(x))\n",
    "\n",
    "    for t in range(n_steps):\n",
    "        # TODO: Implement momentum update\n",
    "        # Hint: First update velocity, then update position\n",
    "        # v = beta * v + grad_f(x)\n",
    "        # x = x - lr * v\n",
    "\n",
    "        pass  # Replace with your implementation\n",
    "\n",
    "        history['x'].append(x.copy())\n",
    "        if f is not None:\n",
    "            history['f'].append(f(x))\n",
    "\n",
    "    history['x'] = np.array(history['x'])\n",
    "    return history\n",
    "\n",
    "# Test on ill-conditioned quadratic\n",
    "A_test = np.array([[20.0, 0.0], [0.0, 1.0]])  # condition number = 20\n",
    "f_test, grad_test = make_quadratic(A_test)\n",
    "x0_test = np.array([5.0, 5.0])\n",
    "\n",
    "# Test your implementation\n",
    "hist_mom = gradient_descent_momentum(grad_test, x0_test, lr=0.05, n_steps=100, beta=0.9, f=f_test)\n",
    "hist_vanilla = gradient_descent(grad_test, x0_test, lr=0.05, n_steps=100, f=f_test)\n",
    "\n",
    "# Verify\n",
    "if len(hist_mom['f']) > 1 and hist_mom['f'][-1] != hist_mom['f'][0]:\n",
    "    print(f\"Momentum final loss:  {hist_mom['f'][-1]:.6f}\")\n",
    "    print(f\"Vanilla final loss:   {hist_vanilla['f'][-1]:.6f}\")\n",
    "    print(f\"Momentum converges faster: {hist_mom['f'][-1] < hist_vanilla['f'][-1]}\")\n",
    "else:\n",
    "    print(\"TODO: Implement the momentum update in the loop above!\")\n",
    "    print(\"Expected: Momentum should converge significantly faster than vanilla GD\")\n",
    "    print(f\"Vanilla GD final loss: {hist_vanilla['f'][-1]:.6f}\")"
   ],
   "id": "cell-35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Cross-Validation for Model Selection\n",
    "\n",
    "Use cross-validation to find the best polynomial degree for a noisy dataset. This connects the bias-variance tradeoff to practical model selection."
   ],
   "id": "cell-36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 2: Find the best polynomial degree using cross-validation\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data\n",
    "n_cv = 100\n",
    "x_cv = np.random.uniform(0, 2*np.pi, n_cv)\n",
    "y_cv = np.sin(x_cv) + 0.3 * np.cos(3*x_cv) + 0.4 * np.random.randn(n_cv)\n",
    "\n",
    "def evaluate_polynomial_degree(x, y, degree, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Evaluate a polynomial regression model using cross-validation.\n",
    "\n",
    "    Args:\n",
    "        x: Input features (1D array)\n",
    "        y: Target values\n",
    "        degree: Polynomial degree to evaluate\n",
    "        cv_folds: Number of cross-validation folds\n",
    "\n",
    "    Returns:\n",
    "        mean_cv_score: Mean cross-validation score (negative MSE)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this!\n",
    "    # Hint:\n",
    "    # 1. Create polynomial features: PolynomialFeatures(degree)\n",
    "    # 2. Transform x: poly.fit_transform(x.reshape(-1, 1))\n",
    "    # 3. Use cross_val_score with LinearRegression and scoring='neg_mean_squared_error'\n",
    "    # 4. Return the mean score\n",
    "\n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "# Test all degrees from 1 to 15\n",
    "degrees_cv = range(1, 16)\n",
    "cv_scores = []\n",
    "\n",
    "for d in degrees_cv:\n",
    "    score = evaluate_polynomial_degree(x_cv, y_cv, d)\n",
    "    if score is not None:\n",
    "        cv_scores.append(-score)  # Negate because sklearn returns negative MSE\n",
    "    else:\n",
    "        cv_scores.append(None)\n",
    "\n",
    "if cv_scores[0] is not None:\n",
    "    best_degree = degrees_cv[np.argmin(cv_scores)]\n",
    "    print(f\"Best polynomial degree by CV: {best_degree}\")\n",
    "    print(f\"CV MSE scores: {[f'{s:.4f}' for s in cv_scores]}\")\n",
    "\n",
    "    # Verify\n",
    "    expected_best = 4  # Approximate expected best (sin + cos terms)\n",
    "    print(f\"\\nExpected best degree: around {expected_best}\")\n",
    "    print(f\"Your best degree: {best_degree}\")\n",
    "    print(f\"Reasonable: {1 <= best_degree <= 7}\")\n",
    "else:\n",
    "    print(\"TODO: Implement evaluate_polynomial_degree above!\")\n",
    "    print(\"Expected: Best degree should be around 3-5 (matching the true function complexity)\")"
   ],
   "id": "cell-37"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Compare L1, L2, and Elastic Net Regularization\n",
    "\n",
    "Fit models with different regularization strategies on a dataset with many irrelevant features. Analyze which method best recovers the true sparse structure."
   ],
   "id": "cell-38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 3: Regularization comparison\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create dataset with sparse true weights\n",
    "n_samples, n_features_ex = 80, 30\n",
    "X_ex = np.random.randn(n_samples, n_features_ex)\n",
    "w_true_ex = np.zeros(n_features_ex)\n",
    "w_true_ex[:8] = [4, -3, 2, -1.5, 1, -0.5, 3, -2]  # Only 8 features matter\n",
    "y_ex = X_ex @ w_true_ex + 0.5 * np.random.randn(n_samples)\n",
    "\n",
    "def compare_regularizers(X, y, w_true, alpha_l1=0.5, alpha_l2=1.0, l1_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Compare L1, L2, and Elastic Net regularization.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target vector\n",
    "        w_true: True weight vector (for comparison)\n",
    "        alpha_l1: Regularization strength for Lasso\n",
    "        alpha_l2: Regularization strength for Ridge\n",
    "        l1_ratio: L1 ratio for Elastic Net\n",
    "\n",
    "    Returns:\n",
    "        dict with 'lasso', 'ridge', 'elastic_net' entries, each containing:\n",
    "            'coef': fitted coefficients\n",
    "            'n_nonzero': number of non-zero coefficients (|w| > 0.01)\n",
    "            'mse': mean squared error of coefficient recovery\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # TODO: Implement this!\n",
    "    # Hint:\n",
    "    # 1. Fit Lasso(alpha=alpha_l1), Ridge(alpha=alpha_l2),\n",
    "    #    and ElasticNet (from sklearn.linear_model import ElasticNet)\n",
    "    # 2. For each, compute:\n",
    "    #    - coef: model.coef_\n",
    "    #    - n_nonzero: np.sum(np.abs(model.coef_) > 0.01)\n",
    "    #    - mse: np.mean((model.coef_ - w_true)**2)\n",
    "\n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test\n",
    "results = compare_regularizers(X_ex, y_ex, w_true_ex)\n",
    "\n",
    "if results:\n",
    "    print(\"Regularization Comparison:\")\n",
    "    print(f\"{'Method':<15} {'Non-zero':>10} {'Coef MSE':>10} {'True non-zero':>15}\")\n",
    "    print(\"-\" * 55)\n",
    "    true_nonzero = np.sum(np.abs(w_true_ex) > 0.01)\n",
    "    for name, res in results.items():\n",
    "        print(f\"{name:<15} {res['n_nonzero']:>10} {res['mse']:>10.4f} {true_nonzero:>15}\")\n",
    "\n",
    "    # Verify\n",
    "    print(f\"\\nExpected: Lasso should have fewest non-zero weights (closest to {true_nonzero})\")\n",
    "    print(f\"Expected: Ridge should have ALL weights non-zero\")\n",
    "    print(f\"Expected: Elastic Net should be in between\")\n",
    "else:\n",
    "    print(\"TODO: Implement compare_regularizers above!\")\n",
    "    print(f\"True non-zero weights: {np.sum(np.abs(w_true_ex) > 0.01)}\")\n",
    "    print(\"Expected: L1 finds sparsity, L2 shrinks all, Elastic Net balances both\")"
   ],
   "id": "cell-39"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 (Bonus): Learning Rate Schedules\n",
    "\n",
    "Implement and compare three learning rate schedules for SGD:\n",
    "1. **Constant**: $\\eta_t = \\eta_0$\n",
    "2. **Step decay**: $\\eta_t = \\eta_0 \\cdot 0.5^{\\lfloor t/50 \\rfloor}$\n",
    "3. **Cosine annealing**: $\\eta_t = \\eta_0 \\cdot \\frac{1}{2}\\left(1 + \\cos\\left(\\frac{\\pi t}{T}\\right)\\right)$"
   ],
   "id": "cell-40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 4 (BONUS): Learning rate schedules\n",
    "np.random.seed(42)\n",
    "\n",
    "def lr_constant(t, lr0=0.01):\n",
    "    return lr0\n",
    "\n",
    "def lr_step_decay(t, lr0=0.01, drop_rate=0.5, drop_every=50):\n",
    "    \"\"\"Step decay: halve the learning rate every drop_every steps.\"\"\"\n",
    "    # TODO: Implement this\n",
    "    # Hint: return lr0 * drop_rate ** (t // drop_every)\n",
    "    pass\n",
    "\n",
    "def lr_cosine(t, lr0=0.01, T=200):\n",
    "    \"\"\"Cosine annealing schedule.\"\"\"\n",
    "    # TODO: Implement this\n",
    "    # Hint: return lr0 * 0.5 * (1 + np.cos(np.pi * t / T))\n",
    "    pass\n",
    "\n",
    "# Compare on the least-squares problem from Section 3\n",
    "schedules = {\n",
    "    'Constant': lr_constant,\n",
    "    'Step Decay': lr_step_decay,\n",
    "    'Cosine Annealing': lr_cosine\n",
    "}\n",
    "\n",
    "n_steps_sched = 200\n",
    "\n",
    "for name, schedule in schedules.items():\n",
    "    if schedule is None or schedule(0) is None:\n",
    "        print(f\"TODO: Implement {name} schedule\")\n",
    "        continue\n",
    "    np.random.seed(42)\n",
    "    hist = sgd(stochastic_grad, w0, schedule, n_steps_sched, N, batch_size=16, f=full_loss)\n",
    "    print(f\"{name}: final loss = {hist['f'][-1]:.6f}\")\n",
    "\n",
    "print(\"\\nExpected: Cosine annealing or step decay should outperform constant LR\")"
   ],
   "id": "cell-41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Gradient descent convergence** depends on convexity and the condition number. Strongly convex functions converge exponentially; convex functions converge as $O(1/t)$.\n",
    "- **Step size selection** is critical: too large causes divergence, too small wastes compute. The optimal step size is $\\eta = 1/L$ where $L$ is the Lipschitz constant.\n",
    "- **SGD** trades exact gradients for computational efficiency, and the noise provides implicit regularization that often improves generalization.\n",
    "- **Bias-variance tradeoff** decomposes test error into underfitting (bias) and overfitting (variance). Model complexity must be matched to the problem.\n",
    "- **VC dimension** measures model capacity \u2014 the largest dataset a model class can perfectly memorize for any labeling.\n",
    "- **PAC learning** provides sample complexity bounds: how much data you need for a given accuracy and confidence level.\n",
    "- **L1 regularization** produces sparse solutions (feature selection) while **L2 regularization** produces small weights. This follows from the geometry of their constraint regions.\n",
    "- **Overparameterized networks** generalize better than classical theory predicts, due to implicit regularization, the double descent phenomenon, and the lottery ticket hypothesis."
   ],
   "id": "cell-42"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to Machine Learning\n",
    "\n",
    "| Concept | ML Application | Why It Matters |\n",
    "|---------|---------------|----------------|\n",
    "| Convergence rates | Learning rate tuning | Knowing the theory prevents trial-and-error |\n",
    "| Condition number | Feature normalization, BatchNorm | Explains why preprocessing helps |\n",
    "| SGD noise | Generalization, escaping local minima | Why SGD often beats full-batch GD |\n",
    "| Bias-variance | Model selection, hyperparameter tuning | The fundamental tradeoff in all of ML |\n",
    "| VC dimension | Choosing model architecture | Capacity must match data complexity |\n",
    "| PAC learning | Dataset size requirements | How much data you actually need |\n",
    "| L1/L2 regularization | Dropout, weight decay, pruning | Most common tools against overfitting |\n",
    "| Double descent | Why large models work | Challenges classical \"simpler is better\" wisdom |\n",
    "| Lottery tickets | Network pruning, efficient inference | 90%+ of weights may be unnecessary |"
   ],
   "id": "cell-43"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checklist\n",
    "\n",
    "- [ ] I can explain why strongly convex functions converge faster than convex ones\n",
    "- [ ] I can choose an appropriate learning rate given a function's properties\n",
    "- [ ] I understand why SGD's noise is a feature, not a bug\n",
    "- [ ] I can decompose test error into bias, variance, and irreducible noise\n",
    "- [ ] I can explain what VC dimension measures and why it matters\n",
    "- [ ] I can use PAC bounds to estimate required dataset sizes\n",
    "- [ ] I understand geometrically why L1 gives sparsity (diamond vs sphere)\n",
    "- [ ] I can explain the double descent curve and why overparameterization helps\n",
    "- [ ] I know what the lottery ticket hypothesis claims and its implications"
   ],
   "id": "cell-44"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "In **Part 2.1: Python OOP for ML** (Notebook 07), we shift from mathematical foundations to software foundations. You'll learn the object-oriented programming patterns that underpin every ML framework \u2014 from PyTorch's `nn.Module` to scikit-learn's estimator API. The optimization theory from this notebook will come alive when you implement gradient descent as a proper `Optimizer` class.\n",
    "\n",
    "**Key connections to look forward to:**\n",
    "- The `Optimizer` class pattern (SGD, Adam, etc.) encapsulates everything from Sections 1-3\n",
    "- Regularization shows up as `weight_decay` parameters in optimizers\n",
    "- Model selection (bias-variance) drives hyperparameter search pipelines\n",
    "- Everything in this notebook becomes practical when you build training loops in Part 3"
   ],
   "id": "cell-45"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}