{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Part 4.2: Computer Vision \u2014 Beyond Classification\n",
    "\n",
    "CNNs can do much more than tell you \"this is a cat.\" The same spatial feature hierarchies that power image classification \u2014 edges, textures, parts, objects \u2014 serve as the backbone for a rich family of vision tasks: **locating** objects in images, **drawing pixel-perfect masks** around them, and even **connecting images to language**. This notebook explores the architectures that take convolutional features and push them far beyond a single class label.\n",
    "\n",
    "We will build up from classification to detection, segmentation, and finally to modern vision transformers and multimodal models like CLIP. Each section introduces the core idea, implements a key component from scratch, and connects it to the broader deep learning landscape.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you should be able to:\n",
    "\n",
    "- [ ] Describe the progression from classification \u2192 localization \u2192 detection \u2192 segmentation\n",
    "- [ ] Implement Intersection over Union (IoU) from scratch and explain its role in detection\n",
    "- [ ] Explain anchor boxes and Non-Maximum Suppression (NMS) and implement both\n",
    "- [ ] Describe the YOLO single-pass detection philosophy and its grid-based output structure\n",
    "- [ ] Explain the U-Net encoder-decoder architecture and implement a mini version in PyTorch\n",
    "- [ ] Contrast semantic segmentation, instance segmentation, and panoptic segmentation\n",
    "- [ ] Implement patch embeddings for Vision Transformers (ViT) and explain positional embeddings\n",
    "- [ ] Describe CLIP's contrastive learning approach for connecting vision and language\n",
    "- [ ] Apply transfer learning with pretrained torchvision models"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. From Classification to Detection\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "In notebook 13, we trained CNNs to answer a single question: **\"What is in this image?\"** That is classification \u2014 one label per image. But real-world vision demands much more:\n",
    "\n",
    "| Task | Question | Output | Example |\n",
    "|------|----------|--------|--------|\n",
    "| **Classification** | \"What is this?\" | One label | \"Cat\" |\n",
    "| **Localization** | \"What is this and where?\" | Label + one bounding box | \"Cat at (x, y, w, h)\" |\n",
    "| **Object Detection** | \"What are all the things and where?\" | Multiple labels + boxes | \"Cat at ..., Dog at ...\" |\n",
    "| **Semantic Segmentation** | \"What is every pixel?\" | Per-pixel class label | Pixel grid of classes |\n",
    "| **Instance Segmentation** | \"What object does every pixel belong to?\" | Per-pixel class + instance ID | \"Cat #1, Cat #2\" |\n",
    "\n",
    "The key insight is that **the same CNN backbone** (VGG, ResNet, etc.) can serve all these tasks. The difference lies in what you attach to the end of the feature extractor \u2014 a classification head, a bounding-box regressor, a pixel-wise decoder, or a combination."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "# Visualize the hierarchy of computer vision tasks\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "\n",
    "# 1. Classification\n",
    "ax = axes[0]\n",
    "ax.set_title('Classification', fontsize=13, fontweight='bold')\n",
    "rect = patches.FancyBboxPatch((1, 1), 8, 8, boxstyle=\"round,pad=0.2\",\n",
    "                               facecolor='lightblue', edgecolor='blue', linewidth=2)\n",
    "ax.add_patch(rect)\n",
    "circle = plt.Circle((5, 4.5), 2, color='gray', alpha=0.6)\n",
    "ax.add_patch(circle)\n",
    "ax.plot([3.5, 3.0, 4.0], [6.3, 7.5, 6.8], color='gray', linewidth=2)\n",
    "ax.plot([6.5, 7.0, 6.0], [6.3, 7.5, 6.8], color='gray', linewidth=2)\n",
    "ax.text(5, 0.3, '\"Cat\"', ha='center', fontsize=12, fontweight='bold', color='blue')\n",
    "\n",
    "# 2. Localization\n",
    "ax = axes[1]\n",
    "ax.set_title('Localization', fontsize=13, fontweight='bold')\n",
    "rect = patches.FancyBboxPatch((1, 1), 8, 8, boxstyle=\"round,pad=0.2\",\n",
    "                               facecolor='lightyellow', edgecolor='gray', linewidth=1)\n",
    "ax.add_patch(rect)\n",
    "circle = plt.Circle((5, 4.5), 2, color='gray', alpha=0.6)\n",
    "ax.add_patch(circle)\n",
    "ax.plot([3.5, 3.0, 4.0], [6.3, 7.5, 6.8], color='gray', linewidth=2)\n",
    "ax.plot([6.5, 7.0, 6.0], [6.3, 7.5, 6.8], color='gray', linewidth=2)\n",
    "bbox = patches.Rectangle((2.5, 2), 5, 6.2, linewidth=2, edgecolor='red',\n",
    "                          facecolor='none', linestyle='--')\n",
    "ax.add_patch(bbox)\n",
    "ax.text(5, 0.3, '\"Cat\" + box', ha='center', fontsize=12, fontweight='bold', color='red')\n",
    "\n",
    "# 3. Detection\n",
    "ax = axes[2]\n",
    "ax.set_title('Detection', fontsize=13, fontweight='bold')\n",
    "rect = patches.FancyBboxPatch((1, 1), 8, 8, boxstyle=\"round,pad=0.2\",\n",
    "                               facecolor='lightyellow', edgecolor='gray', linewidth=1)\n",
    "ax.add_patch(rect)\n",
    "circle1 = plt.Circle((3.5, 4.5), 1.4, color='gray', alpha=0.6)\n",
    "ax.add_patch(circle1)\n",
    "ax.plot([2.5, 2.2, 2.9], [5.7, 6.5, 6.1], color='gray', linewidth=2)\n",
    "ax.plot([4.5, 4.8, 4.1], [5.7, 6.5, 6.1], color='gray', linewidth=2)\n",
    "circle2 = plt.Circle((7, 3.5), 1.2, color='brown', alpha=0.4)\n",
    "ax.add_patch(circle2)\n",
    "bbox1 = patches.Rectangle((1.7, 2.5), 3.6, 4.5, linewidth=2, edgecolor='red',\n",
    "                           facecolor='none', linestyle='--')\n",
    "ax.add_patch(bbox1)\n",
    "bbox2 = patches.Rectangle((5.5, 1.8), 3, 3.4, linewidth=2, edgecolor='green',\n",
    "                           facecolor='none', linestyle='--')\n",
    "ax.add_patch(bbox2)\n",
    "ax.text(3.5, 7.5, 'Cat', fontsize=10, color='red', fontweight='bold', ha='center')\n",
    "ax.text(7, 5.8, 'Dog', fontsize=10, color='green', fontweight='bold', ha='center')\n",
    "\n",
    "# 4. Segmentation\n",
    "ax = axes[3]\n",
    "ax.set_title('Segmentation', fontsize=13, fontweight='bold')\n",
    "rect = patches.FancyBboxPatch((1, 1), 8, 8, boxstyle=\"round,pad=0.2\",\n",
    "                               facecolor='lightyellow', edgecolor='gray', linewidth=1)\n",
    "ax.add_patch(rect)\n",
    "circle1 = plt.Circle((3.5, 4.5), 1.4, color='red', alpha=0.4)\n",
    "ax.add_patch(circle1)\n",
    "circle2 = plt.Circle((7, 3.5), 1.2, color='green', alpha=0.4)\n",
    "ax.add_patch(circle2)\n",
    "ax.text(5, 0.3, 'Per-pixel labels', ha='center', fontsize=12, fontweight='bold', color='purple')\n",
    "\n",
    "plt.suptitle('The Computer Vision Task Hierarchy', fontsize=15, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "### Deep Dive: What CNNs Actually Learn\n",
    "\n",
    "Recall from notebook 13 that CNN layers learn a hierarchy of increasingly abstract features:\n",
    "\n",
    "| Layer Depth | What It Detects | Receptive Field | Analogy |\n",
    "|------------|----------------|-----------------|--------|\n",
    "| Layer 1 | Edges, gradients | 3x3 - 5x5 pixels | Individual brush strokes |\n",
    "| Layer 2 | Textures, corners | 10-20 pixels | Patterns in the paint |\n",
    "| Layer 3 | Parts (eyes, wheels) | 40-80 pixels | Recognizable components |\n",
    "| Layer 4 | Objects (faces, cars) | 100+ pixels | Complete things |\n",
    "| Layer 5 | Scenes, contexts | Entire image | The full picture |\n",
    "\n",
    "**Key insight:** For classification, we only care about the final layer's global summary. For detection and segmentation, we need the **spatial information** from intermediate layers \u2014 we need to know not just *what* features are present, but *where* they are. This is why detection and segmentation architectures carefully preserve and combine features from multiple depths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Object Detection Foundations\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "Object detection answers: \"What objects are in this image, and where is each one?\" This requires predicting both a **class label** and a **bounding box** (x, y, width, height) for every object. The fundamental building blocks are:\n",
    "\n",
    "1. **Anchor boxes** \u2014 a grid of pre-defined candidate regions tiled across the image\n",
    "2. **IoU (Intersection over Union)** \u2014 the metric that measures how well a predicted box matches a ground truth box\n",
    "3. **Non-Maximum Suppression (NMS)** \u2014 the post-processing step that removes duplicate detections\n",
    "\n",
    "Let's implement each one from scratch."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "def compute_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Compute Intersection over Union between two bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        box1: [x1, y1, x2, y2] -- top-left and bottom-right corners\n",
    "        box2: [x1, y1, x2, y2] -- top-left and bottom-right corners\n",
    "\n",
    "    Returns:\n",
    "        IoU value between 0 and 1\n",
    "    \"\"\"\n",
    "    # Compute intersection coordinates\n",
    "    inter_x1 = max(box1[0], box2[0])\n",
    "    inter_y1 = max(box1[1], box2[1])\n",
    "    inter_x2 = min(box1[2], box2[2])\n",
    "    inter_y2 = min(box1[3], box2[3])\n",
    "\n",
    "    # Compute intersection area (0 if no overlap)\n",
    "    inter_width = max(0, inter_x2 - inter_x1)\n",
    "    inter_height = max(0, inter_y2 - inter_y1)\n",
    "    inter_area = inter_width * inter_height\n",
    "\n",
    "    # Compute union area\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union_area = area1 + area2 - inter_area\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if union_area == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return inter_area / union_area\n",
    "\n",
    "# Test IoU with examples\n",
    "box_a = [1, 1, 4, 4]  # 3x3 box\n",
    "box_b = [2, 2, 5, 5]  # 3x3 box, partially overlapping\n",
    "box_c = [5, 5, 8, 8]  # 3x3 box, no overlap\n",
    "box_d = [1, 1, 4, 4]  # identical to box_a\n",
    "\n",
    "print(\"IoU Examples:\")\n",
    "print(f\"  Partial overlap:  IoU(A, B) = {compute_iou(box_a, box_b):.4f}\")\n",
    "print(f\"  No overlap:       IoU(A, C) = {compute_iou(box_a, box_c):.4f}\")\n",
    "print(f\"  Perfect overlap:  IoU(A, D) = {compute_iou(box_a, box_d):.4f}\")\n",
    "\n",
    "# Verify: A and B overlap in a 2x2 region. Union = 9+9-4 = 14. IoU = 4/14\n",
    "print(f\"\\nManual check: 4/14 = {4/14:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "# Visualize IoU with different overlap levels\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "examples = [\n",
    "    (\"No Overlap\\nIoU = 0.00\", [1, 1, 4, 4], [5, 5, 8, 8]),\n",
    "    (\"Small Overlap\\nIoU = 0.07\", [1, 1, 4, 4], [3, 3, 6, 6]),\n",
    "    (\"Large Overlap\\nIoU = 0.29\", [1, 1, 4, 4], [2, 2, 5, 5]),\n",
    "    (\"Perfect Overlap\\nIoU = 1.00\", [1, 1, 4, 4], [1, 1, 4, 4]),\n",
    "]\n",
    "\n",
    "for ax, (title, b1, b2) in zip(axes, examples):\n",
    "    ax.set_xlim(0, 9)\n",
    "    ax.set_ylim(0, 9)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Draw box 1 (blue)\n",
    "    rect1 = patches.Rectangle((b1[0], b1[1]), b1[2]-b1[0], b1[3]-b1[1],\n",
    "                                linewidth=2, edgecolor='blue', facecolor='blue', alpha=0.3)\n",
    "    ax.add_patch(rect1)\n",
    "\n",
    "    # Draw box 2 (red)\n",
    "    rect2 = patches.Rectangle((b2[0], b2[1]), b2[2]-b2[0], b2[3]-b2[1],\n",
    "                                linewidth=2, edgecolor='red', facecolor='red', alpha=0.3)\n",
    "    ax.add_patch(rect2)\n",
    "\n",
    "    # Highlight intersection (green)\n",
    "    ix1, iy1 = max(b1[0], b2[0]), max(b1[1], b2[1])\n",
    "    ix2, iy2 = min(b1[2], b2[2]), min(b1[3], b2[3])\n",
    "    if ix2 > ix1 and iy2 > iy1:\n",
    "        inter = patches.Rectangle((ix1, iy1), ix2-ix1, iy2-iy1,\n",
    "                                   linewidth=2, edgecolor='green', facecolor='green', alpha=0.5)\n",
    "        ax.add_patch(inter)\n",
    "\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "\n",
    "axes[0].legend([patches.Patch(color='blue', alpha=0.3),\n",
    "                patches.Patch(color='red', alpha=0.3),\n",
    "                patches.Patch(color='green', alpha=0.5)],\n",
    "               ['Box A', 'Box B', 'Intersection'], loc='upper right', fontsize=9)\n",
    "\n",
    "plt.suptitle('Intersection over Union (IoU)', fontsize=14, fontweight='bold', y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Anchor Boxes\n",
    "\n",
    "Instead of searching every possible rectangle in an image, detection models tile the image with a set of **anchor boxes** (also called \"priors\" or \"default boxes\") at each spatial location. Each anchor has a predefined **aspect ratio** and **scale**. The network then predicts:\n",
    "\n",
    "1. **Offsets** \u2014 small adjustments (dx, dy, dw, dh) to shift each anchor to better fit an object\n",
    "2. **Objectness score** \u2014 probability that this anchor actually contains an object\n",
    "3. **Class probabilities** \u2014 what kind of object it is\n",
    "\n",
    "This is far more efficient than searching from scratch \u2014 the anchors provide a good starting point, and the network only needs to learn small refinements."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "# Visualize anchor boxes tiled across an image\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: grid of anchor centers\n",
    "ax = axes[0]\n",
    "ax.set_xlim(0, 8)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('Anchor Box Grid (Centers)', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Create a grid of anchor centers\n",
    "grid_size = 4\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        cx = i * 2 + 1\n",
    "        cy = j * 2 + 1\n",
    "        ax.plot(cx, cy, 'k+', markersize=10, markeredgewidth=2)\n",
    "        rect = patches.Rectangle((i*2, j*2), 2, 2, linewidth=1,\n",
    "                                  edgecolor='gray', facecolor='none', linestyle='--')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "# Right: multiple anchors at one location\n",
    "ax = axes[1]\n",
    "ax.set_xlim(0, 8)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('Multiple Anchors at One Location', fontsize=13, fontweight='bold')\n",
    "\n",
    "cx, cy = 4, 4\n",
    "ax.plot(cx, cy, 'k+', markersize=15, markeredgewidth=3)\n",
    "\n",
    "# Different aspect ratios and scales\n",
    "anchors = [\n",
    "    (2, 2, 'blue', '1:1 small'),\n",
    "    (3, 3, 'red', '1:1 large'),\n",
    "    (4, 2, 'green', '2:1'),\n",
    "    (2, 4, 'orange', '1:2'),\n",
    "    (5, 2.5, 'purple', '2:1 large'),\n",
    "]\n",
    "\n",
    "for w, h, color, label in anchors:\n",
    "    rect = patches.Rectangle((cx - w/2, cy - h/2), w, h,\n",
    "                              linewidth=2, edgecolor=color, facecolor='none',\n",
    "                              linestyle='-', label=label)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "plt.suptitle('How Anchor Boxes Work', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"At each grid cell, multiple anchors with different shapes cover various object geometries.\")\n",
    "print(f\"With a {grid_size}x{grid_size} grid and {len(anchors)} anchors per cell: {grid_size**2 * len(anchors)} total anchors.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### Non-Maximum Suppression (NMS)\n",
    "\n",
    "A detection model with many anchors will typically produce **multiple overlapping detections** for the same object. NMS cleans this up:\n",
    "\n",
    "1. Sort all detections by confidence score (highest first)\n",
    "2. Take the highest-scoring detection as a \"keep\"\n",
    "3. Remove all other detections that overlap with it above an IoU threshold (e.g., 0.5)\n",
    "4. Repeat with the next highest-scoring surviving detection\n",
    "5. Continue until no detections remain\n",
    "\n",
    "**What this means:** NMS keeps the single best detection for each object and suppresses the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "def nms(boxes, scores, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Non-Maximum Suppression: remove duplicate detections.\n",
    "\n",
    "    Args:\n",
    "        boxes: list of [x1, y1, x2, y2] bounding boxes\n",
    "        scores: list of confidence scores (one per box)\n",
    "        iou_threshold: IoU above which a box is considered duplicate\n",
    "\n",
    "    Returns:\n",
    "        List of indices of boxes to keep\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # Sort by score (descending)\n",
    "    order = np.argsort(scores)[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while len(order) > 0:\n",
    "        # Keep the highest scoring box\n",
    "        best_idx = order[0]\n",
    "        keep.append(best_idx)\n",
    "\n",
    "        # Compare with all remaining boxes\n",
    "        remaining = order[1:]\n",
    "        suppress = []\n",
    "\n",
    "        for i, idx in enumerate(remaining):\n",
    "            iou = compute_iou(boxes[best_idx], boxes[idx])\n",
    "            if iou >= iou_threshold:\n",
    "                suppress.append(i)\n",
    "\n",
    "        # Remove suppressed boxes\n",
    "        order = np.delete(remaining, suppress)\n",
    "\n",
    "    return keep\n",
    "\n",
    "# Test NMS: three overlapping detections of the same object\n",
    "boxes = [\n",
    "    [1, 1, 4, 4],   # detection A\n",
    "    [1.2, 1.1, 4.2, 4.1],  # detection B (overlaps A heavily)\n",
    "    [1.5, 1.3, 4.5, 4.3],  # detection C (overlaps A)\n",
    "    [6, 6, 9, 9],   # detection D (separate object)\n",
    "]\n",
    "scores = [0.9, 0.75, 0.8, 0.85]\n",
    "\n",
    "kept = nms(boxes, scores, iou_threshold=0.5)\n",
    "print(\"NMS Results:\")\n",
    "print(f\"  Input: {len(boxes)} detections\")\n",
    "print(f\"  Kept indices: {kept}\")\n",
    "print(f\"  Kept scores: {[scores[i] for i in kept]}\")\n",
    "print(f\"  Output: {len(kept)} detections (duplicates removed)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "# Visualize NMS before and after\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "labels = [f'Score={s:.2f}' for s in scores]\n",
    "\n",
    "# Before NMS\n",
    "ax = axes[0]\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('Before NMS (4 detections)', fontsize=13, fontweight='bold')\n",
    "\n",
    "for i, (box, score, color) in enumerate(zip(boxes, scores, colors)):\n",
    "    rect = patches.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1],\n",
    "                              linewidth=2, edgecolor=color, facecolor=color, alpha=0.2,\n",
    "                              label=f'Box {i}: score={score:.2f}')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# After NMS\n",
    "ax = axes[1]\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title(f'After NMS (kept {len(kept)} detections)', fontsize=13, fontweight='bold')\n",
    "\n",
    "for i in kept:\n",
    "    box = boxes[i]\n",
    "    rect = patches.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1],\n",
    "                              linewidth=3, edgecolor=colors[i], facecolor=colors[i], alpha=0.3,\n",
    "                              label=f'Box {i}: score={scores[i]:.2f}')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Non-Maximum Suppression', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. YOLO: You Only Look Once\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "Early detection methods like R-CNN were **two-stage**: first propose candidate regions, then classify each one. This was accurate but slow \u2014 hundreds of separate classification passes per image.\n",
    "\n",
    "**YOLO** revolutionized detection by framing it as a **single regression problem**: one neural network pass predicts all bounding boxes and class probabilities simultaneously.\n",
    "\n",
    "#### How YOLO Works\n",
    "\n",
    "1. **Divide** the image into an S x S grid (e.g., 7x7)\n",
    "2. Each grid cell predicts **B bounding boxes**, each with:\n",
    "   - 4 coordinates: (x, y, w, h) relative to the cell\n",
    "   - 1 objectness confidence: P(object) x IoU(pred, truth)\n",
    "3. Each grid cell also predicts **C class probabilities** (shared across all B boxes)\n",
    "4. The output tensor has shape: **S x S x (B x 5 + C)**\n",
    "\n",
    "For example, with S=7, B=2, and C=20 (PASCAL VOC classes):\n",
    "- Output shape: 7 x 7 x (2 x 5 + 20) = 7 x 7 x 30"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "# Visualize YOLO grid-based detection\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "S = 7  # grid size\n",
    "\n",
    "# 1. The grid\n",
    "ax = axes[0]\n",
    "ax.set_xlim(0, S)\n",
    "ax.set_ylim(0, S)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title(f'{S}x{S} Grid Over Image', fontsize=13, fontweight='bold')\n",
    "\n",
    "for i in range(S + 1):\n",
    "    ax.axhline(y=i, color='gray', linewidth=0.5, alpha=0.5)\n",
    "    ax.axvline(x=i, color='gray', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "ax.fill_between([0, S], 0, S, color='lightblue', alpha=0.3)\n",
    "ax.set_xlabel('Grid columns')\n",
    "ax.set_ylabel('Grid rows')\n",
    "\n",
    "# 2. Cell responsibility\n",
    "ax = axes[1]\n",
    "ax.set_xlim(0, S)\n",
    "ax.set_ylim(0, S)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('Cell Responsible for Object', fontsize=13, fontweight='bold')\n",
    "\n",
    "for i in range(S + 1):\n",
    "    ax.axhline(y=i, color='gray', linewidth=0.5, alpha=0.5)\n",
    "    ax.axvline(x=i, color='gray', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "obj_cx, obj_cy = 3.6, 4.3\n",
    "ax.plot(obj_cx, obj_cy, 'r*', markersize=20, label='Object center')\n",
    "responsible_cell = patches.Rectangle((3, 4), 1, 1, linewidth=3,\n",
    "                                      edgecolor='red', facecolor='red', alpha=0.3,\n",
    "                                      label='Responsible cell')\n",
    "ax.add_patch(responsible_cell)\n",
    "\n",
    "obj_box = patches.Rectangle((2.1, 3.0), 3.0, 2.6, linewidth=2,\n",
    "                             edgecolor='blue', facecolor='none', linestyle='--',\n",
    "                             label='Ground truth box')\n",
    "ax.add_patch(obj_box)\n",
    "ax.legend(loc='upper right', fontsize=9)\n",
    "ax.set_xlabel('Grid columns')\n",
    "ax.set_ylabel('Grid rows')\n",
    "\n",
    "# 3. Output tensor structure\n",
    "ax = axes[2]\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "ax.set_title('Output Tensor per Cell', fontsize=13, fontweight='bold')\n",
    "\n",
    "y_start = 6.5\n",
    "box_h = 0.6\n",
    "box_w = 1.2\n",
    "\n",
    "elements_b1 = ['x1', 'y1', 'w1', 'h1', 'conf1']\n",
    "for i, elem in enumerate(elements_b1):\n",
    "    rect = patches.FancyBboxPatch((i * box_w + 0.2, y_start), box_w - 0.1, box_h,\n",
    "                                   boxstyle=\"round,pad=0.05\", facecolor='lightcoral', edgecolor='red')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(i * box_w + 0.2 + (box_w-0.1)/2, y_start + box_h/2, elem,\n",
    "            ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "ax.text(3.1, y_start + box_h + 0.3, 'Box 1 (5 values)', ha='center', fontsize=10, color='red')\n",
    "\n",
    "y_b2 = y_start - 1.2\n",
    "for i, elem in enumerate(elements_b1):\n",
    "    elem2 = elem.replace('1', '2')\n",
    "    rect = patches.FancyBboxPatch((i * box_w + 0.2, y_b2), box_w - 0.1, box_h,\n",
    "                                   boxstyle=\"round,pad=0.05\", facecolor='lightgreen', edgecolor='green')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(i * box_w + 0.2 + (box_w-0.1)/2, y_b2 + box_h/2, elem2,\n",
    "            ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "ax.text(3.1, y_b2 + box_h + 0.3, 'Box 2 (5 values)', ha='center', fontsize=10, color='green')\n",
    "\n",
    "y_cls = y_b2 - 1.5\n",
    "cls_labels = ['P(cat)', 'P(dog)', 'P(car)', '...', 'P(clsC)']\n",
    "for i, elem in enumerate(cls_labels):\n",
    "    rect = patches.FancyBboxPatch((i * box_w + 0.2, y_cls), box_w - 0.1, box_h,\n",
    "                                   boxstyle=\"round,pad=0.05\", facecolor='lightyellow', edgecolor='orange')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(i * box_w + 0.2 + (box_w-0.1)/2, y_cls + box_h/2, elem,\n",
    "            ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "ax.text(3.1, y_cls + box_h + 0.3, 'Class probs (C values)', ha='center', fontsize=10, color='orange')\n",
    "\n",
    "ax.text(3.1, y_cls - 0.6, 'Total per cell: B*5 + C values', ha='center', fontsize=11,\n",
    "        fontweight='bold', style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "class SimpleYOLOHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified YOLO-style detection head.\n",
    "    Takes a feature map and predicts bounding boxes + classes per grid cell.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, S=7, B=2, C=20):\n",
    "        super().__init__()\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(256, B * 5 + C, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: feature map of shape (batch, in_channels, S, S)\n",
    "        Returns:\n",
    "            predictions: (batch, S, S, B*5 + C)\n",
    "        \"\"\"\n",
    "        out = self.conv(x)\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        return out\n",
    "\n",
    "    def decode(self, predictions):\n",
    "        \"\"\"Decode raw predictions into boxes, confidences, and class probs.\"\"\"\n",
    "        batch_size = predictions.shape[0]\n",
    "\n",
    "        box_preds = predictions[..., :self.B * 5].reshape(\n",
    "            batch_size, self.S, self.S, self.B, 5\n",
    "        )\n",
    "        class_preds = predictions[..., self.B * 5:]\n",
    "\n",
    "        xy = torch.sigmoid(box_preds[..., :2])\n",
    "        wh = box_preds[..., 2:4]\n",
    "        conf = torch.sigmoid(box_preds[..., 4])\n",
    "        class_probs = torch.softmax(class_preds, dim=-1)\n",
    "\n",
    "        return xy, wh, conf, class_probs\n",
    "\n",
    "# Demo\n",
    "S, B, C = 7, 2, 20\n",
    "head = SimpleYOLOHead(in_channels=512, S=S, B=B, C=C)\n",
    "\n",
    "dummy_features = torch.randn(1, 512, S, S)\n",
    "predictions = head(dummy_features)\n",
    "\n",
    "print(f\"Input feature map:  {dummy_features.shape}\")\n",
    "print(f\"Output predictions: {predictions.shape}\")\n",
    "print(f\"  Per cell: {B*5 + C} values = {B} boxes x 5 + {C} classes\")\n",
    "\n",
    "xy, wh, conf, class_probs = head.decode(predictions)\n",
    "print(f\"\\nDecoded:\")\n",
    "print(f\"  Box centers (xy):    {xy.shape}\")\n",
    "print(f\"  Box sizes (wh):      {wh.shape}\")\n",
    "print(f\"  Confidence:          {conf.shape}\")\n",
    "print(f\"  Class probabilities: {class_probs.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### Deep Dive: One-Stage vs Two-Stage Detectors\n",
    "\n",
    "| Aspect | Two-Stage (R-CNN family) | One-Stage (YOLO, SSD) |\n",
    "|--------|------------------------|----------------------|\n",
    "| **Pipeline** | Region proposal, then classify each region | Single pass: grid predictions for all boxes |\n",
    "| **Speed** | Slower (separate steps) | Faster (single forward pass) |\n",
    "| **Accuracy** | Generally higher for small objects | Competitive, sometimes lower on small objects |\n",
    "| **Examples** | R-CNN, Fast R-CNN, Faster R-CNN | YOLO, SSD, RetinaNet |\n",
    "| **Key innovation** | Region Proposal Network (RPN) | Grid-based direct prediction |\n",
    "| **Use case** | When accuracy matters most | When speed matters (real-time) |\n",
    "\n",
    "#### Key Insight\n",
    "\n",
    "The fundamental tradeoff is **thoroughness vs speed**. Two-stage detectors carefully examine each proposed region, while one-stage detectors make predictions everywhere simultaneously. Modern one-stage detectors like YOLOv8 have largely closed the accuracy gap, making them the dominant choice for most real-world applications.\n",
    "\n",
    "#### Common Misconceptions\n",
    "\n",
    "| Misconception | Reality |\n",
    "|---------------|--------|\n",
    "| \"YOLO is always less accurate\" | Modern YOLO versions rival two-stage detectors on most benchmarks |\n",
    "| \"Two-stage is always better for small objects\" | Techniques like Feature Pyramid Networks (FPN) help one-stage detectors with small objects |\n",
    "| \"You must choose one approach\" | Many systems combine ideas from both families |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Semantic Segmentation\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "Semantic segmentation assigns a **class label to every single pixel** in the image. Unlike detection (which draws boxes), segmentation produces a precise mask showing exactly which pixels belong to each category.\n",
    "\n",
    "The core challenge: CNNs for classification progressively **downsample** the spatial resolution (via pooling and striding) to build high-level features. But segmentation needs **pixel-level output** \u2014 the same resolution as the input. How do we get back to full resolution?\n",
    "\n",
    "The answer: **encoder-decoder architectures** that first compress (encode) then expand (decode) the spatial dimensions."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "# Demonstrate upsampling methods\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Create a small \"feature map\"\n",
    "small = np.array([[1, 2],\n",
    "                   [3, 4]], dtype=float)\n",
    "\n",
    "# Method 1: Nearest neighbor upsampling\n",
    "ax = axes[0]\n",
    "nearest = np.repeat(np.repeat(small, 2, axis=0), 2, axis=1)\n",
    "im = ax.imshow(nearest, cmap='Blues', vmin=0, vmax=5)\n",
    "ax.set_title('Nearest Neighbor\\nUpsampling', fontsize=12, fontweight='bold')\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        ax.text(j, i, f'{nearest[i,j]:.0f}', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "# Method 2: Bilinear interpolation\n",
    "ax = axes[1]\n",
    "small_tensor = torch.tensor(small).unsqueeze(0).unsqueeze(0).float()\n",
    "bilinear = F.interpolate(small_tensor, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "bilinear_np = bilinear.squeeze().numpy()\n",
    "im = ax.imshow(bilinear_np, cmap='Blues', vmin=0, vmax=5)\n",
    "ax.set_title('Bilinear\\nInterpolation', fontsize=12, fontweight='bold')\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        ax.text(j, i, f'{bilinear_np[i,j]:.1f}', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "# Method 3: Transposed convolution (learnable!)\n",
    "ax = axes[2]\n",
    "conv_t = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, bias=False)\n",
    "conv_t.weight.data = torch.tensor([[[[1.0, 0.5], [0.5, 0.25]]]])\n",
    "with torch.no_grad():\n",
    "    trans_out = conv_t(small_tensor)\n",
    "trans_np = trans_out.squeeze().numpy()\n",
    "im = ax.imshow(trans_np, cmap='Blues', vmin=0, vmax=5)\n",
    "ax.set_title('Transposed Convolution\\n(Learnable!)', fontsize=12, fontweight='bold')\n",
    "for i in range(trans_np.shape[0]):\n",
    "    for j in range(trans_np.shape[1]):\n",
    "        ax.text(j, i, f'{trans_np[i,j]:.1f}', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "plt.suptitle('Upsampling Methods: From Low-Res Feature Maps to High-Res Output',\n",
    "             fontsize=13, fontweight='bold', y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key difference: Transposed convolution has LEARNABLE parameters.\")\n",
    "print(\"The network can learn the best way to upsample for the task.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### The U-Net Architecture\n",
    "\n",
    "U-Net is the most influential segmentation architecture, originally designed for biomedical image segmentation. Its key innovation is **skip connections** between the encoder and decoder at matching resolutions:\n",
    "\n",
    "```\n",
    "Encoder (Downsampling)          Decoder (Upsampling)\n",
    "\n",
    "Input (256x256)  ------------------>  Output (256x256)\n",
    "    | conv+pool                          ^ upconv+concat\n",
    "    v                                    |\n",
    "  (128x128)  ------------------------> (128x128)\n",
    "    | conv+pool                          ^ upconv+concat\n",
    "    v                                    |\n",
    "   (64x64)  ------------------------->  (64x64)\n",
    "    | conv+pool                          ^ upconv+concat\n",
    "    v                                    |\n",
    "   (32x32)  ------------------------->  (32x32)\n",
    "    | conv+pool                          ^ upconv\n",
    "    v                                    |\n",
    "          (16x16) Bottleneck\n",
    "```\n",
    "\n",
    "#### Why Skip Connections?\n",
    "\n",
    "The encoder captures **what** features are present (semantics) but loses **where** they are (spatial detail). The decoder needs to recover precise boundaries. Skip connections pass the high-resolution spatial information directly from the encoder to the decoder, giving it both:\n",
    "\n",
    "- **Deep features** (from the bottleneck path): \"This region contains a cat\"\n",
    "- **Fine details** (from the skip connections): \"The exact boundary of the cat\"\n",
    "\n",
    "This combination produces sharp, accurate segmentation masks."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "# Visualize U-Net architecture\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_title('U-Net Architecture', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Encoder blocks (left side, going down)\n",
    "encoder_blocks = [\n",
    "    (1, 8, 2.0, 1.2, '3x256x256\\n64 ch', 'lightblue'),\n",
    "    (2, 6.3, 1.6, 1.2, '64x128x128\\n128 ch', 'cornflowerblue'),\n",
    "    (3, 4.6, 1.3, 1.2, '128x64x64\\n256 ch', 'royalblue'),\n",
    "    (4, 2.9, 1.0, 1.2, '256x32x32\\n512 ch', 'mediumblue'),\n",
    "]\n",
    "\n",
    "# Bottleneck\n",
    "bottleneck = (5.5, 1.2, 0.8, 2.0, '512x16x16\\n1024 ch', 'darkblue')\n",
    "\n",
    "# Decoder blocks (right side, going up)\n",
    "decoder_blocks = [\n",
    "    (8.5, 2.9, 1.0, 1.2, '512x32x32', 'lightsalmon'),\n",
    "    (9.5, 4.6, 1.3, 1.2, '256x64x64', 'salmon'),\n",
    "    (10.5, 6.3, 1.6, 1.2, '128x128x128', 'lightcoral'),\n",
    "    (11.5, 8, 2.0, 1.2, '64x256x256', 'indianred'),\n",
    "]\n",
    "\n",
    "# Draw encoder\n",
    "for x, y, w, h, label, color in encoder_blocks:\n",
    "    rect = patches.FancyBboxPatch((x, y), h, w, boxstyle=\"round,pad=0.1\",\n",
    "                                   facecolor=color, edgecolor='black', alpha=0.7)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x + h/2, y + w/2, label, ha='center', va='center', fontsize=7, color='white', fontweight='bold')\n",
    "\n",
    "# Draw bottleneck\n",
    "bx, by, bw, bh, blabel, bcolor = bottleneck\n",
    "rect = patches.FancyBboxPatch((bx, by), bh, bw, boxstyle=\"round,pad=0.1\",\n",
    "                               facecolor=bcolor, edgecolor='black', alpha=0.8)\n",
    "ax.add_patch(rect)\n",
    "ax.text(bx + bh/2, by + bw/2, blabel, ha='center', va='center', fontsize=7, color='white', fontweight='bold')\n",
    "\n",
    "# Draw decoder\n",
    "for x, y, w, h, label, color in decoder_blocks:\n",
    "    rect = patches.FancyBboxPatch((x, y), h, w, boxstyle=\"round,pad=0.1\",\n",
    "                                   facecolor=color, edgecolor='black', alpha=0.7)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x + h/2, y + w/2, label, ha='center', va='center', fontsize=7, color='white', fontweight='bold')\n",
    "\n",
    "# Draw skip connections (horizontal arrows)\n",
    "skip_ys = [8.8, 7.1, 5.3, 3.4]\n",
    "enc_rights = [2.2, 3.2, 4.2, 5.2]\n",
    "dec_lefts = [11.5, 10.5, 9.5, 8.5]\n",
    "for sy, er, dl in zip(skip_ys, enc_rights, dec_lefts):\n",
    "    ax.annotate('', xy=(dl, sy), xytext=(er, sy),\n",
    "                arrowprops=dict(arrowstyle='->', color='green', lw=2, linestyle='--'))\n",
    "    ax.text((er + dl) / 2, sy + 0.15, 'skip', ha='center', fontsize=8, color='green', fontweight='bold')\n",
    "\n",
    "# Down arrows (encoder)\n",
    "for i in range(3):\n",
    "    x_from = encoder_blocks[i][0] + encoder_blocks[i][3]/2\n",
    "    y_from = encoder_blocks[i][1]\n",
    "    x_to = encoder_blocks[i+1][0] + encoder_blocks[i+1][3]/2\n",
    "    y_to = encoder_blocks[i+1][1] + encoder_blocks[i+1][2]\n",
    "    ax.annotate('', xy=(x_to, y_to), xytext=(x_from, y_from),\n",
    "                arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "\n",
    "# Arrow from last encoder to bottleneck\n",
    "ax.annotate('', xy=(bx + bh/2, by + bw),\n",
    "            xytext=(encoder_blocks[-1][0] + encoder_blocks[-1][3]/2, encoder_blocks[-1][1]),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "\n",
    "# Arrow from bottleneck to first decoder\n",
    "ax.annotate('', xy=(decoder_blocks[0][0] + decoder_blocks[0][3]/2, decoder_blocks[0][1] + decoder_blocks[0][2]),\n",
    "            xytext=(bx + bh/2, by + bw),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "\n",
    "# Up arrows (decoder)\n",
    "for i in range(3):\n",
    "    x_from = decoder_blocks[i][0] + decoder_blocks[i][3]/2\n",
    "    y_from = decoder_blocks[i][1] + decoder_blocks[i][2]\n",
    "    x_to = decoder_blocks[i+1][0] + decoder_blocks[i+1][3]/2\n",
    "    y_to = decoder_blocks[i+1][1]\n",
    "    ax.annotate('', xy=(x_to, y_to), xytext=(x_from, y_from),\n",
    "                arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "\n",
    "# Labels\n",
    "ax.text(2, 0.5, 'ENCODER\\n(Downsample)', ha='center', fontsize=12, fontweight='bold', color='blue')\n",
    "ax.text(6.5, 0.5, 'BOTTLENECK', ha='center', fontsize=12, fontweight='bold', color='darkblue')\n",
    "ax.text(11, 0.5, 'DECODER\\n(Upsample)', ha='center', fontsize=12, fontweight='bold', color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"Two consecutive conv-batchnorm-relu blocks (the basic U-Net building block).\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class MiniUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A compact U-Net for semantic segmentation.\n",
    "\n",
    "    Architecture:\n",
    "        Encoder: 3 downsampling stages (channels: 64 -> 128 -> 256)\n",
    "        Bottleneck: 512 channels\n",
    "        Decoder: 3 upsampling stages with skip connections\n",
    "        Output: num_classes channels (one per class)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder path\n",
    "        self.enc1 = DoubleConv(in_channels, 64)\n",
    "        self.enc2 = DoubleConv(64, 128)\n",
    "        self.enc3 = DoubleConv(128, 256)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(256, 512)\n",
    "\n",
    "        # Decoder path (note: input channels = skip + upsampled)\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = DoubleConv(512, 256)  # 256 (up) + 256 (skip) = 512 in\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = DoubleConv(256, 128)  # 128 (up) + 128 (skip) = 256 in\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = DoubleConv(128, 64)   # 64 (up) + 64 (skip) = 128 in\n",
    "\n",
    "        # Final 1x1 convolution to get class predictions\n",
    "        self.final = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)             # (B, 64, H, W)\n",
    "        e2 = self.enc2(self.pool(e1)) # (B, 128, H/2, W/2)\n",
    "        e3 = self.enc3(self.pool(e2)) # (B, 256, H/4, W/4)\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(self.pool(e3))  # (B, 512, H/8, W/8)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        d3 = self.up3(b)                    # (B, 256, H/4, W/4)\n",
    "        d3 = torch.cat([d3, e3], dim=1)     # (B, 512, H/4, W/4)\n",
    "        d3 = self.dec3(d3)                  # (B, 256, H/4, W/4)\n",
    "\n",
    "        d2 = self.up2(d3)                   # (B, 128, H/2, W/2)\n",
    "        d2 = torch.cat([d2, e2], dim=1)     # (B, 256, H/2, W/2)\n",
    "        d2 = self.dec2(d2)                  # (B, 128, H/2, W/2)\n",
    "\n",
    "        d1 = self.up1(d2)                   # (B, 64, H, W)\n",
    "        d1 = torch.cat([d1, e1], dim=1)     # (B, 128, H, W)\n",
    "        d1 = self.dec1(d1)                  # (B, 64, H, W)\n",
    "\n",
    "        return self.final(d1)               # (B, num_classes, H, W)\n",
    "\n",
    "# Test the Mini U-Net\n",
    "model = MiniUNet(in_channels=3, num_classes=10)\n",
    "\n",
    "x = torch.randn(2, 3, 64, 64)\n",
    "output = model(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}  -> (batch, channels, height, width)\")\n",
    "print(f\"Output shape: {output.shape} -> (batch, num_classes, height, width)\")\n",
    "print(f\"\\nOutput is a per-pixel class prediction!\")\n",
    "print(f\"To get the predicted class per pixel: output.argmax(dim=1) -> shape {output.argmax(dim=1).shape}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "# Visualize what the U-Net produces\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "fake_image = torch.randn(1, 3, 64, 64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(fake_image)\n",
    "    pred_classes = pred.argmax(dim=1).squeeze().numpy()\n",
    "\n",
    "# Display input channels\n",
    "ax = axes[0]\n",
    "ax.imshow(fake_image[0, 0].numpy(), cmap='gray')\n",
    "ax.set_title('Input (channel 0)', fontsize=12, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "# Display raw predictions (one channel)\n",
    "ax = axes[1]\n",
    "im = ax.imshow(pred[0, 0].detach().numpy(), cmap='RdBu_r')\n",
    "ax.set_title('Raw Prediction (class 0 logits)', fontsize=12, fontweight='bold')\n",
    "ax.axis('off')\n",
    "plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "\n",
    "# Display argmax segmentation map\n",
    "ax = axes[2]\n",
    "im = ax.imshow(pred_classes, cmap='tab10', vmin=0, vmax=9)\n",
    "ax.set_title('Segmentation Map (argmax)', fontsize=12, fontweight='bold')\n",
    "ax.axis('off')\n",
    "plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "\n",
    "plt.suptitle('Mini U-Net Output (Untrained -- Random Predictions)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: The model is untrained, so predictions are random.\")\n",
    "print(\"After training on labeled data, each pixel would be correctly classified.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Instance Segmentation\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "Semantic segmentation labels every pixel with a class, but it cannot distinguish between **individual instances** of the same class. If two cats overlap, semantic segmentation labels all their pixels as \"cat\" \u2014 but cannot tell you which pixels belong to cat #1 vs cat #2.\n",
    "\n",
    "**Instance segmentation** solves this by combining detection (separate bounding boxes per object) with segmentation (pixel-level mask within each box).\n",
    "\n",
    "| Approach | What It Outputs | Limitation |\n",
    "|----------|----------------|------------|\n",
    "| Semantic Segmentation | Per-pixel class label | Cannot separate overlapping objects of the same class |\n",
    "| Instance Segmentation | Per-pixel class + instance ID | More complex; needs detection + segmentation |\n",
    "| Panoptic Segmentation | Both (all pixels labeled + instances separated) | The full picture; most complex |\n",
    "\n",
    "### Mask R-CNN\n",
    "\n",
    "The dominant instance segmentation architecture is **Mask R-CNN**, which extends Faster R-CNN by adding a small mask prediction branch:\n",
    "\n",
    "1. **Backbone CNN** extracts feature maps\n",
    "2. **Region Proposal Network (RPN)** proposes candidate object regions\n",
    "3. **ROI Align** extracts fixed-size features for each proposed region (improvement over ROI Pooling \u2014 uses bilinear interpolation instead of harsh quantization)\n",
    "4. **Classification head** predicts class and refines bounding box\n",
    "5. **Mask head** predicts a binary mask for each detected object (new in Mask R-CNN)\n",
    "\n",
    "The mask head is a small FCN that outputs a 28x28 binary mask per detected object. This mask is then resized to match the bounding box dimensions.\n",
    "\n",
    "#### Key Insight\n",
    "\n",
    "Mask R-CNN **decouples** class prediction from mask prediction. The mask head predicts a binary mask for *each* class independently, and the classification head decides which class's mask to use. This avoids competition between classes at the mask level and significantly improves quality."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "# Visualize the difference between semantic and instance segmentation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "\n",
    "# Original \"image\"\n",
    "ax = axes[0]\n",
    "ax.set_title('Original Image', fontsize=13, fontweight='bold')\n",
    "ax.add_patch(patches.FancyBboxPatch((0.5, 0.5), 9, 9, boxstyle=\"round,pad=0.1\",\n",
    "             facecolor='lightyellow', edgecolor='gray'))\n",
    "c1 = plt.Circle((3.5, 5), 2, color='gray', alpha=0.6)\n",
    "c2 = plt.Circle((5.5, 5), 2, color='gray', alpha=0.6)\n",
    "c3 = plt.Circle((8, 3), 1.2, color='brown', alpha=0.5)\n",
    "ax.add_patch(c1); ax.add_patch(c2); ax.add_patch(c3)\n",
    "ax.text(3.5, 5, 'Cat', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "ax.text(5.5, 5, 'Cat', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "ax.text(8, 3, 'Dog', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Semantic segmentation\n",
    "ax = axes[1]\n",
    "ax.set_title('Semantic Segmentation', fontsize=13, fontweight='bold')\n",
    "ax.add_patch(patches.FancyBboxPatch((0.5, 0.5), 9, 9, boxstyle=\"round,pad=0.1\",\n",
    "             facecolor='lightyellow', edgecolor='gray'))\n",
    "c1 = plt.Circle((3.5, 5), 2, color='red', alpha=0.4)\n",
    "c2 = plt.Circle((5.5, 5), 2, color='red', alpha=0.4)\n",
    "c3 = plt.Circle((8, 3), 1.2, color='blue', alpha=0.4)\n",
    "ax.add_patch(c1); ax.add_patch(c2); ax.add_patch(c3)\n",
    "ax.text(4.5, 8, 'cat (same color)', fontsize=10, color='red', fontweight='bold', ha='center')\n",
    "ax.text(8, 1.2, 'dog', fontsize=10, color='blue', fontweight='bold', ha='center')\n",
    "\n",
    "# Instance segmentation\n",
    "ax = axes[2]\n",
    "ax.set_title('Instance Segmentation', fontsize=13, fontweight='bold')\n",
    "ax.add_patch(patches.FancyBboxPatch((0.5, 0.5), 9, 9, boxstyle=\"round,pad=0.1\",\n",
    "             facecolor='lightyellow', edgecolor='gray'))\n",
    "c1 = plt.Circle((3.5, 5), 2, color='red', alpha=0.4)\n",
    "c2 = plt.Circle((5.5, 5), 2, color='green', alpha=0.4)\n",
    "c3 = plt.Circle((8, 3), 1.2, color='blue', alpha=0.4)\n",
    "ax.add_patch(c1); ax.add_patch(c2); ax.add_patch(c3)\n",
    "ax.text(3.5, 7.5, 'cat #1', fontsize=10, color='red', fontweight='bold', ha='center')\n",
    "ax.text(5.5, 7.5, 'cat #2', fontsize=10, color='green', fontweight='bold', ha='center')\n",
    "ax.text(8, 1.2, 'dog #1', fontsize=10, color='blue', fontweight='bold', ha='center')\n",
    "\n",
    "plt.suptitle('Semantic vs Instance Segmentation', fontsize=14, fontweight='bold', y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Vision Transformers (ViT)\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "Transformers revolutionized NLP (as we will see in notebooks 16-17). The natural question: can we use the same self-attention mechanism for images?\n",
    "\n",
    "The challenge is scale: a 224x224 image has 50,176 pixels. Self-attention is O(n^2) in sequence length \u2014 applying it to every pixel would be prohibitively expensive.\n",
    "\n",
    "**The ViT solution:** Divide the image into non-overlapping **patches** (e.g., 16x16 pixels), treat each patch as a \"token\" (like a word in NLP), and apply a standard Transformer encoder.\n",
    "\n",
    "#### The ViT Pipeline\n",
    "\n",
    "1. **Split** the image into P x P patches (e.g., 16x16)\n",
    "2. **Flatten** each patch into a vector (16 x 16 x 3 = 768 dimensions)\n",
    "3. **Project** each flattened patch through a linear layer (the \"patch embedding\")\n",
    "4. **Add positional embeddings** (so the model knows where each patch was)\n",
    "5. **Prepend a [CLS] token** (for classification, like BERT)\n",
    "6. **Feed through a standard Transformer encoder** (self-attention + FFN)\n",
    "7. **Use the [CLS] token output** for classification\n",
    "\n",
    "This is remarkably simple \u2014 and it works extremely well, especially with large datasets."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "# Visualize how an image gets split into patches\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "H, W = 224, 224\n",
    "P = 16  # patch size\n",
    "n_patches = (H // P) * (W // P)\n",
    "\n",
    "# Generate a colorful image\n",
    "x = np.linspace(0, 1, W)\n",
    "y = np.linspace(0, 1, H)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "image = np.stack([xx, yy, 1 - xx], axis=-1)  # RGB gradient\n",
    "\n",
    "# 1. Original image\n",
    "ax = axes[0]\n",
    "ax.imshow(image)\n",
    "ax.set_title(f'Original Image ({H}x{W})', fontsize=12, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "# 2. Image with patch grid\n",
    "ax = axes[1]\n",
    "ax.imshow(image)\n",
    "for i in range(0, H + 1, P):\n",
    "    ax.axhline(y=i, color='white', linewidth=0.5, alpha=0.8)\n",
    "    ax.axvline(x=i, color='white', linewidth=0.5, alpha=0.8)\n",
    "ax.set_title(f'Divided into {P}x{P} Patches\\n({n_patches} patches total)', fontsize=12, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "# 3. Patches as a sequence\n",
    "ax = axes[2]\n",
    "n_show = 10\n",
    "patch_sequence = []\n",
    "for i in range(0, min(n_show * P, H), P):\n",
    "    patch = image[0:P, i:i+P]\n",
    "    patch_sequence.append(patch)\n",
    "\n",
    "combined = np.concatenate(patch_sequence, axis=1)\n",
    "ax.imshow(combined)\n",
    "ax.set_title(f'First {n_show} Patches as Sequence', fontsize=12, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "for i in range(n_show):\n",
    "    ax.text(i * P + P/2, P + 3, f'[{i}]', ha='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Vision Transformer: Image -> Patch Sequence', fontsize=14, fontweight='bold', y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image size: {H}x{W}x3 = {H*W*3:,} values\")\n",
    "print(f\"Patch size: {P}x{P}x3 = {P*P*3} values per patch\")\n",
    "print(f\"Number of patches: {n_patches}\")\n",
    "print(f\"Sequence length for Transformer: {n_patches} + 1 (CLS token) = {n_patches + 1}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert an image into a sequence of patch embeddings.\n",
    "\n",
    "    This is the core innovation of ViT: treat an image as a sequence of\n",
    "    flattened patches, just like a sentence is a sequence of words.\n",
    "\n",
    "    Args:\n",
    "        img_size: Input image size (assumes square)\n",
    "        patch_size: Size of each patch (assumes square)\n",
    "        in_channels: Number of input channels (3 for RGB)\n",
    "        embed_dim: Dimension of the embedding space\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Linear projection of flattened patches\n",
    "        # Equivalent to Conv2d with kernel_size=stride=patch_size\n",
    "        self.projection = nn.Conv2d(\n",
    "            in_channels, embed_dim,\n",
    "            kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Project patches: (B, C, H, W) -> (B, embed_dim, H/P, W/P)\n",
    "        x = self.projection(x)\n",
    "        # Flatten spatial dims: (B, embed_dim, H/P, W/P) -> (B, embed_dim, n_patches)\n",
    "        x = x.flatten(2)\n",
    "        # Transpose: (B, embed_dim, n_patches) -> (B, n_patches, embed_dim)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Vision Transformer for classification.\n",
    "\n",
    "    Args:\n",
    "        img_size: Input image size\n",
    "        patch_size: Patch size\n",
    "        in_channels: Input channels\n",
    "        num_classes: Number of output classes\n",
    "        embed_dim: Embedding dimension\n",
    "        num_heads: Number of attention heads\n",
    "        num_layers: Number of transformer layers\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3,\n",
    "                 num_classes=10, embed_dim=768, num_heads=8, num_layers=6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        n_patches = self.patch_embed.n_patches\n",
    "\n",
    "        # Learnable [CLS] token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "\n",
    "        # Positional embeddings (learnable)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, n_patches + 1, embed_dim))\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * 4, dropout=0.1,\n",
    "            activation='gelu', batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Classification head\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)  # (B, n_patches, embed_dim)\n",
    "\n",
    "        # Prepend CLS token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (B, n_patches+1, embed_dim)\n",
    "\n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # Transformer encoder\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Use CLS token for classification\n",
    "        cls_output = self.norm(x[:, 0])\n",
    "        return self.classifier(cls_output)\n",
    "\n",
    "# Test the Vision Transformer\n",
    "torch.manual_seed(42)\n",
    "vit = SimpleViT(img_size=64, patch_size=8, in_channels=3,\n",
    "                num_classes=10, embed_dim=256, num_heads=8, num_layers=4)\n",
    "\n",
    "x = torch.randn(2, 3, 64, 64)\n",
    "output = vit(x)\n",
    "\n",
    "print(f\"Input:  {x.shape} -> (batch, channels, H, W)\")\n",
    "print(f\"Output: {output.shape} -> (batch, num_classes)\")\n",
    "\n",
    "n_patches = (64 // 8) ** 2\n",
    "print(f\"\\nPatch size: 8x8 = 64 pixels per patch\")\n",
    "print(f\"Number of patches: {n_patches}\")\n",
    "print(f\"Sequence length: {n_patches + 1} (patches + CLS)\")\n",
    "\n",
    "total_params = sum(p.numel() for p in vit.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "### Deep Dive: CNN vs Vision Transformer\n",
    "\n",
    "| Aspect | CNN | Vision Transformer (ViT) |\n",
    "|--------|-----|-------------------------|\n",
    "| **Inductive bias** | Strong: locality, translation equivariance | Weak: only sequence order (via positional embeddings) |\n",
    "| **Receptive field** | Grows gradually (layer by layer) | Global from layer 1 (self-attention sees all patches) |\n",
    "| **Data efficiency** | Better with small datasets (biases help) | Needs large datasets (or pretraining) to shine |\n",
    "| **Scalability** | Saturates with more data/compute | Scales well: more data leads to better performance |\n",
    "| **Architecture** | Conv, Pool, Conv, Pool, FC | Patch Embed, Transformer Blocks, CLS head |\n",
    "| **Position awareness** | Built-in (convolution is local) | Must be learned (positional embeddings) |\n",
    "| **Compute** | O(kernel^2 x channels) per position | O(n_patches^2 x embed_dim) for self-attention |\n",
    "\n",
    "#### Key Insight\n",
    "\n",
    "CNNs build in **strong assumptions** about images (local patterns, translation invariance). These assumptions help with limited data but can also limit the model. ViTs make **fewer assumptions**, allowing them to learn more flexible representations \u2014 but they need more data to discover what CNNs get \"for free.\" In practice, the best modern models often combine ideas from both: convolutional patch embeddings, local attention windows, etc.\n",
    "\n",
    "#### Common Misconceptions\n",
    "\n",
    "| Misconception | Reality |\n",
    "|---------------|--------|\n",
    "| \"ViTs completely replaced CNNs\" | Many state-of-the-art models are hybrids (e.g., ConvNeXt, Swin Transformer) |\n",
    "| \"ViTs need no spatial inductive bias\" | Positional embeddings encode position; many variants add locality |\n",
    "| \"Bigger patches are always better\" | Smaller patches capture more detail but increase sequence length quadratically |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. CLIP: Connecting Vision and Language\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "What if, instead of training a vision model on fixed class labels, we trained it to **understand the relationship between images and text**? That is the core idea behind CLIP (Contrastive Language-Image Pre-training) from OpenAI.\n",
    "\n",
    "CLIP trains two separate encoders simultaneously:\n",
    "- An **image encoder** (CNN or ViT) that maps images to a shared embedding space\n",
    "- A **text encoder** (Transformer) that maps text descriptions to the same embedding space\n",
    "\n",
    "The training objective is **contrastive**: in a batch of (image, text) pairs, maximize the similarity between matching pairs and minimize it for non-matching pairs.\n",
    "\n",
    "#### Why CLIP Matters\n",
    "\n",
    "1. **Zero-shot classification**: Classify images into categories never seen during training, just by providing text descriptions\n",
    "2. **Flexible**: No fixed label set \u2014 describe any class in natural language\n",
    "3. **Transfer learning**: CLIP features are remarkably general\n",
    "4. **Multimodal foundation**: CLIP is a building block for text-to-image models (DALL-E, Stable Diffusion), visual question answering, and more"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "# Visualize CLIP's dual-encoder architecture and contrastive learning\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: Architecture\n",
    "ax = axes[0]\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_title('CLIP Architecture', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Image encoder\n",
    "img_box = patches.FancyBboxPatch((0.5, 6.5), 3.5, 2.5, boxstyle=\"round,pad=0.2\",\n",
    "                                  facecolor='lightblue', edgecolor='blue', linewidth=2)\n",
    "ax.add_patch(img_box)\n",
    "ax.text(2.25, 7.75, 'Image\\nEncoder\\n(ViT/CNN)', ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Text encoder\n",
    "txt_box = patches.FancyBboxPatch((6, 6.5), 3.5, 2.5, boxstyle=\"round,pad=0.2\",\n",
    "                                  facecolor='lightyellow', edgecolor='orange', linewidth=2)\n",
    "ax.add_patch(txt_box)\n",
    "ax.text(7.75, 7.75, 'Text\\nEncoder\\n(Transformer)', ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Embeddings\n",
    "ax.annotate('', xy=(2.25, 5.0), xytext=(2.25, 6.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "ax.text(2.25, 5.5, 'Image\\nembedding', ha='center', fontsize=9, color='blue')\n",
    "\n",
    "ax.annotate('', xy=(7.75, 5.0), xytext=(7.75, 6.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='orange', lw=2))\n",
    "ax.text(7.75, 5.5, 'Text\\nembedding', ha='center', fontsize=9, color='orange')\n",
    "\n",
    "# Shared embedding space\n",
    "space_box = patches.FancyBboxPatch((1, 2.5), 8, 2.5, boxstyle=\"round,pad=0.2\",\n",
    "                                    facecolor='lightgreen', edgecolor='green', linewidth=2, alpha=0.3)\n",
    "ax.add_patch(space_box)\n",
    "ax.text(5, 4.5, 'Shared Embedding Space', ha='center', fontsize=12, fontweight='bold', color='green')\n",
    "\n",
    "ax.plot(2.25, 3.5, 'bo', markersize=12)\n",
    "ax.plot(7.75, 3.5, 's', color='orange', markersize=12)\n",
    "ax.annotate('', xy=(7.5, 3.5), xytext=(2.5, 3.5),\n",
    "            arrowprops=dict(arrowstyle='<->', color='green', lw=2))\n",
    "ax.text(5, 3.2, 'cosine similarity', ha='center', fontsize=10, color='green', style='italic')\n",
    "\n",
    "ax.text(2.25, 9.4, 'Image input', ha='center', fontsize=10, style='italic')\n",
    "ax.text(7.75, 9.4, 'Text input', ha='center', fontsize=10, style='italic')\n",
    "\n",
    "# Right: Contrastive learning matrix\n",
    "ax = axes[1]\n",
    "ax.set_title('Contrastive Learning Objective', fontsize=14, fontweight='bold')\n",
    "\n",
    "batch_size = 5\n",
    "np.random.seed(42)\n",
    "sim_matrix = np.random.rand(batch_size, batch_size) * 0.3\n",
    "for i in range(batch_size):\n",
    "    sim_matrix[i, i] = 0.8 + np.random.rand() * 0.2\n",
    "\n",
    "im = ax.imshow(sim_matrix, cmap='RdYlGn', vmin=0, vmax=1)\n",
    "ax.set_xlabel('Text descriptions', fontsize=12)\n",
    "ax.set_ylabel('Images', fontsize=12)\n",
    "ax.set_xticks(range(batch_size))\n",
    "ax.set_yticks(range(batch_size))\n",
    "ax.set_xticklabels([f'Text {i}' for i in range(batch_size)], fontsize=9)\n",
    "ax.set_yticklabels([f'Img {i}' for i in range(batch_size)], fontsize=9)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    for j in range(batch_size):\n",
    "        color = 'white' if sim_matrix[i, j] > 0.5 else 'black'\n",
    "        ax.text(j, i, f'{sim_matrix[i, j]:.2f}', ha='center', va='center',\n",
    "                fontsize=10, color=color, fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='Cosine Similarity')\n",
    "ax.text(2, -1.2, 'Goal: maximize diagonal (matching pairs)\\nminimize off-diagonal (non-matching)',\n",
    "        ha='center', fontsize=10, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "def clip_contrastive_loss(image_embeddings, text_embeddings, temperature=0.07):\n",
    "    \"\"\"\n",
    "    Compute CLIP-style contrastive loss (InfoNCE / NT-Xent).\n",
    "\n",
    "    Args:\n",
    "        image_embeddings: (batch_size, embed_dim) -- L2 normalized\n",
    "        text_embeddings: (batch_size, embed_dim) -- L2 normalized\n",
    "        temperature: scaling factor (lower = sharper distribution)\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    # Normalize embeddings\n",
    "    image_embeddings = F.normalize(image_embeddings, dim=-1)\n",
    "    text_embeddings = F.normalize(text_embeddings, dim=-1)\n",
    "\n",
    "    # Compute similarity matrix: (batch, batch)\n",
    "    logits = image_embeddings @ text_embeddings.T / temperature\n",
    "\n",
    "    # Labels: matching pairs are on the diagonal\n",
    "    batch_size = logits.shape[0]\n",
    "    labels = torch.arange(batch_size)\n",
    "\n",
    "    # Cross-entropy loss in both directions\n",
    "    loss_i2t = F.cross_entropy(logits, labels)       # image -> text\n",
    "    loss_t2i = F.cross_entropy(logits.T, labels)     # text -> image\n",
    "\n",
    "    return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "# Demo: simulate CLIP training on a mini batch\n",
    "torch.manual_seed(42)\n",
    "batch_size = 8\n",
    "embed_dim = 128\n",
    "\n",
    "image_emb = torch.randn(batch_size, embed_dim)\n",
    "text_emb = torch.randn(batch_size, embed_dim)\n",
    "\n",
    "# Make matching pairs slightly similar (simulate partially trained model)\n",
    "text_emb = text_emb + 0.3 * image_emb\n",
    "\n",
    "loss = clip_contrastive_loss(image_emb, text_emb, temperature=0.07)\n",
    "print(f\"CLIP contrastive loss: {loss.item():.4f}\")\n",
    "print(f\"Random baseline (batch={batch_size}): {np.log(batch_size):.4f}\")\n",
    "print(f\"  (Loss should be lower than random baseline since pairs are correlated)\")\n",
    "\n",
    "print(\"\\nEffect of temperature:\")\n",
    "for temp in [0.01, 0.07, 0.1, 0.5, 1.0]:\n",
    "    l = clip_contrastive_loss(image_emb, text_emb, temperature=temp)\n",
    "    print(f\"  temperature={temp:.2f} -> loss={l.item():.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "### Zero-Shot Classification with CLIP\n",
    "\n",
    "One of CLIP's most powerful capabilities is classifying images into categories **it has never been explicitly trained on**. Here is how:\n",
    "\n",
    "1. Given candidate class names: [\"cat\", \"dog\", \"car\", \"airplane\"]\n",
    "2. Create text prompts: [\"a photo of a cat\", \"a photo of a dog\", ...]\n",
    "3. Encode all text prompts with the text encoder\n",
    "4. Encode the input image with the image encoder\n",
    "5. Compute cosine similarity between the image embedding and each text embedding\n",
    "6. The class with the highest similarity wins\n",
    "\n",
    "No retraining needed \u2014 just change the text descriptions to classify into any set of categories.\n",
    "\n",
    "**What this means:** CLIP turns classification from a fixed-label problem into a **language-guided** problem. Instead of retraining a model for every new task, you simply describe the task in words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Transfer Learning for Vision\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "Training a large vision model from scratch requires enormous datasets and compute. **Transfer learning** lets us reuse knowledge from a model pre-trained on a large dataset (like ImageNet with 1.2M images) and adapt it to our specific task.\n",
    "\n",
    "There are two main approaches:\n",
    "\n",
    "| Approach | What You Do | When to Use |\n",
    "|----------|-------------|-------------|\n",
    "| **Feature extraction** | Freeze the pretrained backbone, only train a new classification head | Small dataset, similar domain to pretraining |\n",
    "| **Fine-tuning** | Unfreeze some/all of the pretrained backbone, train end-to-end with a small learning rate | Medium-large dataset, different domain |\n",
    "\n",
    "#### Practical Guidelines\n",
    "\n",
    "1. **Start with feature extraction** \u2014 it is fast and often surprisingly effective\n",
    "2. **If accuracy is insufficient, try fine-tuning** the last few layers first\n",
    "3. **Use a small learning rate for fine-tuning** (10-100x smaller than for the new head)\n",
    "4. **Apply data augmentation** \u2014 especially important with small datasets\n",
    "5. **Freeze batch normalization** layers if fine-tuning with very small batches"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "# Quick transfer learning demo with torchvision models\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load a pretrained ResNet-18\n",
    "resnet = models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "print(\"ResNet-18 final layers:\")\n",
    "print(f\"  Average pool: {resnet.avgpool}\")\n",
    "print(f\"  FC layer: {resnet.fc}\")\n",
    "print(f\"  Original output: {resnet.fc.out_features} classes (ImageNet)\")\n",
    "\n",
    "# Approach 1: Feature extraction (freeze everything, replace head)\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_features = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_features, 10)  # New head for 10 classes\n",
    "\n",
    "# Count trainable vs frozen parameters\n",
    "trainable = sum(p.numel() for p in resnet.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in resnet.parameters())\n",
    "frozen = total - trainable\n",
    "\n",
    "print(f\"\\nAfter modification for 10-class task:\")\n",
    "print(f\"  Total parameters:     {total:>10,}\")\n",
    "print(f\"  Frozen parameters:    {frozen:>10,} ({100*frozen/total:.1f}%)\")\n",
    "print(f\"  Trainable parameters: {trainable:>10,} ({100*trainable/total:.1f}%)\")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "with torch.no_grad():\n",
    "    output = resnet(x)\n",
    "print(f\"\\nInput:  {x.shape}\")\n",
    "print(f\"Output: {output.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "# Approach 2: Fine-tuning (unfreeze last layers)\n",
    "resnet_ft = models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# First freeze everything\n",
    "for param in resnet_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last residual block (layer4) and the new head\n",
    "for param in resnet_ft.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Replace the classification head\n",
    "num_features = resnet_ft.fc.in_features\n",
    "resnet_ft.fc = nn.Linear(num_features, 10)\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_ft = sum(p.numel() for p in resnet_ft.parameters() if p.requires_grad)\n",
    "total_ft = sum(p.numel() for p in resnet_ft.parameters())\n",
    "\n",
    "print(\"Fine-tuning approach (unfreeze layer4 + new head):\")\n",
    "print(f\"  Total parameters:     {total_ft:>10,}\")\n",
    "print(f\"  Trainable parameters: {trainable_ft:>10,} ({100*trainable_ft/total_ft:.1f}%)\")\n",
    "\n",
    "print(\"\\nTypical optimizer setup for fine-tuning:\")\n",
    "print(\"  optimizer = torch.optim.Adam([\")\n",
    "print(\"      {'params': model.layer4.parameters(), 'lr': 1e-4},  # Pretrained: small LR\")\n",
    "print(\"      {'params': model.fc.parameters(), 'lr': 1e-3},      # New head: larger LR\")\n",
    "print(\"  ])\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"{'Approach':<25} {'Trainable Params':>20} {'% of Total':>12}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Feature extraction':<25} {trainable:>20,} {100*trainable/total:>11.1f}%\")\n",
    "print(f\"{'Fine-tune layer4':<25} {trainable_ft:>20,} {100*trainable_ft/total_ft:>11.1f}%\")\n",
    "print(f\"{'Full fine-tuning':<25} {total_ft:>20,} {'100.0':>11}%\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "# Visualize what gets frozen vs fine-tuned\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 5)\n",
    "ax.axis('off')\n",
    "\n",
    "layers = [\n",
    "    ('Conv1', 1.0, 'lightblue'),\n",
    "    ('Layer1', 2.5, 'lightblue'),\n",
    "    ('Layer2', 4.5, 'lightblue'),\n",
    "    ('Layer3', 6.5, 'lightblue'),\n",
    "    ('Layer4', 8.5, 'lightyellow'),\n",
    "    ('FC Head', 11.0, 'lightgreen'),\n",
    "]\n",
    "\n",
    "ax.text(0.5, 4.5, 'Feature\\nExtraction:', fontsize=10, fontweight='bold', va='top')\n",
    "ax.text(0.5, 2.5, 'Fine-\\nTuning:', fontsize=10, fontweight='bold', va='top')\n",
    "\n",
    "for name, x_pos, color in layers:\n",
    "    w = 1.5 if name != 'FC Head' else 2.0\n",
    "\n",
    "    # Feature extraction row (top)\n",
    "    c = 'lightgreen' if name == 'FC Head' else 'lightcoral'\n",
    "    label = 'Train' if name == 'FC Head' else 'Frozen'\n",
    "    rect = patches.FancyBboxPatch((x_pos, 3.5), w, 1.0, boxstyle=\"round,pad=0.1\",\n",
    "                                   facecolor=c, edgecolor='black', alpha=0.7)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x_pos + w/2, 4.0, f'{name}\\n({label})', ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "    # Fine-tuning row (bottom)\n",
    "    if name in ['Layer4', 'FC Head']:\n",
    "        c = 'lightgreen'\n",
    "        label = 'Train'\n",
    "    else:\n",
    "        c = 'lightcoral'\n",
    "        label = 'Frozen'\n",
    "    rect = patches.FancyBboxPatch((x_pos, 1.5), w, 1.0, boxstyle=\"round,pad=0.1\",\n",
    "                                   facecolor=c, edgecolor='black', alpha=0.7)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x_pos + w/2, 2.0, f'{name}\\n({label})', ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "# Arrows\n",
    "for i in range(len(layers)-1):\n",
    "    x_from = layers[i][1] + (1.5 if layers[i][0] != 'FC Head' else 2.0)\n",
    "    x_to = layers[i+1][1]\n",
    "    for y in [4.0, 2.0]:\n",
    "        ax.annotate('', xy=(x_to, y), xytext=(x_from, y),\n",
    "                    arrowprops=dict(arrowstyle='->', color='gray', lw=1.5))\n",
    "\n",
    "# Legend\n",
    "ax.add_patch(patches.Rectangle((1, 0.3), 0.5, 0.3, facecolor='lightcoral', edgecolor='black', alpha=0.7))\n",
    "ax.text(1.7, 0.45, 'Frozen', fontsize=9, va='center')\n",
    "ax.add_patch(patches.Rectangle((3.5, 0.3), 0.5, 0.3, facecolor='lightgreen', edgecolor='black', alpha=0.7))\n",
    "ax.text(4.2, 0.45, 'Trainable', fontsize=9, va='center')\n",
    "\n",
    "ax.set_title('Transfer Learning: What to Freeze vs Train', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Batch IoU Computation\n",
    "\n",
    "Implement a function that computes IoU between every pair of boxes in two sets \u2014 a common operation in detection evaluation. Your function should use vectorized NumPy operations (no loops) for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "# EXERCISE 1: Batch IoU computation\n",
    "def batch_iou(boxes_a, boxes_b):\n",
    "    \"\"\"\n",
    "    Compute pairwise IoU between two sets of boxes.\n",
    "\n",
    "    Args:\n",
    "        boxes_a: numpy array of shape (N, 4), each row is [x1, y1, x2, y2]\n",
    "        boxes_b: numpy array of shape (M, 4), each row is [x1, y1, x2, y2]\n",
    "\n",
    "    Returns:\n",
    "        IoU matrix of shape (N, M) where iou[i, j] = IoU(boxes_a[i], boxes_b[j])\n",
    "\n",
    "    Hint: Use broadcasting! Expand boxes_a to (N, 1, 4) and boxes_b to (1, M, 4)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this!\n",
    "    # Step 1: Compute intersection coordinates using np.maximum and np.minimum\n",
    "    # Step 2: Compute intersection areas (clamp to 0 for non-overlapping)\n",
    "    # Step 3: Compute areas of each box\n",
    "    # Step 4: Compute union = area_a + area_b - intersection\n",
    "    # Step 5: Return intersection / union\n",
    "\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "boxes_a = np.array([\n",
    "    [0, 0, 3, 3],\n",
    "    [1, 1, 4, 4],\n",
    "])\n",
    "boxes_b = np.array([\n",
    "    [0, 0, 3, 3],\n",
    "    [2, 2, 5, 5],\n",
    "    [6, 6, 9, 9],\n",
    "])\n",
    "\n",
    "result = batch_iou(boxes_a, boxes_b)\n",
    "\n",
    "expected = np.array([\n",
    "    [1.0, compute_iou([0,0,3,3], [2,2,5,5]), 0.0],\n",
    "    [compute_iou([1,1,4,4], [0,0,3,3]), compute_iou([1,1,4,4], [2,2,5,5]), 0.0],\n",
    "])\n",
    "\n",
    "if result is not None:\n",
    "    print(f\"Your result:\\n{result}\")\n",
    "    print(f\"\\nExpected:\\n{expected}\")\n",
    "    print(f\"\\nCorrect: {np.allclose(result, expected)}\")\n",
    "else:\n",
    "    print(\"Expected result:\")\n",
    "    print(expected)\n",
    "    print(\"\\nImplement batch_iou to verify!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Patch Embedding from Scratch (No Conv2d)\n",
    "\n",
    "The `PatchEmbedding` class above uses `nn.Conv2d` as a shortcut. Implement it from scratch using only reshaping and a linear layer \u2014 this makes the process more explicit and reinforces understanding."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "# EXERCISE 2: Patch embedding without Conv2d\n",
    "class ManualPatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement patch embedding using explicit reshape + linear projection.\n",
    "    No Conv2d allowed!\n",
    "\n",
    "    Steps:\n",
    "    1. Reshape image from (B, C, H, W) to (B, n_patches, patch_size*patch_size*C)\n",
    "    2. Apply a linear layer to project each flattened patch to embed_dim\n",
    "\n",
    "    Hint: Use tensor.reshape() and careful dimension ordering.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=64, patch_size=8, in_channels=3, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        patch_dim = patch_size * patch_size * in_channels\n",
    "\n",
    "        # TODO: Create a linear layer that maps patch_dim -> embed_dim\n",
    "        self.projection = None  # Replace with nn.Linear(...)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, channels, height, width)\n",
    "        Returns:\n",
    "            (batch, n_patches, embed_dim)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        P = self.patch_size\n",
    "\n",
    "        # TODO: Reshape x into patches\n",
    "        # Step 1: Reshape to (B, C, H//P, P, W//P, P)\n",
    "        # Step 2: Permute to (B, H//P, W//P, C, P, P)\n",
    "        # Step 3: Reshape to (B, n_patches, C*P*P)\n",
    "        # Step 4: Apply self.projection\n",
    "\n",
    "        pass\n",
    "\n",
    "# Test\n",
    "torch.manual_seed(42)\n",
    "# Uncomment after implementing:\n",
    "# manual_pe = ManualPatchEmbedding(img_size=64, patch_size=8, in_channels=3, embed_dim=256)\n",
    "# x = torch.randn(2, 3, 64, 64)\n",
    "# output = manual_pe(x)\n",
    "# print(f\"Input:  {x.shape}\")\n",
    "# print(f\"Output: {output.shape}\")\n",
    "# print(f\"Expected: torch.Size([2, 64, 256])\")\n",
    "# print(f\"Correct shape: {output.shape == torch.Size([2, 64, 256])}\")\n",
    "\n",
    "print(\"Uncomment the test code above after implementing ManualPatchEmbedding!\")\n",
    "print(\"Expected output shape: (2, 64, 256)\")\n",
    "print(\"  2 = batch size\")\n",
    "print(\"  64 = (64/8)^2 = 64 patches\")\n",
    "print(\"  256 = embed_dim\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "### Exercise 3: Add Dice Loss for Segmentation\n",
    "\n",
    "Cross-entropy is the standard loss for classification, but for segmentation it can be problematic when classes are imbalanced (e.g., a small tumor in a large medical image). **Dice loss** directly optimizes the Dice coefficient, which measures overlap between prediction and ground truth.\n",
    "\n",
    "$$\\text{Dice} = \\frac{2 |A \\cap B|}{|A| + |B|}$$\n",
    "\n",
    "Implement Dice loss for binary segmentation."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "# EXERCISE 3: Dice loss for segmentation\n",
    "def dice_loss(pred, target, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Compute Dice loss for binary segmentation.\n",
    "\n",
    "    Args:\n",
    "        pred: (batch, 1, H, W) -- raw logits (apply sigmoid first!)\n",
    "        target: (batch, 1, H, W) -- binary ground truth (0 or 1)\n",
    "        smooth: smoothing factor to avoid division by zero\n",
    "\n",
    "    Returns:\n",
    "        1 - Dice coefficient (so that minimizing loss = maximizing Dice)\n",
    "\n",
    "    Steps:\n",
    "        1. Apply sigmoid to pred to get probabilities\n",
    "        2. Flatten both pred and target to (batch, H*W)\n",
    "        3. Compute intersection: sum(pred * target) per sample\n",
    "        4. Compute Dice = (2 * intersection + smooth) / (sum(pred) + sum(target) + smooth)\n",
    "        5. Return 1 - mean(Dice)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this!\n",
    "\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Perfect prediction\n",
    "target = torch.zeros(1, 1, 8, 8)\n",
    "target[0, 0, 2:6, 2:6] = 1.0  # 4x4 square of 1s\n",
    "\n",
    "# Near-perfect prediction (high logits where target=1)\n",
    "pred_good = torch.zeros(1, 1, 8, 8) - 5.0  # low logits everywhere\n",
    "pred_good[0, 0, 2:6, 2:6] = 5.0  # high logits where target=1\n",
    "\n",
    "# Bad prediction (random)\n",
    "pred_bad = torch.randn(1, 1, 8, 8)\n",
    "\n",
    "if dice_loss(pred_good, target) is not None:\n",
    "    loss_good = dice_loss(pred_good, target)\n",
    "    loss_bad = dice_loss(pred_bad, target)\n",
    "    print(f\"Dice loss (good prediction): {loss_good.item():.4f} (should be close to 0)\")\n",
    "    print(f\"Dice loss (bad prediction):  {loss_bad.item():.4f} (should be higher)\")\n",
    "    print(f\"\\nGood < Bad: {loss_good.item() < loss_bad.item()}\")\n",
    "else:\n",
    "    print(\"Implement dice_loss to verify!\")\n",
    "    print(\"Expected: good prediction loss close to 0.0, bad prediction loss > 0.3\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-43",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Object Detection** decomposes into anchor boxes, IoU scoring, class prediction, and NMS post-processing\n",
    "- **IoU (Intersection over Union)** is the standard metric for measuring bounding box overlap \u2014 values range from 0 (no overlap) to 1 (perfect overlap)\n",
    "- **Non-Maximum Suppression (NMS)** removes duplicate detections by suppressing lower-confidence boxes that overlap with higher-confidence ones\n",
    "- **YOLO** frames detection as a single-pass regression problem, predicting boxes and classes from a grid of cells simultaneously\n",
    "- **Semantic Segmentation** assigns a class to every pixel using encoder-decoder architectures like U-Net\n",
    "- **U-Net** uses skip connections to combine deep semantic features with fine spatial details for precise segmentation\n",
    "- **Transposed convolutions** are the learnable upsampling operation that lets decoders increase spatial resolution\n",
    "- **Instance Segmentation** (Mask R-CNN) combines detection with per-instance mask prediction\n",
    "- **Vision Transformers (ViT)** treat images as sequences of patches and apply Transformer self-attention\n",
    "- **Patch embeddings** convert image patches into token embeddings, analogous to word embeddings in NLP\n",
    "- **CLIP** connects vision and language through contrastive learning in a shared embedding space\n",
    "- **Transfer learning** reuses pretrained features, either by freezing the backbone (feature extraction) or fine-tuning with a small learning rate\n",
    "\n",
    "### Connection to Deep Learning\n",
    "\n",
    "| Concept | Where It Appears | Why It Matters |\n",
    "|---------|-----------------|----------------|\n",
    "| IoU and NMS | Every detection model | Standard evaluation and post-processing |\n",
    "| Anchor boxes | YOLO, SSD, Faster R-CNN | Efficient candidate generation |\n",
    "| Encoder-decoder | U-Net, segmentation, autoencoders | Compress then reconstruct spatial information |\n",
    "| Skip connections | U-Net, ResNet, DenseNet | Preserve information across depth |\n",
    "| Patch embeddings | ViT, DINO, MAE | Bridge images and Transformers |\n",
    "| Contrastive learning | CLIP, SimCLR, DINO | Learn representations without explicit labels |\n",
    "| Transfer learning | Nearly every practical vision system | Train with limited data by reusing pretrained features |\n",
    "\n",
    "### Checklist\n",
    "\n",
    "- [ ] I can compute IoU between two bounding boxes by hand\n",
    "- [ ] I understand how anchor boxes tile an image with candidate regions\n",
    "- [ ] I can explain NMS and why it is needed\n",
    "- [ ] I understand YOLO's grid-based, single-pass detection approach\n",
    "- [ ] I can describe the U-Net encoder-decoder architecture and the role of skip connections\n",
    "- [ ] I know the difference between semantic, instance, and panoptic segmentation\n",
    "- [ ] I can explain how ViT converts an image into a sequence of patch tokens\n",
    "- [ ] I understand CLIP's contrastive learning objective\n",
    "- [ ] I know when to use feature extraction vs fine-tuning for transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In **Notebook 15: Recurrent Neural Networks (RNNs)**, we shift from spatial data (images) to **sequential data** (text, time series, audio). We will explore how RNNs maintain a hidden state that carries information forward through a sequence, why vanilla RNNs struggle with long-range dependencies, and how LSTM and GRU architectures solve the vanishing gradient problem.\n",
    "\n",
    "The ideas from this notebook \u2014 particularly attention mechanisms (ViT) and encoder-decoder architectures (U-Net) \u2014 will reappear in new forms as we move into sequence modeling and eventually the full Transformer architecture."
   ]
  }
 ]
}