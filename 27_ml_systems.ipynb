{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Part 8.3: ML Systems & Experiment Tracking — The Formula 1 Edition\n\nMachine learning in production is 90% engineering. The model is just one component in a complex system of data pipelines, experiment tracking, model registries, and deployment infrastructure. Without proper systems, ML teams drown in untracked experiments, irreproducible results, and models that silently degrade.\n\n**F1 analogy:** The car on the grid is what everyone sees, but it's the *factory* that wins championships. Behind every F1 car is a training infrastructure (the team's simulation farm running thousands of virtual races), data pipelines (continuous telemetry ingestion from car to cloud), feature stores (pre-computed track characteristics, driver profiles, tire degradation curves), and model serving (deploying the strategy model to the pit wall for live races). An F1 team without proper systems is like an ML team without experiment tracking — they might get lucky once, but they can't systematically improve.\n\nThis notebook builds the core infrastructure patterns that every ML team needs — from the ground up.\n\n## Learning Objectives\n\n- [ ] Build an experiment tracking system from scratch (MLflow-style)\n- [ ] Implement systematic hyperparameter search (grid, random, Bayesian)\n- [ ] Understand and build a model registry with versioning\n- [ ] Create reproducible training pipelines with configuration management\n- [ ] Implement a feature store for consistent feature engineering\n- [ ] Build artifact logging for datasets, models, and metrics\n- [ ] Design experiment comparison and visualization dashboards"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom collections import defaultdict, OrderedDict\nimport json\nimport hashlib\nimport time\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nnp.random.seed(42)\ntorch.manual_seed(42)\n\nprint(\"Part 8.3: ML Systems & Experiment Tracking — The Formula 1 Edition\")\nprint(\"=\" * 65)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "---\n\n## 1. The ML Infrastructure Stack\n\nA production ML system has many components beyond the model:\n\n| Layer | Component | Purpose | F1 Parallel |\n|-------|-----------|--------|-------------|\n| **Data** | Feature store, data versioning | Consistent, reproducible features | Telemetry ingestion pipeline — car sensors to cloud, consistent lap-by-lap data |\n| **Training** | Experiment tracker, config management | Track what was tried, reproduce results | The simulation farm log — which virtual setups were tested, which produced fastest laps |\n| **Model** | Model registry, artifact store | Version and stage models | Car spec management — track which aero package version is on which chassis |\n| **Serving** | Model server, API gateway | Serve predictions | Deploying the strategy model to the pit wall for live race decisions |\n| **Monitoring** | Metrics, alerts, drift detection | Keep things working | Real-time telemetry health checks — detecting sensor failures, model drift mid-race |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the ML infrastructure stack\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_title('ML Infrastructure Stack', fontsize=15, fontweight='bold')\n",
    "\n",
    "layers = [\n",
    "    (7, 1.2, 12, 1.5, 'Data Layer', '#3498db',\n",
    "     'Feature Store  |  Data Versioning  |  Data Validation  |  Pipelines'),\n",
    "    (7, 3.0, 12, 1.5, 'Training Layer', '#2ecc71',\n",
    "     'Experiment Tracking  |  Hyperparameter Search  |  Config Management'),\n",
    "    (7, 4.8, 12, 1.5, 'Model Layer', '#f39c12',\n",
    "     'Model Registry  |  Artifact Store  |  Model Versioning  |  Staging'),\n",
    "    (7, 6.6, 12, 1.5, 'Serving Layer', '#e74c3c',\n",
    "     'Model Server  |  API Gateway  |  Caching  |  Load Balancing'),\n",
    "    (7, 8.4, 12, 1.5, 'Monitoring Layer', '#9b59b6',\n",
    "     'Metrics  |  Alerts  |  Drift Detection  |  Logging'),\n",
    "]\n",
    "\n",
    "for x, y, w, h, label, color, desc in layers:\n",
    "    box = mpatches.FancyBboxPatch((x - w/2, y - h/2), w, h,\n",
    "                                   boxstyle=\"round,pad=0.15\", facecolor=color,\n",
    "                                   edgecolor='black', linewidth=2, alpha=0.85)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, y + 0.2, label, ha='center', va='center', fontsize=12,\n",
    "            fontweight='bold', color='white')\n",
    "    ax.text(x, y - 0.3, desc, ha='center', va='center', fontsize=8, color='white')\n",
    "\n",
    "# Arrows connecting layers\n",
    "for i in range(len(layers) - 1):\n",
    "    y_from = layers[i][1] + layers[i][3]/2\n",
    "    y_to = layers[i+1][1] - layers[i+1][3]/2\n",
    "    ax.annotate('', xy=(7, y_to), xytext=(7, y_from),\n",
    "               arrowprops=dict(arrowstyle='->', lw=1.5, color='gray'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": "---\n\n## 2. Experiment Tracking\n\nWithout experiment tracking, ML development is chaos: \"Which hyperparameters gave the best result?\" \"Can I reproduce last week's experiment?\" \"What changed between v1 and v2?\"\n\nAn experiment tracker logs:\n- **Parameters**: hyperparameters, config, code version\n- **Metrics**: loss, accuracy, custom metrics (per step and final)\n- **Artifacts**: model weights, plots, data samples\n- **Metadata**: timestamps, hardware, git commit\n\n**F1 analogy:** Experiment tracking is the simulation farm's logbook. Without it, an F1 team would be running thousands of CFD and simulator sessions without recording which wing angle, ride height, and spring stiffness produced each result. Imagine the chief engineer asking \"What setup gave us the best Sector 2 time at Barcelona last Tuesday?\" and nobody can answer because nobody wrote it down. That's ML without experiment tracking. Every F1 team meticulously logs every simulation run's parameters (setup), metrics (lap time, tire wear), and artifacts (telemetry traces) — and ML teams must do the same."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentTracker:\n",
    "    \"\"\"MLflow-style experiment tracking from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self, project_name='default'):\n",
    "        self.project_name = project_name\n",
    "        self.runs = {}  # run_id -> run data\n",
    "        self.active_run = None\n",
    "    \n",
    "    def start_run(self, run_name=None, tags=None):\n",
    "        \"\"\"Start a new experiment run.\"\"\"\n",
    "        run_id = hashlib.md5(f\"{time.time()}{run_name}\".encode()).hexdigest()[:8]\n",
    "        \n",
    "        self.runs[run_id] = {\n",
    "            'run_id': run_id,\n",
    "            'name': run_name or f'run_{run_id}',\n",
    "            'status': 'running',\n",
    "            'start_time': time.time(),\n",
    "            'params': {},\n",
    "            'metrics': {},         # metric_name -> final value\n",
    "            'metric_history': {},   # metric_name -> [(step, value), ...]\n",
    "            'artifacts': {},\n",
    "            'tags': tags or {},\n",
    "        }\n",
    "        self.active_run = run_id\n",
    "        return run_id\n",
    "    \n",
    "    def log_param(self, key, value):\n",
    "        \"\"\"Log a hyperparameter.\"\"\"\n",
    "        self.runs[self.active_run]['params'][key] = value\n",
    "    \n",
    "    def log_params(self, params_dict):\n",
    "        \"\"\"Log multiple parameters.\"\"\"\n",
    "        self.runs[self.active_run]['params'].update(params_dict)\n",
    "    \n",
    "    def log_metric(self, key, value, step=None):\n",
    "        \"\"\"Log a metric value.\"\"\"\n",
    "        run = self.runs[self.active_run]\n",
    "        run['metrics'][key] = value\n",
    "        \n",
    "        if key not in run['metric_history']:\n",
    "            run['metric_history'][key] = []\n",
    "        run['metric_history'][key].append((step, value))\n",
    "    \n",
    "    def log_artifact(self, name, data):\n",
    "        \"\"\"Log an artifact (model, plot, etc.).\"\"\"\n",
    "        self.runs[self.active_run]['artifacts'][name] = {\n",
    "            'type': type(data).__name__,\n",
    "            'size': len(str(data)),\n",
    "            'logged_at': time.time()\n",
    "        }\n",
    "    \n",
    "    def end_run(self, status='completed'):\n",
    "        \"\"\"End the current run.\"\"\"\n",
    "        run = self.runs[self.active_run]\n",
    "        run['status'] = status\n",
    "        run['end_time'] = time.time()\n",
    "        run['duration'] = run['end_time'] - run['start_time']\n",
    "        self.active_run = None\n",
    "    \n",
    "    def get_best_run(self, metric, maximize=True):\n",
    "        \"\"\"Find the run with the best value for a metric.\"\"\"\n",
    "        best_run = None\n",
    "        best_val = float('-inf') if maximize else float('inf')\n",
    "        \n",
    "        for run_id, run in self.runs.items():\n",
    "            if metric in run['metrics']:\n",
    "                val = run['metrics'][metric]\n",
    "                if (maximize and val > best_val) or (not maximize and val < best_val):\n",
    "                    best_val = val\n",
    "                    best_run = run\n",
    "        \n",
    "        return best_run\n",
    "    \n",
    "    def compare_runs(self, metric_names=None):\n",
    "        \"\"\"Compare all runs in a table format.\"\"\"\n",
    "        rows = []\n",
    "        for run_id, run in self.runs.items():\n",
    "            row = {'run_id': run_id, 'name': run['name'], 'status': run['status']}\n",
    "            row.update(run['params'])\n",
    "            \n",
    "            if metric_names:\n",
    "                for m in metric_names:\n",
    "                    row[m] = run['metrics'].get(m, None)\n",
    "            else:\n",
    "                row.update(run['metrics'])\n",
    "            \n",
    "            rows.append(row)\n",
    "        return rows\n",
    "\n",
    "\n",
    "# Simulate an ML experiment workflow\n",
    "tracker = ExperimentTracker('classification_experiments')\n",
    "\n",
    "# Run multiple experiments with different hyperparameters\n",
    "configs = [\n",
    "    {'lr': 0.001, 'hidden_size': 64, 'dropout': 0.1, 'optimizer': 'adam'},\n",
    "    {'lr': 0.01, 'hidden_size': 128, 'dropout': 0.2, 'optimizer': 'adam'},\n",
    "    {'lr': 0.001, 'hidden_size': 128, 'dropout': 0.1, 'optimizer': 'sgd'},\n",
    "    {'lr': 0.005, 'hidden_size': 256, 'dropout': 0.3, 'optimizer': 'adam'},\n",
    "    {'lr': 0.001, 'hidden_size': 64, 'dropout': 0.0, 'optimizer': 'adam'},\n",
    "]\n",
    "\n",
    "np.random.seed(42)\n",
    "for i, config in enumerate(configs):\n",
    "    run_id = tracker.start_run(run_name=f'exp_{i+1}', tags={'version': 'v1'})\n",
    "    tracker.log_params(config)\n",
    "    \n",
    "    # Simulate training with metrics\n",
    "    base_acc = 0.7 + np.random.normal(0, 0.05)\n",
    "    for epoch in range(20):\n",
    "        # Simulated improvement curve\n",
    "        acc = base_acc + 0.015 * epoch * (1 + config['hidden_size'] / 256) + np.random.normal(0, 0.01)\n",
    "        loss = 2.0 - acc + np.random.normal(0, 0.05)\n",
    "        tracker.log_metric('accuracy', min(0.99, acc), step=epoch)\n",
    "        tracker.log_metric('loss', max(0.1, loss), step=epoch)\n",
    "    \n",
    "    tracker.log_artifact('model_weights', f'model_{i}.pt')\n",
    "    tracker.end_run()\n",
    "\n",
    "# Show results\n",
    "print(\"Experiment Comparison\\n\")\n",
    "comparison = tracker.compare_runs(['accuracy', 'loss'])\n",
    "print(f\"{'Name':>8} {'LR':>8} {'Hidden':>8} {'Dropout':>8} {'Opt':>6} {'Accuracy':>10} {'Loss':>8}\")\n",
    "print(\"-\" * 68)\n",
    "for row in comparison:\n",
    "    print(f\"{row['name']:>8} {row.get('lr', ''):>8} {row.get('hidden_size', ''):>8} \"\n",
    "          f\"{row.get('dropout', ''):>8} {row.get('optimizer', ''):>6} \"\n",
    "          f\"{row.get('accuracy', 0):>10.4f} {row.get('loss', 0):>8.4f}\")\n",
    "\n",
    "best = tracker.get_best_run('accuracy', maximize=True)\n",
    "print(f\"\\nBest run: {best['name']} (accuracy={best['metrics']['accuracy']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize experiment results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training curves\n",
    "ax = axes[0]\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.9, len(tracker.runs)))\n",
    "for (run_id, run), color in zip(tracker.runs.items(), colors):\n",
    "    history = run['metric_history']['accuracy']\n",
    "    steps = [h[0] for h in history]\n",
    "    values = [h[1] for h in history]\n",
    "    ax.plot(steps, values, linewidth=2, color=color, label=run['name'], alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=11)\n",
    "ax.set_ylabel('Accuracy', fontsize=11)\n",
    "ax.set_title('Training Curves Across Experiments', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=9, loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hyperparameter importance (parallel coordinates style)\n",
    "ax = axes[1]\n",
    "final_accs = [run['metrics']['accuracy'] for run in tracker.runs.values()]\n",
    "lrs = [run['params']['lr'] for run in tracker.runs.values()]\n",
    "hiddens = [run['params']['hidden_size'] for run in tracker.runs.values()]\n",
    "dropouts = [run['params']['dropout'] for run in tracker.runs.values()]\n",
    "\n",
    "scatter = ax.scatter(hiddens, final_accs, c=lrs, cmap='coolwarm', s=100,\n",
    "                     edgecolors='black', linewidth=1)\n",
    "plt.colorbar(scatter, ax=ax, label='Learning Rate')\n",
    "\n",
    "for h, acc, d in zip(hiddens, final_accs, dropouts):\n",
    "    ax.annotate(f'd={d}', (h, acc), textcoords='offset points',\n",
    "               xytext=(5, 5), fontsize=8)\n",
    "\n",
    "ax.set_xlabel('Hidden Size', fontsize=11)\n",
    "ax.set_ylabel('Final Accuracy', fontsize=11)\n",
    "ax.set_title('Hyperparameter Space Exploration', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "---\n\n## 3. Hyperparameter Search\n\nManually tuning hyperparameters doesn't scale. Systematic search methods find better configurations more efficiently.\n\n| Method | Strategy | Efficiency | Best For | F1 Parallel |\n|--------|----------|-----------|----------|-------------|\n| **Grid Search** | Try all combinations | Low (exponential) | Few params, small ranges | Testing every combination of wing angle and ride height on a fixed grid — thorough but slow |\n| **Random Search** | Sample randomly | Medium | Medium search spaces | Random sampling of the setup space — surprisingly effective at finding good configurations |\n| **Bayesian** | Model the objective, choose wisely | High | Expensive evaluations | Using past simulation results to intelligently choose the next setup to test — each CFD run costs compute, so choose wisely |\n\n**F1 analogy:** An F1 team can't test every possible car setup — there are too many combinations of wing angles, spring rates, damper settings, and differential maps. Grid search would try every combination on a fixed grid (wing angle 5, 10, 15 degrees x ride height 20, 25, 30mm) — exhaustive but expensive. Random search samples randomly and often finds good setups faster. Bayesian optimization is the smartest: it looks at all previous simulator runs, builds a model of the performance landscape, and picks the next setup most likely to be either very fast or very informative. This is exactly how F1 teams' optimization software works."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparameterSearch:\n",
    "    \"\"\"Hyperparameter search strategies.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def grid_search(param_grid):\n",
    "        \"\"\"Generate all combinations from parameter grid.\n",
    "        \n",
    "        param_grid: {'param_name': [value1, value2, ...]}\n",
    "        \"\"\"\n",
    "        keys = list(param_grid.keys())\n",
    "        values = list(param_grid.values())\n",
    "        \n",
    "        configs = []\n",
    "        def _recurse(idx, current):\n",
    "            if idx == len(keys):\n",
    "                configs.append(dict(current))\n",
    "                return\n",
    "            for val in values[idx]:\n",
    "                current[keys[idx]] = val\n",
    "                _recurse(idx + 1, current)\n",
    "        \n",
    "        _recurse(0, {})\n",
    "        return configs\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_search(param_distributions, n_trials=20):\n",
    "        \"\"\"Sample random configurations.\n",
    "        \n",
    "        param_distributions: {'param': {'type': 'uniform/loguniform/choice', ...}}\n",
    "        \"\"\"\n",
    "        configs = []\n",
    "        for _ in range(n_trials):\n",
    "            config = {}\n",
    "            for param, dist in param_distributions.items():\n",
    "                if dist['type'] == 'uniform':\n",
    "                    config[param] = np.random.uniform(dist['low'], dist['high'])\n",
    "                elif dist['type'] == 'loguniform':\n",
    "                    log_val = np.random.uniform(np.log(dist['low']), np.log(dist['high']))\n",
    "                    config[param] = np.exp(log_val)\n",
    "                elif dist['type'] == 'choice':\n",
    "                    config[param] = np.random.choice(dist['options'])\n",
    "                elif dist['type'] == 'int':\n",
    "                    config[param] = int(np.random.randint(dist['low'], dist['high'] + 1))\n",
    "            configs.append(config)\n",
    "        return configs\n",
    "    \n",
    "    @staticmethod\n",
    "    def bayesian_search(param_distributions, objective_fn, n_trials=20, n_initial=5):\n",
    "        \"\"\"Simple Bayesian optimization using a surrogate model.\n",
    "        \n",
    "        Uses random forest as surrogate (simplified GP-like behavior).\n",
    "        \"\"\"\n",
    "        # Initial random exploration\n",
    "        configs = HyperparameterSearch.random_search(param_distributions, n_initial)\n",
    "        results = [(c, objective_fn(c)) for c in configs]\n",
    "        \n",
    "        for trial in range(n_initial, n_trials):\n",
    "            # Generate candidates\n",
    "            candidates = HyperparameterSearch.random_search(param_distributions, 100)\n",
    "            \n",
    "            # Score candidates using a simple surrogate:\n",
    "            # Expected improvement heuristic based on similarity to best configs\n",
    "            best_score = max(r[1] for r in results)\n",
    "            \n",
    "            best_candidate = None\n",
    "            best_acquisition = -float('inf')\n",
    "            \n",
    "            for cand in candidates:\n",
    "                # Predict score as weighted average of nearby evaluated points\n",
    "                predicted = 0\n",
    "                total_weight = 0\n",
    "                for prev_config, prev_score in results:\n",
    "                    # Distance between configs\n",
    "                    dist = sum((cand.get(k, 0) - prev_config.get(k, 0))**2\n",
    "                              for k in cand if isinstance(cand[k], (int, float)))\n",
    "                    weight = 1 / (dist + 0.01)\n",
    "                    predicted += weight * prev_score\n",
    "                    total_weight += weight\n",
    "                \n",
    "                predicted /= total_weight\n",
    "                # Exploration bonus: prefer unexplored regions\n",
    "                min_dist = min(sum((cand.get(k, 0) - pc.get(k, 0))**2\n",
    "                                  for k in cand if isinstance(cand[k], (int, float)))\n",
    "                              for pc, _ in results)\n",
    "                acquisition = predicted + 0.1 * math.sqrt(min_dist)\n",
    "                \n",
    "                if acquisition > best_acquisition:\n",
    "                    best_acquisition = acquisition\n",
    "                    best_candidate = cand\n",
    "            \n",
    "            score = objective_fn(best_candidate)\n",
    "            results.append((best_candidate, score))\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Define a simulated objective function\n",
    "def simulated_objective(config):\n",
    "    \"\"\"Simulated model training that returns accuracy.\"\"\"\n",
    "    lr = config.get('lr', 0.001)\n",
    "    hidden = config.get('hidden_size', 64)\n",
    "    dropout = config.get('dropout', 0.1)\n",
    "    \n",
    "    # Simulated accuracy surface (peaked around lr=0.003, hidden=128, dropout=0.15)\n",
    "    lr_score = -50 * (np.log10(lr) - np.log10(0.003))**2\n",
    "    hidden_score = -0.0001 * (hidden - 128)**2\n",
    "    dropout_score = -5 * (dropout - 0.15)**2\n",
    "    \n",
    "    accuracy = 0.85 + lr_score + hidden_score + dropout_score + np.random.normal(0, 0.01)\n",
    "    return min(0.99, max(0.5, accuracy))\n",
    "\n",
    "\n",
    "# Compare search strategies\n",
    "param_distributions = {\n",
    "    'lr': {'type': 'loguniform', 'low': 1e-4, 'high': 1e-1},\n",
    "    'hidden_size': {'type': 'int', 'low': 32, 'high': 512},\n",
    "    'dropout': {'type': 'uniform', 'low': 0.0, 'high': 0.5},\n",
    "}\n",
    "\n",
    "search = HyperparameterSearch()\n",
    "\n",
    "# Grid search (limited to specific values)\n",
    "grid = search.grid_search({\n",
    "    'lr': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'hidden_size': [64, 128, 256],\n",
    "    'dropout': [0.0, 0.1, 0.2, 0.3]\n",
    "})\n",
    "grid_results = [(c, simulated_objective(c)) for c in grid[:20]]  # Limit to 20\n",
    "\n",
    "# Random search\n",
    "random_configs = search.random_search(param_distributions, n_trials=20)\n",
    "random_results = [(c, simulated_objective(c)) for c in random_configs]\n",
    "\n",
    "# Bayesian search\n",
    "bayesian_results = search.bayesian_search(param_distributions, simulated_objective, n_trials=20)\n",
    "\n",
    "print(\"Search Strategy Comparison (20 trials each)\\n\")\n",
    "for name, results in [('Grid', grid_results), ('Random', random_results), ('Bayesian', bayesian_results)]:\n",
    "    scores = [r[1] for r in results]\n",
    "    best = max(results, key=lambda x: x[1])\n",
    "    print(f\"  {name:>10}: best={max(scores):.4f}, mean={np.mean(scores):.4f}, \"\n",
    "          f\"std={np.std(scores):.4f}\")\n",
    "    print(f\"             best config: lr={best[0]['lr']:.4f}, \"\n",
    "          f\"hidden={best[0].get('hidden_size', '?')}, dropout={best[0]['dropout']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize search strategies\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for ax, (name, results), color in zip(axes,\n",
    "    [('Grid Search', grid_results), ('Random Search', random_results), ('Bayesian', bayesian_results)],\n",
    "    ['#e74c3c', '#3498db', '#2ecc71']):\n",
    "    \n",
    "    # Best score found over time\n",
    "    scores = [r[1] for r in results]\n",
    "    best_so_far = [max(scores[:i+1]) for i in range(len(scores))]\n",
    "    \n",
    "    ax.plot(range(1, len(scores)+1), scores, 'o', alpha=0.4, color=color, markersize=6)\n",
    "    ax.plot(range(1, len(best_so_far)+1), best_so_far, '-', color=color, linewidth=2,\n",
    "           label='Best so far')\n",
    "    ax.set_xlabel('Trial', fontsize=11)\n",
    "    ax.set_ylabel('Accuracy', fontsize=11)\n",
    "    ax.set_title(f'{name}\\n(best={max(scores):.4f})', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0.6, 0.95)\n",
    "\n",
    "plt.suptitle('Hyperparameter Search Convergence', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": "---\n\n## 4. Model Registry\n\nA **model registry** is a central store for versioned models with staging capabilities:\n- **Version tracking**: Every model gets a version number\n- **Staging**: Models move through stages (development -> staging -> production)\n- **Metadata**: Each version stores metrics, params, and lineage\n- **Rollback**: Easy to revert to a previous version\n\n**F1 analogy:** The model registry is like the FIA's car specification management system. Every car has a precisely documented spec — aero package v3.2, floor v2.1, suspension v4.0. When a team brings an upgrade to a race weekend, it goes through stages: development (designed in CFD), staging (tested in the wind tunnel and simulator), production (fitted to the race car). If the upgrade doesn't work on track, they can roll back to the previous spec — but only because every version was meticulously documented. Without a registry, a team would lose track of which front wing is on which car and which version actually produced results."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRegistry:\n",
    "    \"\"\"Model registry with versioning and staging.\"\"\"\n",
    "    \n",
    "    STAGES = ['development', 'staging', 'production', 'archived']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}  # model_name -> {versions: [...], current_production: version}\n",
    "    \n",
    "    def register_model(self, name, version_info):\n",
    "        \"\"\"Register a new model version.\n",
    "        \n",
    "        version_info: dict with metrics, params, artifacts, etc.\n",
    "        \"\"\"\n",
    "        if name not in self.models:\n",
    "            self.models[name] = {'versions': [], 'production_version': None}\n",
    "        \n",
    "        version = len(self.models[name]['versions']) + 1\n",
    "        entry = {\n",
    "            'version': version,\n",
    "            'stage': 'development',\n",
    "            'registered_at': time.time(),\n",
    "            **version_info\n",
    "        }\n",
    "        self.models[name]['versions'].append(entry)\n",
    "        return version\n",
    "    \n",
    "    def promote(self, name, version, to_stage):\n",
    "        \"\"\"Promote a model version to a new stage.\"\"\"\n",
    "        if to_stage not in self.STAGES:\n",
    "            raise ValueError(f\"Invalid stage: {to_stage}\")\n",
    "        \n",
    "        model = self.models[name]\n",
    "        entry = model['versions'][version - 1]\n",
    "        \n",
    "        # If promoting to production, demote current production\n",
    "        if to_stage == 'production' and model['production_version'] is not None:\n",
    "            old_prod = model['versions'][model['production_version'] - 1]\n",
    "            old_prod['stage'] = 'archived'\n",
    "        \n",
    "        entry['stage'] = to_stage\n",
    "        if to_stage == 'production':\n",
    "            model['production_version'] = version\n",
    "        \n",
    "        return entry\n",
    "    \n",
    "    def get_production_model(self, name):\n",
    "        \"\"\"Get the current production model version.\"\"\"\n",
    "        model = self.models.get(name)\n",
    "        if model and model['production_version']:\n",
    "            return model['versions'][model['production_version'] - 1]\n",
    "        return None\n",
    "    \n",
    "    def list_versions(self, name):\n",
    "        \"\"\"List all versions of a model.\"\"\"\n",
    "        return self.models.get(name, {}).get('versions', [])\n",
    "    \n",
    "    def compare_versions(self, name, v1, v2):\n",
    "        \"\"\"Compare two model versions.\"\"\"\n",
    "        versions = self.models[name]['versions']\n",
    "        entry1 = versions[v1 - 1]\n",
    "        entry2 = versions[v2 - 1]\n",
    "        \n",
    "        comparison = {'version': (v1, v2)}\n",
    "        # Compare metrics\n",
    "        all_metrics = set(entry1.get('metrics', {}).keys()) | set(entry2.get('metrics', {}).keys())\n",
    "        for metric in all_metrics:\n",
    "            val1 = entry1.get('metrics', {}).get(metric)\n",
    "            val2 = entry2.get('metrics', {}).get(metric)\n",
    "            comparison[metric] = {\n",
    "                'v1': val1, 'v2': val2,\n",
    "                'delta': (val2 - val1) if val1 is not None and val2 is not None else None\n",
    "            }\n",
    "        return comparison\n",
    "\n",
    "\n",
    "# Simulate model lifecycle\n",
    "registry = ModelRegistry()\n",
    "\n",
    "# Register several model versions\n",
    "model_versions = [\n",
    "    {'metrics': {'accuracy': 0.82, 'latency_ms': 45, 'f1': 0.80}, 'params': {'lr': 0.01, 'epochs': 10}},\n",
    "    {'metrics': {'accuracy': 0.87, 'latency_ms': 42, 'f1': 0.85}, 'params': {'lr': 0.005, 'epochs': 20}},\n",
    "    {'metrics': {'accuracy': 0.91, 'latency_ms': 50, 'f1': 0.89}, 'params': {'lr': 0.003, 'epochs': 30}},\n",
    "    {'metrics': {'accuracy': 0.89, 'latency_ms': 38, 'f1': 0.87}, 'params': {'lr': 0.003, 'epochs': 30, 'distilled': True}},\n",
    "    {'metrics': {'accuracy': 0.93, 'latency_ms': 48, 'f1': 0.91}, 'params': {'lr': 0.002, 'epochs': 50}},\n",
    "]\n",
    "\n",
    "for info in model_versions:\n",
    "    v = registry.register_model('text_classifier', info)\n",
    "\n",
    "# Promote versions through stages\n",
    "registry.promote('text_classifier', 2, 'staging')\n",
    "registry.promote('text_classifier', 3, 'production')\n",
    "registry.promote('text_classifier', 5, 'staging')  # Testing v5\n",
    "\n",
    "print(\"Model Registry: text_classifier\\n\")\n",
    "for v in registry.list_versions('text_classifier'):\n",
    "    stage_color = {'development': '', 'staging': '*', 'production': '>>>', 'archived': '(old)'}\n",
    "    marker = stage_color.get(v['stage'], '')\n",
    "    print(f\"  v{v['version']} [{v['stage']:>12}] {marker:>4} \"\n",
    "          f\"acc={v['metrics']['accuracy']:.2f}, f1={v['metrics']['f1']:.2f}, \"\n",
    "          f\"latency={v['metrics']['latency_ms']}ms\")\n",
    "\n",
    "prod = registry.get_production_model('text_classifier')\n",
    "print(f\"\\nCurrent production: v{prod['version']} (accuracy={prod['metrics']['accuracy']:.2f})\")\n",
    "\n",
    "# Compare v3 (current prod) vs v5 (staging)\n",
    "comp = registry.compare_versions('text_classifier', 3, 5)\n",
    "print(f\"\\nv3 vs v5 comparison:\")\n",
    "for metric in ['accuracy', 'f1', 'latency_ms']:\n",
    "    if metric in comp:\n",
    "        d = comp[metric]\n",
    "        print(f\"  {metric}: {d['v1']} -> {d['v2']} (delta={d['delta']:+.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": "---\n\n## 5. Feature Store\n\nA **feature store** ensures that features computed for training are identical to those computed at serving time. It solves the \"training-serving skew\" problem.\n\n### Key Properties\n- **Consistent**: Same feature logic for training and serving\n- **Reusable**: Features computed once, used by many models\n- **Versioned**: Feature definitions tracked over time\n- **Fast**: Precomputed features for low-latency serving\n\n**F1 analogy:** The feature store is the pre-computed track characteristics database every F1 team maintains. Before arriving at a circuit, the team pre-computes features for each track: corner radii, elevation changes, surface grip levels, historical weather patterns, overtaking difficulty scores. These features are used by the setup model (to choose car configuration), the strategy model (to predict pit stop windows), and the tire model (to predict degradation) — all consuming the *same* pre-computed track features. Without a feature store, each model might compute \"track grip\" differently, leading to contradictory recommendations. The same features used in pre-race simulation must be identical to those used on the live pit wall — that's the training-serving consistency guarantee."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureStore:\n",
    "    \"\"\"Simple feature store from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_definitions = {}  # name -> {fn, description, version}\n",
    "        self.offline_store = {}        # (entity_id, feature_name) -> value\n",
    "        self.feature_sets = {}         # feature_set_name -> [feature_names]\n",
    "    \n",
    "    def register_feature(self, name, compute_fn, description='', version=1):\n",
    "        \"\"\"Register a feature definition.\"\"\"\n",
    "        self.feature_definitions[name] = {\n",
    "            'fn': compute_fn,\n",
    "            'description': description,\n",
    "            'version': version,\n",
    "            'created_at': time.time()\n",
    "        }\n",
    "    \n",
    "    def register_feature_set(self, name, feature_names):\n",
    "        \"\"\"Group features into a feature set for a model.\"\"\"\n",
    "        self.feature_sets[name] = feature_names\n",
    "    \n",
    "    def compute_and_store(self, entity_id, raw_data):\n",
    "        \"\"\"Compute all features for an entity and store them.\"\"\"\n",
    "        features = {}\n",
    "        for name, defn in self.feature_definitions.items():\n",
    "            value = defn['fn'](raw_data)\n",
    "            self.offline_store[(entity_id, name)] = value\n",
    "            features[name] = value\n",
    "        return features\n",
    "    \n",
    "    def get_features(self, entity_id, feature_set=None):\n",
    "        \"\"\"Retrieve features for an entity.\"\"\"\n",
    "        if feature_set:\n",
    "            names = self.feature_sets.get(feature_set, [])\n",
    "        else:\n",
    "            names = list(self.feature_definitions.keys())\n",
    "        \n",
    "        return {name: self.offline_store.get((entity_id, name)) for name in names}\n",
    "    \n",
    "    def get_training_data(self, entity_ids, feature_set):\n",
    "        \"\"\"Get feature matrix for training.\"\"\"\n",
    "        names = self.feature_sets.get(feature_set, [])\n",
    "        rows = []\n",
    "        for eid in entity_ids:\n",
    "            row = [self.offline_store.get((eid, name), 0) for name in names]\n",
    "            rows.append(row)\n",
    "        return np.array(rows), names\n",
    "\n",
    "\n",
    "# Define features for a user behavior model\n",
    "store = FeatureStore()\n",
    "\n",
    "# Register feature definitions\n",
    "store.register_feature('total_purchases', lambda d: d.get('purchases', 0),\n",
    "                       'Total number of purchases')\n",
    "store.register_feature('avg_order_value', \n",
    "                       lambda d: d.get('total_spend', 0) / max(d.get('purchases', 1), 1),\n",
    "                       'Average order value')\n",
    "store.register_feature('days_since_last', lambda d: d.get('days_since_last', 999),\n",
    "                       'Days since last activity')\n",
    "store.register_feature('session_count', lambda d: d.get('sessions', 0),\n",
    "                       'Number of sessions in last 30 days')\n",
    "store.register_feature('is_premium', lambda d: 1 if d.get('plan') == 'premium' else 0,\n",
    "                       'Premium user flag')\n",
    "\n",
    "store.register_feature_set('churn_model', \n",
    "                           ['total_purchases', 'avg_order_value', 'days_since_last',\n",
    "                            'session_count', 'is_premium'])\n",
    "\n",
    "# Simulate user data\n",
    "np.random.seed(42)\n",
    "users = []\n",
    "for i in range(100):\n",
    "    user_data = {\n",
    "        'purchases': np.random.randint(0, 50),\n",
    "        'total_spend': np.random.uniform(0, 5000),\n",
    "        'days_since_last': np.random.randint(0, 365),\n",
    "        'sessions': np.random.randint(0, 100),\n",
    "        'plan': np.random.choice(['free', 'premium'], p=[0.7, 0.3]),\n",
    "    }\n",
    "    store.compute_and_store(f'user_{i}', user_data)\n",
    "    users.append(f'user_{i}')\n",
    "\n",
    "# Get training data\n",
    "X, feature_names = store.get_training_data(users, 'churn_model')\n",
    "\n",
    "print(\"Feature Store Summary\\n\")\n",
    "print(f\"  Registered features: {len(store.feature_definitions)}\")\n",
    "print(f\"  Feature sets: {list(store.feature_sets.keys())}\")\n",
    "print(f\"  Entities stored: {len(users)}\")\n",
    "print(f\"  Training matrix: {X.shape}\")\n",
    "print(f\"\\n  Features in 'churn_model':\")\n",
    "for name in feature_names:\n",
    "    vals = X[:, feature_names.index(name)]\n",
    "    print(f\"    {name:>20}: mean={np.mean(vals):.2f}, std={np.std(vals):.2f}\")\n",
    "\n",
    "# Show consistency: same features at training and serving time\n",
    "print(f\"\\n  Serving example (user_0):\")\n",
    "serving_features = store.get_features('user_0', 'churn_model')\n",
    "for k, v in serving_features.items():\n",
    "    print(f\"    {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": "---\n\n## 6. Configuration Management & Reproducibility\n\nReproducibility requires capturing **everything** about an experiment: code, data, config, environment, and random seeds.\n\n**F1 analogy:** Reproducibility in F1 is life-or-death serious. If a car passes crash testing at a specific spec, every component must be traceable to that configuration. The team must be able to answer: \"What exact carbon fiber layup, what cure temperature, what adhesive batch was used on the monocoque that passed FIA test #3847?\" Configuration management captures all of this. In ML terms: every experiment must record not just the hyperparameters but the exact code version, data snapshot, random seed, and environment — so that any result can be precisely reproduced. A frozen config, like a homologated car spec, prevents accidental changes mid-experiment."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration management for reproducible experiments.\"\"\"\n",
    "    \n",
    "    def __init__(self, config_dict=None):\n",
    "        self._config = config_dict or {}\n",
    "        self._frozen = False\n",
    "    \n",
    "    def set(self, key, value):\n",
    "        if self._frozen:\n",
    "            raise RuntimeError(\"Config is frozen after experiment starts\")\n",
    "        self._config[key] = value\n",
    "    \n",
    "    def get(self, key, default=None):\n",
    "        return self._config.get(key, default)\n",
    "    \n",
    "    def freeze(self):\n",
    "        \"\"\"Freeze config to prevent accidental changes during training.\"\"\"\n",
    "        self._frozen = True\n",
    "    \n",
    "    def hash(self):\n",
    "        \"\"\"Unique hash for this configuration (for caching/dedup).\"\"\"\n",
    "        config_str = json.dumps(self._config, sort_keys=True, default=str)\n",
    "        return hashlib.sha256(config_str.encode()).hexdigest()[:12]\n",
    "    \n",
    "    def diff(self, other):\n",
    "        \"\"\"Compare with another config.\"\"\"\n",
    "        all_keys = set(self._config.keys()) | set(other._config.keys())\n",
    "        diffs = {}\n",
    "        for key in sorted(all_keys):\n",
    "            v1 = self._config.get(key)\n",
    "            v2 = other._config.get(key)\n",
    "            if v1 != v2:\n",
    "                diffs[key] = {'old': v1, 'new': v2}\n",
    "        return diffs\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return dict(self._config)\n",
    "\n",
    "\n",
    "class ReproducibleExperiment:\n",
    "    \"\"\"Run a fully reproducible experiment.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, tracker):\n",
    "        self.config = config\n",
    "        self.tracker = tracker\n",
    "    \n",
    "    def run(self, train_fn):\n",
    "        \"\"\"Execute training with full tracking.\"\"\"\n",
    "        # Set seeds for reproducibility\n",
    "        seed = self.config.get('seed', 42)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        # Freeze config\n",
    "        self.config.freeze()\n",
    "        config_hash = self.config.hash()\n",
    "        \n",
    "        # Start tracked run\n",
    "        run_id = self.tracker.start_run(\n",
    "            run_name=f'run_{config_hash}',\n",
    "            tags={'config_hash': config_hash}\n",
    "        )\n",
    "        self.tracker.log_params(self.config.to_dict())\n",
    "        \n",
    "        # Execute training\n",
    "        try:\n",
    "            result = train_fn(self.config, self.tracker)\n",
    "            self.tracker.end_run('completed')\n",
    "        except Exception as e:\n",
    "            self.tracker.log_metric('error', 1)\n",
    "            self.tracker.end_run('failed')\n",
    "            raise\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "# Example: reproducible experiment\n",
    "config_v1 = ExperimentConfig({\n",
    "    'model': 'mlp', 'hidden_size': 128, 'lr': 0.003,\n",
    "    'dropout': 0.1, 'epochs': 20, 'batch_size': 32, 'seed': 42\n",
    "})\n",
    "\n",
    "config_v2 = ExperimentConfig({\n",
    "    'model': 'mlp', 'hidden_size': 256, 'lr': 0.001,\n",
    "    'dropout': 0.2, 'epochs': 30, 'batch_size': 32, 'seed': 42\n",
    "})\n",
    "\n",
    "print(\"Configuration Management\\n\")\n",
    "print(f\"  Config v1 hash: {config_v1.hash()}\")\n",
    "print(f\"  Config v2 hash: {config_v2.hash()}\")\n",
    "print(f\"\\n  Diff v1 -> v2:\")\n",
    "for key, diff in config_v1.diff(config_v2).items():\n",
    "    print(f\"    {key}: {diff['old']} -> {diff['new']}\")\n",
    "\n",
    "# Run reproducible experiments\n",
    "tracker2 = ExperimentTracker('reproducibility_demo')\n",
    "\n",
    "def simple_train(config, tracker):\n",
    "    \"\"\"Simulated training function.\"\"\"\n",
    "    acc = 0.7\n",
    "    for epoch in range(config.get('epochs')):\n",
    "        acc += config.get('lr') * (1 + config.get('hidden_size') / 500)\n",
    "        acc = min(0.99, acc + np.random.normal(0, 0.005))\n",
    "        tracker.log_metric('accuracy', acc, step=epoch)\n",
    "    return {'final_accuracy': acc}\n",
    "\n",
    "for cfg in [config_v1, config_v2]:\n",
    "    # Need fresh config since freeze is one-way\n",
    "    fresh_cfg = ExperimentConfig(cfg.to_dict())\n",
    "    exp = ReproducibleExperiment(fresh_cfg, tracker2)\n",
    "    result = exp.run(simple_train)\n",
    "    print(f\"\\n  {fresh_cfg.hash()}: accuracy = {result['final_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": "---\n\n## 7. Putting It All Together: ML Pipeline\n\nLet's build a complete ML pipeline that ties all the pieces together: feature store -> experiment tracking -> model registry.\n\n**F1 analogy:** This is the complete race weekend pipeline: pull pre-computed track features from the feature store (track characteristics database) -> run the simulation with tracking (simulator sessions logged with full parameters and metrics) -> register the best setup in the model registry (commit the final car spec for the race). Every step is tracked, versioned, and reproducible — so when the team arrives at the same track next year, they know exactly what worked."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPipeline:\n",
    "    \"\"\"End-to-end ML pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_store, tracker, registry):\n",
    "        self.feature_store = feature_store\n",
    "        self.tracker = tracker\n",
    "        self.registry = registry\n",
    "    \n",
    "    def train_and_register(self, config, entity_ids, feature_set_name, labels):\n",
    "        \"\"\"Full pipeline: get features -> train -> track -> register.\"\"\"\n",
    "        # Step 1: Get features from feature store\n",
    "        X, feature_names = self.feature_store.get_training_data(\n",
    "            entity_ids, feature_set_name\n",
    "        )\n",
    "        y = np.array(labels)\n",
    "        \n",
    "        # Step 2: Start experiment tracking\n",
    "        run_id = self.tracker.start_run(run_name=f\"pipeline_{config.get('model_name', 'model')}\")\n",
    "        self.tracker.log_params(config)\n",
    "        self.tracker.log_param('n_features', len(feature_names))\n",
    "        self.tracker.log_param('n_samples', len(entity_ids))\n",
    "        \n",
    "        # Step 3: Train (simulated)\n",
    "        # Simple logistic regression equivalent\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "        \n",
    "        # Normalize features\n",
    "        X_mean = X_tensor.mean(dim=0)\n",
    "        X_std = X_tensor.std(dim=0).clamp(min=1e-8)\n",
    "        X_norm = (X_tensor - X_mean) / X_std\n",
    "        \n",
    "        model = nn.Linear(X_norm.shape[1], 2)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.get('lr', 0.01))\n",
    "        \n",
    "        for epoch in range(config.get('epochs', 50)):\n",
    "            logits = model(X_norm)\n",
    "            loss = F.cross_entropy(logits, y_tensor)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc = (logits.argmax(dim=1) == y_tensor).float().mean().item()\n",
    "            self.tracker.log_metric('loss', loss.item(), step=epoch)\n",
    "            self.tracker.log_metric('accuracy', acc, step=epoch)\n",
    "        \n",
    "        # Step 4: Final metrics\n",
    "        final_acc = acc\n",
    "        self.tracker.log_metric('final_accuracy', final_acc)\n",
    "        self.tracker.end_run()\n",
    "        \n",
    "        # Step 5: Register model\n",
    "        version = self.registry.register_model(config.get('model_name', 'model'), {\n",
    "            'metrics': {'accuracy': final_acc, 'loss': loss.item()},\n",
    "            'params': config,\n",
    "            'feature_set': feature_set_name,\n",
    "            'n_samples': len(entity_ids),\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            'run_id': run_id,\n",
    "            'version': version,\n",
    "            'accuracy': final_acc,\n",
    "        }\n",
    "\n",
    "\n",
    "# Run the full pipeline\n",
    "pipeline = MLPipeline(store, ExperimentTracker('pipeline'), ModelRegistry())\n",
    "\n",
    "# Generate labels (simulated churn: 1 = churned)\n",
    "np.random.seed(42)\n",
    "labels = (np.random.random(100) > 0.7).astype(int)\n",
    "\n",
    "configs = [\n",
    "    {'model_name': 'churn_predictor', 'lr': 0.01, 'epochs': 50},\n",
    "    {'model_name': 'churn_predictor', 'lr': 0.005, 'epochs': 100},\n",
    "    {'model_name': 'churn_predictor', 'lr': 0.001, 'epochs': 100},\n",
    "]\n",
    "\n",
    "print(\"ML Pipeline Results\\n\")\n",
    "for config in configs:\n",
    "    result = pipeline.train_and_register(config, users, 'churn_model', labels)\n",
    "    print(f\"  v{result['version']}: lr={config['lr']}, epochs={config['epochs']}, \"\n",
    "          f\"accuracy={result['accuracy']:.4f}\")\n",
    "\n",
    "# Show registry\n",
    "print(\"\\nModel Registry:\")\n",
    "for v in pipeline.registry.list_versions('churn_predictor'):\n",
    "    print(f\"  v{v['version']}: accuracy={v['metrics']['accuracy']:.4f}, \"\n",
    "          f\"stage={v['stage']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pipeline results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Version accuracy progression\n",
    "ax = axes[0]\n",
    "versions = pipeline.registry.list_versions('churn_predictor')\n",
    "v_nums = [v['version'] for v in versions]\n",
    "v_accs = [v['metrics']['accuracy'] for v in versions]\n",
    "v_lrs = [v['params']['lr'] for v in versions]\n",
    "\n",
    "bars = ax.bar(v_nums, v_accs, color=['#3498db', '#2ecc71', '#f39c12'],\n",
    "             edgecolor='black', alpha=0.8)\n",
    "for bar, acc, lr in zip(bars, v_accs, v_lrs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "            f'{acc:.3f}\\nlr={lr}', ha='center', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Model Version', fontsize=11)\n",
    "ax.set_ylabel('Accuracy', fontsize=11)\n",
    "ax.set_title('Model Accuracy by Version', fontsize=13, fontweight='bold')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Pipeline overview diagram\n",
    "ax = axes[1]\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.axis('off')\n",
    "ax.set_title('ML Pipeline Flow', fontsize=13, fontweight='bold')\n",
    "\n",
    "steps = [\n",
    "    (1.5, 3, 'Feature\\nStore', '#3498db'),\n",
    "    (4, 3, 'Train +\\nTrack', '#2ecc71'),\n",
    "    (6.5, 3, 'Model\\nRegistry', '#f39c12'),\n",
    "    (9, 3, 'Deploy', '#e74c3c'),\n",
    "]\n",
    "\n",
    "for x, y, label, color in steps:\n",
    "    box = mpatches.FancyBboxPatch((x - 1, y - 0.8), 2, 1.6, boxstyle=\"round,pad=0.15\",\n",
    "                                   facecolor=color, edgecolor='black', linewidth=2, alpha=0.85)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, y, label, ha='center', va='center', fontsize=10,\n",
    "            fontweight='bold', color='white')\n",
    "\n",
    "for i in range(len(steps) - 1):\n",
    "    ax.annotate('', xy=(steps[i+1][0] - 1, steps[i+1][1]),\n",
    "               xytext=(steps[i][0] + 1, steps[i][1]),\n",
    "               arrowprops=dict(arrowstyle='->', lw=2, color='gray'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": "---\n\n## Exercises\n\n### Exercise 1: Early Stopping with Tracking\n\nAdd early stopping to the ExperimentTracker. Implement a `should_stop(metric, patience)` method that returns True if the metric hasn't improved for `patience` epochs. Use it in a training loop.\n\n**F1 scenario:** During a simulator session, the engineer monitors lap time improvement. If the optimization algorithm hasn't found a faster lap in the last 10 iterations (patience=10), it should stop and move on to testing a different parameter — no point burning CFD hours on a dead end. Implement this early stopping logic."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Hint: Track the best value and number of epochs since improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": "### Exercise 2: A/B Test Integration\n\nExtend the ModelRegistry with an `ab_test(name, v1, v2, traffic_split)` method that sets up an A/B test between two model versions. Simulate serving requests and collecting metrics for both versions.\n\n**F1 scenario:** The team has two strategy models: v3 (current production, conservative) and v5 (new, more aggressive). Set up an A/B test where 70% of simulated race scenarios use v3 and 30% use v5. After enough \"races,\" compare which strategy model produces better outcomes. This is how a team would validate a new pit wall model before trusting it with a real Grand Prix."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": "### Exercise 3: Feature Store with Time Travel\n\nAdd \"point-in-time\" feature retrieval to the FeatureStore. Store features with timestamps, and add a `get_features_at(entity_id, timestamp)` method that returns the feature values as they were at a given point in time. This prevents data leakage in time-series ML.\n\n**F1 scenario:** When building a model to predict tire degradation at lap 30, you must only use features available *before* lap 30 — not the updated track temperature from lap 35 (that's the future!). Implement time-travel feature retrieval so the feature store returns track conditions, tire state, and fuel load exactly as they were at any given lap number. This prevents the classic \"leaking future data into training\" mistake."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": "---\n\n## Summary\n\n### Key Concepts\n\n| Concept | What It Does | F1 Parallel |\n|---------|-------------|-------------|\n| **Experiment tracking** | Logs params, metrics, and artifacts for every run | The simulation farm logbook — every virtual test session recorded with full setup details and results |\n| **Hyperparameter search** | Systematically explores the configuration space | Setup optimization — grid, random, or Bayesian search over wing angles, ride heights, and spring rates |\n| **Model registry** | Versions models and manages staging (dev -> staging -> production -> archived) | Car spec management — tracking which aero package version is homologated and race-ready |\n| **Feature store** | Ensures training-serving consistency and enables feature reuse | Pre-computed track characteristics database shared by setup, strategy, and tire models |\n| **Configuration management** | Freezing and hashing enables reproducibility | FIA homologation — every component traceable to its exact specification |\n| **ML pipelines** | Ties all components into a systematic workflow | The complete race weekend pipeline from pre-race simulation to pit wall deployment |\n\n### The Systems Mindset\n\nThe difference between ML as a hobby and ML as engineering is infrastructure — just as the difference between a hobby racer and an F1 team is the factory. Without experiment tracking, you can't learn from past experiments — like running simulator sessions without saving the results. Without a model registry, you can't safely deploy — like fitting an untested front wing without knowing its spec. Without a feature store, your training and serving features will diverge — like using different track data in simulation vs. on the pit wall. These systems are what make ML teams productive and their models reliable, just as the factory infrastructure is what makes F1 teams consistently competitive."
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": "---\n\n## Next Steps\n\nWe've covered the full ML lifecycle from training infrastructure to deployment — the entire factory behind the race team. The final notebook in our curriculum explores an exciting frontier: **Notebook 28: Multimodal AI** — models that understand images, text, and their connections. In F1 terms, this is about building systems that can simultaneously process onboard camera footage, team radio messages, and telemetry numbers to build a unified understanding of the race."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}