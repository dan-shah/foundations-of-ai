{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Part 9.3: Multimodal AI\n\nHumans understand the world through multiple senses simultaneously — we see an image, read its caption, and connect the two effortlessly. **Multimodal AI** gives models this same ability: understanding and connecting information across different modalities (text, images, audio, video).\n\n**F1 analogy:** An F1 race engineer doesn't rely on a single data source. They're simultaneously watching the onboard camera (vision — track conditions, rival positions, debris), listening to team radio (text/audio — driver feedback, race director messages), and monitoring telemetry dashboards (structured data — speed traces, tire temperatures, fuel load). The magic happens when these streams *fuse*: seeing a rival's car snap sideways on the onboard camera, hearing the driver shout \"He's lost it at Turn 4!\" on the radio, and seeing the corresponding spike in the yellow flag telemetry — all connecting into a unified understanding: \"Safety car incoming, pit NOW.\" That fusion of camera + telemetry + radio into unified race understanding is exactly what multimodal AI achieves.\n\nThe breakthrough came with **CLIP** (Contrastive Language-Image Pre-training), which showed that training on image-text pairs from the internet produces remarkably powerful representations. Think of CLIP as learning to match race images with their telemetry descriptions. This notebook builds multimodal systems from the ground up.\n\n## Learning Objectives\n\n- [ ] Understand the multimodal alignment problem and why it's hard\n- [ ] Implement contrastive learning for aligning modalities\n- [ ] Build a CLIP-style model from scratch (image encoder + text encoder)\n- [ ] Implement zero-shot classification using aligned embeddings\n- [ ] Build cross-modal retrieval (text -> image, image -> text)\n- [ ] Understand vision-language models and how they extend LLMs\n- [ ] Explore multimodal applications: captioning, VQA, generation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom collections import defaultdict\nimport math\nimport re\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nnp.random.seed(42)\ntorch.manual_seed(42)\n\nprint(\"Part 9.3: Multimodal AI\")\nprint(\"=\" * 50)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "---\n\n## 1. The Multimodal Alignment Problem\n\nDifferent modalities live in completely different spaces:\n- **Images**: 3D tensors (H x W x C) of pixel values\n- **Text**: Sequences of discrete tokens\n- **Audio**: 1D waveforms or spectrograms\n\n**Alignment** means learning a shared embedding space where semantically similar content from different modalities is close together:\n\n```\n\"a photo of a cat\" (text) <-> [cat image] (image)  -> close in embedding space\n\"a photo of a dog\" (text) <-> [cat image] (image)  -> far in embedding space\n```\n\n| Approach | How It Aligns | Example | F1 Parallel |\n|----------|--------------|--------|-------------|\n| **Contrastive** | Pull matching pairs together, push non-matching apart | CLIP, ALIGN | Learning that an onboard image of heavy rain matches the radio message \"It's aquaplaning out here\" — and doesn't match \"Track is bone dry\" |\n| **Generative** | Predict one modality from another | Image captioning, DALL-E | Generating a race report from telemetry data, or predicting expected telemetry from a track layout image |\n| **Fusion** | Combine modalities in a joint model | VisualBERT, Flamingo | The pit wall combining camera feed + telemetry + radio into one unified race model |\n\n**F1 analogy:** The alignment problem in F1 is connecting completely different data types that describe the same event. An onboard camera frame (pixels) showing the car braking hard into Turn 1 needs to be aligned with the telemetry trace (numbers) showing brake pressure at 200 bar and the radio message (text) \"Braking point is good, keep pushing.\" These three representations live in completely different mathematical spaces, but they all describe the same moment. Multimodal alignment learns to map them into a shared space where matching moments are close together."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the multimodal alignment concept\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Before alignment: separate spaces\n",
    "ax = axes[0]\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.set_title('Before Alignment', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Text embeddings (random cluster)\n",
    "np.random.seed(42)\n",
    "text_pts = np.random.randn(5, 2) * 0.5 + np.array([-1.5, 1])\n",
    "img_pts = np.random.randn(5, 2) * 0.5 + np.array([1.5, -1])\n",
    "\n",
    "labels = ['cat', 'dog', 'car', 'tree', 'bird']\n",
    "for i, label in enumerate(labels):\n",
    "    ax.scatter(text_pts[i, 0], text_pts[i, 1], c='#3498db', s=100, zorder=5,\n",
    "             edgecolors='black', marker='s')\n",
    "    ax.scatter(img_pts[i, 0], img_pts[i, 1], c='#e74c3c', s=100, zorder=5,\n",
    "             edgecolors='black', marker='o')\n",
    "    ax.annotate(f'\"{label}\"', (text_pts[i, 0], text_pts[i, 1]),\n",
    "               textcoords='offset points', xytext=(-5, 10), fontsize=8, color='#3498db')\n",
    "    ax.annotate(f'img:{label}', (img_pts[i, 0], img_pts[i, 1]),\n",
    "               textcoords='offset points', xytext=(-5, -15), fontsize=8, color='#e74c3c')\n",
    "\n",
    "ax.scatter([], [], c='#3498db', s=60, marker='s', label='Text embeddings')\n",
    "ax.scatter([], [], c='#e74c3c', s=60, marker='o', label='Image embeddings')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# After alignment: shared space\n",
    "ax = axes[1]\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.set_title('After Alignment (CLIP)', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Aligned: matching pairs are close\n",
    "aligned_centers = np.array([[-2, 2], [2, 2], [2, -1], [-1, -2], [0, 0.5]])\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    offset_text = np.random.randn(2) * 0.15\n",
    "    offset_img = np.random.randn(2) * 0.15\n",
    "    \n",
    "    tp = aligned_centers[i] + offset_text\n",
    "    ip = aligned_centers[i] + offset_img\n",
    "    \n",
    "    ax.scatter(tp[0], tp[1], c='#3498db', s=100, zorder=5, edgecolors='black', marker='s')\n",
    "    ax.scatter(ip[0], ip[1], c='#e74c3c', s=100, zorder=5, edgecolors='black', marker='o')\n",
    "    \n",
    "    # Draw connection line\n",
    "    ax.plot([tp[0], ip[0]], [tp[1], ip[1]], 'g--', alpha=0.5, linewidth=1.5)\n",
    "    \n",
    "    ax.annotate(label, aligned_centers[i], textcoords='offset points',\n",
    "               xytext=(10, -5), fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.scatter([], [], c='#3498db', s=60, marker='s', label='Text embeddings')\n",
    "ax.scatter([], [], c='#e74c3c', s=60, marker='o', label='Image embeddings')\n",
    "ax.plot([], [], 'g--', label='Aligned pairs')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Multimodal Alignment', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": "---\n\n## 2. Contrastive Learning\n\nContrastive learning is the key technique behind CLIP. Given a batch of N image-text pairs:\n\n1. Encode all images -> image embeddings\n2. Encode all texts -> text embeddings\n3. Compute similarity matrix (N x N)\n4. The diagonal entries (matching pairs) should be high\n5. Off-diagonal entries (non-matching) should be low\n\n### InfoNCE Loss\n\n$$\\mathcal{L}_i = -\\log \\frac{\\exp(\\text{sim}(I_i, T_i) / \\tau)}{\\sum_{j=1}^{N} \\exp(\\text{sim}(I_i, T_j) / \\tau)}$$\n\nwhere $\\tau$ is a learned temperature parameter.\n\n**F1 analogy:** Imagine training a system with thousands of paired examples: an onboard camera frame matched with its corresponding radio transcript. The contrastive loss says: \"Given this image of a wet track with spray, the matching radio message 'It's really slippery out here' should score high similarity, while all other radio messages in the batch ('Tires feel great', 'Box this lap', 'Gap to car ahead?') should score low.\" The temperature parameter $\\tau$ controls how sharp this distinction is — low temperature means the model must be very confident in its matches, like a race engineer who demands exact telemetry-radio alignment before acting on the information."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"InfoNCE contrastive loss (CLIP-style).\"\"\"\n",
    "    \n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = nn.Parameter(torch.tensor(math.log(1/temperature)))\n",
    "    \n",
    "    def forward(self, image_embeddings, text_embeddings):\n",
    "        \"\"\"Compute symmetric contrastive loss.\n",
    "        \n",
    "        Args:\n",
    "            image_embeddings: (batch_size, embed_dim)\n",
    "            text_embeddings: (batch_size, embed_dim)\n",
    "        \"\"\"\n",
    "        # Normalize embeddings\n",
    "        image_embeddings = F.normalize(image_embeddings, dim=-1)\n",
    "        text_embeddings = F.normalize(text_embeddings, dim=-1)\n",
    "        \n",
    "        # Similarity matrix\n",
    "        temp = self.temperature.exp()\n",
    "        logits = image_embeddings @ text_embeddings.T * temp\n",
    "        \n",
    "        # Labels: diagonal (matching pairs)\n",
    "        batch_size = logits.shape[0]\n",
    "        labels = torch.arange(batch_size, device=logits.device)\n",
    "        \n",
    "        # Symmetric loss: image-to-text + text-to-image\n",
    "        loss_i2t = F.cross_entropy(logits, labels)\n",
    "        loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "        \n",
    "        return (loss_i2t + loss_t2i) / 2, logits\n",
    "\n",
    "\n",
    "# Demonstrate contrastive loss\n",
    "torch.manual_seed(42)\n",
    "batch_size = 8\n",
    "embed_dim = 64\n",
    "\n",
    "# Simulated embeddings (not yet aligned)\n",
    "img_emb = torch.randn(batch_size, embed_dim)\n",
    "txt_emb = torch.randn(batch_size, embed_dim)\n",
    "\n",
    "criterion = ContrastiveLoss(temperature=0.07)\n",
    "loss, logits = criterion(img_emb, txt_emb)\n",
    "\n",
    "print(f\"Batch size: {batch_size}, Embed dim: {embed_dim}\")\n",
    "print(f\"Contrastive loss (random embeddings): {loss.item():.4f}\")\n",
    "print(f\"Expected loss for random (log(N)): {math.log(batch_size):.4f}\")\n",
    "\n",
    "# Now with perfectly aligned embeddings\n",
    "aligned_emb = F.normalize(torch.randn(batch_size, embed_dim), dim=-1)\n",
    "loss_aligned, _ = criterion(aligned_emb, aligned_emb)\n",
    "print(f\"Contrastive loss (perfectly aligned): {loss_aligned.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize similarity matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Random (unaligned) similarity\n",
    "ax = axes[0]\n",
    "sim_random = F.normalize(img_emb) @ F.normalize(txt_emb).T\n",
    "im = ax.imshow(sim_random.detach().numpy(), cmap='RdBu_r', vmin=-0.5, vmax=0.5)\n",
    "ax.set_xlabel('Text', fontsize=11)\n",
    "ax.set_ylabel('Image', fontsize=11)\n",
    "ax.set_title('Similarity: Before Training', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Aligned similarity (diagonal should be bright)\n",
    "ax = axes[1]\n",
    "# Simulate aligned: add identity-like structure\n",
    "aligned_img = F.normalize(torch.randn(batch_size, embed_dim), dim=-1)\n",
    "aligned_txt = aligned_img + torch.randn(batch_size, embed_dim) * 0.1  # Small noise\n",
    "aligned_txt = F.normalize(aligned_txt, dim=-1)\n",
    "\n",
    "sim_aligned = aligned_img @ aligned_txt.T\n",
    "im = ax.imshow(sim_aligned.detach().numpy(), cmap='RdBu_r', vmin=-0.5, vmax=1.0)\n",
    "ax.set_xlabel('Text', fontsize=11)\n",
    "ax.set_ylabel('Image', fontsize=11)\n",
    "ax.set_title('Similarity: After Training (CLIP)', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "---\n\n## 3. Building CLIP from Scratch\n\nCLIP has two encoders:\n1. **Image encoder**: CNN or Vision Transformer -> image embedding\n2. **Text encoder**: Transformer -> text embedding\n\nBoth project into the same embedding space and are trained with contrastive loss.\n\n**F1 analogy:** Building a CLIP-style system for F1 means training two separate encoders — one for onboard camera frames (the image encoder, processing the visual stream) and one for radio transcripts and telemetry descriptions (the text encoder). Both project into the same space so you can ask: \"Which radio message best describes what's happening in this camera frame?\" or \"Which camera frame matches this radio transcript?\" The image encoder learns to see track conditions, car positions, and weather; the text encoder learns to understand the language of racing. They meet in a shared embedding space where a picture of a safety car and the phrase \"safety car deployed\" end up at the same point."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"Simple CNN image encoder (in real CLIP: ViT or ResNet).\"\"\"\n",
    "    \n",
    "    def __init__(self, image_size=32, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.projection = nn.Linear(128, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.features(x).squeeze(-1).squeeze(-1)\n",
    "        return self.projection(features)\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"Simple transformer text encoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=128, max_len=32, n_heads=4, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(max_len, embed_dim)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=n_heads, dim_feedforward=embed_dim * 4,\n",
    "            dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, token_ids):\n",
    "        B, T = token_ids.shape\n",
    "        positions = torch.arange(T, device=token_ids.device).unsqueeze(0).expand(B, T)\n",
    "        \n",
    "        x = self.token_emb(token_ids) + self.pos_emb(positions)\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Use [CLS]-style: mean pooling\n",
    "        pooled = x.mean(dim=1)\n",
    "        return self.projection(pooled)\n",
    "\n",
    "\n",
    "class MiniCLIP(nn.Module):\n",
    "    \"\"\"Minimal CLIP model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=128, image_size=32):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder(image_size, embed_dim)\n",
    "        self.text_encoder = TextEncoder(vocab_size, embed_dim)\n",
    "        self.criterion = ContrastiveLoss(temperature=0.07)\n",
    "    \n",
    "    def forward(self, images, text_ids):\n",
    "        image_emb = self.image_encoder(images)\n",
    "        text_emb = self.text_encoder(text_ids)\n",
    "        loss, logits = self.criterion(image_emb, text_emb)\n",
    "        return loss, image_emb, text_emb, logits\n",
    "    \n",
    "    def encode_image(self, images):\n",
    "        return F.normalize(self.image_encoder(images), dim=-1)\n",
    "    \n",
    "    def encode_text(self, text_ids):\n",
    "        return F.normalize(self.text_encoder(text_ids), dim=-1)\n",
    "\n",
    "\n",
    "# Create synthetic training data\n",
    "# Simulate 5 classes with paired images and text\n",
    "n_classes = 5\n",
    "n_per_class = 20\n",
    "image_size = 16\n",
    "vocab_size = 50\n",
    "max_text_len = 8\n",
    "\n",
    "# Generate synthetic \"images\" (colored patches representing different classes)\n",
    "images = []\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "class_colors = [\n",
    "    [1, 0, 0],   # Red\n",
    "    [0, 1, 0],   # Green\n",
    "    [0, 0, 1],   # Blue\n",
    "    [1, 1, 0],   # Yellow\n",
    "    [1, 0, 1],   # Magenta\n",
    "]\n",
    "\n",
    "# Text tokens per class (simulated captions)\n",
    "class_text_patterns = [\n",
    "    [3, 5, 7, 0, 0, 0, 0, 0],   # \"red object\"\n",
    "    [4, 8, 12, 0, 0, 0, 0, 0],  # \"green thing\"\n",
    "    [6, 9, 15, 0, 0, 0, 0, 0],  # \"blue item\"\n",
    "    [10, 11, 20, 0, 0, 0, 0, 0], # \"yellow shape\"\n",
    "    [13, 14, 25, 0, 0, 0, 0, 0], # \"magenta form\"\n",
    "]\n",
    "\n",
    "for c in range(n_classes):\n",
    "    for _ in range(n_per_class):\n",
    "        # Image: class color + noise\n",
    "        img = np.zeros((3, image_size, image_size), dtype=np.float32)\n",
    "        for ch in range(3):\n",
    "            img[ch] = class_colors[c][ch] + np.random.normal(0, 0.2, (image_size, image_size))\n",
    "        images.append(img)\n",
    "        \n",
    "        # Text: class pattern + noise\n",
    "        text = class_text_patterns[c].copy()\n",
    "        # Add some random variation\n",
    "        for j in range(len(text)):\n",
    "            if text[j] > 0 and np.random.random() < 0.2:\n",
    "                text[j] += np.random.randint(-1, 2)\n",
    "        texts.append(text)\n",
    "        labels.append(c)\n",
    "\n",
    "images_tensor = torch.tensor(np.array(images))\n",
    "texts_tensor = torch.tensor(np.array(texts)).long()\n",
    "labels_tensor = torch.tensor(labels)\n",
    "\n",
    "print(f\"Dataset: {len(images)} image-text pairs, {n_classes} classes\")\n",
    "print(f\"Image shape: {images_tensor.shape}\")\n",
    "print(f\"Text shape: {texts_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MiniCLIP\n",
    "model = MiniCLIP(vocab_size, embed_dim=64, image_size=image_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"MiniCLIP parameters: {n_params:,}\")\n",
    "\n",
    "losses = []\n",
    "n_epochs = 100\n",
    "batch_size = 32\n",
    "n_samples = len(images_tensor)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    # Random batch\n",
    "    idx = torch.randperm(n_samples)[:batch_size]\n",
    "    batch_images = images_tensor[idx]\n",
    "    batch_texts = texts_tensor[idx]\n",
    "    \n",
    "    loss, _, _, _ = model(batch_images, batch_texts)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"  Epoch {epoch+1}/{n_epochs}: loss = {loss.item():.4f}\")\n",
    "\n",
    "print(f\"\\nFinal loss: {losses[-1]:.4f} (random baseline: {math.log(batch_size):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training and learned embeddings\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training loss\n",
    "ax = axes[0]\n",
    "ax.plot(losses, color='#3498db', linewidth=1, alpha=0.3)\n",
    "w = 5\n",
    "smoothed = [np.mean(losses[max(0,i-w):i+1]) for i in range(len(losses))]\n",
    "ax.plot(smoothed, color='#3498db', linewidth=2)\n",
    "ax.axhline(y=math.log(batch_size), color='red', linestyle='--', alpha=0.5,\n",
    "          label=f'Random baseline (log {batch_size})')\n",
    "ax.set_xlabel('Epoch', fontsize=11)\n",
    "ax.set_ylabel('Contrastive Loss', fontsize=11)\n",
    "ax.set_title('CLIP Training Loss', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Learned embedding space (2D projection via PCA)\n",
    "ax = axes[1]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    img_embs = model.encode_image(images_tensor).numpy()\n",
    "    txt_embs = model.encode_text(texts_tensor).numpy()\n",
    "\n",
    "# Simple PCA for visualization\n",
    "all_embs = np.concatenate([img_embs, txt_embs], axis=0)\n",
    "mean = all_embs.mean(axis=0)\n",
    "centered = all_embs - mean\n",
    "cov = centered.T @ centered / len(centered)\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(cov)\n",
    "top2 = eigenvectors[:, -2:]\n",
    "\n",
    "img_2d = (img_embs - mean) @ top2\n",
    "txt_2d = (txt_embs - mean) @ top2\n",
    "\n",
    "class_names = ['Red', 'Green', 'Blue', 'Yellow', 'Magenta']\n",
    "color_map = ['#e74c3c', '#2ecc71', '#3498db', '#f1c40f', '#9b59b6']\n",
    "\n",
    "for c in range(n_classes):\n",
    "    mask = labels_tensor.numpy() == c\n",
    "    ax.scatter(img_2d[mask, 0], img_2d[mask, 1], c=color_map[c], marker='o',\n",
    "             alpha=0.5, s=30, label=f'{class_names[c]} (img)' if c == 0 else None)\n",
    "    ax.scatter(txt_2d[mask, 0], txt_2d[mask, 1], c=color_map[c], marker='s',\n",
    "             alpha=0.5, s=30, label=f'{class_names[c]} (txt)' if c == 0 else None)\n",
    "\n",
    "# Legend\n",
    "ax.scatter([], [], c='gray', marker='o', s=40, label='Image embeddings')\n",
    "ax.scatter([], [], c='gray', marker='s', s=40, label='Text embeddings')\n",
    "ax.set_xlabel('PC 1', fontsize=11)\n",
    "ax.set_ylabel('PC 2', fontsize=11)\n",
    "ax.set_title('Learned Multimodal Embedding Space', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=8, loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "---\n\n## 4. Zero-Shot Classification\n\nCLIP's most remarkable ability: classify images into **any** set of categories without training on those categories.\n\n### How It Works\n1. Encode the image\n2. Encode each candidate label as text: \"a photo of a {label}\"\n3. The label with highest cosine similarity to the image wins\n\nThis works because the shared embedding space captures semantic meaning — the model understands \"cat\" refers to the same concept whether it appears as pixels or as text.\n\n**F1 analogy:** Zero-shot classification is like a system that can label onboard camera frames with *any* set of descriptions — without ever being specifically trained on those labels. Feed it an onboard frame and the candidate descriptions [\"dry track conditions\", \"wet track conditions\", \"safety car period\", \"pit stop in progress\", \"overtaking maneuver\"], and the system picks the best match based purely on its understanding of the shared vision-language space. No labeled training data needed for the specific categories. This is how a CLIP-style system could automatically tag thousands of hours of race footage with descriptive labels it has never explicitly been trained on."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroShotClassifier:\n",
    "    \"\"\"Zero-shot classification using CLIP-style model.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "    \n",
    "    def classify(self, images, label_text_ids):\n",
    "        \"\"\"Classify images into label categories.\n",
    "        \n",
    "        Args:\n",
    "            images: (N, C, H, W) tensor\n",
    "            label_text_ids: (K, T) tensor, one text per label\n",
    "        \n",
    "        Returns:\n",
    "            predictions: (N,) label indices\n",
    "            similarities: (N, K) similarity scores\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            image_embs = self.model.encode_image(images)  # (N, D)\n",
    "            text_embs = self.model.encode_text(label_text_ids)  # (K, D)\n",
    "            \n",
    "            # Cosine similarity\n",
    "            similarities = image_embs @ text_embs.T  # (N, K)\n",
    "            predictions = similarities.argmax(dim=1)\n",
    "        \n",
    "        return predictions, similarities\n",
    "\n",
    "\n",
    "# Zero-shot classification on our dataset\n",
    "classifier = ZeroShotClassifier(model)\n",
    "\n",
    "# Create text embeddings for each class\n",
    "label_texts = torch.tensor(class_text_patterns).long()\n",
    "\n",
    "# Classify all images\n",
    "preds, sims = classifier.classify(images_tensor, label_texts)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = (preds == labels_tensor).float().mean().item()\n",
    "\n",
    "print(f\"Zero-Shot Classification Results\\n\")\n",
    "print(f\"  Overall accuracy: {accuracy:.1%}\")\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\n  Per-class accuracy:\")\n",
    "for c in range(n_classes):\n",
    "    mask = labels_tensor == c\n",
    "    class_acc = (preds[mask] == c).float().mean().item()\n",
    "    print(f\"    {class_names[c]:>10}: {class_acc:.1%}\")\n",
    "\n",
    "# Confusion matrix\n",
    "confusion = np.zeros((n_classes, n_classes), dtype=int)\n",
    "for true, pred in zip(labels_tensor.numpy(), preds.numpy()):\n",
    "    confusion[true][pred] += 1\n",
    "\n",
    "print(\"\\n  Confusion matrix:\")\n",
    "print(f\"{'':>10}\", ''.join(f'{n:>8}' for n in class_names))\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"{name:>10}\", ''.join(f'{confusion[i][j]:>8}' for j in range(n_classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": "---\n\n## 5. Cross-Modal Retrieval\n\nAnother powerful application: retrieve images using text queries (or vice versa).\n\n- **Text -> Image**: \"Find me images of blue objects\"\n- **Image -> Text**: Given an image, find the most relevant captions\n\n**F1 analogy:** Cross-modal retrieval is incredibly useful for F1 broadcast and analysis. **Text -> Image**: A broadcast producer types \"overtaking move into Turn 1\" and the system retrieves the most relevant onboard camera frames from the entire race archive. **Image -> Text**: Given an onboard frame showing a car spinning, the system finds the most relevant radio transcripts (\"I've lost the rear end!\", \"Car has spun at Turn 8\"). This works because both modalities live in the same aligned embedding space — the CLIP-style system has learned that images of spins and radio messages about spins are the same *concept* expressed in different forms."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossModalRetriever:\n",
    "    \"\"\"Cross-modal retrieval using aligned embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.image_index = None\n",
    "        self.text_index = None\n",
    "    \n",
    "    def build_image_index(self, images):\n",
    "        \"\"\"Pre-compute image embeddings.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.image_index = self.model.encode_image(images)\n",
    "    \n",
    "    def build_text_index(self, texts):\n",
    "        \"\"\"Pre-compute text embeddings.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.text_index = self.model.encode_text(texts)\n",
    "    \n",
    "    def text_to_image(self, query_text, top_k=5):\n",
    "        \"\"\"Find images most similar to a text query.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            query_emb = self.model.encode_text(query_text)\n",
    "        \n",
    "        similarities = query_emb @ self.image_index.T\n",
    "        scores, indices = similarities.squeeze().topk(top_k)\n",
    "        return indices, scores\n",
    "    \n",
    "    def image_to_text(self, query_image, top_k=5):\n",
    "        \"\"\"Find texts most similar to an image query.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            query_emb = self.model.encode_image(query_image)\n",
    "        \n",
    "        similarities = query_emb @ self.text_index.T\n",
    "        scores, indices = similarities.squeeze().topk(top_k)\n",
    "        return indices, scores\n",
    "    \n",
    "    def evaluate_retrieval(self, labels, top_k=5):\n",
    "        \"\"\"Evaluate retrieval quality.\"\"\"\n",
    "        n = len(labels)\n",
    "        recalls = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            # Text-to-image: use text i to find images\n",
    "            query = self.text_index[i:i+1]\n",
    "            sims = query @ self.image_index.T\n",
    "            _, top_indices = sims.squeeze().topk(top_k)\n",
    "            \n",
    "            # Check if matching image is in top-k\n",
    "            retrieved_labels = labels[top_indices.numpy()]\n",
    "            correct = labels[i] in retrieved_labels\n",
    "            recalls.append(correct)\n",
    "        \n",
    "        return np.mean(recalls)\n",
    "\n",
    "\n",
    "retriever = CrossModalRetriever(model)\n",
    "retriever.build_image_index(images_tensor)\n",
    "retriever.build_text_index(texts_tensor)\n",
    "\n",
    "# Evaluate retrieval\n",
    "labels_np = labels_tensor.numpy()\n",
    "\n",
    "print(\"Cross-Modal Retrieval Results\\n\")\n",
    "for k in [1, 3, 5, 10]:\n",
    "    recall = retriever.evaluate_retrieval(labels_np, top_k=k)\n",
    "    print(f\"  Recall@{k}: {recall:.1%}\")\n",
    "\n",
    "# Example queries\n",
    "print(\"\\nExample: Text → Image retrieval\")\n",
    "for c in range(n_classes):\n",
    "    query = texts_tensor[c * n_per_class:c * n_per_class + 1]  # First text of each class\n",
    "    indices, scores = retriever.text_to_image(query, top_k=3)\n",
    "    retrieved_classes = [class_names[labels_np[idx]] for idx in indices.numpy()]\n",
    "    print(f\"  Query class '{class_names[c]}' -> Retrieved: {retrieved_classes} \"\n",
    "          f\"(scores: {scores.numpy().round(3)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize retrieval\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Recall@k curve\n",
    "ax = axes[0]\n",
    "ks = [1, 2, 3, 5, 10, 15, 20]\n",
    "recalls = [retriever.evaluate_retrieval(labels_np, top_k=k) for k in ks]\n",
    "ax.plot(ks, recalls, 'bo-', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('k (top-k retrieved)', fontsize=11)\n",
    "ax.set_ylabel('Recall@k', fontsize=11)\n",
    "ax.set_title('Text→Image Retrieval Performance', fontsize=13, fontweight='bold')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Cross-modal similarity heatmap (class-level)\n",
    "ax = axes[1]\n",
    "class_img_embs = []\n",
    "class_txt_embs = []\n",
    "for c in range(n_classes):\n",
    "    mask = labels_np == c\n",
    "    with torch.no_grad():\n",
    "        img_emb = model.encode_image(images_tensor[mask]).mean(dim=0)\n",
    "        txt_emb = model.encode_text(texts_tensor[mask]).mean(dim=0)\n",
    "    class_img_embs.append(img_emb)\n",
    "    class_txt_embs.append(txt_emb)\n",
    "\n",
    "class_img = torch.stack(class_img_embs)\n",
    "class_txt = torch.stack(class_txt_embs)\n",
    "class_sim = (F.normalize(class_img) @ F.normalize(class_txt).T).numpy()\n",
    "\n",
    "im = ax.imshow(class_sim, cmap='YlOrRd', vmin=0, vmax=1)\n",
    "ax.set_xticks(range(n_classes))\n",
    "ax.set_yticks(range(n_classes))\n",
    "ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "ax.set_yticklabels(class_names)\n",
    "ax.set_xlabel('Text Class', fontsize=11)\n",
    "ax.set_ylabel('Image Class', fontsize=11)\n",
    "ax.set_title('Cross-Modal Class Similarity', fontsize=13, fontweight='bold')\n",
    "\n",
    "for i in range(n_classes):\n",
    "    for j in range(n_classes):\n",
    "        color = 'white' if class_sim[i, j] > 0.5 else 'black'\n",
    "        ax.text(j, i, f'{class_sim[i, j]:.2f}', ha='center', va='center',\n",
    "               fontsize=9, color=color, fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": "---\n\n## 6. Vision-Language Models\n\nWhile CLIP aligns modalities for retrieval and classification, **vision-language models** (VLMs) can *generate* text about images. They extend LLMs with visual understanding.\n\n### Architecture Patterns\n\n| Model | Approach | How Vision Connects to LLM | F1 Parallel |\n|-------|---------|---------------------------|-------------|\n| **LLaVA** | Visual tokens | Image patches -> visual tokens prepended to text | Feeding onboard camera frames as \"visual tokens\" alongside telemetry data into the strategy model |\n| **Flamingo** | Cross-attention | Image features injected via cross-attention layers | The strategy model *attending to* camera feeds when relevant, like a race engineer glancing at screens during key moments |\n| **BLIP-2** | Q-Former bridge | Learnable queries extract info from frozen image encoder | A specialized \"visual analyst\" module that extracts key information from camera feeds before passing it to the strategy team |\n| **Claude/GPT-4V** | Native multimodal | Images and text processed jointly | A fully integrated pit wall system where camera, telemetry, and radio are processed as one unified input stream |\n\n**F1 analogy:** Vision-language models are the evolution from \"matching images to descriptions\" (CLIP) to \"understanding images and talking about them\" (VLMs). In F1 terms, CLIP can match an onboard frame to the correct radio transcript. But a VLM can *look at* an onboard frame and generate: \"The car ahead is running wide at Turn 3 exit, there's an opportunity to attack into Turn 4 on the inside. Track surface appears dry but there are dark clouds approaching from the left.\" That's genuine visual understanding combined with language generation — the AI equivalent of a race engineer narrating what they see on the monitors."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVLM(nn.Module):\n",
    "    \"\"\"Simplified Vision-Language Model (LLaVA-style).\n",
    "    \n",
    "    Converts image features to \"visual tokens\" that are prepended\n",
    "    to text tokens, then processed by a language model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=64, n_visual_tokens=4, image_size=16):\n",
    "        super().__init__()\n",
    "        self.n_visual_tokens = n_visual_tokens\n",
    "        \n",
    "        # Image encoder\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        \n",
    "        # Project image features to visual tokens\n",
    "        self.visual_projection = nn.Linear(64, n_visual_tokens * embed_dim)\n",
    "        \n",
    "        # Language model components\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(64, embed_dim)  # Max combined length\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim, nhead=4, dim_feedforward=embed_dim * 4,\n",
    "            dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=2)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, images, text_ids):\n",
    "        \"\"\"Forward pass: image + text -> next token predictions.\"\"\"\n",
    "        B = images.shape[0]\n",
    "        \n",
    "        # Encode image -> visual tokens\n",
    "        img_features = self.image_encoder(images).squeeze(-1).squeeze(-1)  # (B, 64)\n",
    "        visual_tokens = self.visual_projection(img_features)  # (B, n_vis * embed)\n",
    "        visual_tokens = visual_tokens.view(B, self.n_visual_tokens, -1)  # (B, n_vis, embed)\n",
    "        \n",
    "        # Embed text tokens\n",
    "        text_emb = self.token_emb(text_ids)  # (B, T, embed)\n",
    "        \n",
    "        # Concatenate: [visual_tokens, text_tokens]\n",
    "        combined = torch.cat([visual_tokens, text_emb], dim=1)  # (B, n_vis + T, embed)\n",
    "        seq_len = combined.shape[1]\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        positions = torch.arange(seq_len, device=combined.device).unsqueeze(0).expand(B, -1)\n",
    "        combined = combined + self.pos_emb(positions)\n",
    "        \n",
    "        # Causal mask\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=combined.device), diagonal=1).bool()\n",
    "        \n",
    "        # Decode\n",
    "        output = self.decoder(combined, combined, tgt_mask=causal_mask, memory_mask=causal_mask)\n",
    "        logits = self.lm_head(output)\n",
    "        \n",
    "        # Only return logits for text positions (skip visual tokens)\n",
    "        text_logits = logits[:, self.n_visual_tokens:, :]\n",
    "        return text_logits\n",
    "\n",
    "\n",
    "# Demonstrate VLM\n",
    "vlm = SimpleVLM(vocab_size=50, embed_dim=64, n_visual_tokens=4, image_size=16)\n",
    "vlm_params = sum(p.numel() for p in vlm.parameters())\n",
    "\n",
    "# Forward pass\n",
    "sample_images = images_tensor[:4]\n",
    "sample_texts = texts_tensor[:4]\n",
    "\n",
    "vlm.eval()\n",
    "with torch.no_grad():\n",
    "    text_logits = vlm(sample_images, sample_texts)\n",
    "\n",
    "print(f\"Vision-Language Model\")\n",
    "print(f\"  Parameters: {vlm_params:,}\")\n",
    "print(f\"  Visual tokens per image: 4\")\n",
    "print(f\"  Input: {sample_images.shape} images + {sample_texts.shape} text\")\n",
    "print(f\"  Output logits: {text_logits.shape} (text positions only)\")\n",
    "print(f\"  Predicted tokens: {text_logits.argmax(dim=-1).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": "---\n\n## 7. Multimodal Applications Landscape\n\nMultimodal AI powers a wide range of applications today. In Formula 1, the potential applications span every aspect of the sport — from real-time race analysis to fan engagement to safety systems."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the multimodal landscape\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_title('Multimodal AI Application Landscape', fontsize=15, fontweight='bold')\n",
    "\n",
    "apps = [\n",
    "    # (x, y, label, description, color)\n",
    "    (2, 8, 'Image\\nCaptioning', 'Image → Text description', '#3498db'),\n",
    "    (5.5, 8, 'Visual QA', 'Image + Question → Answer', '#2ecc71'),\n",
    "    (9, 8, 'OCR / Doc\\nUnderstanding', 'Document → Structured data', '#f39c12'),\n",
    "    (12, 8, 'Image\\nGeneration', 'Text → Image (DALL-E)', '#e74c3c'),\n",
    "    (2, 5, 'Cross-modal\\nRetrieval', 'Text ↔ Image search', '#9b59b6'),\n",
    "    (5.5, 5, 'Zero-shot\\nClassification', 'Classify without training', '#1abc9c'),\n",
    "    (9, 5, 'Video\\nUnderstanding', 'Video → Description', '#e67e22'),\n",
    "    (12, 5, 'Audio-Visual\\nSpeech', 'Lip reading, dubbing', '#c0392b'),\n",
    "    (3.5, 2, 'Multimodal\\nAgents', 'See + reason + act', '#2c3e50'),\n",
    "    (7, 2, 'Medical\\nImaging + NLP', 'X-ray → Diagnosis report', '#16a085'),\n",
    "    (10.5, 2, 'Autonomous\\nDriving', 'Camera + LiDAR + Maps', '#8e44ad'),\n",
    "]\n",
    "\n",
    "for x, y, label, desc, color in apps:\n",
    "    box = mpatches.FancyBboxPatch((x - 1.3, y - 0.7), 2.6, 1.4,\n",
    "                                   boxstyle=\"round,pad=0.12\", facecolor=color,\n",
    "                                   edgecolor='black', linewidth=1.5, alpha=0.85)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, y + 0.15, label, ha='center', va='center', fontsize=9,\n",
    "            fontweight='bold', color='white')\n",
    "    ax.text(x, y - 0.9, desc, ha='center', va='center', fontsize=7, color='gray')\n",
    "\n",
    "# Category labels\n",
    "ax.text(7, 9.3, 'Generation & Understanding', ha='center', fontsize=11,\n",
    "        fontweight='bold', color='gray', style='italic')\n",
    "ax.text(7, 6.3, 'Retrieval & Classification', ha='center', fontsize=11,\n",
    "        fontweight='bold', color='gray', style='italic')\n",
    "ax.text(7, 3.3, 'Domain Applications', ha='center', fontsize=11,\n",
    "        fontweight='bold', color='gray', style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": "---\n\n## Exercises\n\n### Exercise 1: Hard Negative Mining\n\nImprove the contrastive training by implementing **hard negative mining**: instead of random negatives, select the most confusing non-matching pairs (highest similarity among negatives). Compare training speed and final accuracy.\n\n**F1 scenario:** Instead of random negative pairings, find the *hardest* negatives — the most confusing mismatches. For example, an onboard frame of heavy rain (hard negative) paired with \"light drizzle, track drying\" (close but wrong — it's actually a downpour). Or a pit stop image paired with \"practice start at pit exit\" (visually similar pit lane activity, but completely different event). These hard negatives force the model to learn finer distinctions, just like how F1 drivers improve most by studying their *closest* competitors, not the backmarkers."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Hint: After computing the similarity matrix, for each positive pair,\n",
    "# find the negative with highest similarity and weight it more in the loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": "### Exercise 2: Prompt Engineering for Zero-Shot\n\nReal CLIP uses prompt templates like \"a photo of a {class}\", \"a {class} in the wild\", etc. Implement an ensemble of 5 different prompt templates and show that averaging their text embeddings improves zero-shot accuracy.\n\n**F1 scenario:** Instead of a single text template like \"an image of {condition}\", use multiple phrasings: \"onboard camera showing {condition}\", \"F1 track in {condition}\", \"race conditions: {condition}\", \"a Grand Prix under {condition} conditions\", \"telemetry consistent with {condition}\". Average these embeddings for each category and show that the ensemble is more robust than any single template — just as an F1 team consults multiple data sources before making a strategy call."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Hint: Create multiple text encodings per class using different templates,\n",
    "# average the embeddings, and compare to single-template accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": "### Exercise 3: Multimodal Fusion Model\n\nImplement a simple **fusion model** that combines image and text embeddings for a classification task. Compare three fusion strategies: (1) concatenation, (2) element-wise addition, (3) cross-attention. Which performs best?\n\n**F1 scenario:** Build a model that fuses onboard camera embeddings (image) with radio transcript embeddings (text) to classify race incidents. Try three fusion approaches: (1) concatenation — stack camera and radio features side by side, (2) element-wise addition — directly combine the signals, (3) cross-attention — let the camera features attend to the radio features and vice versa. Which fusion strategy best captures the relationship between what the camera sees and what the driver says? This mirrors the real pit wall challenge of combining visual and verbal information for rapid incident classification."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": "---\n\n## Summary\n\n### Key Concepts\n\n| Concept | What It Does | F1 Parallel |\n|---------|-------------|-------------|\n| **Multimodal alignment** | Maps different modalities into a shared embedding space | Connecting onboard camera frames, radio transcripts, and telemetry into a unified race understanding |\n| **Contrastive learning** (InfoNCE) | Trains by pulling matching pairs together, pushing non-matching apart | Learning that rain images match \"aquaplaning\" radio calls and not \"track is dry\" calls |\n| **CLIP** | Trains dual encoders on internet-scale image-text pairs | Learning to match race images with their telemetry descriptions across millions of examples |\n| **Zero-shot classification** | Classifies without task-specific training data | Labeling race footage with any set of descriptive categories — no labeled training data needed |\n| **Cross-modal retrieval** | Text->image and image->text search | \"Find me overtaking moves into Turn 1\" retrieves matching onboard frames from the archive |\n| **Vision-language models** | Extend LLMs with visual understanding via visual tokens | A pit wall AI that looks at camera feeds and generates natural language race analysis |\n| **Temperature** $\\tau$ | Controls sharpness of similarity distribution | How confident the system must be before matching a camera frame to a radio message |\n\n### The Multimodal Future\n\nThe trend is clear: the most capable AI systems are multimodal. Claude, GPT-4, and Gemini all process text, images, and more. Understanding how modalities are aligned and how information flows between them is essential for building the next generation of AI systems. In Formula 1, this future is already arriving — teams that can fuse onboard camera analysis, team radio understanding, and telemetry data into a unified real-time race intelligence system will have the ultimate competitive advantage. The pit wall of the future doesn't just display data from separate screens; it *understands* the race as a unified multimodal experience, just like a human race engineer does — but at the speed and scale that no human can match."
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": "---\n\n## Congratulations!\n\nYou've completed the full 28-notebook ML/AI curriculum! Here's everything you've learned, from the starting grid to the checkered flag:\n\n- **Part 1-2**: Math foundations and programming tools — the engineering fundamentals, like learning physics and materials science before designing a race car\n- **Part 3**: Neural network fundamentals (perceptrons, backprop, PyTorch, training) — building your first engine\n- **Part 4**: Specialized architectures (CNNs, RNNs, attention) — adding aerodynamics, suspension, and power unit specializations\n- **Part 5**: Modern NLP (transformers, language models, embeddings, fine-tuning) — the radio communication system connecting driver, car, and pit wall\n- **Part 6**: Reinforcement learning (MDPs, Q-learning, policy gradients, PPO, RLHF) — teaching the strategy model to learn from race outcomes\n- **Part 7**: Applied AI (RAG, agents, evaluation, production systems) — race day operations, from qualifying strategy to pit stop timing\n- **Part 8**: LLM engineering (tokenization, inference optimization, ML systems, multimodal AI) — the complete factory, simulation farm, and pit wall technology stack\n\nFrom matrix multiplication to multimodal contrastive learning — from the raw physics to the integrated pit wall intelligence system — you now have the foundations to understand, build, and deploy modern AI systems. The checkered flag is waving, but the real race is just beginning. Keep building!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
