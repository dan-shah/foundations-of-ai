{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Part 3.4: Training Deep Networks â€” The Formula 1 Edition\n\nBuilding a neural network architecture is only half the battle. The real challenge lies in **training** it effectively. In this notebook, we explore the techniques that make deep learning work in practice: optimizers that navigate complex loss landscapes, regularization methods that prevent overfitting, and normalization techniques that stabilize training.\n\n**F1 analogy:** Designing the car (architecture) is one thing. Getting it to go fast on race day is another. This notebook is about the engineering science of car setup: how to tune the setup parameters (optimizers), how to ensure the car works on all tracks and not just one (regularization), and how to keep the car stable through varying conditions (normalization). The difference between a championship-winning team and a backmarker is not the car concept -- it is the quality of the development process.\n\n---\n\n## Learning Objectives\n\nBy the end of this notebook, you should be able to:\n\n- [ ] Explain the intuition behind SGD, Momentum, RMSprop, and Adam optimizers\n- [ ] Choose appropriate learning rate schedules for different training scenarios\n- [ ] Apply regularization techniques (L1, L2, Dropout, Early Stopping) to prevent overfitting\n- [ ] Understand when and why to use Batch Normalization vs LayerNorm\n- [ ] Select proper weight initialization for different activation functions\n- [ ] Build a complete training pipeline with best practices"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 1. Optimizers\n\n### Intuitive Explanation\n\nImagine you're trying to find the lowest point in a mountainous landscape while blindfolded. All you can feel is the slope beneath your feet. **Gradient descent** is your basic strategy: always step downhill. But this simple approach has problems:\n\n1. **Getting stuck in valleys**: You might oscillate back and forth across a narrow valley instead of moving along it\n2. **Slow progress on plateaus**: Flat regions give tiny gradients, leading to tiny steps\n3. **Different terrain scales**: Some directions might be steep, others gentle\n\nDifferent optimizers address these challenges in different ways.\n\n**F1 analogy:** Optimizers are setup tuning strategies. SGD is the conservative engineer who makes one small change at a time and re-tests. Momentum is the engineer who notices \"we have been improving by adding downforce for three sessions, so keep pushing in that direction.\" Adam is the veteran engineer who adapts their approach to each parameter -- making big changes where the car is clearly off and small refinements where it is already close.\n\n| Optimizer | Strategy | F1 Analogy |\n|-----------|----------|------------|\n| **SGD** | Follow the gradient | Conservative: one small setup change at a time |\n| **Momentum** | Build up velocity | Aggressive: if the last 3 changes all went the same way, commit harder |\n| **RMSprop** | Adapt step size per dimension | Smart: big changes for insensitive parameters, small for sensitive ones |\n| **Adam** | Momentum + adaptive step sizes | Veteran: momentum awareness + parameter-specific tuning |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.1 SGD (Stochastic Gradient Descent)\n\nThe simplest optimizer. Update rule:\n\n$$\\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t)$$\n\n#### Breaking down the formula:\n\n| Component | Meaning | Typical Values | F1 Analogy |\n|-----------|---------|----------------|------------|\n| $\\theta_t$ | Current weights | - | Current car setup |\n| $\\eta$ | Learning rate | 0.001 to 0.1 | How big each setup adjustment is |\n| $\\nabla L$ | Gradient of loss | Computed via backprop | Which direction to adjust each parameter |\n\n**What this means:** Move in the opposite direction of the gradient, scaled by learning rate. Simple but can be slow and get stuck oscillating.\n\n**F1 analogy:** This is the most conservative setup strategy. After each session, you look at the data, identify which parameter is most responsible for the time loss, and adjust it by a small fixed amount. It works, but it is slow -- especially when one parameter needs a big change and another needs a tiny one, since SGD uses the same step size for everything."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.2 Momentum\n\n**Intuition: A ball rolling downhill**\n\nInstead of just following the current gradient, momentum keeps track of which direction we've been moving. Like a ball rolling downhill, we build up velocity and can push through small bumps.\n\n$$v_{t+1} = \\beta v_t + \\nabla L(\\theta_t)$$\n$$\\theta_{t+1} = \\theta_t - \\eta v_{t+1}$$\n\n| Component | Meaning | Typical Values |\n|-----------|---------|----------------|\n| $v_t$ | Velocity (accumulated gradient) | - |\n| $\\beta$ | Momentum coefficient | 0.9 |\n\n**What this means:** We blend the current gradient with our previous direction. This helps us:\n- Move faster in consistent directions\n- Dampen oscillations in inconsistent directions\n- Escape shallow local minima\n\n**F1 analogy:** This is the engineer who tracks trends across sessions. If adding downforce has improved lap time for the last three practice sessions, momentum says \"keep going in that direction with confidence\" rather than starting fresh each time. If one session says \"add downforce\" and the next says \"remove downforce,\" momentum dampens that oscillation. The $\\beta = 0.9$ means 90% of the previous direction is retained -- a strong memory of recent trends."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 RMSprop\n",
    "\n",
    "**Intuition: Adaptive step sizes**\n",
    "\n",
    "Some parameters need big updates, others need small ones. RMSprop tracks how variable each gradient has been and adjusts accordingly.\n",
    "\n",
    "$$s_{t+1} = \\beta s_t + (1-\\beta)(\\nabla L)^2$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{s_{t+1} + \\epsilon}} \\nabla L$$\n",
    "\n",
    "| Component | Meaning | Typical Values |\n",
    "|-----------|---------|----------------|\n",
    "| $s_t$ | Running average of squared gradients | - |\n",
    "| $\\epsilon$ | Small constant for stability | $10^{-8}$ |\n",
    "\n",
    "**What this means:**\n",
    "- Parameters with large, variable gradients get smaller updates\n",
    "- Parameters with small, consistent gradients get larger updates\n",
    "- This \"evens out\" the optimization across all parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Adam (Adaptive Moment Estimation)\n",
    "\n",
    "**Intuition: The best of both worlds**\n",
    "\n",
    "Adam combines momentum (first moment) with RMSprop's adaptive learning rates (second moment).\n",
    "\n",
    "$$m_{t+1} = \\beta_1 m_t + (1-\\beta_1) \\nabla L \\quad \\text{(momentum)}$$\n",
    "$$v_{t+1} = \\beta_2 v_t + (1-\\beta_2) (\\nabla L)^2 \\quad \\text{(adaptive rates)}$$\n",
    "$$\\hat{m} = \\frac{m_{t+1}}{1-\\beta_1^t}, \\quad \\hat{v} = \\frac{v_{t+1}}{1-\\beta_2^t} \\quad \\text{(bias correction)}$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}} + \\epsilon} \\hat{m}$$\n",
    "\n",
    "| Component | Meaning | Default Value |\n",
    "|-----------|---------|---------------|\n",
    "| $\\beta_1$ | Momentum decay | 0.9 |\n",
    "| $\\beta_2$ | RMSprop decay | 0.999 |\n",
    "| $\\epsilon$ | Stability constant | $10^{-8}$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Deep Dive: Why Adam is Usually the Default\n\nAdam has become the go-to optimizer for most deep learning tasks. Here's why:\n\n1. **Works well out of the box**: The default hyperparameters ($\\beta_1=0.9$, $\\beta_2=0.999$, $\\eta=0.001$) work well for most problems\n\n2. **Handles sparse gradients**: The adaptive learning rates help when some features appear rarely\n\n3. **Robust to hyperparameter choices**: Less sensitive to learning rate than SGD\n\n4. **Fast convergence**: Combines the speed benefits of momentum with adaptive rates\n\n**F1 analogy:** Adam is like the veteran race engineer who has tuned hundreds of cars across dozens of circuits. They use momentum (remembering what worked in recent sessions) combined with adaptive step sizes (making big changes to the wing but tiny adjustments to the differential). They know the default starting point that works 90% of the time, and they adapt from there. A rookie engineer (SGD) might find a better setup given enough time, but the veteran (Adam) gets you competitive much faster.\n\n#### Key Insight\n\nAdam is like having an experienced hiker guide you through the mountains. They know when to speed up on clear paths and slow down on tricky terrain.\n\n#### Common Misconceptions\n\n| Misconception | Reality |\n|---------------|--------|\n| \"Adam always beats SGD\" | SGD+momentum often achieves better final accuracy on vision tasks |\n| \"Adam doesn't need LR tuning\" | Still benefits from LR scheduling |\n| \"Use Adam for everything\" | For transformers yes, but try SGD for CNNs |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Optimizer Paths on Loss Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x, y):\n",
    "    \"\"\"Rosenbrock function - a classic optimization test function.\n",
    "    \n",
    "    Has a narrow curved valley that's easy to find but hard to follow.\n",
    "    Minimum at (1, 1).\n",
    "    \"\"\"\n",
    "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
    "\n",
    "def rosenbrock_grad(x, y):\n",
    "    \"\"\"Gradient of Rosenbrock function.\"\"\"\n",
    "    dx = -2 * (1 - x) - 400 * x * (y - x**2)\n",
    "    dy = 200 * (y - x**2)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "# Optimizer implementations from scratch\n",
    "class SGDOptimizer:\n",
    "    def __init__(self, lr=0.001):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def step(self, params, grad):\n",
    "        return params - self.lr * grad\n",
    "\n",
    "class MomentumOptimizer:\n",
    "    def __init__(self, lr=0.001, beta=0.9):\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        self.v = None\n",
    "        \n",
    "    def step(self, params, grad):\n",
    "        if self.v is None:\n",
    "            self.v = np.zeros_like(params)\n",
    "        self.v = self.beta * self.v + grad\n",
    "        return params - self.lr * self.v\n",
    "\n",
    "class RMSpropOptimizer:\n",
    "    def __init__(self, lr=0.001, beta=0.9, eps=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "        self.s = None\n",
    "        \n",
    "    def step(self, params, grad):\n",
    "        if self.s is None:\n",
    "            self.s = np.zeros_like(params)\n",
    "        self.s = self.beta * self.s + (1 - self.beta) * grad**2\n",
    "        return params - self.lr * grad / (np.sqrt(self.s) + self.eps)\n",
    "\n",
    "class AdamOptimizer:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "        \n",
    "    def step(self, params, grad):\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(params)\n",
    "            self.v = np.zeros_like(params)\n",
    "        self.t += 1\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * grad\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * grad**2\n",
    "        m_hat = self.m / (1 - self.beta1**self.t)\n",
    "        v_hat = self.v / (1 - self.beta2**self.t)\n",
    "        return params - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_path(optimizer, start, grad_fn, n_steps=500):\n",
    "    \"\"\"Run optimization and record the path.\"\"\"\n",
    "    path = [start.copy()]\n",
    "    params = start.copy()\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        grad = grad_fn(params[0], params[1])\n",
    "        params = optimizer.step(params, grad)\n",
    "        path.append(params.copy())\n",
    "        \n",
    "        # Stop if converged or diverged\n",
    "        if np.linalg.norm(params - np.array([1.0, 1.0])) < 0.01:\n",
    "            break\n",
    "        if np.any(np.abs(params) > 10):\n",
    "            break\n",
    "            \n",
    "    return np.array(path)\n",
    "\n",
    "# Starting point\n",
    "start = np.array([-1.0, 1.5])\n",
    "\n",
    "# Run each optimizer with tuned learning rates\n",
    "optimizers = {\n",
    "    'SGD': SGDOptimizer(lr=0.0002),\n",
    "    'Momentum': MomentumOptimizer(lr=0.0002, beta=0.9),\n",
    "    'RMSprop': RMSpropOptimizer(lr=0.01),\n",
    "    'Adam': AdamOptimizer(lr=0.05)\n",
    "}\n",
    "\n",
    "paths = {}\n",
    "for name, opt in optimizers.items():\n",
    "    paths[name] = optimize_path(opt, start, rosenbrock_grad, n_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss surface and optimization paths\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Create meshgrid for contour plot\n",
    "x = np.linspace(-2, 2, 200)\n",
    "y = np.linspace(-1, 3, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = rosenbrock(X, Y)\n",
    "\n",
    "# Left plot: All paths on contour\n",
    "ax = axes[0]\n",
    "ax.contour(X, Y, Z, levels=np.logspace(0, 3.5, 20), cmap='viridis', alpha=0.7)\n",
    "ax.plot(1, 1, 'r*', markersize=15, label='Minimum (1,1)')\n",
    "ax.plot(start[0], start[1], 'ko', markersize=10, label='Start')\n",
    "\n",
    "colors = {'SGD': 'blue', 'Momentum': 'red', 'RMSprop': 'green', 'Adam': 'orange'}\n",
    "for name, path in paths.items():\n",
    "    ax.plot(path[:, 0], path[:, 1], '-', color=colors[name], \n",
    "            label=f'{name} ({len(path)} steps)', linewidth=2, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Optimizer Paths on Rosenbrock Function')\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-1, 3)\n",
    "\n",
    "# Right plot: Loss over steps\n",
    "ax = axes[1]\n",
    "for name, path in paths.items():\n",
    "    losses = [rosenbrock(p[0], p[1]) for p in path]\n",
    "    ax.plot(losses, color=colors[name], label=name, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Loss (log scale)')\n",
    "ax.set_title('Loss Convergence')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final positions\n",
    "print(\"Final positions and loss:\")\n",
    "for name, path in paths.items():\n",
    "    final = path[-1]\n",
    "    final_loss = rosenbrock(final[0], final[1])\n",
    "    print(f\"  {name:10s}: ({final[0]:.4f}, {final[1]:.4f}), loss={final_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Optimizer Comparison Table\n\n| Optimizer | Pros | Cons | When to Use | F1 Parallel |\n|-----------|------|------|-------------|-------------|\n| **SGD** | Simple, less memory | Slow, sensitive to LR | Final fine-tuning, simple problems | Conservative approach: one change at a time |\n| **Momentum** | Faster than SGD, escapes local minima | Still sensitive to LR | CNNs, when SGD is too slow | Trend-following: commit to directions that keep working |\n| **RMSprop** | Adaptive LR per parameter | Can diverge with wrong settings | RNNs, non-stationary problems | Parameter-specific tuning intensity |\n| **Adam** | Fast, robust, adaptive | Higher memory, can overfit | Default choice for most tasks | Veteran engineer: adapts to each parameter |\n\n### Why This Matters in Machine Learning\n\n| Application | Recommended Optimizer | F1 Parallel |\n|-------------|----------------------|-------------|\n| Computer vision (CNNs) | SGD + Momentum (best accuracy) or Adam (faster) | Fine-tuning downforce: patient, iterative |\n| NLP (Transformers) | Adam or AdamW | Real-time strategy adaptation |\n| GANs | Adam with low beta1 (0.5) | Aggressive, exploratory setup changes |\n| Fine-tuning pretrained | Adam with small LR, or SGD | Gentle refinements to a known-good baseline |\n| Quick prototyping | Adam (works out of the box) | Friday practice: get up to speed fast |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch optimizer examples\n",
    "model = nn.Linear(10, 1)\n",
    "\n",
    "# SGD with momentum\n",
    "sgd_optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Adam (most common default)\n",
    "adam_optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# AdamW (Adam with proper weight decay - often better)\n",
    "adamw_optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "print(\"PyTorch optimizer examples:\")\n",
    "print(f\"  SGD: lr={sgd_optimizer.defaults['lr']}, momentum={sgd_optimizer.defaults['momentum']}\")\n",
    "print(f\"  Adam: lr={adam_optimizer.defaults['lr']}, betas={adam_optimizer.defaults['betas']}\")\n",
    "print(f\"  AdamW: lr={adamw_optimizer.defaults['lr']}, weight_decay={adamw_optimizer.defaults['weight_decay']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 2. Learning Rate\n\n### Intuitive Explanation\n\nThe **learning rate** is perhaps the single most important hyperparameter. It controls how big of a step you take with each update:\n\n- **Too high**: You overshoot the minimum, bouncing around wildly or even diverging\n- **Too low**: Training takes forever and may get stuck in poor solutions\n- **Just right**: Steady progress toward the minimum\n\n**F1 analogy:** The learning rate is how big each setup adjustment is between sessions. If you change the front wing by 5 degrees at a time (too high), you will overshoot the sweet spot and oscillate wildly between understeer and oversteer. If you change it by 0.01 degrees (too low), you will never converge on the optimal setting before the weekend is over. The art is finding the right adjustment size -- big enough to make progress, small enough not to overshoot. And as you get closer to the optimal setup, you should make smaller and smaller adjustments (learning rate scheduling)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Too High vs Too Low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_loss(x):\n",
    "    \"\"\"Simple 1D loss function: x^2\"\"\"\n",
    "    return x**2\n",
    "\n",
    "def simple_grad(x):\n",
    "    \"\"\"Gradient: 2x\"\"\"\n",
    "    return 2 * x\n",
    "\n",
    "def gradient_descent_1d(start, lr, n_steps):\n",
    "    \"\"\"Run 1D gradient descent.\"\"\"\n",
    "    path = [start]\n",
    "    x = start\n",
    "    for _ in range(n_steps):\n",
    "        x = x - lr * simple_grad(x)\n",
    "        path.append(x)\n",
    "        if abs(x) > 100:  # Diverged\n",
    "            break\n",
    "    return path\n",
    "\n",
    "# Test different learning rates\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "start = 5.0\n",
    "n_steps = 20\n",
    "\n",
    "configs = [\n",
    "    ('Too High (lr=1.1)', 1.1, 'red'),\n",
    "    ('Just Right (lr=0.3)', 0.3, 'green'),\n",
    "    ('Too Low (lr=0.01)', 0.01, 'blue')\n",
    "]\n",
    "\n",
    "for ax, (title, lr, color) in zip(axes, configs):\n",
    "    path = gradient_descent_1d(start, lr, n_steps)\n",
    "    \n",
    "    # Plot x value over steps\n",
    "    ax.plot(path, 'o-', color=color, markersize=6, linewidth=2)\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('x value')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    if lr > 1:\n",
    "        ax.set_ylim(-10, 10)\n",
    "    else:\n",
    "        ax.set_ylim(-1, 6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.1 Learning Rate Schedulers\n\nA common strategy is to **start with a larger learning rate** for fast initial progress, then **gradually reduce it** to fine-tune the solution.\n\n**F1 analogy:** This is exactly how F1 teams approach a race weekend. In FP1 (Friday practice), you make bold setup changes to explore the performance landscape. In FP2, you narrow the range. By qualifying, you are making tiny refinements. Learning rate schedulers automate this progression.\n\n| Scheduler | Strategy | Use Case | F1 Parallel |\n|-----------|----------|----------|-------------|\n| **StepLR** | Multiply by gamma every N epochs | Simple, predictable decay | Cut adjustment size at scheduled points (FP1 -> FP2 -> Quali) |\n| **ExponentialLR** | Multiply by gamma each epoch | Smooth continuous decay | Gradually smaller changes every session |\n| **CosineAnnealingLR** | Smooth cosine curve decay | Transformers, good generalization | Smooth transition from exploration to refinement |\n| **ReduceLROnPlateau** | Reduce when metric stops improving | Adaptive, data-driven | \"If lap time stops improving, try smaller changes\" |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different learning rate schedules\n",
    "epochs = 100\n",
    "base_lr = 0.1\n",
    "\n",
    "def get_scheduler_lrs(scheduler_class, epochs, **kwargs):\n",
    "    \"\"\"Get learning rates for a scheduler over epochs.\"\"\"\n",
    "    model = nn.Linear(10, 1)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=base_lr)\n",
    "    scheduler = scheduler_class(optimizer, **kwargs)\n",
    "    \n",
    "    lrs = []\n",
    "    for _ in range(epochs):\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "        scheduler.step()\n",
    "    return lrs\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# StepLR\n",
    "ax = axes[0, 0]\n",
    "lrs = get_scheduler_lrs(optim.lr_scheduler.StepLR, epochs, step_size=30, gamma=0.1)\n",
    "ax.plot(lrs, 'b-', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('StepLR (step=30, gamma=0.1)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# ExponentialLR\n",
    "ax = axes[0, 1]\n",
    "lrs = get_scheduler_lrs(optim.lr_scheduler.ExponentialLR, epochs, gamma=0.95)\n",
    "ax.plot(lrs, 'g-', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('ExponentialLR (gamma=0.95)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# CosineAnnealingLR\n",
    "ax = axes[1, 0]\n",
    "lrs = get_scheduler_lrs(optim.lr_scheduler.CosineAnnealingLR, epochs, T_max=epochs)\n",
    "ax.plot(lrs, 'r-', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('CosineAnnealingLR')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# MultiStepLR\n",
    "ax = axes[1, 1]\n",
    "lrs = get_scheduler_lrs(optim.lr_scheduler.MultiStepLR, epochs, milestones=[30, 60, 80], gamma=0.1)\n",
    "ax.plot(lrs, 'purple', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('MultiStepLR (milestones=[30,60,80])')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Learning Rate Schedulers', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Warmup Explained\n",
    "\n",
    "**Warmup** starts training with a very small learning rate and gradually increases it. This helps because:\n",
    "\n",
    "1. **Early gradients are unreliable**: Before the model has learned anything, gradients point in somewhat random directions\n",
    "2. **Batch normalization needs time**: BatchNorm statistics aren't accurate initially\n",
    "3. **Prevents early divergence**: Large initial updates can push weights to bad regions\n",
    "\n",
    "Warmup is especially important for:\n",
    "- Very deep networks\n",
    "- Large batch sizes\n",
    "- Transformer architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_cosine_schedule(epoch, warmup_epochs, total_epochs, max_lr):\n",
    "    \"\"\"Linear warmup followed by cosine decay.\"\"\"\n",
    "    if epoch < warmup_epochs:\n",
    "        return max_lr * (epoch / warmup_epochs)\n",
    "    else:\n",
    "        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n",
    "        return max_lr * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "\n",
    "# Visualize warmup\n",
    "epochs = 100\n",
    "warmup_epochs = 10\n",
    "max_lr = 0.1\n",
    "\n",
    "# Different schedules\n",
    "warmup_cosine = [warmup_cosine_schedule(e, warmup_epochs, epochs, max_lr) for e in range(epochs)]\n",
    "no_warmup = [max_lr * 0.5 * (1 + np.cos(np.pi * e / epochs)) for e in range(epochs)]\n",
    "constant = [max_lr] * epochs\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(warmup_cosine, 'b-', linewidth=2, label='Warmup + Cosine')\n",
    "ax.plot(no_warmup, 'g--', linewidth=2, label='Cosine (no warmup)', alpha=0.7)\n",
    "ax.plot(constant, 'gray', linestyle=':', linewidth=2, label='Constant', alpha=0.5)\n",
    "\n",
    "ax.axvline(x=warmup_epochs, color='red', linestyle='--', alpha=0.5, label=f'End warmup (epoch {warmup_epochs})')\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Learning Rate', fontsize=12)\n",
    "ax.set_title('Learning Rate Warmup', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Learning Rate Finder (Brief Mention)\n",
    "\n",
    "The **learning rate finder** is a technique to automatically find a good learning rate:\n",
    "\n",
    "1. Start with a very small learning rate\n",
    "2. Train for a few iterations, gradually increasing the LR\n",
    "3. Plot loss vs learning rate\n",
    "4. Choose the LR where loss is decreasing fastest (steepest slope)\n",
    "\n",
    "**Rule of thumb:** Pick a learning rate where the loss is clearly decreasing but hasn't started to explode. Often about 10x smaller than where loss starts increasing.\n",
    "\n",
    "Libraries like `pytorch-lightning` and `fastai` implement this automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 3. Regularization\n\n### Intuitive Explanation\n\n**Overfitting** happens when your model memorizes the training data instead of learning general patterns. It's like a student who memorizes answers to practice problems but can't solve new ones.\n\n**Regularization** techniques prevent overfitting by constraining the model's capacity.\n\n**F1 analogy:** Overfitting is when the car is perfectly tuned for one specific track but falls apart everywhere else. A car that is overfit to Monaco (tight, slow corners) will have massive downforce and soft springs -- but it will be hopeless on Monza (long straights, fast corners). Regularization is the engineering discipline of building a car that works well across the entire calendar, not just the track you tested on. It is the difference between winning one race and winning a championship."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: The Overfitting Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with noise\n",
    "np.random.seed(42)\n",
    "n_samples = 30\n",
    "X_train = np.linspace(0, 1, n_samples).reshape(-1, 1)\n",
    "y_true = np.sin(2 * np.pi * X_train).ravel()\n",
    "y_train = y_true + 0.3 * np.random.randn(n_samples)\n",
    "\n",
    "# Fit polynomials of different degrees\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "X_test = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "\n",
    "degrees = [1, 4, 15]\n",
    "titles = ['Underfitting (degree=1)', 'Good Fit (degree=4)', 'Overfitting (degree=15)']\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "for ax, degree, title, color in zip(axes, degrees, titles, colors):\n",
    "    # Fit polynomial\n",
    "    coeffs = np.polyfit(X_train.ravel(), y_train, degree)\n",
    "    y_pred = np.polyval(coeffs, X_test.ravel())\n",
    "    \n",
    "    # Calculate errors\n",
    "    train_pred = np.polyval(coeffs, X_train.ravel())\n",
    "    train_mse = np.mean((y_train - train_pred)**2)\n",
    "    test_mse = np.mean((np.sin(2 * np.pi * X_test.ravel()) - y_pred)**2)\n",
    "    \n",
    "    # Plot\n",
    "    ax.scatter(X_train, y_train, color='black', s=50, label='Training data', zorder=5)\n",
    "    ax.plot(X_test, np.sin(2 * np.pi * X_test), 'g--', alpha=0.5, label='True function')\n",
    "    ax.plot(X_test, y_pred, color=color, linewidth=2, label='Model')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f'{title}\\nTrain MSE: {train_mse:.3f}, Test MSE: {test_mse:.3f}')\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('The Bias-Variance Tradeoff', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.1 L2 Regularization (Weight Decay)\n\n**Intuition:** Penalize large weights to keep the model simple.\n\n$$L_{total} = L_{data} + \\lambda \\sum_i w_i^2$$\n\n| Component | Meaning | F1 Analogy |\n|-----------|--------|------------|\n| $L_{data}$ | Original loss (e.g., cross-entropy) | Lap time on the current track |\n| $\\lambda$ | Regularization strength (weight_decay) | How much you care about all-track performance |\n| $\\sum w_i^2$ | Sum of squared weights | How \"extreme\" your setup is from baseline |\n\n**What this means:** Large weights are costly, so the model prefers smaller weights. This leads to:\n- Smoother decision boundaries\n- Less sensitivity to individual features\n- Better generalization\n\n**F1 analogy:** L2 regularization is like penalizing extreme setup deviations. A car with wing angle at max, springs at minimum, and differential locked tight might be fast at one track, but it is \"overfit\" to those conditions. L2 says \"there is a cost to being extreme.\" The model (car) is incentivized to find a balanced setup that performs well broadly, rather than an extreme setup that only works in one specific condition."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 L1 Regularization (Sparsity)\n",
    "\n",
    "**Intuition:** Encourage weights to be exactly zero.\n",
    "\n",
    "$$L_{total} = L_{data} + \\lambda \\sum_i |w_i|$$\n",
    "\n",
    "**What this means:** Unlike L2 which makes weights small, L1 pushes weights all the way to zero. This creates **sparse** models where many weights are exactly 0.\n",
    "\n",
    "| Regularization | Effect on Weights | Use Case |\n",
    "|----------------|-------------------|----------|\n",
    "| L2 (Ridge) | Small but non-zero | General regularization |\n",
    "| L1 (Lasso) | Many exactly zero | Feature selection |\n",
    "| L1 + L2 (Elastic Net) | Some zero, others small | Best of both |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize L1 vs L2 regularization effect on weights\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Simulate weight distributions after regularization\n",
    "np.random.seed(42)\n",
    "n_weights = 1000\n",
    "\n",
    "# No regularization - weights can be large\n",
    "no_reg = np.random.randn(n_weights) * 1.5\n",
    "\n",
    "# L2 regularization - weights are small but non-zero\n",
    "l2_reg = np.random.randn(n_weights) * 0.3\n",
    "\n",
    "# L1 regularization - many weights exactly zero (Laplace distribution approximation)\n",
    "l1_reg = np.random.laplace(0, 0.2, n_weights)\n",
    "l1_reg[np.abs(l1_reg) < 0.1] = 0  # More weights become exactly 0\n",
    "\n",
    "ax = axes[0]\n",
    "ax.hist(no_reg, bins=50, alpha=0.7, color='gray', edgecolor='black')\n",
    "ax.set_xlabel('Weight Value')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('No Regularization')\n",
    "ax.set_xlim(-4, 4)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.hist(l2_reg, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "ax.set_xlabel('Weight Value')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('L2 Regularization\\n(small but non-zero)')\n",
    "ax.set_xlim(-4, 4)\n",
    "\n",
    "ax = axes[2]\n",
    "ax.hist(l1_reg, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "ax.set_xlabel('Weight Value')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title(f'L1 Regularization\\n({np.sum(l1_reg == 0)} weights exactly 0)')\n",
    "ax.set_xlim(-4, 4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.3 Dropout\n\n**Intuition: Training an ensemble of networks**\n\nDuring training, dropout randomly \"turns off\" neurons with probability $p$. This is like training many different smaller networks simultaneously.\n\n**Why it works:**\n1. **Prevents co-adaptation**: Neurons can't rely on specific other neurons always being there\n2. **Ensemble effect**: Like training many different networks and averaging them\n3. **Forces redundancy**: The network must learn robust features\n\n**Key insight:** Dropout forces neurons to learn features that are useful on their own, not just in combination with specific other neurons.\n\n**F1 analogy:** Dropout is like randomly disabling sensors during testing to build robustness. Imagine running practice sessions where you randomly turn off the tire temperature sensor, or the brake temperature sensor, or the wind speed anemometer. The telemetry system cannot rely on any single sensor always being available -- it must learn to make good predictions even with missing data. When race day comes and all sensors are active, the system is more robust because it never became dependent on any one input. This is exactly what dropout does to neural network layers."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dropout as creating sub-networks\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "def draw_network(ax, title, dropout_rate=0):\n",
    "    \"\"\"Draw a simple neural network with dropout visualization.\"\"\"\n",
    "    np.random.seed(42 + int(dropout_rate * 100))  # Different seeds for variety\n",
    "    \n",
    "    layers = [4, 6, 6, 3]\n",
    "    positions = []\n",
    "    \n",
    "    for layer_idx, n_neurons in enumerate(layers):\n",
    "        layer_pos = []\n",
    "        for i in range(n_neurons):\n",
    "            y = (i - (n_neurons - 1) / 2) * 0.15\n",
    "            x = layer_idx * 0.3\n",
    "            \n",
    "            # Determine if neuron is dropped (not for input/output layers)\n",
    "            is_dropped = False\n",
    "            if dropout_rate > 0 and layer_idx in [1, 2]:\n",
    "                is_dropped = np.random.random() < dropout_rate\n",
    "            \n",
    "            layer_pos.append((x, y, is_dropped))\n",
    "            \n",
    "            # Draw neuron\n",
    "            color = 'lightgray' if is_dropped else 'steelblue'\n",
    "            edge = 'gray' if is_dropped else 'darkblue'\n",
    "            ax.scatter(x, y, s=300, c=color, edgecolors=edge, linewidth=2, zorder=3)\n",
    "        \n",
    "        positions.append(layer_pos)\n",
    "    \n",
    "    # Draw connections\n",
    "    for l in range(len(positions) - 1):\n",
    "        for x1, y1, d1 in positions[l]:\n",
    "            for x2, y2, d2 in positions[l + 1]:\n",
    "                if not d1 and not d2:\n",
    "                    ax.plot([x1, x2], [y1, y2], 'gray', alpha=0.3, linewidth=0.5, zorder=1)\n",
    "    \n",
    "    ax.set_xlim(-0.1, 1.0)\n",
    "    ax.set_ylim(-0.6, 0.6)\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "\n",
    "draw_network(axes[0], 'No Dropout', 0)\n",
    "draw_network(axes[1], 'Dropout p=0.3\\n(training step 1)', 0.3)\n",
    "np.random.seed(99)\n",
    "draw_network(axes[2], 'Dropout p=0.3\\n(training step 2)', 0.3)\n",
    "\n",
    "plt.suptitle('Dropout Creates Different Sub-Networks Each Step', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Gray neurons are 'dropped' - they don't participate in this forward/backward pass.\")\n",
    "print(\"Each training step uses a different random sub-network!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout in PyTorch: Training vs Eval mode\n",
    "dropout = nn.Dropout(p=0.5)\n",
    "x = torch.ones(1, 10)\n",
    "\n",
    "# Training mode (dropout active)\n",
    "dropout.train()\n",
    "print(\"Training mode (dropout ACTIVE):\")\n",
    "for i in range(3):\n",
    "    out = dropout(x)\n",
    "    print(f\"  Sample {i+1}: {out.numpy().round(1)}\")\n",
    "\n",
    "# Evaluation mode (dropout disabled)\n",
    "dropout.eval()\n",
    "print(\"\\nEvaluation mode (dropout DISABLED):\")\n",
    "for i in range(3):\n",
    "    out = dropout(x)\n",
    "    print(f\"  Sample {i+1}: {out.numpy().round(1)}\")\n",
    "\n",
    "print(\"\\nNote: In training mode, surviving values are scaled by 1/(1-p)=2 to maintain expected value.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.4 Early Stopping\n\n**Intuition:** Stop training when validation performance starts to degrade.\n\nEarly stopping is a simple but effective form of regularization:\n1. Monitor validation loss during training\n2. Save the model when validation loss improves\n3. Stop if validation loss hasn't improved for N epochs (patience)\n4. Restore the best saved model\n\n**F1 analogy:** Early stopping is knowing when to stop chasing setup changes. There is a point in every race weekend where additional setup tweaks start making the car worse, not better -- you have passed the optimum and are now overfitting to noise in the data (track temperature variations, wind gusts, tire batch differences). The experienced engineer knows when to say \"the car is as good as it is going to get, lock it in.\" The patience parameter is like giving yourself 3 more sessions to beat the current best before accepting it."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate training with/without early stopping\n",
    "np.random.seed(42)\n",
    "\n",
    "epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training loss keeps decreasing\n",
    "    train_loss = 1.0 * np.exp(-epoch/30) + 0.05 + 0.01 * np.random.randn()\n",
    "    \n",
    "    # Validation loss increases after epoch 40 (overfitting)\n",
    "    if epoch < 40:\n",
    "        val_loss = 1.0 * np.exp(-epoch/30) + 0.1 + 0.02 * np.random.randn()\n",
    "    else:\n",
    "        val_loss = 0.2 + 0.004 * (epoch - 40) + 0.02 * np.random.randn()\n",
    "    \n",
    "    train_losses.append(max(0.05, train_loss))\n",
    "    val_losses.append(max(0.1, val_loss))\n",
    "\n",
    "best_epoch = np.argmin(val_losses)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(train_losses, 'b-', linewidth=2, label='Training Loss')\n",
    "ax.plot(val_losses, 'r-', linewidth=2, label='Validation Loss')\n",
    "ax.axvline(x=best_epoch, color='green', linestyle='--', linewidth=2, \n",
    "           label=f'Best model (epoch {best_epoch})')\n",
    "ax.axvspan(best_epoch, epochs, alpha=0.1, color='red', label='Overfitting region')\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('Early Stopping Prevents Overfitting', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best epoch: {best_epoch} with validation loss: {val_losses[best_epoch]:.4f}\")\n",
    "print(f\"Final epoch: {epochs-1} with validation loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"Early stopping saves {val_losses[-1] - val_losses[best_epoch]:.4f} in validation loss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Regularization Comparison Table\n\n| Technique | How It Works | Hyperparameter | When to Use | F1 Parallel |\n|-----------|--------------|----------------|-------------|-------------|\n| **L2 (Weight Decay)** | Penalizes large weights | weight_decay (0.01-0.1) | Always (default choice) | Penalizing extreme setup deviations |\n| **L1** | Encourages sparse weights | lambda | Feature selection needed | Identifying which sensors actually matter |\n| **Dropout** | Randomly drops neurons | p (0.1-0.5) | Deep networks, overfitting | Randomly disabling sensors to build robustness |\n| **Early Stopping** | Stop when val loss increases | patience (5-20 epochs) | Always monitor | Knowing when to lock in the setup |\n| **Data Augmentation** | Artificially expand dataset | Aug. parameters | Images, audio, text | Simulating varied track conditions |\n\n### Why This Matters in Machine Learning\n\n| Scenario | Recommended Regularization | F1 Parallel |\n|----------|---------------------------|-------------|\n| Small dataset | Dropout + weight decay + data augmentation | Limited testing: maximize learning from few sessions |\n| Large dataset | Lighter regularization, early stopping | Full test schedule: data speaks for itself |\n| Very deep network | Dropout (0.2-0.5) between dense layers | Complex telemetry pipeline: prevent over-specialization |\n| Transformers | Dropout in attention + AdamW weight decay | Modern F1 analytics with many interacting systems |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 4. Batch Normalization\n\n### Intuitive Explanation: The Problem\n\nAs data flows through a deep network, the distribution of activations can shift dramatically between layers. This is called **internal covariate shift**.\n\n**Analogy:** Imagine trying to learn to catch balls, but the thrower keeps changing how they throw - sometimes fast, sometimes slow, sometimes high, sometimes low. It would be much easier if every throw was similar.\n\n**Batch Normalization** normalizes the activations within each mini-batch, making training faster and more stable.\n\n**F1 analogy:** Batch normalization is normalizing telemetry data across different track conditions. Tire temperature readings at Bahrain (50C ambient) look completely different from Finland testing (-5C ambient). If the downstream analysis system expects a certain range of inputs, these wild variations cause instability. BatchNorm standardizes the inputs at each processing stage so that the downstream systems always see data in a consistent range, regardless of whether it was collected at Bahrain or Spa. The learnable scale ($\\gamma$) and shift ($\\beta$) parameters let each layer find its own optimal operating range."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### What BatchNorm Does\n\nFor each mini-batch, BatchNorm:\n\n1. **Normalize**: Subtract mean, divide by standard deviation\n$$\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n\n2. **Scale and Shift**: Apply learnable parameters $\\gamma$ and $\\beta$\n$$y = \\gamma \\hat{x} + \\beta$$\n\n| Component | Meaning | F1 Analogy |\n|-----------|--------|------------|\n| $\\mu_B$ | Mean of the batch | Average sensor reading across current conditions |\n| $\\sigma_B^2$ | Variance of the batch | How spread out the readings are |\n| $\\gamma$ | Learnable scale | Optimal sensitivity range for this processing stage |\n| $\\beta$ | Learnable shift | Optimal baseline for this processing stage |\n\n**Why the learnable parameters?** They let the network undo the normalization if needed. The network learns the optimal distribution for each layer."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what BatchNorm does\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate activations before BatchNorm (shifted and scaled)\n",
    "batch_size = 1000\n",
    "before_bn = np.random.exponential(2, batch_size) + np.random.randn(batch_size) * 0.5 + 5\n",
    "\n",
    "# Apply BatchNorm manually\n",
    "mu = before_bn.mean()\n",
    "sigma = before_bn.std()\n",
    "normalized = (before_bn - mu) / (sigma + 1e-8)\n",
    "\n",
    "# Learnable scale and shift\n",
    "gamma, beta = 1.5, 0.5\n",
    "after_bn = gamma * normalized + beta\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.hist(before_bn, bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "ax.axvline(mu, color='black', linestyle='--', linewidth=2, label=f'Mean: {mu:.1f}')\n",
    "ax.set_xlabel('Activation Value')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Before BatchNorm\\n(shifted, varying scale)')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1]\n",
    "ax.hist(normalized, bins=50, alpha=0.7, color='yellow', edgecolor='black')\n",
    "ax.axvline(0, color='black', linestyle='--', linewidth=2, label='Mean: 0, Std: 1')\n",
    "ax.set_xlabel('Activation Value')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('After Normalization\\n(zero mean, unit variance)')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[2]\n",
    "ax.hist(after_bn, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "ax.axvline(beta, color='black', linestyle='--', linewidth=2, label=f'Mean: {after_bn.mean():.2f}')\n",
    "ax.set_xlabel('Activation Value')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title(f'After Scale/Shift\\n(gamma={gamma}, beta={beta})')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training vs Eval Mode\n",
    "\n",
    "BatchNorm behaves differently during training and evaluation:\n",
    "\n",
    "| Mode | Mean/Variance | Why |\n",
    "|------|---------------|-----|\n",
    "| **Training** | Computed from current batch | Different each batch, adds noise |\n",
    "| **Evaluation** | Running averages from training | Consistent, deterministic predictions |\n",
    "\n",
    "**Critical:** Always call `model.train()` before training and `model.eval()` before inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchNorm: Training vs Eval mode\n",
    "bn = nn.BatchNorm1d(num_features=4)\n",
    "\n",
    "# Training mode - uses batch statistics\n",
    "bn.train()\n",
    "x_batch = torch.randn(32, 4) * 5 + 10  # batch of 32\n",
    "\n",
    "print(\"Training mode:\")\n",
    "print(f\"  Input mean: {x_batch.mean(dim=0).numpy().round(2)}\")\n",
    "print(f\"  Input std:  {x_batch.std(dim=0).numpy().round(2)}\")\n",
    "\n",
    "y_batch = bn(x_batch)\n",
    "print(f\"  Output mean: {y_batch.mean(dim=0).detach().numpy().round(2)} (approx 0)\")\n",
    "print(f\"  Output std:  {y_batch.std(dim=0).detach().numpy().round(2)} (approx 1)\")\n",
    "\n",
    "# After some training, running statistics are updated\n",
    "print(f\"\\n  Running mean: {bn.running_mean.numpy().round(2)}\")\n",
    "print(f\"  Running var:  {bn.running_var.numpy().round(2)}\")\n",
    "\n",
    "# Evaluation mode - uses running statistics\n",
    "bn.eval()\n",
    "x_single = torch.randn(1, 4) * 5 + 10  # Single sample\n",
    "\n",
    "print(\"\\nEvaluation mode (single sample):\")\n",
    "print(f\"  Input: {x_single.numpy().round(2)}\")\n",
    "y_single = bn(x_single)\n",
    "print(f\"  Output: {y_single.detach().numpy().round(2)}\")\n",
    "print(\"  (Uses running statistics, not batch statistics)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Deep Dive: Why BatchNorm Helps\n\nBatchNorm provides several benefits:\n\n1. **Faster training**: Allows higher learning rates without diverging\n2. **Regularization effect**: Batch statistics add noise, like dropout\n3. **Reduces initialization sensitivity**: Normalization prevents extreme activations\n4. **Smoother loss landscape**: Makes optimization easier\n\n#### Key Insight\n\nBatchNorm doesn't just normalize - it gives each layer a \"fresh start\" at each training step.\n\n**F1 analogy:** BatchNorm is like recalibrating every sensor at the start of each session. Without recalibration, the tire temperature sensor that read 90C in Bahrain and 40C in Barcelona gives the downstream systems wildly different inputs. With recalibration (normalization), the system always sees \"this tire is 1.5 standard deviations above the session mean\" -- a consistent, comparable signal regardless of absolute conditions.\n\n#### Common Misconceptions\n\n| Misconception | Reality |\n|---------------|--------|\n| Always put BatchNorm after activation | Before activation is more common and often better |\n| BatchNorm eliminates need for good init | Still helps to initialize properly |\n| BatchNorm always helps | Can hurt with very small batches |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### LayerNorm: For Transformers\n\n**Layer Normalization** normalizes across features instead of across the batch.\n\n| Normalization | Normalizes Across | Use Case | F1 Analogy |\n|---------------|-------------------|----------|------------|\n| **BatchNorm** | Batch dimension | CNNs, large batches | Normalize each sensor across all laps in a session |\n| **LayerNorm** | Feature dimension | Transformers, RNNs | Normalize all sensors within a single lap |\n| **InstanceNorm** | Spatial dimensions | Style transfer | Normalize within a single corner trace |\n| **GroupNorm** | Groups of channels | Small batches, detection | Normalize groups of related sensors together |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize BatchNorm vs LayerNorm\n",
    "batch_size, features = 4, 8\n",
    "x = torch.randn(batch_size, features) * 3 + 2\n",
    "\n",
    "# BatchNorm normalizes each column (feature) across the batch\n",
    "batch_norm = nn.BatchNorm1d(features)\n",
    "batch_norm.train()\n",
    "bn_out = batch_norm(x)\n",
    "\n",
    "# LayerNorm normalizes each row (sample) across features\n",
    "layer_norm = nn.LayerNorm(features)\n",
    "ln_out = layer_norm(x)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "im0 = axes[0].imshow(x.detach().numpy(), cmap='RdBu', aspect='auto', vmin=-6, vmax=6)\n",
    "axes[0].set_xlabel('Features')\n",
    "axes[0].set_ylabel('Batch samples')\n",
    "axes[0].set_title('Original Input')\n",
    "plt.colorbar(im0, ax=axes[0])\n",
    "\n",
    "im1 = axes[1].imshow(bn_out.detach().numpy(), cmap='RdBu', aspect='auto', vmin=-3, vmax=3)\n",
    "axes[1].set_xlabel('Features')\n",
    "axes[1].set_ylabel('Batch samples')\n",
    "axes[1].set_title('BatchNorm\\n(normalizes columns)')\n",
    "plt.colorbar(im1, ax=axes[1])\n",
    "\n",
    "im2 = axes[2].imshow(ln_out.detach().numpy(), cmap='RdBu', aspect='auto', vmin=-3, vmax=3)\n",
    "axes[2].set_xlabel('Features')\n",
    "axes[2].set_ylabel('Batch samples')\n",
    "axes[2].set_title('LayerNorm\\n(normalizes rows)')\n",
    "plt.colorbar(im2, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"BatchNorm: Each COLUMN has mean~0, std~1\")\n",
    "print(\"LayerNorm: Each ROW has mean~0, std~1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 5. Weight Initialization\n\n### Intuitive Explanation\n\nHow we initialize weights dramatically affects whether training succeeds. Poor initialization can cause:\n- **Vanishing activations**: All outputs become near-zero, gradients vanish\n- **Exploding activations**: Outputs grow without bound, gradients explode\n- **Dead neurons**: Some neurons never activate (especially with ReLU)\n\nThe goal is to keep activations and gradients at reasonable scales throughout the network.\n\n**F1 analogy:** Weight initialization is choosing your baseline setup before you start tuning. If you start with a completely random setup -- wing at max, springs at minimum, ride height at maximum -- the car might not even be drivable (exploding activations) or it might be so slow it provides no useful data (vanishing activations). A good starting point, like using last year's setup at a similar track, gives you a drivable car from which you can tune effectively. Xavier and Kaiming initialization are the engineering equivalent of \"start from a known-good baseline.\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Why Initialization Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_activations(layers, x, activation=torch.relu):\n",
    "    \"\"\"Track activation magnitudes through layers.\"\"\"\n",
    "    magnitudes = [x.abs().mean().item()]\n",
    "    \n",
    "    for layer in layers:\n",
    "        x = activation(layer(x))\n",
    "        magnitudes.append(x.abs().mean().item())\n",
    "    \n",
    "    return magnitudes\n",
    "\n",
    "def create_layers(n_layers, hidden_dim, init_scale):\n",
    "    \"\"\"Create linear layers with specified initialization scale.\"\"\"\n",
    "    layers = []\n",
    "    for _ in range(n_layers):\n",
    "        layer = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        nn.init.normal_(layer.weight, mean=0, std=init_scale)\n",
    "        layers.append(layer)\n",
    "    return layers\n",
    "\n",
    "# Test different initialization scales\n",
    "torch.manual_seed(42)\n",
    "n_layers = 20\n",
    "hidden_dim = 256\n",
    "x = torch.randn(100, hidden_dim)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "scales = [0.01, 0.1, 1.0]\n",
    "colors = ['blue', 'green', 'red']\n",
    "labels = ['Too Small (std=0.01)', 'Better (std=0.1)', 'Too Large (std=1.0)']\n",
    "\n",
    "for scale, color, label in zip(scales, colors, labels):\n",
    "    layers = create_layers(n_layers, hidden_dim, scale)\n",
    "    mags = forward_activations(layers, x.clone())\n",
    "    ax.plot(mags, '-o', color=color, label=label, markersize=4, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Layer', fontsize=12)\n",
    "ax.set_ylabel('Mean Activation Magnitude', fontsize=12)\n",
    "ax.set_title('Effect of Weight Initialization Scale', fontsize=14)\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Too small: Activations vanish (shrink to 0)\")\n",
    "print(\"Too large: Activations explode (grow unbounded)\")\n",
    "print(\"Just right: Activations stay roughly constant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Xavier/Glorot Initialization\n",
    "\n",
    "**For tanh and sigmoid activations.**\n",
    "\n",
    "$$W \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{in} + n_{out}}\\right)$$\n",
    "\n",
    "**Intuition:** Balance the variance of inputs and outputs to maintain signal magnitude through layers. Works well when activations are symmetric around zero.\n",
    "\n",
    "### 5.2 Kaiming/He Initialization\n",
    "\n",
    "**For ReLU activations.**\n",
    "\n",
    "$$W \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{in}}\\right)$$\n",
    "\n",
    "**Intuition:** ReLU zeros out half the activations (negative values become 0), so we need larger initial weights to compensate. The factor of 2 accounts for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Xavier vs Kaiming with ReLU\n",
    "torch.manual_seed(42)\n",
    "\n",
    "n_layers = 30\n",
    "hidden_dim = 256\n",
    "x = torch.randn(100, hidden_dim)\n",
    "\n",
    "def create_initialized_layers(n_layers, hidden_dim, init_type):\n",
    "    \"\"\"Create layers with proper initialization.\"\"\"\n",
    "    layers = []\n",
    "    for _ in range(n_layers):\n",
    "        layer = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        if init_type == 'xavier':\n",
    "            nn.init.xavier_normal_(layer.weight)\n",
    "        elif init_type == 'kaiming':\n",
    "            nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')\n",
    "        elif init_type == 'random':\n",
    "            nn.init.normal_(layer.weight, mean=0, std=0.1)\n",
    "        layers.append(layer)\n",
    "    return layers\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# With ReLU activation\n",
    "ax = axes[0]\n",
    "for init_type, color in [('random', 'red'), ('xavier', 'blue'), ('kaiming', 'green')]:\n",
    "    layers = create_initialized_layers(n_layers, hidden_dim, init_type)\n",
    "    mags = forward_activations(layers, x.clone(), torch.relu)\n",
    "    ax.plot(mags, '-o', color=color, label=init_type.capitalize(), markersize=3, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Mean Activation Magnitude')\n",
    "ax.set_title('ReLU Activation\\n(Kaiming is designed for ReLU)')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(1e-6, 1e2)\n",
    "\n",
    "# With Tanh activation\n",
    "ax = axes[1]\n",
    "for init_type, color in [('random', 'red'), ('xavier', 'blue'), ('kaiming', 'orange')]:\n",
    "    layers = create_initialized_layers(n_layers, hidden_dim, init_type)\n",
    "    mags = forward_activations(layers, x.clone(), torch.tanh)\n",
    "    ax.plot(mags, '-o', color=color, label=init_type.capitalize(), markersize=3, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Mean Activation Magnitude')\n",
    "ax.set_title('Tanh Activation\\n(Xavier is designed for symmetric activations)')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(1e-6, 1e2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Which Initialization to Use?\n\n| Activation | Recommended Init | PyTorch Function | F1 Analogy |\n|------------|------------------|------------------|------------|\n| ReLU, LeakyReLU | Kaiming (He) | `nn.init.kaiming_normal_` | Baseline for threshold-based systems |\n| Tanh, Sigmoid | Xavier (Glorot) | `nn.init.xavier_normal_` | Baseline for smooth-response systems |\n| GELU, SiLU | Kaiming | `nn.init.kaiming_normal_` | Baseline for modern nonlinear responses |\n| Linear (no activation) | Xavier | `nn.init.xavier_normal_` | Baseline for linear processing stages |\n\n**Good news:** PyTorch's default initialization works well for most cases!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch initialization examples\n",
    "layer = nn.Linear(512, 256)\n",
    "\n",
    "# Xavier initialization (for tanh/sigmoid)\n",
    "nn.init.xavier_normal_(layer.weight)\n",
    "print(f\"Xavier: mean={layer.weight.mean():.6f}, std={layer.weight.std():.4f}\")\n",
    "\n",
    "# Kaiming initialization (for ReLU)\n",
    "nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')\n",
    "print(f\"Kaiming: mean={layer.weight.mean():.6f}, std={layer.weight.std():.4f}\")\n",
    "\n",
    "# For biases, usually zero\n",
    "nn.init.zeros_(layer.bias)\n",
    "print(f\"Bias: all zeros = {torch.all(layer.bias == 0).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Putting It All Together\n",
    "\n",
    "Now let's build a complete training pipeline that incorporates all the techniques:\n",
    "- Adam optimizer with learning rate scheduling\n",
    "- BatchNorm and Dropout for regularization\n",
    "- Proper Kaiming initialization\n",
    "- Training with validation monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic classification dataset\n",
    "def make_moons_data(n_samples=1000, noise=0.2):\n",
    "    \"\"\"Generate two interleaving half circles (moons).\"\"\"\n",
    "    np.random.seed(42)\n",
    "    n = n_samples // 2\n",
    "    \n",
    "    # First moon\n",
    "    theta1 = np.linspace(0, np.pi, n)\n",
    "    x1 = np.cos(theta1)\n",
    "    y1 = np.sin(theta1)\n",
    "    \n",
    "    # Second moon (shifted and flipped)\n",
    "    theta2 = np.linspace(0, np.pi, n)\n",
    "    x2 = 1 - np.cos(theta2)\n",
    "    y2 = 0.5 - np.sin(theta2)\n",
    "    \n",
    "    X = np.vstack([\n",
    "        np.column_stack([x1, y1]),\n",
    "        np.column_stack([x2, y2])\n",
    "    ])\n",
    "    X += np.random.randn(*X.shape) * noise\n",
    "    y = np.hstack([np.zeros(n), np.ones(n)])\n",
    "    \n",
    "    # Shuffle\n",
    "    idx = np.random.permutation(n_samples)\n",
    "    return X[idx].astype(np.float32), y[idx].astype(np.float32)\n",
    "\n",
    "# Create train and validation data\n",
    "X_train, y_train = make_moons_data(n_samples=800, noise=0.2)\n",
    "X_val, y_val = make_moons_data(n_samples=200, noise=0.2)\n",
    "\n",
    "# Convert to PyTorch\n",
    "train_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_train), \n",
    "    torch.from_numpy(y_train).unsqueeze(1)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_val), \n",
    "    torch.from_numpy(y_val).unsqueeze(1)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], c='blue', label='Class 0', alpha=0.6)\n",
    "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], c='red', label='Class 1', alpha=0.6)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Training Data (Two Moons)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestPracticesNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Network with all best practices:\n",
    "    - BatchNorm after linear layers\n",
    "    - Dropout for regularization\n",
    "    - ReLU activation\n",
    "    - Kaiming initialization\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2, hidden_dims=[64, 32], output_dim=1, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            # Linear layer with Kaiming init\n",
    "            linear = nn.Linear(prev_dim, hidden_dim)\n",
    "            nn.init.kaiming_normal_(linear.weight, mode='fan_in', nonlinearity='relu')\n",
    "            nn.init.zeros_(linear.bias)\n",
    "            layers.append(linear)\n",
    "            \n",
    "            # BatchNorm\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            \n",
    "            # Activation\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "            # Dropout\n",
    "            if dropout_rate > 0:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "            \n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer (no BatchNorm, no Dropout)\n",
    "        output_layer = nn.Linear(prev_dim, output_dim)\n",
    "        nn.init.xavier_normal_(output_layer.weight)\n",
    "        nn.init.zeros_(output_layer.bias)\n",
    "        layers.append(output_layer)\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "model = BestPracticesNetwork(input_dim=2, hidden_dims=[64, 32], output_dim=1, dropout_rate=0.3)\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=100, lr=0.01, weight_decay=0.01):\n",
    "    \"\"\"\n",
    "    Complete training pipeline with:\n",
    "    - AdamW optimizer with weight decay\n",
    "    - Cosine annealing learning rate schedule\n",
    "    - Training and validation tracking\n",
    "    \"\"\"\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'lr': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            train_correct += (predictions == y_batch).sum().item()\n",
    "            train_total += X_batch.size(0)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "                predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                val_correct += (predictions == y_batch).sum().item()\n",
    "                val_total += X_batch.size(0)\n",
    "        \n",
    "        # Record and update\n",
    "        history['train_loss'].append(train_loss / train_total)\n",
    "        history['val_loss'].append(val_loss / val_total)\n",
    "        history['train_acc'].append(train_correct / train_total)\n",
    "        history['val_acc'].append(val_correct / val_total)\n",
    "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}: train_loss={history['train_loss'][-1]:.4f}, \"\n",
    "                  f\"val_loss={history['val_loss'][-1]:.4f}, \"\n",
    "                  f\"val_acc={history['val_acc'][-1]:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Train the model\n",
    "torch.manual_seed(42)\n",
    "model = BestPracticesNetwork(input_dim=2, hidden_dims=[64, 32], output_dim=1, dropout_rate=0.3)\n",
    "history = train_model(model, train_loader, val_loader, epochs=100, lr=0.01, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "ax = axes[0]\n",
    "ax.plot(history['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "ax.plot(history['val_loss'], 'r-', label='Validation', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax = axes[1]\n",
    "ax.plot(history['train_acc'], 'b-', label='Train', linewidth=2)\n",
    "ax.plot(history['val_acc'], 'r-', label='Validation', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning Rate\n",
    "ax = axes[2]\n",
    "ax.plot(history['lr'], 'g-', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('Learning Rate (Cosine Annealing)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundary\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    \"\"\"Plot the decision boundary.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    grid = np.column_stack([xx.ravel(), yy.ravel()]).astype(np.float32)\n",
    "    with torch.no_grad():\n",
    "        Z = torch.sigmoid(model(torch.from_numpy(grid))).numpy()\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, levels=50, cmap='RdBu', alpha=0.7)\n",
    "    plt.colorbar(label='P(Class 1)')\n",
    "    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', edgecolors='white', label='Class 0')\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', edgecolors='white', label='Class 1')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.legend()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_decision_boundary(model, X_val, y_val)\n",
    "plt.title('Trained Model Decision Boundary')\n",
    "plt.show()\n",
    "\n",
    "# Final accuracy\n",
    "print(f\"\\nFinal validation accuracy: {history['val_acc'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Optimizer Comparison\n",
    "\n",
    "Compare the performance of different optimizers on the moons dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 1: Compare optimizers\n",
    "def train_with_optimizer(optimizer_name, lr=0.01, epochs=50):\n",
    "    \"\"\"\n",
    "    Train a model with a specific optimizer and return validation accuracy.\n",
    "    \n",
    "    Args:\n",
    "        optimizer_name: 'sgd', 'momentum', 'rmsprop', or 'adam'\n",
    "        lr: learning rate\n",
    "        epochs: number of epochs\n",
    "    \n",
    "    Returns:\n",
    "        List of validation accuracies per epoch\n",
    "    \"\"\"\n",
    "    # TODO: Implement this!\n",
    "    # Hint: Create a fresh model\n",
    "    # Hint: Choose optimizer based on optimizer_name:\n",
    "    #   - 'sgd': optim.SGD(params, lr=lr)\n",
    "    #   - 'momentum': optim.SGD(params, lr=lr, momentum=0.9)\n",
    "    #   - 'rmsprop': optim.RMSprop(params, lr=lr)\n",
    "    #   - 'adam': optim.Adam(params, lr=lr)\n",
    "    # Hint: Train and record validation accuracy each epoch\n",
    "    \n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "# Test your implementation\n",
    "# val_accs = train_with_optimizer('adam', lr=0.01, epochs=50)\n",
    "# print(f\"Adam final accuracy: {val_accs[-1]:.4f}\")\n",
    "\n",
    "# Expected output: Compare all optimizers\n",
    "# optimizers = ['sgd', 'momentum', 'adam']\n",
    "# for opt in optimizers:\n",
    "#     accs = train_with_optimizer(opt, lr=0.01)\n",
    "#     plt.plot(accs, label=opt)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Early Stopping\n",
    "\n",
    "Implement an early stopping class that stops training when validation loss stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 2: Implement early stopping\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to prevent overfitting.\n",
    "    \n",
    "    Usage:\n",
    "        early_stop = EarlyStopping(patience=5)\n",
    "        for epoch in range(epochs):\n",
    "            # ... training code ...\n",
    "            if early_stop(val_loss):\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience: Number of epochs to wait before stopping\n",
    "            min_delta: Minimum improvement to count as improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        # TODO: Initialize tracking variables\n",
    "        # Hint: Track best_loss and counter\n",
    "        pass\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        \"\"\"\n",
    "        Check if training should stop.\n",
    "        \n",
    "        Args:\n",
    "            val_loss: Current validation loss\n",
    "            \n",
    "        Returns:\n",
    "            True if training should stop, False otherwise\n",
    "        \"\"\"\n",
    "        # TODO: Implement this!\n",
    "        # Hint: If val_loss improved by min_delta, reset counter\n",
    "        # Hint: Otherwise, increment counter\n",
    "        # Hint: Return True if counter >= patience\n",
    "        \n",
    "        pass  # Replace with your implementation\n",
    "\n",
    "# Test your implementation\n",
    "early_stop = EarlyStopping(patience=3)\n",
    "losses = [1.0, 0.9, 0.8, 0.85, 0.83, 0.84, 0.86, 0.7]\n",
    "\n",
    "print(\"Testing Early Stopping:\")\n",
    "for epoch, loss in enumerate(losses):\n",
    "    should_stop = early_stop(loss)\n",
    "    print(f\"Epoch {epoch}: loss={loss}, stop={should_stop}\")\n",
    "    if should_stop:\n",
    "        print(f\"Training stopped at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "# Expected: Should stop at epoch 6 (after 3 epochs without improvement from 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Dropout Rate Experiment\n",
    "\n",
    "Find the optimal dropout rate for the moons classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 3: Find optimal dropout rate\n",
    "def test_dropout_rate(dropout_rate, epochs=50):\n",
    "    \"\"\"\n",
    "    Train a model with a specific dropout rate.\n",
    "    \n",
    "    Args:\n",
    "        dropout_rate: float between 0 and 1\n",
    "        epochs: number of training epochs\n",
    "    \n",
    "    Returns:\n",
    "        Final validation accuracy\n",
    "    \"\"\"\n",
    "    # TODO: Implement this!\n",
    "    # Hint: Create BestPracticesNetwork with the given dropout_rate\n",
    "    # Hint: Train the model\n",
    "    # Hint: Return the final validation accuracy\n",
    "    \n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "# Test your implementation\n",
    "# dropout_rates = [0.0, 0.1, 0.2, 0.3, 0.5, 0.7]\n",
    "# accuracies = [test_dropout_rate(dr) for dr in dropout_rates]\n",
    "#\n",
    "# plt.plot(dropout_rates, accuracies, 'bo-')\n",
    "# plt.xlabel('Dropout Rate')\n",
    "# plt.ylabel('Validation Accuracy')\n",
    "# plt.title('Effect of Dropout Rate')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.show()\n",
    "#\n",
    "# best_idx = np.argmax(accuracies)\n",
    "# print(f\"Best dropout rate: {dropout_rates[best_idx]} with accuracy {accuracies[best_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Summary\n\n### Key Concepts\n\n| Concept | Definition | F1 Parallel |\n|---------|-----------|-------------|\n| **SGD** | Simple gradient following | Conservative: one small setup change at a time |\n| **Momentum** | Accumulate velocity from past gradients | Trend-following: if it kept working, keep going |\n| **Adam** | Momentum + adaptive per-parameter rates | Veteran engineer: adapts strategy to each parameter |\n| **Learning rate** | Step size for weight updates | How big each setup adjustment is |\n| **LR schedulers** | Reduce LR over time | Bold changes early (FP1), tiny refinements late (qualifying) |\n| **L2 regularization** | Penalize large weights | Penalizing extreme setup deviations from baseline |\n| **Dropout** | Randomly disable neurons during training | Randomly disabling sensors to build robustness |\n| **Early stopping** | Stop when validation degrades | Knowing when to lock in the setup |\n| **BatchNorm** | Normalize activations per batch | Normalizing telemetry across different track conditions |\n| **LayerNorm** | Normalize activations per sample | Normalizing all sensors within a single lap |\n| **Weight init** | Smart starting values for parameters | Starting from a known-good baseline setup |\n\n### Connection to Deep Learning\n\n| Technique | Application | F1 Parallel |\n|-----------|------------|-------------|\n| Adam/AdamW | Default for most tasks | Veteran engineer's adaptive approach |\n| SGD + Momentum | Fine-tuning, achieving SOTA on vision | Patient, iterative refinement |\n| Cosine LR Schedule | Modern training recipes | Smooth FP1 -> qualifying transition |\n| Dropout | Fully connected layers | Sensor robustness training |\n| BatchNorm | CNNs, faster training | Cross-condition telemetry normalization |\n| LayerNorm | Transformers, RNNs | Within-sample feature normalization |\n| Kaiming Init | Any ReLU network | Engineering-informed baseline setup |\n\n### Checklist\n\n- [ ] I can explain why Adam is often the default optimizer\n- [ ] I understand the effect of learning rate on training\n- [ ] I can choose appropriate regularization for different scenarios\n- [ ] I know when to use BatchNorm vs LayerNorm\n- [ ] I can select the right initialization for my activation function\n- [ ] I can build a complete training pipeline with best practices"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Next Steps\n\nNow that you understand how to train deep networks effectively, you're ready to explore:\n\n1. **Convolutional Neural Networks (CNNs)**: Specialized architectures for image data\n2. **Recurrent Neural Networks (RNNs)**: Handling sequential data\n3. **Transfer Learning**: Using pretrained models as starting points\n4. **Transformers**: The architecture behind modern NLP and beyond\n\nThe training techniques you learned here apply to all these architectures. Whether you're training a simple classifier or a billion-parameter language model, you'll use:\n- Optimizers (usually Adam or AdamW) -- your setup tuning strategy\n- Learning rate schedules (often cosine with warmup) -- bold changes early, refinements late\n- Regularization (dropout, weight decay) -- preventing the car from being overfit to one track\n- Normalization (BatchNorm for CNNs, LayerNorm for Transformers) -- normalizing telemetry across conditions\n\n**Practical next steps:**\n- Train a model on MNIST or CIFAR-10\n- Experiment with different optimizer/scheduler combinations\n- Use TensorBoard or Weights & Biases to visualize training\n- Try implementing gradient clipping for very deep networks\n\nYou now have the complete engineering toolkit to train deep neural networks. Like an F1 team heading into a race weekend, you know the car (architecture), the tuning process (optimizers), the development discipline (regularization), and the calibration systems (normalization). Time to go racing."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}