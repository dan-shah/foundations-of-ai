{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Part 4.4: Attention Mechanisms â€” The Formula 1 Edition\n\nAttention is the single most important idea in modern deep learning. It solves a fundamental problem with sequence models: how do you let a network **focus** on the most relevant parts of its input? Before attention, models had to compress an entire input sequence into a single fixed-size vector -- a brutal bottleneck. Attention removes that bottleneck and has become the foundation of Transformers, the architecture behind GPT, BERT, and virtually every state-of-the-art model today.\n\n**F1 analogy:** Imagine you are a race strategist trying to predict how a driver will perform on lap 45. Without attention, you have to cram the entire 44-lap history into a single summary vector -- losing critical detail. With attention, you can ask: \"Which past laps are most relevant to predicting THIS lap?\" Maybe lap 1 (tire compound choice) and lap 38 (the last pit stop) matter enormously, while laps 10-30 (uneventful clean-air running) are nearly irrelevant. Attention lets the model dynamically decide what to focus on, just as a strategist zeroes in on the moments that matter.\n\n---\n\n## Learning Objectives\n\nBy the end of this notebook, you should be able to:\n\n- [ ] Explain why fixed-length encoding is a bottleneck for sequence models\n- [ ] Describe the Query, Key, Value framework using the database lookup analogy\n- [ ] Compute dot-product attention step by step\n- [ ] Explain why we scale by sqrt(d_k) in scaled dot-product attention\n- [ ] Implement self-attention from scratch in PyTorch\n- [ ] Implement multi-head attention and explain why multiple heads help\n- [ ] Compare additive (Bahdanau) vs multiplicative (Luong) attention\n- [ ] Build and train a sequence model with attention"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "---\n\n## 1. The Problem with Fixed-Length Encoding\n\n### Intuitive Explanation\n\nImagine you're reading a long novel, and someone asks you to summarize the **entire book in a single sentence**. No matter how good you are, you'll lose details. Early words, subtle plot points, and minor characters will be forgotten.\n\nThis is exactly the problem with encoder-decoder models (like sequence-to-sequence RNNs). The encoder reads the entire input sequence and compresses it into a **single fixed-size vector** -- the \"context vector.\" The decoder then has to generate the entire output from only that one vector.\n\n**The bottleneck:** As the input sequence gets longer, more and more information gets squeezed into the same small vector, and early parts of the sequence are gradually overwritten.\n\n| Sequence Length | What Happens | Analogy | F1 Parallel |\n|---------------|--------------|---------|-------------|\n| Short (5-10 tokens) | Works okay | Summarize a paragraph in one sentence | Predicting the next lap from 5 laps of data -- manageable |\n| Medium (20-50 tokens) | Starts losing detail | Summarize a chapter in one sentence | Compressing a full stint into one vector -- losing corner-by-corner detail |\n| Long (100+ tokens) | Severe information loss | Summarize a book in one sentence | Cramming an entire 78-lap race into a single vector -- lap 1 details are gone |\n\n**The key insight:** Instead of forcing the decoder to work from a single summary vector, let it **look back at all the encoder states** and decide which ones are relevant at each step.\n\n**F1 analogy:** A fixed-length encoding is like asking an engineer to summarize an entire Grand Prix in a single number. Attention is like giving them the full lap chart and letting them look up any specific lap whenever they need to. When predicting lap 45, they can glance at lap 38 (last pit stop), lap 1 (compound choice), and lap 44 (current degradation rate) -- different queries pull different information."
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "### Visualization: Information Loss as Sequence Grows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate how much information about each position is retained\n",
    "# in a fixed-length context vector as sequence length grows\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, seq_len, title in zip(axes, [5, 15, 50], \n",
    "                               ['Short Sequence (5 tokens)', \n",
    "                                'Medium Sequence (15 tokens)',\n",
    "                                'Long Sequence (50 tokens)']):\n",
    "    # Simulate exponential decay of information retention\n",
    "    # Earlier positions lose more info as they get overwritten\n",
    "    positions = np.arange(seq_len)\n",
    "    decay_rate = 0.15\n",
    "    retention = np.exp(-decay_rate * (seq_len - 1 - positions))\n",
    "    retention = retention / retention.max()  # Normalize\n",
    "    \n",
    "    colors = plt.cm.RdYlGn(retention)  # Red=lost, Green=retained\n",
    "    ax.bar(positions, retention, color=colors, edgecolor='black', linewidth=0.5)\n",
    "    ax.set_xlabel('Position in Sequence')\n",
    "    ax.set_ylabel('Information Retained')\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.3)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Fixed-Length Encoding: Early Positions Lose Information', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: As the sequence gets longer, early positions retain almost no information.\")\n",
    "print(\"This is the fundamental bottleneck that attention solves.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: Fixed encoding vs Attention\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Fixed-length encoding (bottleneck)\n",
    "ax = axes[0]\n",
    "encoder_positions = np.arange(6)\n",
    "encoder_y = np.ones(6) * 2\n",
    "decoder_positions = np.arange(4)\n",
    "decoder_y = np.ones(4) * 0\n",
    "\n",
    "# Draw encoder states\n",
    "for i in range(6):\n",
    "    ax.add_patch(plt.Rectangle((i - 0.3, 1.7), 0.6, 0.6, \n",
    "                               facecolor='steelblue', edgecolor='black', linewidth=1.5))\n",
    "    ax.text(i, 2.0, f'h{i}', ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "\n",
    "# Draw bottleneck\n",
    "ax.add_patch(plt.Rectangle((2.2, 0.9), 1.6, 0.6, \n",
    "                           facecolor='red', edgecolor='black', linewidth=2, alpha=0.8))\n",
    "ax.text(3, 1.2, 'Context\\nVector', ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Arrows from all encoder states to bottleneck\n",
    "for i in range(6):\n",
    "    ax.annotate('', xy=(3, 1.5), xytext=(i, 1.7),\n",
    "               arrowprops=dict(arrowstyle='->', color='gray', lw=1))\n",
    "\n",
    "# Draw decoder states\n",
    "for i in range(4):\n",
    "    ax.add_patch(plt.Rectangle((i + 1 - 0.3, -0.3), 0.6, 0.6, \n",
    "                               facecolor='orange', edgecolor='black', linewidth=1.5))\n",
    "    ax.text(i + 1, 0.0, f's{i}', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Arrow from bottleneck to decoder\n",
    "for i in range(4):\n",
    "    ax.annotate('', xy=(i + 1, 0.3), xytext=(3, 0.9),\n",
    "               arrowprops=dict(arrowstyle='->', color='red', lw=1.5))\n",
    "\n",
    "ax.set_xlim(-1, 6.5)\n",
    "ax.set_ylim(-1, 3)\n",
    "ax.set_title('Without Attention: Bottleneck', fontsize=13)\n",
    "ax.text(3, 2.8, 'Encoder', ha='center', fontsize=11, color='steelblue', fontweight='bold')\n",
    "ax.text(2.5, -0.8, 'Decoder', ha='center', fontsize=11, color='orange', fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "# Right: Attention (direct connections)\n",
    "ax = axes[1]\n",
    "\n",
    "# Draw encoder states\n",
    "for i in range(6):\n",
    "    ax.add_patch(plt.Rectangle((i - 0.3, 1.7), 0.6, 0.6, \n",
    "                               facecolor='steelblue', edgecolor='black', linewidth=1.5))\n",
    "    ax.text(i, 2.0, f'h{i}', ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "\n",
    "# Draw decoder states\n",
    "for i in range(4):\n",
    "    ax.add_patch(plt.Rectangle((i + 1 - 0.3, -0.3), 0.6, 0.6, \n",
    "                               facecolor='orange', edgecolor='black', linewidth=1.5))\n",
    "    ax.text(i + 1, 0.0, f's{i}', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Attention connections with varying thickness\n",
    "np.random.seed(42)\n",
    "for j in range(4):\n",
    "    weights = np.random.dirichlet(np.ones(6) * 0.5)  # Random attention weights\n",
    "    for i in range(6):\n",
    "        alpha = weights[i]\n",
    "        ax.annotate('', xy=(j + 1, 0.3), xytext=(i, 1.7),\n",
    "                   arrowprops=dict(arrowstyle='->', color='green', \n",
    "                                  lw=alpha * 5, alpha=max(alpha, 0.1)))\n",
    "\n",
    "ax.set_xlim(-1, 6.5)\n",
    "ax.set_ylim(-1, 3)\n",
    "ax.set_title('With Attention: Direct Access', fontsize=13)\n",
    "ax.text(3, 2.8, 'Encoder', ha='center', fontsize=11, color='steelblue', fontweight='bold')\n",
    "ax.text(2.5, -0.8, 'Decoder', ha='center', fontsize=11, color='orange', fontweight='bold')\n",
    "ax.text(3, 0.95, 'Attention\\nweights', ha='center', fontsize=9, color='green', fontstyle='italic')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wyanwr3ui3f",
   "source": "---\n\n## 2. Attention Intuition\n\n### The \"Spotlight\" Analogy\n\nImagine you're in a dark room full of objects. You have a flashlight (a **query**) and you're looking for something specific. As you sweep the beam around:\n\n- Each object in the room is a potential **value** (the information you might want)\n- Each object has a label describing it -- that's its **key**\n- Your flashlight beam (query) is compared against every label (key)\n- Objects whose labels **match** your query get brightly illuminated\n- You combine the information from all visible objects, weighted by how brightly they're lit\n\nThis is exactly how attention works: **a learned, differentiable spotlight**.\n\n### The Database Lookup Analogy\n\nAn even more precise analogy: attention is like a **soft database lookup**.\n\n| Database Concept | Attention Concept | Example | F1 Parallel |\n|-----------------|-------------------|---------|-------------|\n| Search query | **Query (Q)** | \"What adjective describes the subject?\" | \"What was the tire state at the last pit stop?\" |\n| Index/key in database | **Key (K)** | Each word's \"matchability\" representation | Each lap's \"type label\" (pit lap, push lap, safety car lap) |\n| Stored record | **Value (V)** | Each word's content representation | Each lap's actual data (time, tire temp, fuel, gap) |\n| Exact match | Hard attention | Only look at one item | Look at exactly one lap |\n| Fuzzy/weighted match | **Soft attention** | Look at everything, weighted by relevance | Blend info from multiple laps, weighted by relevance |\n\n**What this means:** Instead of retrieving one exact match (like a SQL query), attention retrieves a **weighted combination** of all values, where the weights depend on how well each key matches the query.\n\n**F1 analogy:** Think of the query as the strategist asking: \"Which past laps are most relevant to predicting THIS lap's performance?\" Each past lap has a key (its characteristics: was it a push lap? safety car? post-pit?) and a value (its actual telemetry data). The attention weights tell you: \"lap 38 is 40% relevant, lap 1 is 25% relevant, lap 44 is 20% relevant, everything else shares the remaining 15%.\" The output is a weighted blend of the most informative laps.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "7ldjx29hxho",
   "source": "### Visualization: Attention as Soft Lookup",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "s6zsj3beywa",
   "source": "# Visualize attention as a soft database lookup\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: Hard lookup (traditional database)\nax = axes[0]\nkeys = ['cat', 'dog', 'fish', 'bird', 'cat']\nvalues = [0.9, 0.1, 0.2, 0.3, 0.8]\nquery = 'cat'\nmatches = [1 if k == query else 0 for k in keys]\n\ncolors = ['green' if m else 'lightgray' for m in matches]\nbars = ax.bar(range(len(keys)), matches, color=colors, edgecolor='black', linewidth=1.5)\nax.set_xticks(range(len(keys)))\nax.set_xticklabels([f'Key: {k}\\nVal: {v}' for k, v in zip(keys, values)], fontsize=9)\nax.set_ylabel('Match Weight')\nax.set_title(f'Hard Lookup: Query = \"{query}\"\\n(exact match only)', fontsize=12)\nax.set_ylim(0, 1.3)\nax.text(2, 1.15, 'Result: average of matching values', ha='center', fontsize=10, fontstyle='italic')\nax.grid(True, alpha=0.3)\n\n# Right: Soft attention lookup\nax = axes[1]\n# Simulated attention weights (soft matching)\nattention_weights = np.array([0.35, 0.05, 0.08, 0.12, 0.40])\ncolors_soft = plt.cm.Greens(attention_weights / attention_weights.max())\n\nbars = ax.bar(range(len(keys)), attention_weights, color=colors_soft, edgecolor='black', linewidth=1.5)\nax.set_xticks(range(len(keys)))\nax.set_xticklabels([f'Key: {k}\\nVal: {v}' for k, v in zip(keys, values)], fontsize=9)\nax.set_ylabel('Attention Weight')\nax.set_title(f'Soft Attention: Query = \"{query}\"\\n(weighted combination)', fontsize=12)\nax.set_ylim(0, 0.6)\n\nweighted_result = sum(w * v for w, v in zip(attention_weights, values))\nax.text(2, 0.52, f'Result: weighted sum = {weighted_result:.3f}', ha='center', fontsize=10, fontstyle='italic')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Hard lookup: Returns exact matches only (not differentiable)\")\nprint(\"Soft attention: Returns weighted combination of ALL values (differentiable!)\")\nprint(\"This differentiability is what makes attention trainable with backpropagation.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "pwd7e7jejg",
   "source": "### Example: Translation Attention\n\nConsider translating \"The black cat sat on the mat\" to French. When generating each French word, the model should attend to different source words:\n\n- Generating \"Le\" (The) --> attend to \"The\"\n- Generating \"chat\" (cat) --> attend to \"cat\"  \n- Generating \"noir\" (black) --> attend to \"black\"\n- Generating \"assis\" (sat) --> attend to \"sat\"\n\nNotice how **word order changes** between languages! The adjective \"black\" comes before \"cat\" in English but \"noir\" comes after \"chat\" in French. Attention handles this naturally by letting the decoder look at any position.\n\n**F1 analogy:** This is like translating one team's raw telemetry into another team's format. The \"channels\" may be in a different order, some may be combined differently, and the sampling rates may differ. Attention lets the translation model look at any source channel at any time, regardless of order mismatches -- just as a strategist can pull any piece of data from the lap chart at any moment.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "zft0xwkzve",
   "source": "# Visualize translation attention pattern\nsource = ['The', 'black', 'cat', 'sat', 'on', 'the', 'mat']\ntarget = ['Le', 'chat', 'noir', 'etait', 'assis', 'sur', 'le', 'tapis']\n\n# Simulated attention weights (which source words each target word attends to)\nattention_matrix = np.array([\n    [0.85, 0.02, 0.03, 0.02, 0.02, 0.04, 0.02],  # Le -> The\n    [0.02, 0.05, 0.82, 0.03, 0.02, 0.03, 0.03],  # chat -> cat\n    [0.02, 0.80, 0.10, 0.02, 0.02, 0.02, 0.02],  # noir -> black\n    [0.02, 0.02, 0.05, 0.78, 0.05, 0.03, 0.05],  # etait -> sat\n    [0.02, 0.02, 0.05, 0.75, 0.08, 0.03, 0.05],  # assis -> sat\n    [0.02, 0.02, 0.02, 0.03, 0.78, 0.05, 0.08],  # sur -> on\n    [0.03, 0.02, 0.02, 0.02, 0.03, 0.80, 0.08],  # le -> the\n    [0.02, 0.02, 0.02, 0.03, 0.05, 0.06, 0.80],  # tapis -> mat\n])\n\nfig, ax = plt.subplots(figsize=(10, 8))\nim = ax.imshow(attention_matrix, cmap='Blues', aspect='auto', vmin=0, vmax=1)\n\nax.set_xticks(range(len(source)))\nax.set_xticklabels(source, fontsize=12, fontweight='bold')\nax.set_yticks(range(len(target)))\nax.set_yticklabels(target, fontsize=12, fontweight='bold')\nax.set_xlabel('Source (English)', fontsize=13)\nax.set_ylabel('Target (French)', fontsize=13)\nax.set_title('Attention Weights in Translation\\nBrighter = More Attention', fontsize=14)\n\n# Add text annotations\nfor i in range(len(target)):\n    for j in range(len(source)):\n        val = attention_matrix[i, j]\n        color = 'white' if val > 0.5 else 'black'\n        ax.text(j, i, f'{val:.2f}', ha='center', va='center', fontsize=8, color=color)\n\nplt.colorbar(im, label='Attention Weight')\nplt.tight_layout()\nplt.show()\n\nprint('Notice: \"noir\" (row 3) attends to \"black\" (col 2), NOT \"cat\" (col 3)')\nprint('Attention naturally handles word reordering between languages!')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "kv5pleax9yr",
   "source": "---\n\n## 3. Dot-Product Attention\n\n### Intuitive Explanation\n\nNow let's make the \"soft lookup\" concrete. The simplest form of attention uses the **dot product** to measure how well a query matches each key. Remember from linear algebra: the dot product measures similarity between two vectors.\n\nThe three-step recipe:\n\n1. **Score:** Compute similarity between query and each key using dot product\n2. **Normalize:** Pass scores through softmax to get weights that sum to 1\n3. **Aggregate:** Compute weighted sum of values using those weights\n\n$$\\text{Attention}(q, K, V) = \\text{softmax}(q \\cdot K^T) \\cdot V$$\n\n#### Breaking down the formula:\n\n| Component | Shape | Meaning | F1 Parallel |\n|-----------|-------|---------|-------------|\n| $q$ | $(d_k,)$ | The query vector -- \"what am I looking for?\" | \"I need laps with high tire degradation\" |\n| $K$ | $(n, d_k)$ | Matrix of key vectors -- one per position | Each lap's characteristic signature |\n| $V$ | $(n, d_v)$ | Matrix of value vectors -- the information to retrieve | Each lap's actual telemetry data |\n| $q \\cdot K^T$ | $(n,)$ | Similarity scores (one per key) | How relevant each past lap is to the current query |\n| $\\text{softmax}(\\cdot)$ | $(n,)$ | Attention weights (sum to 1) | Normalized relevance: \"60% from lap 38, 25% from lap 1...\" |\n| Output | $(d_v,)$ | Weighted combination of values | Blended telemetry from the most relevant laps |\n\n**What this means:** The dot product $q \\cdot k_i$ is large when the query and key point in the same direction (similar), and small (or negative) when they point in different directions. Softmax converts these raw scores into a probability distribution.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "qhsfkmzgfzf",
   "source": "### Step-by-Step Calculation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "aqa9tjc2de",
   "source": "# Step-by-step dot-product attention with a small example\nprint(\"=\" * 60)\nprint(\"DOT-PRODUCT ATTENTION: Step by Step\")\nprint(\"=\" * 60)\n\n# Small example: d_k = 3, n = 4 keys/values\nd_k = 3\n\n# Query: \"What am I looking for?\"\nquery = np.array([1.0, 0.5, 0.0])\nprint(f\"\\nQuery vector: {query}\")\n\n# Keys: each position has a key vector\nkeys = np.array([\n    [1.0, 0.0, 0.0],   # Key 0: mostly dimension 0\n    [0.0, 1.0, 0.0],   # Key 1: mostly dimension 1\n    [0.8, 0.6, 0.0],   # Key 2: similar to query!\n    [0.0, 0.0, 1.0],   # Key 3: mostly dimension 2\n])\nprint(f\"\\nKeys (4 x {d_k}):\")\nfor i, k in enumerate(keys):\n    print(f\"  Key {i}: {k}\")\n\n# Values: the actual information to retrieve\nvalues = np.array([\n    [10, 0],\n    [0, 10],\n    [5, 5],\n    [0, 0],\n])\nprint(f\"\\nValues (4 x 2):\")\nfor i, v in enumerate(values):\n    print(f\"  Value {i}: {v}\")\n\n# STEP 1: Compute scores (dot products)\nprint(f\"\\n{'='*60}\")\nprint(\"STEP 1: Compute scores = query . key_i\")\nprint(\"=\" * 60)\nscores = keys @ query  # Same as np.dot(keys, query)\nfor i in range(len(keys)):\n    dot_detail = \" + \".join([f\"{query[j]:.1f}*{keys[i,j]:.1f}\" for j in range(d_k)])\n    print(f\"  score[{i}] = {dot_detail} = {scores[i]:.2f}\")\n\n# STEP 2: Apply softmax\nprint(f\"\\n{'='*60}\")\nprint(\"STEP 2: Attention weights = softmax(scores)\")\nprint(\"=\" * 60)\nexp_scores = np.exp(scores - scores.max())  # Numerically stable softmax\nweights = exp_scores / exp_scores.sum()\nfor i in range(len(weights)):\n    print(f\"  weight[{i}] = {weights[i]:.4f}\")\nprint(f\"  Sum of weights: {weights.sum():.4f} (always sums to 1)\")\n\n# STEP 3: Weighted sum of values\nprint(f\"\\n{'='*60}\")\nprint(\"STEP 3: Output = weighted sum of values\")\nprint(\"=\" * 60)\noutput = weights @ values\nfor i in range(len(weights)):\n    print(f\"  {weights[i]:.4f} * {values[i]} = {weights[i] * values[i]}\")\nprint(f\"  Output = {output}\")\nprint(f\"\\nThe output is closest to Value 2 [5,5] because Key 2 was most similar to the Query!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "yyp5apb3y4",
   "source": "### Visualization: Attention Weights as Heatmap",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "o01p3u3wnf",
   "source": "# Visualize the attention computation we just did\nfig, axes = plt.subplots(1, 4, figsize=(16, 4), \n                         gridspec_kw={'width_ratios': [1, 1, 0.5, 1]})\n\n# Plot 1: Query vs Keys similarity\nax = axes[0]\nsimilarity = keys @ query\ncolors = plt.cm.Greens(similarity / similarity.max())\nax.barh(range(len(keys)), similarity, color=colors, edgecolor='black')\nax.set_yticks(range(len(keys)))\nax.set_yticklabels([f'Key {i}' for i in range(len(keys))])\nax.set_xlabel('Dot Product Score')\nax.set_title('Step 1: Scores\\n(query . key)')\nax.invert_yaxis()\nax.grid(True, alpha=0.3)\n\n# Plot 2: Attention weights (after softmax)\nax = axes[1]\ncolors_w = plt.cm.Greens(weights / weights.max())\nax.barh(range(len(weights)), weights, color=colors_w, edgecolor='black')\nax.set_yticks(range(len(weights)))\nax.set_yticklabels([f'w[{i}]={weights[i]:.3f}' for i in range(len(weights))])\nax.set_xlabel('Attention Weight')\nax.set_title('Step 2: Softmax\\n(normalize scores)')\nax.invert_yaxis()\nax.grid(True, alpha=0.3)\n\n# Plot 3: Arrow\nax = axes[2]\nax.annotate('', xy=(0.8, 0.5), xytext=(0.2, 0.5),\n           arrowprops=dict(arrowstyle='->', lw=3, color='black'))\nax.text(0.5, 0.65, 'Weighted\\nSum', ha='center', va='center', fontsize=11, fontweight='bold')\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.axis('off')\n\n# Plot 4: Output\nax = axes[3]\nax.bar(['dim 0', 'dim 1'], output, color=['steelblue', 'orange'], edgecolor='black', linewidth=1.5)\nax.set_ylabel('Value')\nax.set_title(f'Step 3: Output\\n= [{output[0]:.2f}, {output[1]:.2f}]')\nax.grid(True, alpha=0.3)\n\nplt.suptitle('Dot-Product Attention: Complete Pipeline', fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "umusvr974y",
   "source": "---\n\n## 4. Scaled Dot-Product Attention\n\n### Intuitive Explanation\n\nThere's a subtle problem with plain dot-product attention: as the dimension $d_k$ grows, dot products get **larger in magnitude**. Why? Each element of the dot product adds a term, so with more dimensions, the sum grows.\n\nWhen dot products are large, softmax produces outputs that are very close to one-hot vectors (nearly all the weight on one key). This means:\n- Gradients become tiny (softmax saturation)\n- The model can't learn to spread attention across multiple keys\n- Training becomes unstable\n\n**The fix:** Divide by $\\sqrt{d_k}$ to keep the variance of scores roughly constant regardless of dimension.\n\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n\n#### Breaking down the formula:\n\n| Component | Meaning | Why It's There | F1 Parallel |\n|-----------|---------|----------------|-------------|\n| $Q$ | Query matrix $(n_q, d_k)$ | What we're looking for | Current lap's questions about the race history |\n| $K$ | Key matrix $(n_k, d_k)$ | What's available to match | Each past lap's matchable signature |\n| $V$ | Value matrix $(n_k, d_v)$ | Information to retrieve | Each past lap's actual data |\n| $QK^T$ | Score matrix $(n_q, n_k)$ | Raw similarities | Raw relevance of every past lap to every query |\n| $\\sqrt{d_k}$ | Scaling factor | Keeps scores in good range for softmax | Prevents the model from being overconfident about one lap |\n| softmax | Row-wise normalization | Convert scores to weights | Turn raw scores into a proper \"attention budget\" |\n\n**What this means:** The $\\sqrt{d_k}$ scaling is a simple but critical trick. Without it, attention in high dimensions would collapse to hard attention (looking at only one position), losing the benefit of soft weighting.\n\n**F1 analogy:** Without scaling, the model becomes an overconfident strategist who only looks at the single most similar past lap and ignores everything else. With scaling, the model blends information from several relevant laps -- which is almost always a better strategy.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "ic1ugyqvmz",
   "source": "### Visualization: Why Scaling Matters",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "366fhdin3ay",
   "source": "# Show how dot product magnitude grows with dimension\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Left: Dot product magnitude vs dimension\nax = axes[0]\ndims = [4, 16, 64, 128, 256, 512]\ndot_stds = []\n\nfor d in dims:\n    # Random unit-variance vectors\n    q = np.random.randn(1000, d)\n    k = np.random.randn(1000, d)\n    dots = np.sum(q * k, axis=1)\n    dot_stds.append(dots.std())\n\nax.plot(dims, dot_stds, 'bo-', linewidth=2, markersize=8)\nax.plot(dims, [np.sqrt(d) for d in dims], 'r--', linewidth=2, label=r'$\\sqrt{d_k}$')\nax.set_xlabel('Dimension $d_k$')\nax.set_ylabel('Std of dot products')\nax.set_title('Dot Product Magnitude\\nGrows with Dimension')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Middle: Softmax on unscaled scores\nax = axes[1]\nnp.random.seed(42)\nfor d_k in [4, 64, 512]:\n    q = np.random.randn(d_k)\n    K = np.random.randn(8, d_k)\n    scores = K @ q  # Unscaled\n    weights = np.exp(scores - scores.max()) / np.exp(scores - scores.max()).sum()\n    ax.plot(range(8), sorted(weights, reverse=True), 'o-', \n            label=f'd_k={d_k}', linewidth=2, markersize=6)\n\nax.set_xlabel('Key index (sorted)')\nax.set_ylabel('Attention weight')\nax.set_title('WITHOUT Scaling\\n(higher dim = more peaked)')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Right: Softmax on scaled scores\nax = axes[2]\nnp.random.seed(42)\nfor d_k in [4, 64, 512]:\n    q = np.random.randn(d_k)\n    K = np.random.randn(8, d_k)\n    scores = (K @ q) / np.sqrt(d_k)  # Scaled!\n    weights = np.exp(scores - scores.max()) / np.exp(scores - scores.max()).sum()\n    ax.plot(range(8), sorted(weights, reverse=True), 'o-', \n            label=f'd_k={d_k}', linewidth=2, markersize=6)\n\nax.set_xlabel('Key index (sorted)')\nax.set_ylabel('Attention weight')\nax.set_title('WITH Scaling (/$\\\\sqrt{d_k}$)\\n(consistent regardless of dim)')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Without scaling: high dimensions -> almost one-hot attention (bad for learning)\")\nprint(\"With scaling: attention distribution is consistent across dimensions (good!)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6mh5dwb30ku",
   "source": "### Implement Scaled Dot-Product Attention from Scratch",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "8m2fy9zwacs",
   "source": "def scaled_dot_product_attention(Q, K, V, mask=None):\n    \"\"\"\n    Compute scaled dot-product attention.\n    \n    Args:\n        Q: Query tensor (..., seq_len_q, d_k)\n        K: Key tensor (..., seq_len_k, d_k)\n        V: Value tensor (..., seq_len_k, d_v)\n        mask: Optional mask tensor (broadcastable to score shape)\n    \n    Returns:\n        output: Weighted sum of values (..., seq_len_q, d_v)\n        weights: Attention weights (..., seq_len_q, seq_len_k)\n    \"\"\"\n    d_k = Q.shape[-1]\n    \n    # Step 1: Compute scaled scores\n    scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n    \n    # Step 2: Apply mask (if provided) - set masked positions to -inf\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, float('-inf'))\n    \n    # Step 3: Softmax to get weights\n    weights = F.softmax(scores, dim=-1)\n    \n    # Step 4: Weighted sum of values\n    output = torch.matmul(weights, V)\n    \n    return output, weights\n\n# Test it!\ntorch.manual_seed(42)\n\n# Batch of 1, sequence length 5, dimension 4\nseq_len, d_k, d_v = 5, 4, 4\nQ = torch.randn(1, seq_len, d_k)\nK = torch.randn(1, seq_len, d_k)\nV = torch.randn(1, seq_len, d_v)\n\noutput, weights = scaled_dot_product_attention(Q, K, V)\n\nprint(f\"Q shape: {Q.shape}\")\nprint(f\"K shape: {K.shape}\")\nprint(f\"V shape: {V.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Weights shape: {weights.shape}\")\nprint(f\"\\nAttention weights (each row sums to 1):\")\nprint(weights[0].detach().numpy().round(3))\nprint(f\"Row sums: {weights[0].sum(dim=-1).detach().numpy().round(4)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9vi8ydi7ag6",
   "source": "---\n\n## 5. Self-Attention\n\n### Intuitive Explanation\n\nIn the examples so far, queries came from one sequence (decoder) and keys/values from another (encoder). **Self-attention** is the special case where queries, keys, and values all come from the **same sequence**.\n\n**Key insight:** Self-attention lets each position in a sequence \"look at\" every other position in the same sequence. This is how a model understands context:\n\n- In \"The animal didn't cross the street because **it** was too tired\" -- the word \"it\" needs to attend to \"animal\" to understand the sentence\n- In \"The animal didn't cross the street because **it** was too wide\" -- now \"it\" should attend to \"street\"\n\nSelf-attention allows the model to figure out these relationships by learning what to attend to.\n\n**F1 analogy:** Self-attention is each lap attending to every other lap in the same race. Lap 45 can directly \"look at\" lap 1, lap 20, and lap 44 to understand its own context. Did a safety car on lap 20 bunch up the field? Did a pit stop on lap 38 change the tire compound? Did a rain shower on lap 1 affect track evolution? Each lap gathers context from whichever other laps are most relevant to understanding itself.\n\n### How Self-Attention Works\n\nGiven an input sequence $X$ of shape $(n, d_{model})$:\n\n1. **Project** $X$ into three different spaces using learned weight matrices:\n   - $Q = XW^Q$ (queries)\n   - $K = XW^K$ (keys) \n   - $V = XW^V$ (values)\n\n2. **Apply** scaled dot-product attention: $\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$\n\n| Step | What Happens | Analogy | F1 Parallel |\n|------|-------------|---------|-------------|\n| Compute Q | Each position asks \"what am I looking for?\" | Formulating a question | Lap 45 asks: \"which past laps had similar tire conditions?\" |\n| Compute K | Each position broadcasts \"here's what I offer\" | Creating a matchable label | Each lap broadcasts: \"I'm a push lap on hard tires, lap 12 of stint\" |\n| Compute V | Each position says \"here's my content\" | Storing information | Each lap stores its full telemetry vector |\n| Q . K^T | Every position scores against every other | Checking all question-label pairs | Lap 45 scores every other lap's relevance |\n| Softmax | Normalize scores to weights | Decide how much to focus on each | Allocate attention budget across the race |\n| Weights . V | Gather relevant information | Retrieve and combine answers | Blend telemetry from the most relevant laps |\n\n**What this means:** The Q, K, V projections let the **same word** play different roles depending on context. \"Cat\" as a query asks \"who modifies me?\"; as a key it answers \"I'm a noun, an animal\"; as a value it provides its embedding content.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "x1mditahpn",
   "source": "### Visualization: Self-Attention in a Sentence",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "njhzpd6l3vp",
   "source": "# Visualize self-attention: which words attend to which in a sentence\nwords = ['The', 'cat', 'sat', 'on', 'the', 'mat']\nn = len(words)\n\n# Simulated self-attention weights (what each word attends to)\n# Designed to show realistic patterns\nself_attn = np.array([\n    [0.30, 0.40, 0.05, 0.05, 0.10, 0.10],  # \"The\" -> attends to \"cat\" (its noun)\n    [0.15, 0.20, 0.35, 0.05, 0.05, 0.20],  # \"cat\" -> attends to \"sat\" (its verb) and \"mat\"\n    [0.05, 0.45, 0.15, 0.10, 0.05, 0.20],  # \"sat\" -> attends to \"cat\" (subject)\n    [0.05, 0.05, 0.30, 0.10, 0.10, 0.40],  # \"on\" -> attends to \"sat\" and \"mat\"\n    [0.05, 0.05, 0.05, 0.05, 0.30, 0.50],  # \"the\" -> attends to \"mat\" (its noun)\n    [0.05, 0.15, 0.20, 0.25, 0.10, 0.25],  # \"mat\" -> attends to \"sat\", \"on\", itself\n])\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: Heatmap\nax = axes[0]\nim = ax.imshow(self_attn, cmap='Blues', aspect='auto', vmin=0, vmax=0.5)\nax.set_xticks(range(n))\nax.set_xticklabels(words, fontsize=12, fontweight='bold')\nax.set_yticks(range(n))\nax.set_yticklabels(words, fontsize=12, fontweight='bold')\nax.set_xlabel('Attending TO (keys)', fontsize=12)\nax.set_ylabel('Attending FROM (queries)', fontsize=12)\nax.set_title('Self-Attention Weights', fontsize=13)\nfor i in range(n):\n    for j in range(n):\n        val = self_attn[i, j]\n        color = 'white' if val > 0.3 else 'black'\n        ax.text(j, i, f'{val:.2f}', ha='center', va='center', fontsize=9, color=color)\nplt.colorbar(im, ax=ax)\n\n# Right: Attention as arrows for a specific word\nax = axes[1]\nfocus_word_idx = 2  # \"sat\"\nfocus_weights = self_attn[focus_word_idx]\n\n# Position words horizontally\nx_positions = np.linspace(0, 5, n)\ny_base = 0.5\n\nfor i, (word, pos) in enumerate(zip(words, x_positions)):\n    fontsize = 14 + focus_weights[i] * 20  # Bigger = more attention\n    alpha = 0.3 + focus_weights[i] * 1.5\n    color = 'green' if i == focus_word_idx else 'steelblue'\n    ax.text(pos, y_base, word, fontsize=fontsize, ha='center', va='center',\n            fontweight='bold', alpha=min(alpha, 1.0), color=color,\n            bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow' if i == focus_word_idx else 'lightblue',\n                      alpha=min(alpha, 0.8), edgecolor='gray'))\n    \n    # Draw attention arrows from focus word\n    if i != focus_word_idx:\n        ax.annotate('', xy=(pos, y_base + 0.15), xytext=(x_positions[focus_word_idx], y_base + 0.15),\n                   arrowprops=dict(arrowstyle='->', color='red', \n                                  lw=focus_weights[i] * 8, alpha=focus_weights[i] * 2))\n        ax.text(pos, y_base + 0.3, f'{focus_weights[i]:.2f}', \n                ha='center', fontsize=9, color='red', alpha=max(focus_weights[i] * 2, 0.3))\n\nax.set_xlim(-0.5, 5.5)\nax.set_ylim(-0.2, 1.0)\nax.set_title(f'What does \"{words[focus_word_idx]}\" attend to?', fontsize=13)\nax.axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(f'The word \"{words[focus_word_idx]}\" attends most to \"{words[np.argmax(focus_weights)]}\"')\nprint(\"This is how self-attention captures syntactic relationships!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "i67jzmvzal",
   "source": "### Implement Self-Attention from Scratch",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "uwea45dhyc",
   "source": "class SelfAttention(nn.Module):\n    \"\"\"\n    Self-attention layer.\n    \n    Takes a sequence and lets each position attend to all positions\n    in the same sequence using learned Q, K, V projections.\n    \"\"\"\n    def __init__(self, d_model, d_k=None, d_v=None):\n        \"\"\"\n        Args:\n            d_model: Input/output dimension\n            d_k: Key/query dimension (defaults to d_model)\n            d_v: Value dimension (defaults to d_model)\n        \"\"\"\n        super().__init__()\n        d_k = d_k or d_model\n        d_v = d_v or d_model\n        \n        self.d_k = d_k\n        \n        # Learned projection matrices\n        self.W_q = nn.Linear(d_model, d_k, bias=False)  # Project to queries\n        self.W_k = nn.Linear(d_model, d_k, bias=False)  # Project to keys\n        self.W_v = nn.Linear(d_model, d_v, bias=False)  # Project to values\n    \n    def forward(self, x, mask=None):\n        \"\"\"\n        Args:\n            x: Input tensor (batch, seq_len, d_model)\n            mask: Optional attention mask\n            \n        Returns:\n            output: (batch, seq_len, d_v)\n            weights: (batch, seq_len, seq_len)\n        \"\"\"\n        # Step 1: Project input to Q, K, V\n        Q = self.W_q(x)  # (batch, seq_len, d_k)\n        K = self.W_k(x)  # (batch, seq_len, d_k)\n        V = self.W_v(x)  # (batch, seq_len, d_v)\n        \n        # Step 2: Compute scaled dot-product attention\n        output, weights = scaled_dot_product_attention(Q, K, V, mask)\n        \n        return output, weights\n\n# Test it!\ntorch.manual_seed(42)\n\nbatch_size, seq_len, d_model = 2, 6, 8\nx = torch.randn(batch_size, seq_len, d_model)\n\nself_attn_layer = SelfAttention(d_model=8, d_k=8, d_v=8)\noutput, weights = self_attn_layer(x)\n\nprint(f\"Input shape:  {x.shape}  (batch=2, seq=6, d_model=8)\")\nprint(f\"Output shape: {output.shape}  (same as input!)\")\nprint(f\"Weight shape: {weights.shape}  (batch=2, 6x6 attention matrix)\")\nprint(f\"\\nAttention weights for first sample (each row sums to 1):\")\nprint(weights[0].detach().numpy().round(3))\n\n# Count parameters\nn_params = sum(p.numel() for p in self_attn_layer.parameters())\nprint(f\"\\nTotal parameters: {n_params} (3 projection matrices of {d_model}x{8})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "he10zrjjh5n",
   "source": "### Deep Dive: Why Self-Attention Captures Long-Range Dependencies\n\nSelf-attention has a key advantage over RNNs: **any position can directly attend to any other position in one step**. In an RNN, information from position 1 must travel through every intermediate position to reach position 100. At each step, it gets processed and potentially distorted.\n\n| Property | RNN | Self-Attention |\n|----------|-----|----------------|\n| Max path length | O(n) | **O(1)** |\n| Computation per layer | O(n) | O(n^2) |\n| Parallelizable | No (sequential) | **Yes** |\n| Long-range dependencies | Difficult | **Easy** |\n\n**F1 analogy:** With an RNN, information about lap 1 has to pass through 49 intermediate laps to influence the prediction for lap 50 -- degrading at each step (the vanishing gradient problem from the previous notebook). With self-attention, lap 50 can directly \"look at\" lap 1 in a single step. It is as if the strategist has the full lap chart open and can jump to any lap instantly, rather than having to mentally replay the race forward lap by lap.\n\n#### Key Insight\n\nSelf-attention trades sequential processing for parallel processing. The O(n^2) cost is a limitation for very long sequences, but the ability to directly connect distant positions makes it far better at capturing long-range patterns.\n\n#### Common Misconceptions\n\n| Misconception | Reality |\n|---------------|---------|\n| \"Q, K, V are three different inputs\" | They're three different **projections** of the same input |\n| \"Self-attention knows word order\" | It doesn't! You need positional encoding (covered in the Transformer notebook) |\n| \"Self-attention replaces RNNs everywhere\" | For very long sequences, efficient attention variants are needed |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "clfp8cv4kct",
   "source": "---\n\n## 6. Multi-Head Attention\n\n### Intuitive Explanation\n\nA single attention head can only focus on one type of relationship at a time. But language has many types of relationships happening simultaneously:\n\n- **Syntactic:** \"cat\" relates to \"sat\" (subject-verb)\n- **Semantic:** \"cat\" relates to \"animal\" (meaning)\n- **Positional:** \"the\" relates to the next word (article-noun)\n- **Coreference:** \"it\" relates to \"cat\" (pronoun reference)\n\n**Multi-head attention** runs multiple attention operations in parallel, each with its own learned projections. Each \"head\" can learn to focus on a different type of relationship.\n\n**F1 analogy:** Multi-head attention is like having multiple specialists on the pit wall, each attending to a different aspect of the race simultaneously. Head 1 tracks **pace** (which laps had similar lap times?). Head 2 tracks **tire wear** (which laps had similar degradation rates?). Head 3 tracks **fuel load** (which laps had similar fuel levels?). Head 4 tracks **track evolution** (which laps had similar grip levels?). Each head independently decides what is relevant from its own perspective, and the results are combined into a rich, multi-faceted understanding of the current race state.\n\n$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W^O$$\n\nwhere $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$\n\n#### Breaking down the formula:\n\n| Component | Shape | Meaning | F1 Parallel |\n|-----------|-------|---------|-------------|\n| $h$ | scalar | Number of heads (typically 8 or 16) | Number of specialist engineers on the pit wall |\n| $W_i^Q, W_i^K$ | $(d_{model}, d_k)$ | Per-head query/key projections | Each specialist's \"lens\" for viewing the data |\n| $W_i^V$ | $(d_{model}, d_v)$ | Per-head value projection | What information each specialist extracts |\n| $d_k = d_v = d_{model}/h$ | scalar | Each head uses a fraction of the total dimension | Each specialist gets a share of the total bandwidth |\n| $W^O$ | $(hd_v, d_{model})$ | Output projection to combine heads | Strategy meeting: combine all specialist inputs into one decision |\n\n**What this means:** Instead of one big attention with $d_{model}$-dimensional Q/K/V, we run $h$ smaller attentions in parallel (each with $d_{model}/h$ dimensions), then concatenate and project. This costs the same as single-head attention but gives the model multiple \"perspectives.\"",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "etpd3foczzb",
   "source": "### Visualization: Different Heads Attend to Different Things",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "xdvr063b8d",
   "source": "# Visualize how different attention heads learn different patterns\nwords = ['The', 'cat', 'sat', 'on', 'the', 'mat']\nn = len(words)\n\n# Simulated attention patterns for 4 different heads\nheads = {\n    'Head 1: Positional\\n(next word)': np.array([\n        [0.1, 0.7, 0.05, 0.05, 0.05, 0.05],\n        [0.05, 0.1, 0.7, 0.05, 0.05, 0.05],\n        [0.05, 0.05, 0.1, 0.7, 0.05, 0.05],\n        [0.05, 0.05, 0.05, 0.1, 0.7, 0.05],\n        [0.05, 0.05, 0.05, 0.05, 0.1, 0.7],\n        [0.05, 0.05, 0.05, 0.05, 0.15, 0.7],\n    ]),\n    'Head 2: Syntactic\\n(subject-verb)': np.array([\n        [0.3, 0.5, 0.05, 0.05, 0.05, 0.05],\n        [0.1, 0.2, 0.5, 0.05, 0.05, 0.1],\n        [0.05, 0.6, 0.15, 0.05, 0.05, 0.1],\n        [0.05, 0.05, 0.5, 0.15, 0.05, 0.2],\n        [0.05, 0.05, 0.05, 0.05, 0.3, 0.5],\n        [0.05, 0.1, 0.4, 0.2, 0.05, 0.2],\n    ]),\n    'Head 3: Semantic\\n(related nouns)': np.array([\n        [0.3, 0.2, 0.1, 0.1, 0.15, 0.15],\n        [0.05, 0.3, 0.1, 0.05, 0.05, 0.45],\n        [0.1, 0.1, 0.3, 0.1, 0.2, 0.2],\n        [0.1, 0.1, 0.1, 0.3, 0.2, 0.2],\n        [0.15, 0.1, 0.1, 0.1, 0.3, 0.25],\n        [0.05, 0.45, 0.1, 0.05, 0.05, 0.3],\n    ]),\n    'Head 4: Determiner\\n(article-noun)': np.array([\n        [0.2, 0.6, 0.05, 0.05, 0.05, 0.05],\n        [0.3, 0.3, 0.1, 0.1, 0.1, 0.1],\n        [0.1, 0.1, 0.3, 0.1, 0.2, 0.2],\n        [0.1, 0.1, 0.1, 0.3, 0.1, 0.3],\n        [0.05, 0.05, 0.05, 0.05, 0.2, 0.6],\n        [0.1, 0.1, 0.1, 0.1, 0.3, 0.3],\n    ]),\n}\n\nfig, axes = plt.subplots(1, 4, figsize=(18, 4))\n\nfor ax, (title, attn) in zip(axes, heads.items()):\n    im = ax.imshow(attn, cmap='Purples', aspect='auto', vmin=0, vmax=0.7)\n    ax.set_xticks(range(n))\n    ax.set_xticklabels(words, fontsize=9, rotation=45)\n    ax.set_yticks(range(n))\n    ax.set_yticklabels(words, fontsize=9)\n    ax.set_title(title, fontsize=10)\n\nplt.suptitle('Multi-Head Attention: Each Head Learns Different Patterns', fontsize=14, y=1.05)\nplt.tight_layout()\nplt.show()\n\nprint(\"Each head captures a different type of relationship:\")\nprint(\"  Head 1: Adjacent word patterns (local context)\")\nprint(\"  Head 2: Subject-verb agreement (syntactic structure)\")\nprint(\"  Head 3: Semantically related words (meaning)\")\nprint(\"  Head 4: Determiner-noun pairs (grammatical role)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "j0efr77cad",
   "source": "### Implement Multi-Head Attention from Scratch",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "4rboqfi6ji9",
   "source": "class MultiHeadAttention(nn.Module):\n    \"\"\"\n    Multi-head attention from scratch.\n    \n    Runs h parallel attention heads, each operating on d_model/h dimensions,\n    then concatenates and projects the results.\n    \"\"\"\n    def __init__(self, d_model, n_heads):\n        \"\"\"\n        Args:\n            d_model: Model dimension (must be divisible by n_heads)\n            n_heads: Number of attention heads\n        \"\"\"\n        super().__init__()\n        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n        \n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads  # Dimension per head\n        \n        # Single large projection matrices (more efficient than separate per-head)\n        self.W_q = nn.Linear(d_model, d_model, bias=False)\n        self.W_k = nn.Linear(d_model, d_model, bias=False)\n        self.W_v = nn.Linear(d_model, d_model, bias=False)\n        self.W_o = nn.Linear(d_model, d_model, bias=False)  # Output projection\n    \n    def forward(self, query, key, value, mask=None):\n        \"\"\"\n        Args:\n            query: (batch, seq_len_q, d_model)\n            key:   (batch, seq_len_k, d_model)\n            value: (batch, seq_len_k, d_model)\n            mask:  Optional mask\n            \n        Returns:\n            output: (batch, seq_len_q, d_model)\n            weights: (batch, n_heads, seq_len_q, seq_len_k)\n        \"\"\"\n        batch_size = query.shape[0]\n        \n        # Step 1: Project Q, K, V\n        Q = self.W_q(query)  # (batch, seq_q, d_model)\n        K = self.W_k(key)    # (batch, seq_k, d_model)\n        V = self.W_v(value)  # (batch, seq_k, d_model)\n        \n        # Step 2: Reshape to (batch, n_heads, seq_len, d_k)\n        # This splits d_model into n_heads separate d_k-dimensional spaces\n        Q = Q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n        K = K.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n        V = V.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n        \n        # Step 3: Apply scaled dot-product attention (works on last 2 dims)\n        output, weights = scaled_dot_product_attention(Q, K, V, mask)\n        # output: (batch, n_heads, seq_q, d_k)\n        # weights: (batch, n_heads, seq_q, seq_k)\n        \n        # Step 4: Concatenate heads\n        # Transpose back and reshape: (batch, seq_q, n_heads * d_k) = (batch, seq_q, d_model)\n        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        \n        # Step 5: Final linear projection\n        output = self.W_o(output)\n        \n        return output, weights\n\n# Test it!\ntorch.manual_seed(42)\n\nd_model, n_heads = 32, 4\nmha = MultiHeadAttention(d_model=d_model, n_heads=n_heads)\n\nbatch_size, seq_len = 2, 8\nx = torch.randn(batch_size, seq_len, d_model)\n\n# Self-attention: Q=K=V=x\noutput, weights = mha(x, x, x)\n\nprint(f\"Input shape:   {x.shape}\")\nprint(f\"Output shape:  {output.shape}  (same as input)\")\nprint(f\"Weights shape: {weights.shape}  (batch, heads, seq_q, seq_k)\")\nprint(f\"\\nEach head has its own {seq_len}x{seq_len} attention matrix\")\nprint(f\"Head dimension: d_k = d_model/n_heads = {d_model}/{n_heads} = {d_model//n_heads}\")\nprint(f\"\\nTotal parameters: {sum(p.numel() for p in mha.parameters())}\")\nprint(f\"  = 4 matrices of {d_model}x{d_model} = {4 * d_model * d_model}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ysrplit15ch",
   "source": "# Visualize the attention weights from each head\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\n\nfor head_idx in range(n_heads):\n    ax = axes[head_idx]\n    head_weights = weights[0, head_idx].detach().numpy()  # First sample\n    im = ax.imshow(head_weights, cmap='Blues', aspect='auto', vmin=0, vmax=0.5)\n    ax.set_xlabel('Key position')\n    ax.set_ylabel('Query position')\n    ax.set_title(f'Head {head_idx + 1}')\n\nplt.suptitle('Multi-Head Attention: Each Head Learns Different Patterns', fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"Even with random initialization, the heads already show different attention patterns.\")\nprint(\"After training, these differences become much more pronounced.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "vx267k92o68",
   "source": "### Interactive Exploration: Number of Heads\n\n**F1 analogy:** As you increase the number of heads, each specialist gets a narrower slice of the total bandwidth. With 1 head, one generalist sees everything in full resolution. With 64 heads, each specialist works with a single dimension -- hyper-specialized but potentially too narrow. The sweet spot depends on the task, just as the optimal pit wall staffing depends on the complexity of the race scenario.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "weqvj50jnv",
   "source": "# Explore: how does the number of heads affect the model?\nd_model = 64\nhead_configs = [1, 2, 4, 8, 16, 64]\n\nprint(f\"d_model = {d_model}\")\nprint(f\"{'Heads':>6} | {'d_k (per head)':>14} | {'Total Params':>12} | {'Same Total?':>11}\")\nprint(\"-\" * 55)\n\nfor n_h in head_configs:\n    d_k = d_model // n_h\n    # 4 matrices of d_model x d_model (Q, K, V, O projections)\n    total_params = 4 * d_model * d_model\n    print(f\"{n_h:>6} | {d_k:>14} | {total_params:>12} | {'Yes':>11}\")\n\nprint(f\"\\nKey insight: The number of heads does NOT change the parameter count!\")\nprint(f\"More heads = smaller per-head dimension, but same total computation.\")\nprint(f\"Typical choice: 8-16 heads for d_model=512-1024\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "lq14zb17ene",
   "source": "---\n\n## 7. Attention Variants\n\n### Overview\n\nThe scaled dot-product attention we've been studying is not the only type. Different attention mechanisms compute the query-key similarity score differently.\n\n### 7.1 Additive (Bahdanau) Attention\n\nThe original attention mechanism (2014). Uses a small neural network to compute scores:\n\n$$\\text{score}(q, k) = v^T \\tanh(W_1 q + W_2 k)$$\n\n**Intuition:** Instead of assuming that similarity is measured by dot product, learn a small network that determines how well a query matches a key.\n\n### 7.2 Multiplicative (Luong) Attention\n\nA simpler approach using a learned weight matrix:\n\n$$\\text{score}(q, k) = q^T W k$$\n\n**Intuition:** Like dot-product attention, but with a learned transformation in between. This lets the model learn which dimensions of the query should match which dimensions of the key.\n\n### 7.3 Dot-Product / Scaled Dot-Product\n\nWhat we've already studied:\n\n$$\\text{score}(q, k) = \\frac{q \\cdot k}{\\sqrt{d_k}}$$\n\n### Comparison Table\n\n| Attention Type | Score Function | Pros | Cons | Used In | F1 Parallel |\n|----------------|---------------|------|------|---------|-------------|\n| **Additive (Bahdanau)** | $v^T \\tanh(W_1 q + W_2 k)$ | Flexible, works well for different Q/K dims | Slower (MLP forward pass) | Original seq2seq attention | A neural network learns complex lap-to-lap relevance |\n| **Multiplicative (Luong)** | $q^T W k$ | Learns cross-dim interactions | Extra parameters | Seq2seq variants | Learns which telemetry channels should match which |\n| **Dot-Product** | $q \\cdot k$ | Fastest, no extra params | Assumes Q and K in same space | Simple models | Direct similarity between lap profiles |\n| **Scaled Dot-Product** | $\\frac{q \\cdot k}{\\sqrt{d_k}}$ | Fast + stable gradients | Assumes Q and K in same space | **Transformers** | Stable lap comparison at any telemetry resolution |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "f4osh4pp7i9",
   "source": "# Implement all three attention scoring functions\nclass AdditiveAttention(nn.Module):\n    \"\"\"Bahdanau (additive) attention.\"\"\"\n    def __init__(self, d_query, d_key, d_hidden):\n        super().__init__()\n        self.W1 = nn.Linear(d_query, d_hidden, bias=False)\n        self.W2 = nn.Linear(d_key, d_hidden, bias=False)\n        self.v = nn.Linear(d_hidden, 1, bias=False)\n    \n    def forward(self, query, keys, values):\n        # query: (batch, 1, d_query), keys: (batch, seq, d_key)\n        scores = self.v(torch.tanh(self.W1(query) + self.W2(keys)))  # (batch, seq, 1)\n        scores = scores.squeeze(-1)  # (batch, seq)\n        weights = F.softmax(scores, dim=-1)  # (batch, seq)\n        output = torch.bmm(weights.unsqueeze(1), values)  # (batch, 1, d_v)\n        return output.squeeze(1), weights\n\nclass MultiplicativeAttention(nn.Module):\n    \"\"\"Luong (multiplicative) attention.\"\"\"\n    def __init__(self, d_query, d_key):\n        super().__init__()\n        self.W = nn.Linear(d_key, d_query, bias=False)\n    \n    def forward(self, query, keys, values):\n        # query: (batch, 1, d_query), keys: (batch, seq, d_key)\n        transformed_keys = self.W(keys)  # (batch, seq, d_query)\n        scores = torch.bmm(query, transformed_keys.transpose(1, 2))  # (batch, 1, seq)\n        scores = scores.squeeze(1)  # (batch, seq)\n        weights = F.softmax(scores, dim=-1)\n        output = torch.bmm(weights.unsqueeze(1), values)\n        return output.squeeze(1), weights\n\n# Compare the three attention types\ntorch.manual_seed(42)\nbatch, seq_len, d = 1, 6, 8\n\nquery = torch.randn(batch, 1, d)\nkeys = torch.randn(batch, seq_len, d)\nvalues = torch.randn(batch, seq_len, d)\n\nadditive = AdditiveAttention(d, d, d)\nmultiplicative = MultiplicativeAttention(d, d)\n\nout_add, w_add = additive(query, keys, values)\nout_mul, w_mul = multiplicative(query, keys, values)\n\n# Scaled dot-product for comparison\nscores_dot = torch.bmm(query, keys.transpose(1, 2)) / np.sqrt(d)\nw_dot = F.softmax(scores_dot.squeeze(1), dim=-1)\nout_dot = torch.bmm(w_dot.unsqueeze(1), values).squeeze(1)\n\n# Visualize weight distributions\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\nnames = ['Additive (Bahdanau)', 'Multiplicative (Luong)', 'Scaled Dot-Product']\nall_weights = [w_add[0].detach().numpy(), w_mul[0].detach().numpy(), w_dot[0].detach().numpy()]\ncolors = ['blue', 'green', 'red']\n\nfor ax, name, w, color in zip(axes, names, all_weights, colors):\n    ax.bar(range(seq_len), w, color=color, edgecolor='black', alpha=0.7)\n    ax.set_xlabel('Key Position')\n    ax.set_ylabel('Attention Weight')\n    ax.set_title(name)\n    ax.set_ylim(0, 0.5)\n    ax.grid(True, alpha=0.3)\n\nplt.suptitle('Attention Weight Distribution by Type\\n(same Q, K, V -- different scoring)', fontsize=13)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cia6hvg4mvq",
   "source": "### 7.4 Causal (Masked) Attention\n\nIn autoregressive models (like GPT), each position should only attend to **previous** positions -- it shouldn't be able to \"peek\" at future tokens it hasn't generated yet. We achieve this by masking out future positions with $-\\infty$ before softmax.\n\n**F1 analogy:** Causal masking is like real-time race prediction where you can only use data from laps that have already happened. When predicting lap 30, you cannot look ahead at laps 31-56 -- those have not occurred yet. This is the constraint that autoregressive models operate under, and it mirrors exactly how a strategist makes decisions during a live race.\n\n### 7.5 Cross-Attention vs Self-Attention\n\n| Type | Queries From | Keys/Values From | Use Case | F1 Parallel |\n|------|-------------|-----------------|----------|-------------|\n| **Self-Attention** | Same sequence | Same sequence | Understanding context within one sequence | Each lap attending to every other lap in the same race |\n| **Cross-Attention** | Target sequence | Source sequence | Connecting encoder and decoder (translation) | Comparing your car's laps against a rival's data |\n| **Causal Self-Attention** | Same sequence | Same sequence (past only) | Autoregressive generation (GPT) | Real-time strategy: only use data from completed laps |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "s0rme2hde3",
   "source": "# Demonstrate causal masking\ntorch.manual_seed(42)\nseq_len = 6\n\n# Create a causal mask: 1 = attend, 0 = mask out\ncausal_mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)\n\nQ = torch.randn(1, seq_len, 8)\nK = torch.randn(1, seq_len, 8)\nV = torch.randn(1, seq_len, 8)\n\n# Without mask (bidirectional)\n_, weights_full = scaled_dot_product_attention(Q, K, V)\n\n# With causal mask (autoregressive)\n_, weights_causal = scaled_dot_product_attention(Q, K, V, mask=causal_mask)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Mask\nax = axes[0]\nim = ax.imshow(causal_mask[0, 0].numpy(), cmap='Greens', aspect='auto')\nax.set_xlabel('Key Position')\nax.set_ylabel('Query Position')\nax.set_title('Causal Mask\\n(1=attend, 0=block)')\nfor i in range(seq_len):\n    for j in range(seq_len):\n        val = int(causal_mask[0, 0, i, j].item())\n        ax.text(j, i, str(val), ha='center', va='center', fontsize=12,\n               color='white' if val else 'black')\n\n# Full attention\nax = axes[1]\nim = ax.imshow(weights_full[0].detach().numpy(), cmap='Blues', aspect='auto', vmin=0, vmax=0.5)\nax.set_xlabel('Key Position')\nax.set_ylabel('Query Position')\nax.set_title('Full (Bidirectional) Attention\\n(can see everything)')\n\n# Causal attention\nax = axes[2]\nim = ax.imshow(weights_causal[0].detach().numpy(), cmap='Blues', aspect='auto', vmin=0, vmax=0.5)\nax.set_xlabel('Key Position')\nax.set_ylabel('Query Position')\nax.set_title('Causal (Masked) Attention\\n(can only see past)')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Causal attention: position i can only attend to positions 0, 1, ..., i\")\nprint(\"This prevents the model from cheating by looking at future tokens.\")\nprint(\"Used in GPT and all autoregressive language models.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ul2dclpxyhm",
   "source": "### Why This Matters in Machine Learning\n\n| Application | Attention Type Used | F1 Parallel |\n|-------------|-------------------|-------------|\n| Machine translation (BERT encoder) | Bidirectional self-attention | Post-race analysis: look at entire race both ways |\n| Text generation (GPT) | Causal self-attention | Live strategy: only see completed laps |\n| Translation decoder | Causal self-attention + cross-attention | Real-time comparison with rival car data |\n| Image recognition (ViT) | Bidirectional self-attention on patches | Scanning a track map where each patch is a corner |\n| Speech recognition | Self-attention + cross-attention | Decoding team radio with full audio context |\n| Protein structure (AlphaFold) | Multi-head self-attention | Multi-aspect telemetry analysis |\n\n---\n\n## 8. Practical Example: Sequence Reversal with Attention\n\n### Building a Complete Model\n\nLet's build a simple sequence-to-sequence model with attention and train it on a concrete task: **reversing a sequence of numbers**. This is simple enough to train quickly but complex enough to show how attention works.\n\nFor example: `[1, 3, 5, 7, 9]` --> `[9, 7, 5, 3, 1]`\n\nWithout attention, this task is hard because the decoder needs to remember the entire input. With attention, the decoder can simply look at the right position!\n\n**F1 analogy:** Think of this as taking the lap chart in chronological order and producing it in reverse -- last lap first. With attention, the model can look directly at whatever position it needs. When generating the first output (the last lap), it attends strongly to the last input position. This is a toy task, but it cleanly demonstrates how attention weights align with the structure of the problem.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "p069kkbunb",
   "source": "# Dataset: reverse sequences of integers\ndef generate_reversal_data(n_samples, seq_len, vocab_size=10):\n    \"\"\"\n    Generate sequence reversal pairs.\n    \n    Args:\n        n_samples: Number of training examples\n        seq_len: Length of each sequence\n        vocab_size: Number of distinct tokens (0 to vocab_size-1)\n    \n    Returns:\n        src: Source sequences (n_samples, seq_len)\n        tgt: Target sequences (n_samples, seq_len) - reversed source\n    \"\"\"\n    src = torch.randint(1, vocab_size, (n_samples, seq_len))  # 0 reserved for padding\n    tgt = src.flip(dims=[1])  # Reverse along sequence dimension\n    return src, tgt\n\n# Generate data\ntorch.manual_seed(42)\nseq_len = 8\nvocab_size = 10\nn_train = 5000\nn_test = 500\n\ntrain_src, train_tgt = generate_reversal_data(n_train, seq_len, vocab_size)\ntest_src, test_tgt = generate_reversal_data(n_test, seq_len, vocab_size)\n\nprint(\"Example pairs (source -> target):\")\nfor i in range(5):\n    print(f\"  {train_src[i].tolist()} -> {train_tgt[i].tolist()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "n68y65371fl",
   "source": "class Seq2SeqWithAttention(nn.Module):\n    \"\"\"\n    Simple sequence-to-sequence model with attention.\n    \n    Uses embeddings + self-attention (no RNN!) to process sequences.\n    This is a simplified \"transformer-style\" approach.\n    \"\"\"\n    def __init__(self, vocab_size, d_model=32, n_heads=4, n_layers=2):\n        \"\"\"\n        Args:\n            vocab_size: Size of token vocabulary\n            d_model: Embedding and model dimension\n            n_heads: Number of attention heads\n            n_layers: Number of self-attention layers\n        \"\"\"\n        super().__init__()\n        self.d_model = d_model\n        \n        # Token embeddings\n        self.src_embed = nn.Embedding(vocab_size, d_model)\n        self.tgt_embed = nn.Embedding(vocab_size, d_model)\n        \n        # Positional encoding (learned)\n        self.src_pos = nn.Embedding(50, d_model)\n        self.tgt_pos = nn.Embedding(50, d_model)\n        \n        # Encoder: self-attention layers\n        self.encoder_layers = nn.ModuleList([\n            MultiHeadAttention(d_model, n_heads) for _ in range(n_layers)\n        ])\n        self.encoder_norms = nn.ModuleList([\n            nn.LayerNorm(d_model) for _ in range(n_layers)\n        ])\n        \n        # Decoder: cross-attention to encoder output\n        self.cross_attention = MultiHeadAttention(d_model, n_heads)\n        self.cross_norm = nn.LayerNorm(d_model)\n        \n        # Output projection\n        self.output_proj = nn.Linear(d_model, vocab_size)\n        \n        # Store attention weights for visualization\n        self.attention_weights = None\n    \n    def encode(self, src):\n        \"\"\"Encode source sequence.\"\"\"\n        batch_size, seq_len = src.shape\n        positions = torch.arange(seq_len, device=src.device).unsqueeze(0)\n        \n        x = self.src_embed(src) + self.src_pos(positions)\n        \n        for attn, norm in zip(self.encoder_layers, self.encoder_norms):\n            residual = x\n            attn_out, _ = attn(x, x, x)\n            x = norm(residual + attn_out)\n        \n        return x\n    \n    def decode(self, tgt, encoder_output):\n        \"\"\"Decode target sequence with cross-attention to encoder.\"\"\"\n        batch_size, seq_len = tgt.shape\n        positions = torch.arange(seq_len, device=tgt.device).unsqueeze(0)\n        \n        x = self.tgt_embed(tgt) + self.tgt_pos(positions)\n        \n        # Cross-attention: target queries, encoder keys/values\n        residual = x\n        cross_out, self.attention_weights = self.cross_attention(x, encoder_output, encoder_output)\n        x = self.cross_norm(residual + cross_out)\n        \n        return self.output_proj(x)\n    \n    def forward(self, src, tgt):\n        \"\"\"\n        Args:\n            src: Source tokens (batch, src_len)\n            tgt: Target tokens (batch, tgt_len)\n        \n        Returns:\n            logits: (batch, tgt_len, vocab_size)\n        \"\"\"\n        encoder_output = self.encode(src)\n        logits = self.decode(tgt, encoder_output)\n        return logits\n\n# Create model\ntorch.manual_seed(42)\nmodel = Seq2SeqWithAttention(vocab_size=vocab_size, d_model=32, n_heads=4, n_layers=2)\nn_params = sum(p.numel() for p in model.parameters())\nprint(f\"Model architecture:\")\nprint(model)\nprint(f\"\\nTotal parameters: {n_params:,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7vw3r3xci72",
   "source": "# Training loop\nfrom torch.utils.data import TensorDataset, DataLoader\n\ntorch.manual_seed(42)\nmodel = Seq2SeqWithAttention(vocab_size=vocab_size, d_model=32, n_heads=4, n_layers=2)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\ntrain_dataset = TensorDataset(train_src, train_tgt)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\nlosses = []\naccuracies = []\n\nprint(\"Training sequence reversal model with attention...\")\nfor epoch in range(30):\n    model.train()\n    epoch_loss = 0\n    correct = 0\n    total = 0\n    \n    for src_batch, tgt_batch in train_loader:\n        optimizer.zero_grad()\n        \n        # Teacher forcing: feed true target as input\n        logits = model(src_batch, tgt_batch)\n        \n        # Compute loss\n        loss = criterion(logits.view(-1, vocab_size), tgt_batch.view(-1))\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item() * src_batch.size(0)\n        \n        # Compute accuracy\n        preds = logits.argmax(dim=-1)\n        correct += (preds == tgt_batch).all(dim=-1).sum().item()\n        total += src_batch.size(0)\n    \n    avg_loss = epoch_loss / n_train\n    accuracy = correct / total\n    losses.append(avg_loss)\n    accuracies.append(accuracy)\n    \n    if (epoch + 1) % 5 == 0:\n        print(f\"  Epoch {epoch+1:3d}: loss={avg_loss:.4f}, seq_accuracy={accuracy:.4f}\")\n\n# Test accuracy\nmodel.eval()\nwith torch.no_grad():\n    test_logits = model(test_src, test_tgt)\n    test_preds = test_logits.argmax(dim=-1)\n    test_acc = (test_preds == test_tgt).all(dim=-1).float().mean()\n    print(f\"\\nTest accuracy (full sequence correct): {test_acc:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "jkdog9rnmip",
   "source": "# Plot training curves\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nax = axes[0]\nax.plot(losses, 'b-', linewidth=2)\nax.set_xlabel('Epoch')\nax.set_ylabel('Loss')\nax.set_title('Training Loss')\nax.grid(True, alpha=0.3)\n\nax = axes[1]\nax.plot(accuracies, 'g-', linewidth=2)\nax.set_xlabel('Epoch')\nax.set_ylabel('Sequence Accuracy')\nax.set_title('Training Accuracy (entire sequence correct)')\nax.set_ylim(0, 1.05)\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "zyxr8qicz1",
   "source": "### Visualize Attention Weights During Inference\n\nThe real power of attention: we can **see** what the model is looking at!\n\n**F1 analogy:** This is one of the most valuable properties of attention-based models -- interpretability. When a model predicts \"pit now,\" you can look at the attention weights and see exactly which past laps it was focusing on. Was it the degradation trend from the last 5 laps? The competitor's pit stop 3 laps ago? The fuel model from lap 1? Attention weights make the model's reasoning visible, just like a strategist explaining their call.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ycyl3wpt18a",
   "source": "# Visualize attention patterns for sequence reversal\nmodel.eval()\n\n# Pick a few test examples\nn_examples = 3\nfig, axes = plt.subplots(n_examples, 2, figsize=(14, 4 * n_examples))\n\nfor ex_idx in range(n_examples):\n    src = test_src[ex_idx:ex_idx+1]\n    tgt = test_tgt[ex_idx:ex_idx+1]\n    \n    with torch.no_grad():\n        logits = model(src, tgt)\n        preds = logits.argmax(dim=-1)\n    \n    # Get cross-attention weights (averaged across heads)\n    # Shape: (1, n_heads, tgt_len, src_len)\n    attn_weights = model.attention_weights[0].mean(dim=0).numpy()  # Average over heads\n    \n    src_tokens = src[0].tolist()\n    tgt_tokens = tgt[0].tolist()\n    pred_tokens = preds[0].tolist()\n    \n    # Heatmap\n    ax = axes[ex_idx, 0]\n    im = ax.imshow(attn_weights, cmap='Blues', aspect='auto', vmin=0, vmax=0.5)\n    ax.set_xticks(range(len(src_tokens)))\n    ax.set_xticklabels(src_tokens, fontsize=11, fontweight='bold')\n    ax.set_yticks(range(len(tgt_tokens)))\n    ax.set_yticklabels(tgt_tokens, fontsize=11, fontweight='bold')\n    ax.set_xlabel('Source Position')\n    ax.set_ylabel('Target Position')\n    ax.set_title(f'Cross-Attention Weights (avg over heads)')\n    plt.colorbar(im, ax=ax)\n    \n    # Per-head view\n    ax = axes[ex_idx, 1]\n    head_weights = model.attention_weights[0].numpy()  # (n_heads, tgt, src)\n    for h in range(min(4, head_weights.shape[0])):\n        # Show which source position each target position attends to most\n        max_attn = head_weights[h].argmax(axis=1)\n        ax.plot(range(len(tgt_tokens)), max_attn, 'o-', label=f'Head {h+1}', \n                markersize=8, linewidth=2, alpha=0.7)\n    \n    # Perfect reversal line\n    ax.plot(range(len(tgt_tokens)), list(range(len(src_tokens)-1, -1, -1)), \n            'k--', linewidth=2, alpha=0.5, label='Perfect reversal')\n    ax.set_xlabel('Target Position')\n    ax.set_ylabel('Most-Attended Source Position')\n    ax.set_title(f'Src: {src_tokens} -> Pred: {pred_tokens}')\n    ax.legend(fontsize=8)\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"The anti-diagonal pattern in the heatmap confirms the model learned to reverse!\")\nprint(\"Target position 0 attends to source position 7 (last), and so on.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "gpr616iiu5j",
   "source": "---\n\n## Exercises\n\n### Exercise 1: Implement Dot-Product Attention from Scratch\n\nImplement the basic (unscaled) dot-product attention function using only NumPy.\n\n**F1 scenario:** You are building the core of a race analysis tool. Given a query (\"what kind of lap am I trying to predict?\"), a set of keys (each past lap's signature), and values (each past lap's telemetry), compute the attention-weighted blend of past laps. This is the fundamental operation your strategy model will use to look back at the race history.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "nqkz0jf3xx",
   "source": "# EXERCISE 1: Implement dot-product attention with NumPy\ndef dot_product_attention_numpy(query, keys, values):\n    \"\"\"\n    Compute dot-product attention using NumPy.\n    \n    Args:\n        query: Query vector of shape (d_k,)\n        keys: Key matrix of shape (n, d_k)\n        values: Value matrix of shape (n, d_v)\n    \n    Returns:\n        output: Weighted sum of values, shape (d_v,)\n        weights: Attention weights, shape (n,)\n    \"\"\"\n    # TODO: Implement the three steps:\n    # 1. Compute scores = keys @ query  (dot product of each key with query)\n    # 2. Compute weights = softmax(scores)\n    #    Hint: Use numerically stable softmax: exp(x - max(x)) / sum(exp(x - max(x)))\n    # 3. Compute output = weights @ values (weighted sum)\n    \n    pass  # Replace with your implementation\n\n# Test\nnp.random.seed(42)\nq = np.array([1.0, 0.0, 1.0])\nK = np.array([[1.0, 0.0, 0.0],\n              [0.0, 1.0, 0.0],\n              [1.0, 0.0, 1.0],  # This key matches the query best!\n              [0.0, 0.0, 1.0]])\nV = np.array([[1, 0],\n              [0, 1],\n              [1, 1],\n              [0, 0]])\n\noutput, weights = dot_product_attention_numpy(q, K, V)\n\nexpected_scores = np.array([1.0, 0.0, 2.0, 1.0])\nexpected_weights_unnorm = np.exp(expected_scores - expected_scores.max())\nexpected_weights = expected_weights_unnorm / expected_weights_unnorm.sum()\nexpected_output = expected_weights @ V\n\nprint(f\"Your weights:    {weights}\")\nprint(f\"Expected weights: {expected_weights.round(4)}\")\nprint(f\"Your output:     {output}\")\nprint(f\"Expected output:  {expected_output.round(4)}\")\nprint(f\"Correct: {np.allclose(weights, expected_weights) and np.allclose(output, expected_output)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6ub2zmvbiui",
   "source": "### Exercise 2: Add Causal Masking to Self-Attention\n\nModify the SelfAttention class to support causal masking.\n\n**F1 scenario:** Convert your race analysis tool from post-race mode (can see all laps) to live-race mode (can only see completed laps). When predicting lap 30, the model must not have access to laps 31 and beyond -- that would be looking into the future. Implement the causal mask that enforces this constraint.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "gbkm4a3gotq",
   "source": "# EXERCISE 2: Implement causal self-attention\nclass CausalSelfAttention(nn.Module):\n    \"\"\"\n    Self-attention with causal masking.\n    Each position can only attend to itself and previous positions.\n    \"\"\"\n    def __init__(self, d_model):\n        super().__init__()\n        self.d_model = d_model\n        self.W_q = nn.Linear(d_model, d_model, bias=False)\n        self.W_k = nn.Linear(d_model, d_model, bias=False)\n        self.W_v = nn.Linear(d_model, d_model, bias=False)\n    \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input tensor (batch, seq_len, d_model)\n        \n        Returns:\n            output: (batch, seq_len, d_model)\n            weights: (batch, seq_len, seq_len)\n        \"\"\"\n        # TODO: Implement causal self-attention\n        # Step 1: Compute Q, K, V projections\n        # Step 2: Create a causal mask using torch.tril\n        #   Hint: mask = torch.tril(torch.ones(seq_len, seq_len))\n        #   Hint: Reshape mask for broadcasting: (1, 1, seq_len, seq_len)\n        # Step 3: Use scaled_dot_product_attention with the mask\n        \n        pass  # Replace with your implementation\n\n# Test\ntorch.manual_seed(42)\ncausal_attn = CausalSelfAttention(d_model=8)\nx = torch.randn(1, 5, 8)\n\noutput, weights = causal_attn(x)\n\n# Check that future positions have zero attention weight\nprint(\"Attention weights:\")\nprint(weights[0].detach().numpy().round(3))\n\nprint(f\"\\nUpper triangle should be ~0:\")\nupper_triangle = weights[0].detach().numpy()[np.triu_indices(5, k=1)]\nprint(f\"Max value in upper triangle: {upper_triangle.max():.6f}\")\nprint(f\"Causal masking working: {upper_triangle.max() < 1e-6}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "y4g8qaanawn",
   "source": "### Exercise 3: Multi-Head Attention Comparison\n\nCompare 1-head, 4-head, and 8-head attention on the sequence reversal task.\n\n**F1 scenario:** Compare having 1 generalist engineer vs 4 specialists vs 8 specialists on the pit wall. With more heads, each specialist focuses on a narrower aspect of the race data (pace, tires, fuel, gaps) but with finer granularity. Does having more specialized perspectives improve the model's ability to learn the task? Run the experiment and find out.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5fj3uto89g6",
   "source": "# EXERCISE 3: Compare different numbers of heads\ndef train_and_evaluate(n_heads, d_model=32, epochs=20):\n    \"\"\"\n    Train a Seq2SeqWithAttention model with a given number of heads.\n    \n    Args:\n        n_heads: Number of attention heads\n        d_model: Model dimension (must be divisible by n_heads)\n        epochs: Number of training epochs\n    \n    Returns:\n        test_accuracy: Fraction of test sequences fully reversed correctly\n        losses: List of training losses per epoch\n    \"\"\"\n    # TODO: Implement this!\n    # Hint: Create a Seq2SeqWithAttention model with the given n_heads\n    # Hint: Train it using the same training loop as Section 8\n    # Hint: Evaluate on test data and return accuracy\n    \n    pass  # Replace with your implementation\n\n# Test your implementation:\n# head_configs = [1, 2, 4, 8]\n# results = {}\n# for n_h in head_configs:\n#     acc, losses = train_and_evaluate(n_h)\n#     results[n_h] = (acc, losses)\n#     print(f\"  {n_h} heads: test accuracy = {acc:.4f}\")\n#\n# # Plot loss curves\n# for n_h, (acc, losses) in results.items():\n#     plt.plot(losses, label=f'{n_h} heads (acc={acc:.3f})')\n# plt.xlabel('Epoch')\n# plt.ylabel('Loss')\n# plt.title('Effect of Number of Attention Heads')\n# plt.legend()\n# plt.grid(True, alpha=0.3)\n# plt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "qx43ka5t5l",
   "source": "---\n\n## Summary\n\n### Key Concepts\n\n**The Bottleneck Problem:**\n- Fixed-length encoding forces entire sequences into one vector\n- Information loss grows with sequence length\n- Attention solves this by letting the decoder look at all encoder states\n- **F1 parallel:** Instead of compressing 78 laps into one summary vector, attention lets the model look up any specific lap whenever it needs to\n\n**The Q, K, V Framework:**\n- **Query:** \"What am I looking for?\"\n- **Key:** \"What do I have to offer?\"\n- **Value:** \"Here is my content\"\n- Attention = softmax(similarity(Q, K)) * V\n- **F1 parallel:** Query = \"which past laps are relevant to predicting THIS lap?\" Keys = each lap's signature. Values = each lap's actual telemetry data.\n\n**Scaled Dot-Product Attention:**\n- Score = Q . K^T / sqrt(d_k)\n- Scaling prevents softmax saturation in high dimensions\n- Foundation of all modern attention mechanisms\n- **F1 parallel:** Scaling prevents the model from fixating on a single lap -- it blends information from several relevant laps\n\n**Self-Attention:**\n- Q, K, V are all projected from the same input\n- Each position can attend to every other position\n- O(1) path length between any two positions (vs O(n) for RNNs)\n- **F1 parallel:** Each lap attending to every other lap in the same race -- lap 50 can directly access lap 1 without information degradation\n\n**Multi-Head Attention:**\n- Multiple parallel attention \"perspectives\"\n- Each head can learn different relationship types\n- Same parameter count as single-head (just splits d_model)\n- **F1 parallel:** Multiple specialist engineers simultaneously attending to pace, tire wear, fuel load, and track evolution\n\n**Attention Variants:**\n- Additive (Bahdanau): MLP-based scoring\n- Multiplicative (Luong): Learned bilinear scoring\n- Scaled dot-product: Used in Transformers\n- Causal masking: For autoregressive generation (live race, no peeking at future laps)\n\n### Connection to Deep Learning\n\n| Concept | Where It's Used | F1 Parallel |\n|---------|----------------|-------------|\n| Scaled dot-product attention | Core of every Transformer | Foundation of race-state prediction |\n| Multi-head attention | Encoder and decoder blocks | Multiple specialists on the pit wall |\n| Self-attention | BERT, GPT, ViT, and almost every modern model | Each lap understanding its context from all other laps |\n| Cross-attention | Encoder-decoder models, image captioning | Comparing your telemetry against a rival's |\n| Causal masking | GPT, autoregressive language models | Live-race prediction (no future data) |\n| Q, K, V projections | All attention-based architectures | The query-key-value lookup that powers the entire strategy system |\n\n### Checklist\n\n- [ ] I can explain the bottleneck problem with fixed-length encoding\n- [ ] I understand the Q, K, V framework and the database analogy\n- [ ] I can compute dot-product attention by hand on small examples\n- [ ] I know why we divide by sqrt(d_k) and what happens without it\n- [ ] I can implement self-attention from scratch\n- [ ] I can implement multi-head attention and explain the head dimension split\n- [ ] I can compare additive, multiplicative, and dot-product attention\n- [ ] I understand causal masking and when it's needed\n- [ ] I can build and train a model with attention and visualize its attention weights",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "hc2ychu2ps4",
   "source": "---\n\n## Next Steps\n\nYou now understand the **attention mechanism** -- the single most important building block in modern deep learning. Everything you've learned here feeds directly into the **Transformer architecture**, which is the next notebook.\n\nIn the Transformer notebook, you'll see how attention is combined with:\n1. **Positional encoding** -- giving the model a sense of word order (remember, self-attention alone has no notion of position!) In F1 terms: telling the model that lap 1 comes before lap 2 -- something attention alone does not know.\n2. **Feed-forward layers** -- adding per-position nonlinear processing\n3. **Layer normalization and residual connections** -- stabilizing deep attention networks\n4. **The full encoder-decoder architecture** -- stacking multiple attention layers\n\n**The key takeaway:** Attention lets a model dynamically decide what to focus on. Instead of processing sequences step-by-step (RNNs) or looking at fixed windows (CNNs), attention can connect any two positions in a sequence directly. This is why Transformers have replaced RNNs and CNNs for most sequence tasks, and why attention is now used even in vision (ViT), audio, and protein folding. In F1 terms, attention replaced the \"relay every lap sequentially through memory\" approach with a \"look up any lap instantly\" approach -- and that single change transformed the entire field.\n\n**Practical next steps:**\n- Try modifying the sequence reversal model to handle variable-length sequences\n- Experiment with different attention types (additive vs dot-product) on the same task\n- Read the \"Attention Is All You Need\" paper (Vaswani et al., 2017) -- you now have the background to understand it\n- Explore PyTorch's built-in `nn.MultiheadAttention` and compare with your implementation",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}