{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2.1: Python OOP for Deep Learning\n",
    "\n",
    "Object-Oriented Programming is essential for deep learning because:\n",
    "- PyTorch models are classes (`nn.Module`)\n",
    "- Datasets are classes (`torch.utils.data.Dataset`)\n",
    "- Training loops use objects with state\n",
    "- Clean, reusable code requires good OOP design\n",
    "\n",
    "## Learning Objectives\n",
    "- [ ] Design clean class hierarchies\n",
    "- [ ] Use magic methods to create Pythonic APIs\n",
    "- [ ] Write and use decorators\n",
    "- [ ] Add type hints for better code documentation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Setup - run this cell first\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('seaborn-v0_8-whitegrid')\nnp.random.seed(42)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### What this means:\n\n**Classes** are like blueprints for building things. Think of a class as a cookie cutter - it defines the shape, but each cookie (object) you make is separate and can have different decorations.\n\nIn deep learning:\n- The `nn.Module` class is the cookie cutter for all neural networks\n- When you write `model = MyNetwork()`, you're making one specific cookie\n- Each model you create has its own weights, even though they follow the same blueprint",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classes and Objects\n",
    "\n",
    "A **class** is a blueprint for creating objects. An **object** is an instance of a class.\n",
    "\n",
    "### Why Classes Matter in Deep Learning\n",
    "\n",
    "| PyTorch Concept | Implemented As |\n",
    "|-----------------|----------------|\n",
    "| Neural network | Class extending `nn.Module` |\n",
    "| Dataset | Class extending `Dataset` |\n",
    "| Optimizer | Class with `step()` and `zero_grad()` |\n",
    "| Loss function | Class with `forward()` method |\n",
    "| Data loader | Class with `__iter__()` method |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic class structure\n",
    "class Neuron:\n",
    "    \"\"\"A simple artificial neuron.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs):\n",
    "        \"\"\"Initialize neuron with random weights.\n",
    "        \n",
    "        Args:\n",
    "            n_inputs: Number of input connections\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        self.weights = np.random.randn(n_inputs)\n",
    "        self.bias = 0.0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Compute neuron output.\"\"\"\n",
    "        import numpy as np\n",
    "        z = np.dot(self.weights, x) + self.bias\n",
    "        return 1 / (1 + np.exp(-z))  # Sigmoid activation\n",
    "\n",
    "# Create an instance (object)\n",
    "neuron = Neuron(n_inputs=3)\n",
    "print(f\"Weights: {neuron.weights}\")\n",
    "print(f\"Bias: {neuron.bias}\")\n",
    "\n",
    "# Use the neuron\n",
    "import numpy as np\n",
    "x = np.array([1.0, 2.0, 3.0])\n",
    "output = neuron.forward(x)\n",
    "print(f\"Output for input {x}: {output:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: Understanding `self`\n",
    "\n",
    "`self` refers to the specific instance of the class. It's how the object accesses its own data.\n",
    "\n",
    "```python\n",
    "neuron1 = Neuron(3)  # self = neuron1 inside methods\n",
    "neuron2 = Neuron(3)  # self = neuron2 inside methods\n",
    "```\n",
    "\n",
    "Each object has its own `weights` and `bias` - they don't share!\n",
    "\n",
    "| Term | Meaning |\n",
    "|------|--------|\n",
    "| `self.weights` | Instance attribute (each object has its own) |\n",
    "| `self.forward(x)` | Instance method (operates on this object's data) |\n",
    "| `Neuron.forward` | The method definition in the class |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating that each instance has its own state\n",
    "np.random.seed(42)\n",
    "neuron1 = Neuron(3)\n",
    "neuron2 = Neuron(3)\n",
    "\n",
    "print(\"neuron1 weights:\", neuron1.weights)\n",
    "print(\"neuron2 weights:\", neuron2.weights)\n",
    "print(\"\\nThey're different! Each object has its own state.\")\n",
    "\n",
    "# Modify one, the other is unaffected\n",
    "neuron1.bias = 1.0\n",
    "print(f\"\\nneuron1.bias = {neuron1.bias}\")\n",
    "print(f\"neuron2.bias = {neuron2.bias}  (unchanged)\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Visualization: Object Memory Layout\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Left: Two separate neuron objects\nax = axes[0]\nax.set_xlim(0, 10)\nax.set_ylim(0, 10)\nax.axis('off')\nax.set_title('Two Neuron Objects in Memory', fontsize=12, fontweight='bold')\n\n# Object 1 box\nrect1 = plt.Rectangle((0.5, 5), 4, 4.5, fill=True, facecolor='lightblue', \n                        edgecolor='black', linewidth=2)\nax.add_patch(rect1)\nax.text(2.5, 9, 'neuron1', ha='center', va='center', fontsize=11, fontweight='bold')\nax.text(2.5, 8.2, 'weights: [0.49, -0.13, 0.64]', ha='center', fontsize=9)\nax.text(2.5, 7.4, 'bias: 1.0', ha='center', fontsize=9)\nax.text(2.5, 6.5, 'forward: <method>', ha='center', fontsize=9, style='italic')\nax.text(2.5, 5.4, 'id: 0x7f...a1b0', ha='center', fontsize=8, color='gray')\n\n# Object 2 box\nrect2 = plt.Rectangle((5.5, 5), 4, 4.5, fill=True, facecolor='lightgreen', \n                        edgecolor='black', linewidth=2)\nax.add_patch(rect2)\nax.text(7.5, 9, 'neuron2', ha='center', va='center', fontsize=11, fontweight='bold')\nax.text(7.5, 8.2, 'weights: [1.52, -0.23, 0.54]', ha='center', fontsize=9)\nax.text(7.5, 7.4, 'bias: 0.0', ha='center', fontsize=9)\nax.text(7.5, 6.5, 'forward: <method>', ha='center', fontsize=9, style='italic')\nax.text(7.5, 5.4, 'id: 0x7f...c2d0', ha='center', fontsize=8, color='gray')\n\nax.text(5, 3.5, 'Each object has its own copy\\nof instance attributes!', \n        ha='center', fontsize=10, style='italic')\n\n# Right: Class vs Instance attributes\nax = axes[1]\nax.set_xlim(0, 10)\nax.set_ylim(0, 10)\nax.axis('off')\nax.set_title('Class vs Instance Attributes', fontsize=12, fontweight='bold')\n\n# Class (shared)\nrect_class = plt.Rectangle((2, 6.5), 6, 2.5, fill=True, facecolor='lightyellow', \n                             edgecolor='black', linewidth=2)\nax.add_patch(rect_class)\nax.text(5, 8.5, 'Layer (class)', ha='center', fontweight='bold', fontsize=11)\nax.text(5, 7.5, 'count = 3  (shared by all)', ha='center', fontsize=10)\n\n# Instances\nfor i, (x, color, name) in enumerate([(1.5, 'lightblue', 'layer1'), \n                                        (5, 'lightgreen', 'layer2'),\n                                        (8.5, 'lightcoral', 'layer3')]):\n    rect = plt.Rectangle((x-1.2, 2), 2.4, 3, fill=True, facecolor=color, \n                          edgecolor='black', linewidth=1.5)\n    ax.add_patch(rect)\n    ax.text(x, 4.5, name, ha='center', fontweight='bold', fontsize=10)\n    ax.text(x, 3.8, f'id = {i}', ha='center', fontsize=9)\n    ax.text(x, 3.1, f'n_neurons = {[64, 128, 10][i]}', ha='center', fontsize=8)\n    # Arrow to class\n    ax.annotate('', xy=(x, 6.5), xytext=(x, 5),\n                arrowprops=dict(arrowstyle='->', color='gray', lw=1.5))\n\nax.text(5, 1, 'Instance attributes are unique;\\nclass attributes are shared', \n        ha='center', fontsize=10, style='italic')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Attributes vs Instance Attributes\n",
    "\n",
    "| Type | Defined | Shared? | Use Case |\n",
    "|------|---------|---------|----------|\n",
    "| Class attribute | In class body | Yes, by all instances | Constants, counters |\n",
    "| Instance attribute | In `__init__` with `self.` | No, each instance has own | Object-specific data |"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### What this means:\n\n**Inheritance** is like genetic inheritance - children get traits from parents, but can also have their own unique features.\n\nIn deep learning:\n- `nn.Module` is the \"parent\" that knows how to track parameters, move to GPU, save/load, etc.\n- Your custom model is the \"child\" that inherits all those abilities\n- You only need to write what's unique (your architecture), not reinvent parameter tracking!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualization: Class Hierarchy Diagram\nfig, ax = plt.subplots(figsize=(10, 6))\nax.set_xlim(0, 10)\nax.set_ylim(0, 8)\nax.axis('off')\nax.set_title('Class Hierarchy: Inheritance in Deep Learning', fontsize=14, fontweight='bold')\n\n# Draw boxes\ndef draw_box(ax, x, y, text, color='lightblue'):\n    box = plt.Rectangle((x-0.8, y-0.3), 1.6, 0.6, fill=True, \n                         facecolor=color, edgecolor='black', linewidth=2)\n    ax.add_patch(box)\n    ax.text(x, y, text, ha='center', va='center', fontsize=10, fontweight='bold')\n\n# Parent class\ndraw_box(ax, 5, 7, 'nn.Module', 'lightyellow')\n\n# Child classes (level 1)\ndraw_box(ax, 2, 5, 'nn.Linear', 'lightblue')\ndraw_box(ax, 5, 5, 'nn.Conv2d', 'lightblue')\ndraw_box(ax, 8, 5, 'nn.LSTM', 'lightblue')\n\n# Your custom models (level 2)\ndraw_box(ax, 2, 3, 'MyMLP', 'lightgreen')\ndraw_box(ax, 5, 3, 'MyCNN', 'lightgreen')\ndraw_box(ax, 8, 3, 'MyRNN', 'lightgreen')\n\n# Draw inheritance arrows\nfor x in [2, 5, 8]:\n    ax.annotate('', xy=(x, 6.7), xytext=(x, 5.3),\n                arrowprops=dict(arrowstyle='->', color='black', lw=2))\n    ax.annotate('', xy=(x, 4.7), xytext=(x, 3.3),\n                arrowprops=dict(arrowstyle='->', color='black', lw=2))\n\n# Legend\nax.text(5, 1.5, 'Arrows show \"inherits from\" relationship', ha='center', fontsize=10, style='italic')\nax.text(5, 1, 'Yellow = Base class | Blue = PyTorch built-in | Green = Your custom classes', \n        ha='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"A neural network layer.\"\"\"\n",
    "    \n",
    "    # Class attribute - shared by all instances\n",
    "    count = 0\n",
    "    \n",
    "    def __init__(self, n_neurons):\n",
    "        # Instance attributes - unique to each instance\n",
    "        self.n_neurons = n_neurons\n",
    "        self.id = Layer.count\n",
    "        Layer.count += 1  # Increment the shared counter\n",
    "\n",
    "# Create layers\n",
    "layer1 = Layer(64)\n",
    "layer2 = Layer(128)\n",
    "layer3 = Layer(10)\n",
    "\n",
    "print(f\"layer1: id={layer1.id}, neurons={layer1.n_neurons}\")\n",
    "print(f\"layer2: id={layer2.id}, neurons={layer2.n_neurons}\")\n",
    "print(f\"layer3: id={layer3.id}, neurons={layer3.n_neurons}\")\n",
    "print(f\"\\nTotal layers created: {Layer.count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Inheritance\n",
    "\n",
    "**Inheritance** lets you create a new class based on an existing class. The new class \"inherits\" all the methods and attributes of the parent.\n",
    "\n",
    "### Why Inheritance Matters in Deep Learning\n",
    "\n",
    "```python\n",
    "class MyModel(nn.Module):      # Inherit from nn.Module\n",
    "class MyDataset(Dataset):       # Inherit from Dataset\n",
    "class MyOptimizer(Optimizer):   # Inherit from Optimizer\n",
    "```\n",
    "\n",
    "You get all the PyTorch machinery for free, then customize what you need!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### What this means:\n\n**Magic methods** are special methods that Python calls automatically when you use operators or built-in functions. The double underscores (\"dunders\") tell Python \"this is special.\"\n\nThink of it this way:\n- When you write `len(dataset)`, Python secretly calls `dataset.__len__()`\n- When you write `model(x)`, Python secretly calls `model.__call__(x)`\n- You're teaching Python how to treat YOUR objects like built-in types!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class\n",
    "class Activation:\n",
    "    \"\"\"Base class for activation functions.\"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply activation - must be implemented by subclass.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement forward()\")\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}()\"\n",
    "\n",
    "\n",
    "# Child classes - inherit from Activation\n",
    "class ReLU(Activation):\n",
    "    \"\"\"Rectified Linear Unit.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"ReLU\")  # Call parent's __init__\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    \"\"\"Sigmoid activation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"Sigmoid\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "class Tanh(Activation):\n",
    "    \"\"\"Hyperbolic tangent activation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"Tanh\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "\n",
    "# Use them\n",
    "activations = [ReLU(), Sigmoid(), Tanh()]\n",
    "x = np.linspace(-3, 3, 100)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 4))\n",
    "for act in activations:\n",
    "    plt.plot(x, act.forward(x), label=act.name, linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('activation(x)')\n",
    "plt.title('Activation Functions (via inheritance)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# All share the same interface!\n",
    "print(\"All activations share the same interface:\")\n",
    "for act in activations:\n",
    "    print(f\"  {act} -> forward(0) = {act.forward(np.array([0]))[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: `super()` and Method Resolution Order\n",
    "\n",
    "`super()` calls the parent class's method. This is essential when you want to extend (not replace) the parent's behavior.\n",
    "\n",
    "```python\n",
    "class Child(Parent):\n",
    "    def __init__(self, child_arg, parent_arg):\n",
    "        super().__init__(parent_arg)  # Initialize parent first!\n",
    "        self.child_attr = child_arg   # Then add child-specific stuff\n",
    "```\n",
    "\n",
    "**In PyTorch, you MUST call `super().__init__()`** in your model's `__init__`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating PyTorch's nn.Module pattern\n",
    "class Module:\n",
    "    \"\"\"Simplified version of nn.Module.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._parameters = {}\n",
    "        self._modules = {}\n",
    "        \n",
    "    def register_parameter(self, name, value):\n",
    "        self._parameters[name] = value\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"Return all parameters.\"\"\"\n",
    "        params = list(self._parameters.values())\n",
    "        for module in self._modules.values():\n",
    "            params.extend(module.parameters())\n",
    "        return params\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "    \"\"\"Linear layer: y = xW + b\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()  # MUST call parent's __init__!\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.weight = np.random.randn(in_features, out_features) * 0.01\n",
    "        self.bias = np.zeros(out_features)\n",
    "        \n",
    "        # Register as parameters\n",
    "        self.register_parameter('weight', self.weight)\n",
    "        self.register_parameter('bias', self.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x @ self.weight + self.bias\n",
    "\n",
    "\n",
    "# Use it\n",
    "layer = Linear(10, 5)\n",
    "x = np.random.randn(3, 10)  # Batch of 3 samples, 10 features each\n",
    "output = layer(x)  # Calls __call__ -> forward\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of parameters: {len(layer.parameters())}\")\n",
    "print(f\"Weight shape: {layer.weight.shape}\")\n",
    "print(f\"Bias shape: {layer.bias.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Magic Methods (Dunder Methods)\n",
    "\n",
    "**Magic methods** (also called \"dunder\" methods for \"double underscore\") let you define how your objects behave with Python's built-in operations.\n",
    "\n",
    "### Why Magic Methods Matter in Deep Learning\n",
    "\n",
    "| Magic Method | Enables | PyTorch Example |\n",
    "|--------------|---------|----------------|\n",
    "| `__init__` | Creating objects | `model = MyModel()` |\n",
    "| `__call__` | Calling like function | `output = model(input)` |\n",
    "| `__len__` | `len()` function | `len(dataset)` |\n",
    "| `__getitem__` | Indexing with `[]` | `dataset[0]` |\n",
    "| `__iter__` | For loops | `for batch in dataloader:` |\n",
    "| `__repr__` | Nice printing | `print(model)` |\n",
    "| `__add__` | The `+` operator | `tensor1 + tensor2` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    \"\"\"A simple tensor class demonstrating magic methods.\"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \"\"\"Called when you do: t = Tensor(data)\"\"\"\n",
    "        self.data = np.array(data)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        \"\"\"Called when you do: print(t) or just t in REPL\"\"\"\n",
    "        return f\"Tensor({self.data.tolist()})\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Called when you do: len(t)\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Called when you do: t[idx]\"\"\"\n",
    "        return Tensor(self.data[idx])\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        \"\"\"Called when you do: t1 + t2\"\"\"\n",
    "        if isinstance(other, Tensor):\n",
    "            return Tensor(self.data + other.data)\n",
    "        return Tensor(self.data + other)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        \"\"\"Called when you do: t1 * t2\"\"\"\n",
    "        if isinstance(other, Tensor):\n",
    "            return Tensor(self.data * other.data)\n",
    "        return Tensor(self.data * other)\n",
    "    \n",
    "    def __matmul__(self, other):\n",
    "        \"\"\"Called when you do: t1 @ t2\"\"\"\n",
    "        return Tensor(self.data @ other.data)\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        \"\"\"Access like t.shape (not t.shape())\"\"\"\n",
    "        return self.data.shape\n",
    "\n",
    "\n",
    "# Demonstrate magic methods\n",
    "t1 = Tensor([1, 2, 3])\n",
    "t2 = Tensor([4, 5, 6])\n",
    "\n",
    "print(\"__repr__:\", t1)\n",
    "print(\"__len__:\", len(t1))\n",
    "print(\"__getitem__:\", t1[0])\n",
    "print(\"__add__:\", t1 + t2)\n",
    "print(\"__mul__:\", t1 * t2)\n",
    "print(\"scalar add:\", t1 + 10)\n",
    "print(\"shape property:\", t1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `__call__`: Making Objects Callable\n",
    "\n",
    "This is **THE** most important magic method for deep learning. It lets you call an object like a function:\n",
    "\n",
    "```python\n",
    "model = MyModel()    # __init__\n",
    "output = model(x)    # __call__ -> forward\n",
    "```\n",
    "\n",
    "PyTorch's `nn.Module.__call__` does:\n",
    "1. Calls hooks (if any)\n",
    "2. Calls your `forward()` method\n",
    "3. Calls more hooks\n",
    "4. Returns the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"Simple neural network demonstrating __call__.\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "        self.activation = ReLU()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through all layers.\"\"\"\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            # Apply activation to all but last layer\n",
    "            if i < len(self.layers) - 1:\n",
    "                x = self.activation.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Makes the network callable like a function.\"\"\"\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        layers_str = \"\\n  \".join([f\"Linear({l.weight.shape[0]} -> {l.weight.shape[1]})\" \n",
    "                                   for l in self.layers])\n",
    "        return f\"NeuralNetwork(\\n  {layers_str}\\n)\"\n",
    "\n",
    "\n",
    "# Create and use network\n",
    "net = NeuralNetwork([784, 128, 64, 10])\n",
    "print(net)\n",
    "\n",
    "# Forward pass - note we call net(x), not net.forward(x)\n",
    "x = np.random.randn(32, 784)  # Batch of 32 images (flattened 28x28)\n",
    "output = net(x)  # This calls __call__ -> forward\n",
    "\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `__getitem__` and `__len__`: Building a Dataset\n",
    "\n",
    "These methods let you create custom datasets that work with PyTorch's DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"Base dataset class (like torch.utils.data.Dataset).\"\"\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SyntheticDataset(Dataset):\n",
    "    \"\"\"A synthetic dataset for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_samples, n_features, n_classes):\n",
    "        self.X = np.random.randn(n_samples, n_features)\n",
    "        self.y = np.random.randint(0, n_classes, n_samples)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of samples.\"\"\"\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return (features, label) for given index.\"\"\"\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataset = SyntheticDataset(n_samples=1000, n_features=10, n_classes=3)\n",
    "\n",
    "# Now we can use len() and indexing!\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"First sample: X={dataset[0][0][:3]}..., y={dataset[0][1]}\")\n",
    "print(f\"Last sample: X={dataset[-1][0][:3]}..., y={dataset[-1][1]}\")\n",
    "\n",
    "# We can even iterate!\n",
    "print(\"\\nFirst 3 labels:\")\n",
    "for i in range(3):\n",
    "    x, y = dataset[i]\n",
    "    print(f\"  Sample {i}: label = {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Magic Methods Reference\n",
    "\n",
    "| Method | Triggered By | Example |\n",
    "|--------|--------------|--------|\n",
    "| `__init__(self, ...)` | `obj = Class(...)` | Constructor |\n",
    "| `__repr__(self)` | `print(obj)`, `repr(obj)` | String representation |\n",
    "| `__str__(self)` | `str(obj)` | Human-readable string |\n",
    "| `__len__(self)` | `len(obj)` | Length |\n",
    "| `__getitem__(self, key)` | `obj[key]` | Indexing |\n",
    "| `__setitem__(self, key, val)` | `obj[key] = val` | Index assignment |\n",
    "| `__call__(self, ...)` | `obj(...)` | Call like function |\n",
    "| `__iter__(self)` | `for x in obj:` | Iteration |\n",
    "| `__next__(self)` | `next(obj)` | Next item |\n",
    "| `__add__(self, other)` | `obj + other` | Addition |\n",
    "| `__sub__(self, other)` | `obj - other` | Subtraction |\n",
    "| `__mul__(self, other)` | `obj * other` | Multiplication |\n",
    "| `__matmul__(self, other)` | `obj @ other` | Matrix multiplication |\n",
    "| `__eq__(self, other)` | `obj == other` | Equality |\n",
    "| `__lt__(self, other)` | `obj < other` | Less than |\n",
    "| `__enter__(self)` | `with obj:` | Context manager enter |\n",
    "| `__exit__(self, ...)` | End of `with` block | Context manager exit |"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### What this means:\n\n**Decorators** are like gift wrapping - they wrap your function with extra functionality without changing the gift (function) inside.\n\nIn deep learning:\n- `@torch.no_grad()` wraps your function to disable gradient tracking (faster inference)\n- `@property` lets you access a method like an attribute (cleaner code)\n- Custom decorators can add timing, logging, or caching to any function",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Decorators\n",
    "\n",
    "A **decorator** modifies or enhances a function without changing its code. It's a function that takes a function and returns a new function.\n",
    "\n",
    "### Why Decorators Matter in Deep Learning\n",
    "\n",
    "| Decorator | Use |\n",
    "|-----------|-----|\n",
    "| `@property` | Access method like attribute (`model.device`) |\n",
    "| `@staticmethod` | Method that doesn't need `self` |\n",
    "| `@classmethod` | Method that operates on class, not instance |\n",
    "| `@torch.no_grad()` | Disable gradient computation (inference) |\n",
    "| `@torch.jit.script` | JIT compile for speed |\n",
    "| Custom timing decorator | Profile your code |"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Visualization: Decorator Flow Diagram\nfig, ax = plt.subplots(figsize=(12, 6))\nax.set_xlim(0, 12)\nax.set_ylim(0, 8)\nax.axis('off')\nax.set_title('How Decorators Work: @timer Example', fontsize=14, fontweight='bold')\n\n# Helper function to draw boxes\ndef draw_flow_box(ax, x, y, w, h, text, color='lightblue', fontsize=10):\n    rect = plt.Rectangle((x, y), w, h, fill=True, facecolor=color, \n                          edgecolor='black', linewidth=2, alpha=0.8)\n    ax.add_patch(rect)\n    ax.text(x + w/2, y + h/2, text, ha='center', va='center', fontsize=fontsize)\n\n# Step 1: Original function\ndraw_flow_box(ax, 0.5, 5.5, 2.5, 1.5, 'my_func()', 'lightyellow', 11)\nax.text(1.75, 7.3, '1. Original\\nfunction', ha='center', fontsize=9)\n\n# Arrow\nax.annotate('', xy=(3.3, 6.25), xytext=(3, 6.25),\n            arrowprops=dict(arrowstyle='->', color='black', lw=2))\n\n# Step 2: @timer decorator\ndraw_flow_box(ax, 3.5, 5, 3, 2.5, '@timer\\ndef my_func():\\n    ...', 'lightcoral', 10)\nax.text(5, 7.8, '2. Apply\\ndecorator', ha='center', fontsize=9)\n\n# Arrow\nax.annotate('', xy=(6.8, 6.25), xytext=(6.5, 6.25),\n            arrowprops=dict(arrowstyle='->', color='black', lw=2))\n\n# Step 3: Wrapper function\ndraw_flow_box(ax, 7, 4.5, 4.5, 3.5, 'wrapper():\\n  start = time()\\n  result = my_func()\\n  print(elapsed)\\n  return result', \n              'lightgreen', 9)\nax.text(9.25, 8.3, '3. Returns wrapped\\nfunction', ha='center', fontsize=9)\n\n# What happens when you call\nax.text(6, 2.5, 'When you call my_func():', fontsize=11, fontweight='bold')\nax.text(6, 1.8, 'Python actually calls wrapper() which:', fontsize=10)\nax.text(6, 1.2, '1. Records start time  2. Calls original my_func()  3. Prints elapsed time', fontsize=9)\n\n# Code example box\ncode_box = plt.Rectangle((0.5, 0.3, ), 5, 1.5, fill=True, facecolor='#f0f0f0', \n                          edgecolor='gray', linewidth=1)\nax.add_patch(code_box)\nax.text(3, 1.35, '@timer', fontsize=10, fontfamily='monospace')\nax.text(3, 0.85, 'def train(): ...', fontsize=10, fontfamily='monospace')\nax.text(3, 0.5, '# Same as: train = timer(train)', fontsize=9, fontfamily='monospace', color='gray')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "# A simple timer decorator\n",
    "def timer(func):\n",
    "    \"\"\"Decorator that times function execution.\"\"\"\n",
    "    @wraps(func)  # Preserves function name and docstring\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"{func.__name__} took {end - start:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@timer\n",
    "def slow_function():\n",
    "    \"\"\"A slow function for demonstration.\"\"\"\n",
    "    time.sleep(0.1)\n",
    "    return \"done\"\n",
    "\n",
    "\n",
    "@timer\n",
    "def matrix_multiply(size):\n",
    "    \"\"\"Multiply two random matrices.\"\"\"\n",
    "    A = np.random.randn(size, size)\n",
    "    B = np.random.randn(size, size)\n",
    "    return A @ B\n",
    "\n",
    "\n",
    "# Use the decorated functions\n",
    "result1 = slow_function()\n",
    "result2 = matrix_multiply(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More useful decorators\n",
    "\n",
    "def debug(func):\n",
    "    \"\"\"Print function arguments and return value.\"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        args_str = \", \".join([repr(a) for a in args])\n",
    "        kwargs_str = \", \".join([f\"{k}={v!r}\" for k, v in kwargs.items()])\n",
    "        all_args = \", \".join(filter(None, [args_str, kwargs_str]))\n",
    "        print(f\"Calling {func.__name__}({all_args})\")\n",
    "        result = func(*args, **kwargs)\n",
    "        print(f\"  -> {result!r}\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def retry(max_attempts=3):\n",
    "    \"\"\"Retry a function if it raises an exception.\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for attempt in range(max_attempts):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                    if attempt == max_attempts - 1:\n",
    "                        raise\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "\n",
    "@debug\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "result = add(3, 5)\n",
    "result = add(3, b=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Decorators: `@property`, `@staticmethod`, `@classmethod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"Demonstrating built-in decorators.\"\"\"\n",
    "    \n",
    "    _model_count = 0\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self._parameters = {'weight': np.random.randn(10, 5)}\n",
    "        Model._model_count += 1\n",
    "        \n",
    "    @property\n",
    "    def num_parameters(self):\n",
    "        \"\"\"Access like an attribute, but computed on the fly.\"\"\"\n",
    "        return sum(p.size for p in self._parameters.values())\n",
    "    \n",
    "    @property\n",
    "    def shape_str(self):\n",
    "        \"\"\"Another computed property.\"\"\"\n",
    "        return \", \".join([f\"{k}: {v.shape}\" for k, v in self._parameters.items()])\n",
    "    \n",
    "    @staticmethod\n",
    "    def activation(x):\n",
    "        \"\"\"Static method - doesn't need self, just a utility function.\"\"\"\n",
    "        return np.maximum(0, x)  # ReLU\n",
    "    \n",
    "    @classmethod\n",
    "    def get_model_count(cls):\n",
    "        \"\"\"Class method - operates on the class, not instance.\"\"\"\n",
    "        return cls._model_count\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"Alternative constructor from a config dict.\"\"\"\n",
    "        return cls(name=config['name'])\n",
    "\n",
    "\n",
    "# Using @property - note NO parentheses!\n",
    "model = Model(\"MyModel\")\n",
    "print(f\"Number of parameters: {model.num_parameters}\")  # Not num_parameters()!\n",
    "print(f\"Shapes: {model.shape_str}\")\n",
    "\n",
    "# Using @staticmethod - can call on class or instance\n",
    "print(f\"\\nReLU([-1, 0, 1]): {Model.activation(np.array([-1, 0, 1]))}\")\n",
    "\n",
    "# Using @classmethod\n",
    "model2 = Model(\"Model2\")\n",
    "print(f\"\\nTotal models created: {Model.get_model_count()}\")\n",
    "\n",
    "# Alternative constructor\n",
    "config = {'name': 'ConfigModel'}\n",
    "model3 = Model.from_config(config)\n",
    "print(f\"Created from config: {model3.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Context Managers\n",
    "\n",
    "Context managers handle setup and cleanup automatically using the `with` statement.\n",
    "\n",
    "### Why Context Managers Matter in Deep Learning\n",
    "\n",
    "```python\n",
    "# Disable gradients during inference\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_data)\n",
    "\n",
    "# Automatic mixed precision training\n",
    "with torch.cuda.amp.autocast():\n",
    "    output = model(input)\n",
    "\n",
    "# Timer context\n",
    "with Timer(\"Training\"):\n",
    "    train_one_epoch()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    \"\"\"Context manager for timing code blocks.\"\"\"\n",
    "    \n",
    "    def __init__(self, name=\"Block\"):\n",
    "        self.name = name\n",
    "        \n",
    "    def __enter__(self):\n",
    "        \"\"\"Called when entering the 'with' block.\"\"\"\n",
    "        self.start = time.time()\n",
    "        return self  # This is what 'as' binds to\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Called when exiting the 'with' block.\"\"\"\n",
    "        self.elapsed = time.time() - self.start\n",
    "        print(f\"{self.name} took {self.elapsed:.4f} seconds\")\n",
    "        return False  # Don't suppress exceptions\n",
    "\n",
    "\n",
    "# Use it\n",
    "with Timer(\"Matrix operations\"):\n",
    "    A = np.random.randn(500, 500)\n",
    "    B = np.random.randn(500, 500)\n",
    "    C = A @ B\n",
    "    \n",
    "with Timer(\"Loop\"):\n",
    "    total = 0\n",
    "    for i in range(100000):\n",
    "        total += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating torch.no_grad()\n",
    "class NoGrad:\n",
    "    \"\"\"Context manager to disable gradient tracking.\"\"\"\n",
    "    \n",
    "    # Class variable to track global state\n",
    "    grad_enabled = True\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.prev_state = NoGrad.grad_enabled\n",
    "        NoGrad.grad_enabled = False\n",
    "        print(\"Gradients disabled\")\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        NoGrad.grad_enabled = self.prev_state\n",
    "        print(\"Gradients restored\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def compute_something():\n",
    "    \"\"\"Function that checks gradient state.\"\"\"\n",
    "    if NoGrad.grad_enabled:\n",
    "        print(\"  Computing with gradients\")\n",
    "    else:\n",
    "        print(\"  Computing WITHOUT gradients (faster!)\")\n",
    "\n",
    "\n",
    "print(\"Training mode:\")\n",
    "compute_something()\n",
    "\n",
    "print(\"\\nInference mode:\")\n",
    "with NoGrad():\n",
    "    compute_something()\n",
    "\n",
    "print(\"\\nBack to training mode:\")\n",
    "compute_something()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Type Hints\n",
    "\n",
    "Type hints document what types your functions expect and return. They don't enforce types at runtime but help with:\n",
    "- Documentation\n",
    "- IDE autocomplete\n",
    "- Static analysis tools (mypy)\n",
    "\n",
    "### Basic Type Hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Optional, Union, Callable\n",
    "import numpy as np\n",
    "\n",
    "# Basic types\n",
    "def greet(name: str) -> str:\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    return a + b\n",
    "\n",
    "# Collections\n",
    "def sum_list(numbers: List[float]) -> float:\n",
    "    return sum(numbers)\n",
    "\n",
    "def get_layer_shapes(model: Dict[str, np.ndarray]) -> List[Tuple[int, ...]]:\n",
    "    return [v.shape for v in model.values()]\n",
    "\n",
    "# Optional (can be None)\n",
    "def get_activation(name: Optional[str] = None) -> Callable:\n",
    "    if name is None or name == 'relu':\n",
    "        return lambda x: np.maximum(0, x)\n",
    "    elif name == 'sigmoid':\n",
    "        return lambda x: 1 / (1 + np.exp(-x))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown activation: {name}\")\n",
    "\n",
    "# Union (can be multiple types)\n",
    "def normalize(data: Union[List[float], np.ndarray]) -> np.ndarray:\n",
    "    arr = np.array(data)\n",
    "    return (arr - arr.mean()) / arr.std()\n",
    "\n",
    "\n",
    "# Examples\n",
    "print(greet(\"Alice\"))\n",
    "print(sum_list([1.0, 2.0, 3.0]))\n",
    "print(normalize([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type hints in classes\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for training a model.\"\"\"\n",
    "    learning_rate: float = 0.001\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 100\n",
    "    hidden_sizes: List[int] = None\n",
    "    dropout: Optional[float] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.hidden_sizes is None:\n",
    "            self.hidden_sizes = [128, 64]\n",
    "\n",
    "\n",
    "# @dataclass automatically generates __init__, __repr__, etc.\n",
    "config = TrainingConfig(learning_rate=0.01, epochs=50)\n",
    "print(config)\n",
    "\n",
    "# Access fields\n",
    "print(f\"\\nLearning rate: {config.learning_rate}\")\n",
    "print(f\"Hidden sizes: {config.hidden_sizes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Putting It All Together: A Mini Deep Learning Framework\n",
    "\n",
    "Let's build a minimal neural network framework using everything we've learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable, Optional\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Module(ABC):\n",
    "    \"\"\"Base class for all neural network modules.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._parameters: Dict[str, np.ndarray] = {}\n",
    "        self._modules: Dict[str, 'Module'] = {}\n",
    "        self.training: bool = True\n",
    "        \n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass - must be implemented by subclasses.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make module callable.\"\"\"\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def parameters(self) -> List[np.ndarray]:\n",
    "        \"\"\"Return all parameters.\"\"\"\n",
    "        params = list(self._parameters.values())\n",
    "        for module in self._modules.values():\n",
    "            params.extend(module.parameters())\n",
    "        return params\n",
    "    \n",
    "    def train(self, mode: bool = True):\n",
    "        \"\"\"Set training mode.\"\"\"\n",
    "        self.training = mode\n",
    "        for module in self._modules.values():\n",
    "            module.train(mode)\n",
    "        return self\n",
    "    \n",
    "    def eval(self):\n",
    "        \"\"\"Set evaluation mode.\"\"\"\n",
    "        return self.train(False)\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "    \"\"\"Fully connected layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        # Xavier initialization\n",
    "        scale = np.sqrt(2.0 / (in_features + out_features))\n",
    "        self._parameters['weight'] = np.random.randn(in_features, out_features) * scale\n",
    "        self._parameters['bias'] = np.zeros(out_features)\n",
    "        \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        return x @ self._parameters['weight'] + self._parameters['bias']\n",
    "    \n",
    "    def __repr__(self):\n",
    "        w = self._parameters['weight']\n",
    "        return f\"Linear({w.shape[0]}, {w.shape[1]})\"\n",
    "\n",
    "\n",
    "class ReLU(Module):\n",
    "    \"\"\"ReLU activation.\"\"\"\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ReLU()\"\n",
    "\n",
    "\n",
    "class Sequential(Module):\n",
    "    \"\"\"Sequential container for modules.\"\"\"\n",
    "    \n",
    "    def __init__(self, *modules: Module):\n",
    "        super().__init__()\n",
    "        for i, module in enumerate(modules):\n",
    "            self._modules[str(i)] = module\n",
    "            \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        for module in self._modules.values():\n",
    "            x = module(x)\n",
    "        return x\n",
    "    \n",
    "    def __repr__(self):\n",
    "        lines = [\"Sequential(\"]\n",
    "        for name, module in self._modules.items():\n",
    "            lines.append(f\"  ({name}): {module}\")\n",
    "        lines.append(\")\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# Build a model!\n",
    "model = Sequential(\n",
    "    Linear(784, 256),\n",
    "    ReLU(),\n",
    "    Linear(256, 128),\n",
    "    ReLU(),\n",
    "    Linear(128, 10)\n",
    ")\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nNumber of parameter arrays: {len(model.parameters())}\")\n",
    "print(f\"Total parameters: {sum(p.size for p in model.parameters()):,}\")\n",
    "\n",
    "# Forward pass\n",
    "x = np.random.randn(32, 784)  # Batch of 32 images\n",
    "output = model(x)\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Build a Dropout Layer\n",
    "\n",
    "Implement dropout that:\n",
    "- During training: randomly zeros elements with probability `p`\n",
    "- During evaluation: does nothing (but scales output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    \"\"\"Dropout layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, p: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        # TODO: Implement dropout\n",
    "        # Hint: \n",
    "        # - During training (self.training == True): create mask, apply it, scale by 1/(1-p)\n",
    "        # - During eval: just return x\n",
    "        \n",
    "        if self.training:\n",
    "            mask = np.random.binomial(1, 1 - self.p, x.shape)\n",
    "            return x * mask / (1 - self.p)\n",
    "        return x\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Dropout(p={self.p})\"\n",
    "\n",
    "\n",
    "# Test\n",
    "dropout = Dropout(p=0.5)\n",
    "x = np.ones((2, 10))\n",
    "\n",
    "print(\"Training mode:\")\n",
    "dropout.train()\n",
    "print(dropout(x))\n",
    "\n",
    "print(\"\\nEval mode:\")\n",
    "dropout.eval()\n",
    "print(dropout(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Interactive Example: How Dropout Rate Affects Output\n\nTry changing the dropout rate to see how it affects the network's activations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Interactive: Visualizing dropout at different rates\n# Try changing dropout_rate to see the effect!\n\ndef visualize_dropout_effect(dropout_rates=[0.0, 0.25, 0.5, 0.75]):\n    \"\"\"Visualize how different dropout rates affect network activations.\"\"\"\n    np.random.seed(42)\n    \n    # Create input (simulating activations from a hidden layer)\n    x = np.abs(np.random.randn(1, 100))  # 100 neurons, positive values\n    \n    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n    axes = axes.flatten()\n    \n    for ax, p in zip(axes, dropout_rates):\n        # Apply dropout\n        if p > 0:\n            mask = np.random.binomial(1, 1 - p, x.shape)\n            output = x * mask / (1 - p)  # Inverted dropout scaling\n        else:\n            output = x.copy()\n            mask = np.ones_like(x)\n        \n        # Visualize\n        colors = ['green' if m else 'red' for m in mask.flatten()]\n        ax.bar(range(100), output.flatten(), color=colors, alpha=0.7, width=1.0)\n        ax.axhline(y=x.mean(), color='blue', linestyle='--', \n                   label=f'Original mean: {x.mean():.2f}', linewidth=2)\n        ax.axhline(y=output.mean(), color='orange', linestyle='-', \n                   label=f'After dropout: {output.mean():.2f}', linewidth=2)\n        \n        n_dropped = int(p * 100)\n        ax.set_title(f'Dropout p={p}\\n({n_dropped} neurons dropped, {100-n_dropped} active)', \n                     fontsize=11, fontweight='bold')\n        ax.set_xlabel('Neuron index')\n        ax.set_ylabel('Activation value')\n        ax.legend(loc='upper right', fontsize=8)\n        ax.set_xlim(-1, 101)\n    \n    plt.suptitle('Effect of Dropout Rate on Activations\\n(Green = active, Red = dropped)', \n                 fontsize=13, fontweight='bold', y=1.02)\n    plt.tight_layout()\n    plt.show()\n    \n    print(\"Key insight: Notice how the mean stays roughly the same!\")\n    print(\"This is because we scale by 1/(1-p) during training.\")\n    print(\"\\nTry different rates:\")\n    print(\"  - p=0.0: No dropout (all neurons active)\")\n    print(\"  - p=0.5: Typical dropout rate (50% dropped)\")\n    print(\"  - p=0.75: Aggressive dropout (may hurt performance)\")\n\n# Run the visualization\nvisualize_dropout_effect()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Create a DataLoader\n",
    "\n",
    "Build a DataLoader that:\n",
    "- Takes a dataset and batch_size\n",
    "- Is iterable (use `__iter__` and `__next__`)\n",
    "- Optionally shuffles data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Simple DataLoader.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, batch_size: int = 32, shuffle: bool = False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches.\"\"\"\n",
    "        return (len(self.dataset) + self.batch_size - 1) // self.batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Return iterator.\"\"\"\n",
    "        # TODO: Create indices, optionally shuffle, reset position\n",
    "        self.indices = np.arange(len(self.dataset))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        self.pos = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        \"\"\"Get next batch.\"\"\"\n",
    "        # TODO: Return next batch or raise StopIteration\n",
    "        if self.pos >= len(self.dataset):\n",
    "            raise StopIteration\n",
    "            \n",
    "        batch_indices = self.indices[self.pos:self.pos + self.batch_size]\n",
    "        self.pos += self.batch_size\n",
    "        \n",
    "        # Collect batch\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        for idx in batch_indices:\n",
    "            x, y = self.dataset[idx]\n",
    "            batch_x.append(x)\n",
    "            batch_y.append(y)\n",
    "            \n",
    "        return np.array(batch_x), np.array(batch_y)\n",
    "\n",
    "\n",
    "# Test\n",
    "dataset = SyntheticDataset(100, 10, 3)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Number of batches: {len(loader)}\")\n",
    "\n",
    "print(\"\\nIterating through batches:\")\n",
    "for i, (x, y) in enumerate(loader):\n",
    "    print(f\"  Batch {i}: x.shape={x.shape}, y.shape={y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | What It Does | PyTorch Example |\n",
    "|---------|--------------|------------------|\n",
    "| Classes | Bundle data and methods | `nn.Module`, `Dataset` |\n",
    "| Inheritance | Extend existing classes | `class MyModel(nn.Module)` |\n",
    "| `__init__` | Initialize object | Setup layers and parameters |\n",
    "| `__call__` | Make callable | `model(input)` |\n",
    "| `__len__` | Enable `len()` | `len(dataset)` |\n",
    "| `__getitem__` | Enable indexing | `dataset[0]` |\n",
    "| `@property` | Computed attributes | `model.device` |\n",
    "| Decorators | Modify functions | `@torch.no_grad()` |\n",
    "| Context managers | Setup/cleanup | `with torch.no_grad():` |\n",
    "| Type hints | Document types | `def forward(self, x: Tensor) -> Tensor:` |\n",
    "\n",
    "### Checklist\n",
    "- [ ] I can create classes with `__init__` and methods\n",
    "- [ ] I understand inheritance and `super()`\n",
    "- [ ] I can use magic methods (`__call__`, `__len__`, `__getitem__`)\n",
    "- [ ] I can write and use decorators\n",
    "- [ ] I can create context managers\n",
    "- [ ] I can add type hints to functions and classes"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Connection to Deep Learning\n\n| Concept | PyTorch Example | Why It Matters |\n|---------|-----------------|----------------|\n| **Classes & `__init__`** | `class MyModel(nn.Module): def __init__(self): ...` | Every neural network is a class; `__init__` sets up layers and registers parameters |\n| **Inheritance** | `class MyModel(nn.Module)` | You inherit GPU support, parameter tracking, save/load, and gradient computation for free |\n| **`__call__` & `forward`** | `output = model(x)` calls `forward()` | PyTorch adds hooks and gradient tracking when you call the model, so always use `model(x)` not `model.forward(x)` |\n| **`__len__` & `__getitem__`** | `class MyDataset(Dataset)` | DataLoader uses these to batch and shuffle your data automatically |\n| **`@property`** | `model.device`, `tensor.shape` | Access computed values cleanly without parentheses |\n| **`@torch.no_grad()`** | `with torch.no_grad(): pred = model(x)` | Disables gradient tracking during inference for speed and memory savings |\n| **Context managers** | `with autocast(): ...` | Mixed precision training, gradient scaling, and resource management |\n| **Type hints** | `def forward(self, x: Tensor) -> Tensor` | IDE autocomplete, better documentation, catch bugs early |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to **Part 2.2: NumPy Deep Dive** where we'll cover:\n",
    "- Advanced array operations\n",
    "- Broadcasting in depth\n",
    "- Vectorization for performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}