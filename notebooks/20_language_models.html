
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Part 6.4: Language Models &#8212; Foundations of AI</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=9c3e77be" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=1ae7504c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/20_language_models';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Part 6.5: Fine-tuning &amp; PEFT" href="21_finetuning_and_peft.html" />
    <link rel="prev" title="Part 6.3: Tokenization &amp; Language Model Training" href="19_tokenization_lm_training.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Foundations of AI</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 1: Mathematical Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_linear_algebra.html">Part 1.1: Linear Algebra for Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_calculus.html">Part 1.2: Calculus for Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_probability_statistics.html">Part 1.3: Probability &amp; Statistics for Deep Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 2: Programming Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_python_oop.html">Part 2.1: Python OOP for Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_numpy_deep_dive.html">Part 2.2: NumPy Deep Dive</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 3: Classical ML &amp; Optimization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="06_classical_ml.html">Part 3.1: Classical Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_optimization_linear_programming.html">Part 3.2: Optimization &amp; Linear Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_optimization_theory.html">Part 3.3: Optimization Theory for Machine Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 4: Neural Network Fundamentals</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="09_perceptrons_basic_networks.html">Part 4.1: Perceptrons &amp; Basic Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_backpropagation.html">Part 4.2: Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_pytorch_fundamentals.html">Part 4.3: PyTorch Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_training_deep_networks.html">Part 4.4: Training Deep Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 5: Neural Network Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="13_convolutional_neural_networks.html">Part 5.1: Convolutional Neural Networks (CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_computer_vision_depth.html">Part 5.2: Computer Vision — Beyond Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_recurrent_neural_networks.html">Part 5.3: Recurrent Neural Networks (RNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_attention_mechanisms.html">Part 5.4: Attention Mechanisms</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 6: Transformers &amp; LLMs</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="17_transformer_architecture.html">Part 6.1: Transformer Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_embeddings.html">Part 6.2: Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="19_tokenization_lm_training.html">Part 6.3: Tokenization &amp; Language Model Training</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Part 6.4: Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="21_finetuning_and_peft.html">Part 6.5: Fine-tuning &amp; PEFT</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 7: Reinforcement Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="22_rl_fundamentals.html">Part 7.1: Reinforcement Learning Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="23_q_learning_dqn.html">Part 7.2: Q-Learning and Deep Q-Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="24_policy_gradients.html">Part 7.3: Policy Gradient Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="25_ppo_modern_rl.html">Part 7.4: PPO and Modern RL</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 8: Applied AI Systems</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="26_rag.html">Part 8.1: Retrieval-Augmented Generation (RAG)</a></li>
<li class="toctree-l1"><a class="reference internal" href="27_ai_agents.html">Part 8.2: AI Agents and Tool Use</a></li>
<li class="toctree-l1"><a class="reference internal" href="28_ai_evals.html">Part 8.3: Evaluating AI Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="29_production_monitoring.html">Part 8.4: Production AI Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 9: Advanced Topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="30_inference_optimization.html">Part 9.1: LLM Inference Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="31_ml_systems.html">Part 9.2: ML Systems &amp; Experiment Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="32_multimodal_ai.html">Part 9.3: Multimodal AI</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/dan-shah/foundations-of-ai/blob/main/notebooks/20_language_models.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/dan-shah/foundations-of-ai" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/dan-shah/foundations-of-ai/edit/main/notebooks/20_language_models.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/dan-shah/foundations-of-ai/issues/new?title=Issue%20on%20page%20%2Fnotebooks/20_language_models.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notebooks/20_language_models.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Part 6.4: Language Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-language-model">1. What Is a Language Model?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuitive-explanation">Intuitive Explanation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#breaking-down-the-formula">Breaking down the formula:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-this-is-useful">Why This Is Useful</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#brief-history-from-n-grams-to-transformers">Brief History: From N-grams to Transformers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">2. Tokenization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#byte-pair-encoding-bpe-the-dominant-tokenization-algorithm">Byte Pair Encoding (BPE): The Dominant Tokenization Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-why-bpe-works-so-well">Deep Dive: Why BPE Works So Well</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#common-misconceptions">Common Misconceptions</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-bpe-from-scratch">Implementing BPE from Scratch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#special-tokens">Special Tokens</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization-comparison-table">Tokenization Comparison Table</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-architecture-decoder-only">3. GPT Architecture (Decoder-Only)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-overview">Architecture Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-a-mini-gpt-model">Implementing a Mini-GPT Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-decoder-only-vs-encoder-decoder">Comparison: Decoder-Only vs Encoder-Decoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-gpt-scaling-history">Deep Dive: GPT Scaling History</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#key-insight">Key Insight</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bert-architecture-encoder-only">4. BERT Architecture (Encoder-Only)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bert-s-two-training-objectives">BERT’s Two Training Objectives</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mlm-details">MLM Details</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-vs-bert-complete-comparison">GPT vs BERT: Complete Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use-which">When to Use Which?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Key Insight</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-a-small-language-model">5. Training a Small Language Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#temperature-sampling">Temperature Sampling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-laws">6. Scaling Laws</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-scaling-insights">Key Scaling Insights</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-chinchilla-finding">The Chinchilla Finding</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-extend-bpe-with-encoding">Exercise 1: Extend BPE with Encoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-top-p-nucleus-sampling">Exercise 2: Top-p (Nucleus) Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-calculate-perplexity">Exercise 3: Calculate Perplexity</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts">Key Concepts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-deep-learning">Connection to Deep Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checklist">Checklist</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">Next Steps</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="part-6-4-language-models">
<h1>Part 6.4: Language Models<a class="headerlink" href="#part-6-4-language-models" title="Link to this heading">#</a></h1>
<p>Language models are the foundation of modern AI. At their core, they do something deceptively simple: <strong>predict the next token in a sequence</strong>. Yet from this single objective emerges the ability to write essays, translate languages, answer questions, and even reason about code. In notebook 17 you built a Transformer from scratch – now we’ll see how that architecture becomes GPT, BERT, and the large language models that are reshaping every field.</p>
<p><strong>F1 analogy:</strong> A language model is like an F1 race prediction system. Given everything that has happened so far in a race – “Safety car deployed on lap 12, Verstappen pits on lap 13, rain starts on lap 14…” – it predicts the most likely next event. Just as a language model assigns probabilities to every possible next word, a race predictor assigns probabilities to every possible next event: pit stop, overtake, mechanical failure, safety car. The better the model, the better its predictions.</p>
<hr class="docutils" />
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<p>By the end of this notebook, you should be able to:</p>
<ul class="simple">
<li><p>[ ] Explain what a language model is and why next-token prediction is so powerful</p></li>
<li><p>[ ] Implement Byte Pair Encoding (BPE) tokenization from scratch</p></li>
<li><p>[ ] Describe the GPT (decoder-only) architecture and why it suits generation</p></li>
<li><p>[ ] Describe the BERT (encoder-only) architecture and why it suits understanding</p></li>
<li><p>[ ] Train a small character-level GPT and generate text from it</p></li>
<li><p>[ ] Explain scaling laws and how model size, data, and compute interact</p></li>
<li><p>[ ] Choose the right architecture (GPT vs BERT) for a given task</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">Counter</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn-v0_8-whitegrid&#39;</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="what-is-a-language-model">
<h2>1. What Is a Language Model?<a class="headerlink" href="#what-is-a-language-model" title="Link to this heading">#</a></h2>
<section id="intuitive-explanation">
<h3>Intuitive Explanation<a class="headerlink" href="#intuitive-explanation" title="Link to this heading">#</a></h3>
<p>Imagine someone starts a sentence: <em>“The cat sat on the ___”</em>. Your brain instantly predicts likely next words: “mat”, “chair”, “roof”. You assign high probability to sensible continuations and low probability to nonsensical ones like “democracy” or “purple”.</p>
<p>A <strong>language model</strong> does exactly this – it learns a probability distribution over the next token given all previous tokens:</p>
<div class="math notranslate nohighlight">
\[P(w_t \mid w_1, w_2, \ldots, w_{t-1})\]</div>
<section id="breaking-down-the-formula">
<h4>Breaking down the formula:<a class="headerlink" href="#breaking-down-the-formula" title="Link to this heading">#</a></h4>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Meaning</p></th>
<th class="head"><p>Intuition</p></th>
<th class="head"><p>F1 Analogy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(w_t\)</span></p></td>
<td><p>The next token to predict</p></td>
<td><p>The word we’re guessing</p></td>
<td><p>The next race event to predict</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(w_1, \ldots, w_{t-1}\)</span></p></td>
<td><p>All previous tokens</p></td>
<td><p>The context we’ve seen so far</p></td>
<td><p>Everything that has happened in the race so far</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(P(\cdot \mid \cdot)\)</span></p></td>
<td><p>Conditional probability</p></td>
<td><p>How likely is this word <em>given</em> what came before?</p></td>
<td><p>How likely is a pit stop <em>given</em> the current race state?</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>What this means:</strong> A language model is a probability machine. Given any prefix of text, it outputs a probability for every possible next token in its vocabulary. The better the model, the higher probability it assigns to tokens that actually make sense.</p>
<p><strong>F1 analogy:</strong> Think of the language model as a race prediction engine. Given the sequence of events so far – “Lap 1: Verstappen leads, Lap 2: Norris overtakes Leclerc, Lap 3: …” – it assigns a probability to every possible next event. The model trained on thousands of historical races learns patterns: safety cars often trigger pit stops, tire degradation increases over stint length, drivers on fresh tires tend to overtake. This is <em>autoregressive prediction</em> applied to racing.</p>
</section>
</section>
<section id="why-this-is-useful">
<h3>Why This Is Useful<a class="headerlink" href="#why-this-is-useful" title="Link to this heading">#</a></h3>
<p>Language modeling isn’t just about prediction – it’s a gateway to many capabilities:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Capability</p></th>
<th class="head"><p>How LM Enables It</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Text Generation</strong></p></td>
<td><p>Sample from the predicted distribution repeatedly</p></td>
<td><p>Generate race commentary lap by lap</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Translation</strong></p></td>
<td><p>Model P(target language | source language)</p></td>
<td><p>Translate telemetry data into strategy calls</p></td>
</tr>
<tr class="row-even"><td><p><strong>Summarization</strong></p></td>
<td><p>Generate condensed version conditioned on original</p></td>
<td><p>Race report: 58 laps summarized in 3 paragraphs</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Question Answering</strong></p></td>
<td><p>Generate answer conditioned on context + question</p></td>
<td><p>“When should we pit?” given current race state</p></td>
</tr>
<tr class="row-even"><td><p><strong>Code Writing</strong></p></td>
<td><p>Code is just another language to model</p></td>
<td><p>Generate strategy algorithms from requirements</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Reasoning</strong></p></td>
<td><p>Chain-of-thought emerges from predicting logical next steps</p></td>
<td><p>Multi-step strategic reasoning about race scenarios</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="brief-history-from-n-grams-to-transformers">
<h3>Brief History: From N-grams to Transformers<a class="headerlink" href="#brief-history-from-n-grams-to-transformers" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Era</p></th>
<th class="head"><p>Approach</p></th>
<th class="head"><p>Key Idea</p></th>
<th class="head"><p>Limitation</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1990s-2000s</p></td>
<td><p><strong>N-gram models</strong></p></td>
<td><p>Count word sequences in data</p></td>
<td><p>Fixed context window (2-5 words)</p></td>
<td><p>Strategy based on last 2-3 laps only</p></td>
</tr>
<tr class="row-odd"><td><p>2010-2015</p></td>
<td><p><strong>RNN/LSTM</strong></p></td>
<td><p>Recurrent hidden state carries context</p></td>
<td><p>Sequential processing, vanishing gradients</p></td>
<td><p>Pit wall processing one car at a time</p></td>
</tr>
<tr class="row-even"><td><p>2017-present</p></td>
<td><p><strong>Transformers</strong></p></td>
<td><p>Self-attention over all positions</p></td>
<td><p>Quadratic cost in sequence length</p></td>
<td><p>Full parallel telemetry processing</p></td>
</tr>
<tr class="row-odd"><td><p>2018+</p></td>
<td><p><strong>Large LMs (GPT, BERT)</strong></p></td>
<td><p>Scale Transformers + massive data</p></td>
<td><p>Compute-intensive training</p></td>
<td><p>Strategy AI trained on decades of race data</p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualization: Next-token probability distribution</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Left: probability distribution for a predictable context</span>
<span class="n">context1</span> <span class="o">=</span> <span class="s1">&#39;&quot;The cat sat on the ___&quot;&#39;</span>
<span class="n">words1</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mat&#39;</span><span class="p">,</span> <span class="s1">&#39;floor&#39;</span><span class="p">,</span> <span class="s1">&#39;chair&#39;</span><span class="p">,</span> <span class="s1">&#39;roof&#39;</span><span class="p">,</span> <span class="s1">&#39;table&#39;</span><span class="p">,</span> <span class="s1">&#39;bed&#39;</span><span class="p">,</span> <span class="s1">&#39;dog&#39;</span><span class="p">,</span> <span class="s1">&#39;moon&#39;</span><span class="p">,</span> <span class="s1">&#39;idea&#39;</span><span class="p">,</span> <span class="s1">&#39;purple&#39;</span><span class="p">]</span>
<span class="n">probs1</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.30</span><span class="p">,</span> <span class="mf">0.18</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">]</span>

<span class="n">colors1</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;#2ecc71&#39;</span> <span class="k">if</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mf">0.1</span> <span class="k">else</span> <span class="s1">&#39;#3498db&#39;</span> <span class="k">if</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mf">0.04</span> <span class="k">else</span> <span class="s1">&#39;#e74c3c&#39;</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">probs1</span><span class="p">]</span>
<span class="n">bars1</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words1</span><span class="p">)),</span> <span class="n">probs1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors1</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words1</span><span class="p">)))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">words1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Next word after:</span><span class="se">\n</span><span class="si">{</span><span class="n">context1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">invert_yaxis</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">probs1</span><span class="p">):</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">p</span> <span class="o">+</span> <span class="mf">0.005</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s1">.0%</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Right: probability distribution for an ambiguous context</span>
<span class="n">context2</span> <span class="o">=</span> <span class="s1">&#39;&quot;I need to go to the ___&quot;&#39;</span>
<span class="n">words2</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;store&#39;</span><span class="p">,</span> <span class="s1">&#39;bank&#39;</span><span class="p">,</span> <span class="s1">&#39;doctor&#39;</span><span class="p">,</span> <span class="s1">&#39;office&#39;</span><span class="p">,</span> <span class="s1">&#39;gym&#39;</span><span class="p">,</span> <span class="s1">&#39;park&#39;</span><span class="p">,</span> <span class="s1">&#39;school&#39;</span><span class="p">,</span> <span class="s1">&#39;airport&#39;</span><span class="p">,</span> <span class="s1">&#39;dentist&#39;</span><span class="p">,</span> <span class="s1">&#39;library&#39;</span><span class="p">]</span>
<span class="n">probs2</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.13</span><span class="p">,</span> <span class="mf">0.12</span><span class="p">,</span> <span class="mf">0.11</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.09</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.07</span><span class="p">,</span> <span class="mf">0.07</span><span class="p">]</span>

<span class="n">colors2</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;#2ecc71&#39;</span> <span class="k">if</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mf">0.1</span> <span class="k">else</span> <span class="s1">&#39;#3498db&#39;</span> <span class="k">if</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mf">0.04</span> <span class="k">else</span> <span class="s1">&#39;#e74c3c&#39;</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">probs2</span><span class="p">]</span>
<span class="n">bars2</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words2</span><span class="p">)),</span> <span class="n">probs2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors2</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words2</span><span class="p">)))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">words2</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Next word after:</span><span class="se">\n</span><span class="si">{</span><span class="n">context2</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">invert_yaxis</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">probs2</span><span class="p">):</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">p</span> <span class="o">+</span> <span class="mf">0.005</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s1">.0%</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Language Models Output Probability Distributions Over Next Tokens&#39;</span><span class="p">,</span> 
             <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.02</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Key insight: More context = more peaked (confident) distributions&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The model&#39;s job is to learn these distributions from data&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="tokenization">
<h2>2. Tokenization<a class="headerlink" href="#tokenization" title="Link to this heading">#</a></h2>
<section id="id1">
<h3>Intuitive Explanation<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>Before a language model can process text, it needs to break text into discrete units called <strong>tokens</strong>. But how should we split text?</p>
<p><strong>The vocabulary problem:</strong> If we use whole words, we need a massive vocabulary (English has 170,000+ words, plus names, technical terms, misspellings…). If we use single characters, sequences become very long and the model must learn spelling from scratch.</p>
<p><strong>Subword tokenization</strong> is the elegant middle ground: common words stay whole (“the”, “and”), while rare words are split into meaningful pieces (“un” + “happi” + “ness”).</p>
<p><strong>F1 analogy:</strong> Tokenization is like breaking radio messages into meaningful units. The message “Box box box, we’re switching to hards” could be tokenized at different granularities:</p>
<ul class="simple">
<li><p><strong>Character-level:</strong> B, o, x, (space), b, o, x, … – too fine-grained, loses meaning</p></li>
<li><p><strong>Word-level:</strong> “Box”, “box”, “box”, “we’re”, “switching”, “to”, “hards” – works but cannot handle rare compound terms like “undercut-opportunity”</p></li>
<li><p><strong>Subword (BPE):</strong> “Box”, “box”, “box”, “we”, “‘re”, “switch”, “ing”, “to”, “hard”, “s” – the sweet spot</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Approach</p></th>
<th class="head"><p>Example: “unhappiness”</p></th>
<th class="head"><p>Vocab Size</p></th>
<th class="head"><p>Pros</p></th>
<th class="head"><p>Cons</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Character-level</strong></p></td>
<td><p>u, n, h, a, p, p, i, n, e, s, s</p></td>
<td><p>~256</p></td>
<td><p>Handles any text</p></td>
<td><p>Very long sequences</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Word-level</strong></p></td>
<td><p>unhappiness</p></td>
<td><p>100K+</p></td>
<td><p>Semantically meaningful</p></td>
<td><p>Can’t handle unseen words</p></td>
</tr>
<tr class="row-even"><td><p><strong>Subword (BPE)</strong></p></td>
<td><p>un, happi, ness</p></td>
<td><p>30K-50K</p></td>
<td><p>Best of both worlds</p></td>
<td><p>Requires training</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="byte-pair-encoding-bpe-the-dominant-tokenization-algorithm">
<h3>Byte Pair Encoding (BPE): The Dominant Tokenization Algorithm<a class="headerlink" href="#byte-pair-encoding-bpe-the-dominant-tokenization-algorithm" title="Link to this heading">#</a></h3>
<p>BPE is beautifully simple. It starts with individual characters and repeatedly merges the most frequent pair:</p>
<p><strong>Algorithm:</strong></p>
<ol class="arabic simple">
<li><p>Start with a vocabulary of all individual characters in the text</p></li>
<li><p>Count all adjacent pairs of tokens in the text</p></li>
<li><p>Merge the most frequent pair into a new token</p></li>
<li><p>Repeat steps 2-3 until desired vocabulary size is reached</p></li>
</ol>
<p><strong>What this means:</strong> BPE discovers the natural building blocks of language. Frequent subwords like “ing”, “tion”, “un” emerge automatically from the data. Common words end up as single tokens, while rare words get split into recognizable pieces.</p>
<p><strong>F1 analogy:</strong> BPE is like how F1 engineers develop shorthand for common telemetry patterns. At first, everything is described in raw terms (“throttle 100%, brake 0%, speed increasing”). Over time, frequent patterns get their own names: “full traction zone,” “brake point,” “power unit deployment.” The most common patterns become single tokens in the team’s vocabulary, while rare events are still described by combining known sub-patterns.</p>
</section>
<section id="deep-dive-why-bpe-works-so-well">
<h3>Deep Dive: Why BPE Works So Well<a class="headerlink" href="#deep-dive-why-bpe-works-so-well" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Property</p></th>
<th class="head"><p>Why It Matters</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Data-driven</strong></p></td>
<td><p>Learns from actual text, not handcrafted rules</p></td>
<td><p>Vocabulary emerges from real race data, not imposed</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Open vocabulary</strong></p></td>
<td><p>Any text can be encoded (falls back to characters)</p></td>
<td><p>Any radio message can be parsed, even unusual ones</p></td>
</tr>
<tr class="row-even"><td><p><strong>Compression</strong></p></td>
<td><p>Common patterns get short representations</p></td>
<td><p>“Box box box” becomes one token with enough data</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Morphological</strong></p></td>
<td><p>Naturally discovers prefixes, suffixes, stems</p></td>
<td><p>Discovers “under-cut”, “over-cut”, “out-braked”</p></td>
</tr>
<tr class="row-even"><td><p><strong>Language-agnostic</strong></p></td>
<td><p>Works for English, Chinese, code, math – anything</p></td>
<td><p>Works for team radio, timing data, technical reports</p></td>
</tr>
</tbody>
</table>
</div>
<section id="common-misconceptions">
<h4>Common Misconceptions<a class="headerlink" href="#common-misconceptions" title="Link to this heading">#</a></h4>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Misconception</p></th>
<th class="head"><p>Reality</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>BPE splits on word boundaries</p></td>
<td><p>BPE operates on characters/bytes, ignoring word boundaries</p></td>
</tr>
<tr class="row-odd"><td><p>All words are single tokens</p></td>
<td><p>Only frequent words; rare ones are split</p></td>
</tr>
<tr class="row-even"><td><p>Tokenization is trivial</p></td>
<td><p>It significantly impacts model performance</p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualization: BPE step by step on a small corpus</span>
<span class="k">def</span><span class="w"> </span><span class="nf">visualize_bpe_steps</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">num_merges</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Run BPE on a small corpus and visualize each merge step.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        corpus: list of words (each word is a list of characters + end marker)</span>
<span class="sd">        num_merges: number of merge operations to perform</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Initialize: split each word into characters with end-of-word marker</span>
    <span class="n">word_freqs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">:</span>
        <span class="n">word_freqs</span><span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">word</span><span class="p">)]</span> <span class="o">=</span> <span class="n">freq</span>
    
    <span class="n">merge_history</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_merges</span><span class="p">):</span>
        <span class="c1"># Count all adjacent pairs</span>
        <span class="n">pair_counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">pair_counts</span><span class="p">[(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])]</span> <span class="o">+=</span> <span class="n">freq</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_counts</span><span class="p">:</span>
            <span class="k">break</span>
        
        <span class="c1"># Find most frequent pair</span>
        <span class="n">best_pair</span> <span class="o">=</span> <span class="n">pair_counts</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">pair</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="n">best_pair</span>
        <span class="n">merged</span> <span class="o">=</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="n">merge_history</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">pair</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">merged</span><span class="p">))</span>
        
        <span class="c1"># Apply merge</span>
        <span class="n">new_word_freqs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">new_word</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                    <span class="n">new_word</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">merged</span><span class="p">)</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_word</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">new_word_freqs</span><span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">new_word</span><span class="p">)]</span> <span class="o">=</span> <span class="n">freq</span>
        <span class="n">word_freqs</span> <span class="o">=</span> <span class="n">new_word_freqs</span>
    
    <span class="k">return</span> <span class="n">merge_history</span><span class="p">,</span> <span class="n">word_freqs</span>

<span class="c1"># Small corpus with frequencies</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="s2">&quot;low&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;_&quot;</span><span class="p">],</span> <span class="mi">5</span><span class="p">),</span>
    <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="s2">&quot;lower&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;_&quot;</span><span class="p">],</span> <span class="mi">2</span><span class="p">),</span>
    <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="s2">&quot;newest&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;_&quot;</span><span class="p">],</span> <span class="mi">6</span><span class="p">),</span>
    <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="s2">&quot;widest&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;_&quot;</span><span class="p">],</span> <span class="mi">3</span><span class="p">),</span>
    <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="s2">&quot;new&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;_&quot;</span><span class="p">],</span> <span class="mi">2</span><span class="p">),</span>
<span class="p">]</span>

<span class="n">merge_history</span><span class="p">,</span> <span class="n">final_vocab</span> <span class="o">=</span> <span class="n">visualize_bpe_steps</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">num_merges</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Visualize merge history</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">steps</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">merge_history</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;&quot;</span><span class="si">{</span><span class="n">h</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">&quot; + &quot;</span><span class="si">{</span><span class="n">h</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">&quot; </span><span class="se">\u2192</span><span class="s1"> &quot;</span><span class="si">{</span><span class="n">h</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s1">&quot;&#39;</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">merge_history</span><span class="p">]</span>
<span class="n">counts</span> <span class="o">=</span> <span class="p">[</span><span class="n">h</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">merge_history</span><span class="p">]</span>

<span class="n">colors</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">viridis</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">merge_history</span><span class="p">)))</span>
<span class="n">bars</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">merge_history</span><span class="p">)),</span> <span class="n">counts</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">merge_history</span><span class="p">)))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39;Step </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">merge_history</span><span class="p">))],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Pair Frequency in Corpus&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;BPE Merge Operations (Most Frequent Pairs First)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">invert_yaxis</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">counts</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">c</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">c</span><span class="p">),</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Corpus words: low(5), lower(2), newest(6), widest(3), new(2)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Final tokenization of each word:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">final_vocab</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">word</span><span class="p">)</span><span class="si">:</span><span class="s2">15s</span><span class="si">}</span><span class="s2"> (frequency: </span><span class="si">{</span><span class="n">freq</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="implementing-bpe-from-scratch">
<h3>Implementing BPE from Scratch<a class="headerlink" href="#implementing-bpe-from-scratch" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SimpleBPE</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A simple Byte Pair Encoding tokenizer built from scratch.</span>
<span class="sd">    </span>
<span class="sd">    This implements the core BPE algorithm: start with characters,</span>
<span class="sd">    iteratively merge the most frequent adjacent pair.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">256</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">merges</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># List of (pair, merged_token) in order</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="p">{}</span>   <span class="c1"># token -&gt; index</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Train BPE on a text corpus.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            text: Training text string</span>
<span class="sd">            verbose: If True, print each merge step</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Split text into words, add end-of-word marker</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">word_freqs</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
        
        <span class="c1"># Initialize: each word as tuple of characters + end marker</span>
        <span class="n">splits</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">splits</span><span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;&lt;/w&gt;&quot;</span><span class="p">])]</span> <span class="o">=</span> <span class="n">freq</span>
        
        <span class="c1"># Build initial character vocabulary</span>
        <span class="n">chars</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">:</span>
            <span class="n">chars</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        
        <span class="n">num_merges</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_merges</span><span class="p">):</span>
            <span class="c1"># Count pairs</span>
            <span class="n">pair_counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">splits</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
                    <span class="n">pair_counts</span><span class="p">[(</span><span class="n">word</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">word</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">])]</span> <span class="o">+=</span> <span class="n">freq</span>
            
            <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_counts</span><span class="p">:</span>
                <span class="k">break</span>
            
            <span class="c1"># Find best pair</span>
            <span class="n">best_pair</span> <span class="o">=</span> <span class="n">pair_counts</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">merged</span> <span class="o">=</span> <span class="n">best_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">best_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            
            <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">15</span><span class="p">:</span>
                <span class="n">count</span> <span class="o">=</span> <span class="n">pair_counts</span><span class="p">[</span><span class="n">best_pair</span><span class="p">]</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Merge </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: &#39;</span><span class="si">{</span><span class="n">best_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39; + &#39;</span><span class="si">{</span><span class="n">best_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39; -&gt; &#39;</span><span class="si">{</span><span class="n">merged</span><span class="si">}</span><span class="s2">&#39; (freq: </span><span class="si">{</span><span class="n">count</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
            
            <span class="bp">self</span><span class="o">.</span><span class="n">merges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">best_pair</span><span class="p">,</span> <span class="n">merged</span><span class="p">))</span>
            
            <span class="c1"># Apply merge</span>
            <span class="n">new_splits</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">splits</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">new_word</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">while</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">word</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="n">best_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">word</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">best_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                        <span class="n">new_word</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">merged</span><span class="p">)</span>
                        <span class="n">j</span> <span class="o">+=</span> <span class="mi">2</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">new_word</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
                        <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">new_splits</span><span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">new_word</span><span class="p">)]</span> <span class="o">=</span> <span class="n">freq</span>
            <span class="n">splits</span> <span class="o">=</span> <span class="n">new_splits</span>
        
        <span class="c1"># Build vocabulary</span>
        <span class="n">all_tokens</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">:</span>
            <span class="n">all_tokens</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">all_tokens</span><span class="p">))}</span>
        
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Final vocabulary size: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Tokenize a string using learned merges.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            text: Input string</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            List of tokens</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">all_tokens</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;&lt;/w&gt;&quot;</span><span class="p">]</span>
            
            <span class="c1"># Apply each merge rule in order</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">merged</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">merges</span><span class="p">:</span>
                <span class="n">new_tokens</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">a</span> <span class="ow">and</span> <span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">b</span><span class="p">:</span>
                        <span class="n">new_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">merged</span><span class="p">)</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">new_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">tokens</span> <span class="o">=</span> <span class="n">new_tokens</span>
            
            <span class="n">all_tokens</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">all_tokens</span>

<span class="c1"># Train on a small corpus</span>
<span class="n">training_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;the cat sat on the mat the cat sat on the hat</span>
<span class="s2">the dog sat on the log the dog sat on the rug</span>
<span class="s2">a new cat and a new dog sat on a new mat</span>
<span class="s2">the newest cat sat on the newest mat happily</span>
<span class="s2">the lower cat and the lowest dog were unhappy&quot;&quot;&quot;</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training BPE tokenizer...&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">bpe</span> <span class="o">=</span> <span class="n">SimpleBPE</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="n">bpe</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">training_text</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Tokenizing example sentences:&quot;</span><span class="p">)</span>
<span class="n">test_sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;the cat sat&quot;</span><span class="p">,</span> <span class="s2">&quot;newest dog&quot;</span><span class="p">,</span> <span class="s2">&quot;unhappy cat&quot;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">test_sentences</span><span class="p">:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">bpe</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  &#39;</span><span class="si">{</span><span class="n">sent</span><span class="si">}</span><span class="s2">&#39; -&gt; </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualization: How &quot;unhappiness&quot; gets tokenized at different vocab sizes</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># Simulate different tokenization granularities</span>
<span class="n">tokenizations</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s2">&quot;Character-level</span><span class="se">\n</span><span class="s2">(vocab ~30)&quot;</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="s2">&quot;unhappiness&quot;</span><span class="p">),</span> <span class="s1">&#39;#e74c3c&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;Small subword</span><span class="se">\n</span><span class="s2">(vocab ~100)&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;un&quot;</span><span class="p">,</span> <span class="s2">&quot;h&quot;</span><span class="p">,</span> <span class="s2">&quot;app&quot;</span><span class="p">,</span> <span class="s2">&quot;in&quot;</span><span class="p">,</span> <span class="s2">&quot;ess&quot;</span><span class="p">],</span> <span class="s1">&#39;#e67e22&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;Medium subword</span><span class="se">\n</span><span class="s2">(vocab ~1000)&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;un&quot;</span><span class="p">,</span> <span class="s2">&quot;happi&quot;</span><span class="p">,</span> <span class="s2">&quot;ness&quot;</span><span class="p">],</span> <span class="s1">&#39;#2ecc71&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;Large subword</span><span class="se">\n</span><span class="s2">(vocab ~50000)&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;unhappiness&quot;</span><span class="p">],</span> <span class="s1">&#39;#3498db&#39;</span><span class="p">),</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">color</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">tokenizations</span><span class="p">):</span>
    <span class="c1"># Draw token boxes</span>
    <span class="n">x_pos</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
        <span class="n">width</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.8</span> <span class="o">+</span> <span class="mf">0.4</span>
        <span class="n">rect</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="n">x_pos</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span> <span class="n">width</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> 
                            <span class="n">facecolor</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">rect</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x_pos</span> <span class="o">+</span> <span class="n">width</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">token</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> 
               <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>
        <span class="n">x_pos</span> <span class="o">+=</span> <span class="n">width</span> <span class="o">+</span> <span class="mf">0.15</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;top&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;right&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;bottom&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;left&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    
    <span class="c1"># Show token count</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">11.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span><span class="si">}</span><span class="s1"> tokens&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> 
           <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Tokenizing &quot;unhappiness&quot; at Different Vocabulary Sizes&#39;</span><span class="p">,</span> 
             <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Trade-off: Smaller vocab = longer sequences, Larger vocab = shorter sequences&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Most modern LLMs use 30K-100K subword tokens (the sweet spot)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="special-tokens">
<h3>Special Tokens<a class="headerlink" href="#special-tokens" title="Link to this heading">#</a></h3>
<p>Language models use special tokens to mark structure and boundaries:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Token</p></th>
<th class="head"><p>Name</p></th>
<th class="head"><p>Purpose</p></th>
<th class="head"><p>Used By</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">[PAD]</span></code></p></td>
<td><p>Padding</p></td>
<td><p>Fill shorter sequences to equal length in a batch</p></td>
<td><p>All models</p></td>
<td><p>Padding short stints to match longest stint length</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">[UNK]</span></code></p></td>
<td><p>Unknown</p></td>
<td><p>Represent tokens not in vocabulary</p></td>
<td><p>Word-level models</p></td>
<td><p>“Unknown flag condition” – never seen before</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">[BOS]</span></code> / <code class="docutils literal notranslate"><span class="pre">&lt;s&gt;</span></code></p></td>
<td><p>Beginning of Sequence</p></td>
<td><p>Mark where text starts</p></td>
<td><p>GPT, generation models</p></td>
<td><p>Race start / lights out</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">[EOS]</span></code> / <code class="docutils literal notranslate"><span class="pre">&lt;/s&gt;</span></code></p></td>
<td><p>End of Sequence</p></td>
<td><p>Signal the model to stop generating</p></td>
<td><p>GPT, generation models</p></td>
<td><p>Chequered flag</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">[CLS]</span></code></p></td>
<td><p>Classification</p></td>
<td><p>Aggregate representation for classification tasks</p></td>
<td><p>BERT</p></td>
<td><p>Race summary token</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">[SEP]</span></code></p></td>
<td><p>Separator</p></td>
<td><p>Separate two sentences in a pair</p></td>
<td><p>BERT</p></td>
<td><p>Separating qualifying data from race data</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">[MASK]</span></code></p></td>
<td><p>Mask</p></td>
<td><p>Placeholder for masked language modeling</p></td>
<td><p>BERT</p></td>
<td><p>Hidden event: “On lap 15, [MASK] pitted” – predict who</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>What this means:</strong> Special tokens are the “punctuation” of the model’s internal language. They tell the model where sequences start and end, how to separate inputs, and where to output predictions.</p>
</section>
<section id="tokenization-comparison-table">
<h3>Tokenization Comparison Table<a class="headerlink" href="#tokenization-comparison-table" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>How It Works</p></th>
<th class="head"><p>Vocab Size</p></th>
<th class="head"><p>Used By</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>BPE</strong></p></td>
<td><p>Merge most frequent character pairs</p></td>
<td><p>30K-50K</p></td>
<td><p>GPT-2, GPT-3, GPT-4, LLaMA</p></td>
</tr>
<tr class="row-odd"><td><p><strong>WordPiece</strong></p></td>
<td><p>Like BPE but uses likelihood, not frequency</p></td>
<td><p>30K</p></td>
<td><p>BERT, DistilBERT</p></td>
</tr>
<tr class="row-even"><td><p><strong>SentencePiece</strong></p></td>
<td><p>BPE/Unigram on raw text (no pre-tokenization)</p></td>
<td><p>32K-256K</p></td>
<td><p>T5, LLaMA, multilingual models</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Tiktoken</strong></p></td>
<td><p>BPE with byte-level fallback</p></td>
<td><p>100K</p></td>
<td><p>GPT-4, Claude</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<hr class="docutils" />
<section id="gpt-architecture-decoder-only">
<h2>3. GPT Architecture (Decoder-Only)<a class="headerlink" href="#gpt-architecture-decoder-only" title="Link to this heading">#</a></h2>
<section id="id2">
<h3>Intuitive Explanation<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>GPT stands for <strong>Generative Pre-trained Transformer</strong>. It uses only the <strong>decoder</strong> part of the Transformer (from notebook 17) to generate text left-to-right.</p>
<p><strong>Why decoder-only works for generation:</strong> When you write a sentence, you write one word at a time, left to right. Each word depends only on what came before it. A decoder-only model mirrors this natural process with <strong>causal masking</strong> – each position can only attend to positions before it (and itself), never to future positions.</p>
<p><strong>Key insight:</strong> The same architecture that predicts the next token during training can generate text at inference time by repeatedly sampling from its predictions.</p>
<p><strong>F1 analogy:</strong> GPT is like a <strong>race commentary generator</strong> or a <strong>lap time predictor from history</strong>. Given everything that has happened so far in the race, it predicts the next event – and then feeds that prediction back in to predict the event after that. “Verstappen leads lap 1” -&gt; predicts “Norris closes gap in sector 2” -&gt; predicts “DRS enabled on lap 3” -&gt; and so on. It can only look backward (causal masking), just as a commentator can only describe what has already happened when predicting what comes next.</p>
</section>
<section id="architecture-overview">
<h3>Architecture Overview<a class="headerlink" href="#architecture-overview" title="Link to this heading">#</a></h3>
<p>The GPT architecture is a stack of identical decoder blocks:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Input</span> <span class="n">Text</span><span class="p">:</span> <span class="s2">&quot;The cat sat&quot;</span>
    <span class="o">|</span>
    <span class="n">v</span>
<span class="p">[</span><span class="n">Token</span> <span class="n">Embedding</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">Positional</span> <span class="n">Encoding</span><span class="p">]</span>
    <span class="o">|</span>
    <span class="n">v</span>
<span class="p">[</span><span class="n">Decoder</span> <span class="n">Block</span> <span class="mi">1</span><span class="p">]</span> <span class="o">--</span> <span class="n">Masked</span> <span class="n">Self</span><span class="o">-</span><span class="n">Attention</span> <span class="o">-&gt;</span> <span class="n">Feed</span><span class="o">-</span><span class="n">Forward</span>
    <span class="o">|</span>
    <span class="n">v</span>
<span class="p">[</span><span class="n">Decoder</span> <span class="n">Block</span> <span class="mi">2</span><span class="p">]</span> <span class="o">--</span> <span class="n">Masked</span> <span class="n">Self</span><span class="o">-</span><span class="n">Attention</span> <span class="o">-&gt;</span> <span class="n">Feed</span><span class="o">-</span><span class="n">Forward</span>
    <span class="o">|</span>
    <span class="n">v</span>
   <span class="o">...</span><span class="n">N</span> <span class="n">blocks</span><span class="o">...</span>
    <span class="o">|</span>
    <span class="n">v</span>
<span class="p">[</span><span class="n">Layer</span> <span class="n">Norm</span><span class="p">]</span>
    <span class="o">|</span>
    <span class="n">v</span>
<span class="p">[</span><span class="n">Linear</span> <span class="n">Head</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="n">Vocabulary</span><span class="o">-</span><span class="n">sized</span> <span class="n">logits</span>
    <span class="o">|</span>
    <span class="n">v</span>
<span class="n">P</span><span class="p">(</span><span class="s2">&quot;on&quot;</span><span class="p">)</span> <span class="o">=</span> <span class="mf">0.35</span><span class="p">,</span> <span class="n">P</span><span class="p">(</span><span class="s2">&quot;down&quot;</span><span class="p">)</span> <span class="o">=</span> <span class="mf">0.12</span><span class="p">,</span> <span class="o">...</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualization: GPT Architecture and Causal Masking</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Left: Causal masking pattern</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;The&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;sat&quot;</span><span class="p">,</span> <span class="s2">&quot;on&quot;</span><span class="p">,</span> <span class="s2">&quot;the&quot;</span><span class="p">]</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>

<span class="c1"># Create causal mask (lower triangular)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">)))</span>

<span class="n">im</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Key (can attend to)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Query (predicting from)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;GPT: Causal (Left-to-Right) Mask&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># Add text annotations</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;white&#39;</span> <span class="k">if</span> <span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="k">else</span> <span class="s1">&#39;black&#39;</span>
        <span class="n">symbol</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\u2713</span><span class="s1">&#39;</span> <span class="k">if</span> <span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="k">else</span> <span class="s1">&#39;</span><span class="se">\u2717</span><span class="s1">&#39;</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">symbol</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> 
                   <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># Right: What each position predicts</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;GPT: Each Position Predicts Next Token&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s2">&quot;The&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s1">&#39;#3498db&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;The cat&quot;</span><span class="p">,</span> <span class="s2">&quot;sat&quot;</span><span class="p">,</span> <span class="s1">&#39;#2ecc71&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;The cat sat&quot;</span><span class="p">,</span> <span class="s2">&quot;on&quot;</span><span class="p">,</span> <span class="s1">&#39;#e67e22&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;The cat sat on&quot;</span><span class="p">,</span> <span class="s2">&quot;the&quot;</span><span class="p">,</span> <span class="s1">&#39;#9b59b6&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;The cat sat on the&quot;</span><span class="p">,</span> <span class="s2">&quot;mat&quot;</span><span class="p">,</span> <span class="s1">&#39;#e74c3c&#39;</span><span class="p">),</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">color</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">predictions</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="mf">4.5</span> <span class="o">-</span> <span class="n">i</span>
    <span class="c1"># Context box</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">y</span> <span class="o">-</span> <span class="mf">0.2</span><span class="p">),</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> 
                     <span class="n">facecolor</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">2.45</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
    <span class="c1"># Arrow</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">6.5</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span>
                    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
    <span class="c1"># Prediction</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">7.3</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="se">\u2192</span><span class="s1"> &quot;</span><span class="si">{</span><span class="n">pred</span><span class="si">}</span><span class="s1">&quot;&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> 
               <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;top&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;right&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;bottom&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;left&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Left: Each row shows which positions that token can attend to&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Right: GPT is trained so every position predicts the NEXT token&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">This is why GPT is autoregressive: it generates one token at a time, left to right&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="implementing-a-mini-gpt-model">
<h3>Implementing a Mini-GPT Model<a class="headerlink" href="#implementing-a-mini-gpt-model" title="Link to this heading">#</a></h3>
<p>Let’s build a small GPT model. This uses the same components from notebook 17 (multi-head attention, feed-forward layers) but arranged as a <strong>decoder-only</strong> model with causal masking.</p>
<p><strong>F1 framing:</strong> We are building a mini race-event predictor. Feed in the sequence of events, and it predicts the next one – all using causal attention so it can only look at what has already happened, never peek into the future.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">CausalSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multi-head self-attention with causal (autoregressive) masking.</span>
<span class="sd">    Each position can only attend to itself and earlier positions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">n_heads</span> <span class="o">==</span> <span class="mi">0</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_heads</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">W_qkv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        
        <span class="c1"># Causal mask: lower triangular matrix</span>
        <span class="c1"># Register as buffer so it moves to GPU with model but isn&#39;t a parameter</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;mask&#39;</span><span class="p">,</span> <span class="n">mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">))</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        
        <span class="c1"># Compute Q, K, V in one projection</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_qkv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">qkv</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Reshape for multi-head attention</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Scaled dot-product attention with causal mask</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">T</span><span class="p">,</span> <span class="p">:</span><span class="n">T</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
        
        <span class="c1"># Apply attention to values</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">attn</span> <span class="o">@</span> <span class="n">v</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">GPTBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A single GPT decoder block: LayerNorm -&gt; Causal Attention -&gt; LayerNorm -&gt; FFN</span>
<span class="sd">    Uses pre-norm (LayerNorm before attention) like GPT-2.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">d_ff</span> <span class="o">=</span> <span class="n">d_ff</span> <span class="ow">or</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">d_model</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">ln1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">CausalSelfAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ln2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">),</span>
        <span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ln1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>   <span class="c1"># Residual + attention</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ln2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>     <span class="c1"># Residual + FFN</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span><span class="w"> </span><span class="nc">MiniGPT</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A minimal GPT language model.</span>
<span class="sd">    </span>
<span class="sd">    Architecture: Token Embed + Pos Embed -&gt; N x GPTBlock -&gt; LayerNorm -&gt; Linear</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        vocab_size: Number of tokens in vocabulary</span>
<span class="sd">        d_model: Embedding dimension</span>
<span class="sd">        n_heads: Number of attention heads</span>
<span class="sd">        n_layers: Number of decoder blocks</span>
<span class="sd">        max_seq_len: Maximum sequence length</span>
<span class="sd">        dropout: Dropout rate</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> 
                 <span class="n">max_seq_len</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">token_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="p">[</span>
            <span class="n">GPTBlock</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)</span>
        <span class="p">])</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">ln_final</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
        <span class="c1"># Weight tying: share weights between token embedding and output head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_embedding</span><span class="o">.</span><span class="n">weight</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="o">=</span> <span class="n">max_seq_len</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">()</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize weights following GPT-2 conventions.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            idx: Token indices, shape (batch, seq_len)</span>
<span class="sd">            targets: Target token indices for loss computation</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            logits: Shape (batch, seq_len, vocab_size)</span>
<span class="sd">            loss: Cross-entropy loss (if targets provided)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">idx</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">assert</span> <span class="n">T</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Sequence length </span><span class="si">{</span><span class="n">T</span><span class="si">}</span><span class="s2"> exceeds max </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span><span class="si">}</span><span class="s2">&quot;</span>
        
        <span class="c1"># Token + positional embeddings</span>
        <span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">idx</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_embedding</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding</span><span class="p">(</span><span class="n">positions</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Transformer blocks</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ln_final</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Project to vocabulary</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">targets</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        
        <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>
    
    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate tokens autoregressively.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            idx: Starting token indices, shape (batch, seq_len)</span>
<span class="sd">            max_new_tokens: Number of new tokens to generate</span>
<span class="sd">            temperature: Controls randomness (higher = more random)</span>
<span class="sd">            top_k: If set, only sample from top-k most likely tokens</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            Token indices including generated tokens</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
            <span class="c1"># Crop to max_seq_len if needed</span>
            <span class="n">idx_cond</span> <span class="o">=</span> <span class="n">idx</span> <span class="k">if</span> <span class="n">idx</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="k">else</span> <span class="n">idx</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">:]</span>
            
            <span class="c1"># Forward pass</span>
            <span class="n">logits</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">idx_cond</span><span class="p">)</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">/</span> <span class="n">temperature</span>  <span class="c1"># Take last position, apply temperature</span>
            
            <span class="c1"># Optional top-k filtering</span>
            <span class="k">if</span> <span class="n">top_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">v</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">top_k</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
                <span class="n">logits</span><span class="p">[</span><span class="n">logits</span> <span class="o">&lt;</span> <span class="n">v</span><span class="p">[:,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">)</span>
            
            <span class="c1"># Sample from distribution</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">next_token</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            
            <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">idx</span><span class="p">,</span> <span class="n">next_token</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">idx</span>

<span class="c1"># Create a mini-GPT and inspect it</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MiniGPT</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>

<span class="n">total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mini-GPT Architecture&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Vocabulary size: 100&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Embedding dimension: 64&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attention heads: 4&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Decoder layers: 2&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max sequence length: 32&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total parameters: </span><span class="si">{</span><span class="n">total_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model structure:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="comparison-decoder-only-vs-encoder-decoder">
<h3>Comparison: Decoder-Only vs Encoder-Decoder<a class="headerlink" href="#comparison-decoder-only-vs-encoder-decoder" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Aspect</p></th>
<th class="head"><p>Decoder-Only (GPT)</p></th>
<th class="head"><p>Encoder-Decoder (T5, Original Transformer)</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Architecture</strong></p></td>
<td><p>Only decoder blocks with causal masking</p></td>
<td><p>Separate encoder (bidirectional) + decoder (causal)</p></td>
<td><p>Commentary generator vs. telemetry-to-strategy translator</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Attention</strong></p></td>
<td><p>Causal self-attention only</p></td>
<td><p>Encoder: full self-attention; Decoder: causal + cross-attention</p></td>
<td><p>Only past events visible vs. full data + sequential output</p></td>
</tr>
<tr class="row-even"><td><p><strong>Training</strong></p></td>
<td><p>Predict next token</p></td>
<td><p>Map input sequence to output sequence</p></td>
<td><p>Predict next lap event vs. translate telemetry to strategy</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Input/Output</strong></p></td>
<td><p>Single text stream</p></td>
<td><p>Separate input and output streams</p></td>
<td><p>One continuous race narrative vs. separate input/output</p></td>
</tr>
<tr class="row-even"><td><p><strong>Best for</strong></p></td>
<td><p>Generation, completion, chat</p></td>
<td><p>Translation, summarization (structured input-&gt;output)</p></td>
<td><p>Race commentary, predictions vs. data translation tasks</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Examples</strong></p></td>
<td><p>GPT-1/2/3/4, LLaMA, Claude</p></td>
<td><p>T5, BART, original Transformer</p></td>
<td><p>–</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Key insight:</strong> Decoder-only models turn out to be surprisingly versatile. By framing any task as “continue this text,” GPT can do translation, Q&amp;A, summarization, and more – all as text generation.</p>
</section>
<section id="deep-dive-gpt-scaling-history">
<h3>Deep Dive: GPT Scaling History<a class="headerlink" href="#deep-dive-gpt-scaling-history" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Year</p></th>
<th class="head"><p>Parameters</p></th>
<th class="head"><p>Training Data</p></th>
<th class="head"><p>Context Length</p></th>
<th class="head"><p>Key Innovation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>GPT-1</strong></p></td>
<td><p>2018</p></td>
<td><p>117M</p></td>
<td><p>BookCorpus (5GB)</p></td>
<td><p>512</p></td>
<td><p>Proved unsupervised pre-training works</p></td>
</tr>
<tr class="row-odd"><td><p><strong>GPT-2</strong></p></td>
<td><p>2019</p></td>
<td><p>1.5B</p></td>
<td><p>WebText (40GB)</p></td>
<td><p>1024</p></td>
<td><p>Zero-shot task transfer</p></td>
</tr>
<tr class="row-even"><td><p><strong>GPT-3</strong></p></td>
<td><p>2020</p></td>
<td><p>175B</p></td>
<td><p>300B tokens</p></td>
<td><p>2048</p></td>
<td><p>In-context learning, few-shot prompting</p></td>
</tr>
<tr class="row-odd"><td><p><strong>GPT-4</strong></p></td>
<td><p>2023</p></td>
<td><p>~1.8T (est.)</p></td>
<td><p>~13T tokens (est.)</p></td>
<td><p>8K-128K</p></td>
<td><p>Multimodal, instruction following</p></td>
</tr>
</tbody>
</table>
</div>
<section id="key-insight">
<h4>Key Insight<a class="headerlink" href="#key-insight" title="Link to this heading">#</a></h4>
<p>Each generation didn’t change the core architecture much – it primarily <strong>scaled up</strong> parameters, data, and compute. The Transformer architecture from 2017 has proven remarkably scalable. In F1 terms, the fundamental car concept (ground effect, for example) stays the same – the teams that win are the ones that invest the most in refining and developing that concept.</p>
</section>
</section>
</section>
<hr class="docutils" />
<section id="bert-architecture-encoder-only">
<h2>4. BERT Architecture (Encoder-Only)<a class="headerlink" href="#bert-architecture-encoder-only" title="Link to this heading">#</a></h2>
<section id="id3">
<h3>Intuitive Explanation<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>While GPT generates text left-to-right, BERT takes a completely different approach: it reads text <strong>bidirectionally</strong> – looking at both left and right context simultaneously.</p>
<p><strong>Analogy:</strong> Imagine filling in a blank in a sentence: <em>“The [MASK] chased the mouse.”</em> You need to see both what comes before AND after the blank to determine the answer is “cat.” This is exactly what BERT does – it’s trained to fill in randomly masked tokens.</p>
<p><strong>F1 analogy:</strong> BERT is like understanding the <strong>full context of a race situation</strong>. When analyzing “On lap 32, [MASK] made a critical overtake at turn 4,” you need to see both the preceding context (who was in position, what the gaps were) and the following context (the overtake was into P3, the driver was on fresh softs) to determine it was Norris. GPT can only look left; BERT sees the whole picture – that is why BERT excels at <em>understanding</em> rather than <em>generating</em>.</p>
<p><strong>Why encoder-only works for understanding:</strong> Many NLP tasks don’t require generating text – they require <em>understanding</em> it:</p>
<ul class="simple">
<li><p>Is this email spam or not? (classification)</p></li>
<li><p>What is the sentiment of this review? (sentiment analysis)</p></li>
<li><p>Which word does “it” refer to? (coreference resolution)</p></li>
<li><p>Where is the answer in this paragraph? (extractive QA)</p></li>
</ul>
<p>For all these tasks, looking at the <strong>full context</strong> (both directions) gives better understanding than only looking left-to-right.</p>
</section>
<section id="bert-s-two-training-objectives">
<h3>BERT’s Two Training Objectives<a class="headerlink" href="#bert-s-two-training-objectives" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Objective</p></th>
<th class="head"><p>How It Works</p></th>
<th class="head"><p>What It Teaches</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Masked Language Modeling (MLM)</strong></p></td>
<td><p>Randomly mask 15% of tokens, predict them</p></td>
<td><p>Deep bidirectional understanding of language</p></td>
<td><p>Predict hidden race events from full context</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Next Sentence Prediction (NSP)</strong></p></td>
<td><p>Given two sentences, predict if B follows A</p></td>
<td><p>Understanding relationships between sentences</p></td>
<td><p>“Did this strategy call follow that telemetry reading?”</p></td>
</tr>
</tbody>
</table>
</div>
<section id="mlm-details">
<h4>MLM Details<a class="headerlink" href="#mlm-details" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>15% of tokens are selected for prediction</p></li>
<li><p>Of those: 80% replaced with [MASK], 10% replaced with random token, 10% kept unchanged</p></li>
<li><p>The model must predict the original token for all selected positions</p></li>
</ul>
<p><strong>What this means:</strong> MLM forces BERT to build rich representations that capture meaning from both directions. Unlike GPT which only predicts forward, BERT sees the full picture.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualization: BERT vs GPT Attention Patterns</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;The&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;[MASK]&quot;</span><span class="p">,</span> <span class="s2">&quot;the&quot;</span><span class="p">,</span> <span class="s2">&quot;mouse&quot;</span><span class="p">]</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>

<span class="c1"># GPT: Causal mask (lower triangular)</span>
<span class="n">causal_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">)))</span>
<span class="n">im1</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">causal_mask</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Oranges&#39;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;GPT: Causal Attention</span><span class="se">\n</span><span class="s1">(left-to-right only)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Keys (attends to)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Queries&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">symbol</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\u2713</span><span class="s1">&#39;</span> <span class="k">if</span> <span class="n">causal_mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
        <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;white&#39;</span> <span class="k">if</span> <span class="n">causal_mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="s1">&#39;gray&#39;</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">symbol</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> 
                   <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># BERT: Full bidirectional attention</span>
<span class="n">full_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="n">im2</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">full_mask</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;BERT: Bidirectional Attention</span><span class="se">\n</span><span class="s1">(sees everything)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Keys (attends to)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Queries&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\u2713</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> 
                   <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># Side comparison diagram</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Context Available for &quot;[MASK]&quot;&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># GPT context for position 2</span>
<span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="mi">9</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;#e67e22&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> 
             <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;#e67e22&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">7.2</span><span class="p">,</span> <span class="s1">&#39;GPT at position 3&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#e67e22&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">6.5</span><span class="p">,</span> <span class="s1">&#39;&quot;The&quot;  &quot;cat&quot;&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#e67e22&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">6.5</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\u2190</span><span class="s1"> can only see these&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#e67e22&#39;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;italic&#39;</span><span class="p">)</span>

<span class="c1"># BERT context for [MASK]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="mi">9</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;#2980b9&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> 
             <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;#2980b9&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">4.2</span><span class="p">,</span> <span class="s1">&#39;BERT at [MASK]&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#2980b9&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="s1">&#39;&quot;The&quot; &quot;cat&quot; ... &quot;the&quot; &quot;mouse&quot;&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#2980b9&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\u2190</span><span class="s1"> sees ALL&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#2980b9&#39;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;italic&#39;</span><span class="p">)</span>

<span class="c1"># Verdict</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="s1">&#39;BERT: &quot;chased&quot; (confident)&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> 
       <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#2980b9&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="s1">&#39;GPT: &quot;sat&quot;? &quot;ate&quot;? (uncertain)&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> 
       <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#e67e22&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="k">for</span> <span class="n">spine</span> <span class="ow">in</span> <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
    <span class="n">spine</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;BERT sees both left and right context =&gt; better understanding&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GPT sees only left context =&gt; suited for generation&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="gpt-vs-bert-complete-comparison">
<h3>GPT vs BERT: Complete Comparison<a class="headerlink" href="#gpt-vs-bert-complete-comparison" title="Link to this heading">#</a></h3>
<p>This is one of the most important architectural distinctions in modern NLP:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Aspect</p></th>
<th class="head"><p>GPT (Decoder-Only)</p></th>
<th class="head"><p>BERT (Encoder-Only)</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Direction</strong></p></td>
<td><p>Left-to-right (autoregressive)</p></td>
<td><p>Bidirectional</p></td>
<td><p>Predicting next lap vs. analyzing full race</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Attention mask</strong></p></td>
<td><p>Causal (triangular)</p></td>
<td><p>Full (no mask)</p></td>
<td><p>Only past events vs. full race context</p></td>
</tr>
<tr class="row-even"><td><p><strong>Training objective</strong></p></td>
<td><p>Predict next token</p></td>
<td><p>Predict masked tokens + NSP</p></td>
<td><p>Predict next event vs. fill in hidden events</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Pre-training task</strong></p></td>
<td><p>Language modeling</p></td>
<td><p>Masked language modeling</p></td>
<td><p>Race commentary vs. race analysis</p></td>
</tr>
<tr class="row-even"><td><p><strong>Output</strong></p></td>
<td><p>Next-token probabilities</p></td>
<td><p>Contextualized embeddings</p></td>
<td><p>“What happens next?” vs. “What does this data mean?”</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Generation</strong></p></td>
<td><p>Natural (sample next token)</p></td>
<td><p>Unnatural (not designed for it)</p></td>
<td><p>Generates commentary vs. not a generator</p></td>
</tr>
<tr class="row-even"><td><p><strong>Understanding</strong></p></td>
<td><p>Limited (only left context)</p></td>
<td><p>Superior (full context)</p></td>
<td><p>Partial picture vs. full picture</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Fine-tuning</strong></p></td>
<td><p>Prompt-based / instruction-tuning</p></td>
<td><p>Add classification head on [CLS]</p></td>
<td><p>Adapt via prompts vs. add decision layer</p></td>
</tr>
<tr class="row-even"><td><p><strong>Parameters (base)</strong></p></td>
<td><p>GPT-2: 117M - 1.5B</p></td>
<td><p>BERT-base: 110M, BERT-large: 340M</p></td>
<td><p>–</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Best for</strong></p></td>
<td><p>Text generation, chatbots, coding</p></td>
<td><p>Classification, NER, QA, search</p></td>
<td><p>Commentary, prediction vs. classification, analysis</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="when-to-use-which">
<h3>When to Use Which?<a class="headerlink" href="#when-to-use-which" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Task</p></th>
<th class="head"><p>Best Architecture</p></th>
<th class="head"><p>Why</p></th>
<th class="head"><p>F1 Framing</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Chatbot / dialogue</p></td>
<td><p><strong>GPT</strong></p></td>
<td><p>Needs to generate fluent responses</p></td>
<td><p>Race commentary / team radio generation</p></td>
</tr>
<tr class="row-odd"><td><p>Text classification</p></td>
<td><p><strong>BERT</strong></p></td>
<td><p>Needs to understand full document</p></td>
<td><p>“Was this a good strategy?” (yes/no)</p></td>
</tr>
<tr class="row-even"><td><p>Code generation</p></td>
<td><p><strong>GPT</strong></p></td>
<td><p>Code is written left-to-right</p></td>
<td><p>Strategy algorithm generation</p></td>
</tr>
<tr class="row-odd"><td><p>Named entity recognition</p></td>
<td><p><strong>BERT</strong></p></td>
<td><p>Needs bidirectional context for each token</p></td>
<td><p>Identify drivers, teams, circuits in text</p></td>
</tr>
<tr class="row-even"><td><p>Creative writing</p></td>
<td><p><strong>GPT</strong></p></td>
<td><p>Generation task</p></td>
<td><p>Generate race preview articles</p></td>
</tr>
<tr class="row-odd"><td><p>Semantic search</p></td>
<td><p><strong>BERT</strong></p></td>
<td><p>Needs rich sentence embeddings</p></td>
<td><p>Find similar race situations in the archive</p></td>
</tr>
<tr class="row-even"><td><p>Translation</p></td>
<td><p><strong>Encoder-Decoder</strong></p></td>
<td><p>Structured input-to-output mapping</p></td>
<td><p>Telemetry-to-English translation</p></td>
</tr>
<tr class="row-odd"><td><p>Summarization</p></td>
<td><p><strong>GPT</strong> or <strong>Encoder-Decoder</strong></p></td>
<td><p>Generation with input understanding</p></td>
<td><p>Post-race summary generation</p></td>
</tr>
</tbody>
</table>
</div>
<section id="id4">
<h4>Key Insight<a class="headerlink" href="#id4" title="Link to this heading">#</a></h4>
<p>The trend in 2023-2024+ has been toward <strong>large decoder-only models</strong> (GPT-4, Claude, LLaMA, Gemini). It turns out that with enough scale and the right training, decoder-only models can match or exceed BERT-style models even on understanding tasks. But BERT-style models remain popular for efficient, focused applications where generation isn’t needed.</p>
</section>
</section>
</section>
<hr class="docutils" />
<section id="training-a-small-language-model">
<h2>5. Training a Small Language Model<a class="headerlink" href="#training-a-small-language-model" title="Link to this heading">#</a></h2>
<section id="id5">
<h3>Intuitive Explanation<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<p>Let’s put theory into practice by training a <strong>character-level GPT</strong> on a small dataset. Character-level means each token is a single character – this keeps our vocabulary tiny and training fast, while still demonstrating all the core concepts.</p>
<p><strong>F1 framing:</strong> Think of this as training a tiny race commentator. We feed it examples of text, and it learns to predict what character comes next – eventually generating coherent-looking sequences. With a small model and dataset, we will not get Brundle-quality commentary, but we will see the fundamental mechanism in action.</p>
<p>We’ll:</p>
<ol class="arabic simple">
<li><p>Prepare a simple text dataset</p></li>
<li><p>Create character-level tokenization</p></li>
<li><p>Train our MiniGPT model</p></li>
<li><p>Generate text and explore temperature sampling</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Training data: simple repetitive patterns for fast learning</span>
<span class="n">training_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">the cat sat on the mat. the cat ran to the hat.</span>
<span class="s2">the dog sat on the log. the dog ran to the fog.</span>
<span class="s2">a big cat sat on a big mat. a small dog sat on a small log.</span>
<span class="s2">the cat and the dog sat on the mat.</span>
<span class="s2">the cat saw the dog. the dog saw the cat.</span>
<span class="s2">&quot;&quot;&quot;</span> <span class="o">*</span> <span class="mi">100</span>  <span class="c1"># Repeat for more training data</span>

<span class="c1"># Character-level tokenization</span>
<span class="n">chars</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">training_text</span><span class="p">)))</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
<span class="n">char_to_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">ch</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>
<span class="n">idx_to_char</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">ch</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>

<span class="c1"># Encode the text</span>
<span class="k">def</span><span class="w"> </span><span class="nf">encode</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">char_to_idx</span><span class="p">[</span><span class="n">ch</span><span class="p">]</span> <span class="k">for</span> <span class="n">ch</span> <span class="ow">in</span> <span class="n">text</span><span class="p">]</span>

<span class="k">def</span><span class="w"> </span><span class="nf">decode</span><span class="p">(</span><span class="n">indices</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">idx_to_char</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">encode</span><span class="p">(</span><span class="n">training_text</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Vocabulary size: </span><span class="si">{</span><span class="n">vocab_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Characters: </span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">chars</span><span class="p">))</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total characters in training data: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Example encoding: &#39;cat&#39; -&gt; </span><span class="si">{</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;cat&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Example decoding: </span><span class="si">{</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;cat&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2"> -&gt; &#39;</span><span class="si">{</span><span class="n">decode</span><span class="p">(</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;cat&#39;</span><span class="p">))</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Data loader for training</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_batch</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">block_size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get a random batch of training examples.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        data: Full training data as tensor</span>
<span class="sd">        batch_size: Number of sequences per batch</span>
<span class="sd">        block_size: Length of each sequence</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        x: Input sequences (batch_size, block_size)</span>
<span class="sd">        y: Target sequences (batch_size, block_size) - shifted by 1</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">-</span> <span class="n">block_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">block_size</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ix</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">block_size</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ix</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>

<span class="c1"># Test the batch function</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input batch shape: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Target batch shape: </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Example input:  &#39;</span><span class="si">{</span><span class="n">decode</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Example target: &#39;</span><span class="si">{</span><span class="n">decode</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Notice: Target is the input shifted by one character (next-char prediction)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train the model</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Model hyperparameters</span>
<span class="n">block_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">n_heads</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">n_layers</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">3e-4</span>
<span class="n">n_steps</span> <span class="o">=</span> <span class="mi">2000</span>

<span class="c1"># Create model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MiniGPT</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="o">=</span><span class="n">n_heads</span><span class="p">,</span>
    <span class="n">n_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span>
    <span class="n">max_seq_len</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span>
<span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="c1"># Training loop</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">samples_during_training</span> <span class="o">=</span> <span class="p">[]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training MiniGPT (</span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> parameters)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="c1"># Get batch</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">block_size</span><span class="p">)</span>
    
    <span class="c1"># Forward pass</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    
    <span class="c1"># Backward pass</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    
    <span class="c1"># Log progress</span>
    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">400</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">step</span> <span class="o">==</span> <span class="n">n_steps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># Generate a sample</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="n">encode</span><span class="p">(</span><span class="s2">&quot;the &quot;</span><span class="p">)</span>
        <span class="n">prompt_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">prompt</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
        <span class="n">generated</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt_tensor</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">decode</span><span class="p">(</span><span class="n">generated</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
        <span class="n">samples_during_training</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">step</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">sample</span><span class="p">))</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Step </span><span class="si">{</span><span class="n">step</span><span class="si">:</span><span class="s2">4d</span><span class="si">}</span><span class="s2"> | Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | Sample: &#39;</span><span class="si">{</span><span class="n">sample</span><span class="p">[:</span><span class="mi">50</span><span class="p">]</span><span class="si">}</span><span class="s2">...&#39;&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Training complete!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize training progress</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Loss curve</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Raw loss&#39;</span><span class="p">)</span>
<span class="c1"># Smoothed loss</span>
<span class="n">window</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">smoothed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">window</span><span class="p">)</span><span class="o">/</span><span class="n">window</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">window</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">)),</span> <span class="n">smoothed</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Smoothed&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Training Step&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Cross-Entropy Loss&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Training Loss Over Time&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Generation quality over time</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Generation Quality During Training&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="n">y_pos</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">samples_during_training</span><span class="p">:</span>
    <span class="c1"># Color based on loss</span>
    <span class="n">color</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">RdYlGn</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="nb">min</span><span class="p">(</span><span class="n">loss</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">truncated</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[:</span><span class="mi">45</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;...&#39;</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">45</span> <span class="k">else</span> <span class="n">sample</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">y_pos</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Step </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> 
           <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">y_pos</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;&#39;</span><span class="si">{</span><span class="n">truncated</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> 
           <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="s1">&#39;monospace&#39;</span><span class="p">)</span>
    <span class="n">y_pos</span> <span class="o">-=</span> <span class="mf">0.15</span>

<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="s2">&quot;Green = low loss (good), Red = high loss (bad)&quot;</span><span class="p">,</span> 
       <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;italic&#39;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="k">for</span> <span class="n">spine</span> <span class="ow">in</span> <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
    <span class="n">spine</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Watch how generation quality improves as loss decreases!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="temperature-sampling">
<h3>Temperature Sampling<a class="headerlink" href="#temperature-sampling" title="Link to this heading">#</a></h3>
<p><strong>Temperature</strong> controls the randomness of generation:</p>
<div class="math notranslate nohighlight">
\[P(token) = \frac{\exp(logit_i / T)}{\sum_j \exp(logit_j / T)}\]</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Temperature</p></th>
<th class="head"><p>Effect</p></th>
<th class="head"><p>Use Case</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>T &lt; 1.0</p></td>
<td><p>Sharper distribution, more deterministic</p></td>
<td><p>Factual answers, code</p></td>
<td><p>Conservative strategy: “Stay on plan, no surprises”</p></td>
</tr>
<tr class="row-odd"><td><p>T = 1.0</p></td>
<td><p>Original distribution</p></td>
<td><p>General use</p></td>
<td><p>Balanced strategy: weigh all options fairly</p></td>
</tr>
<tr class="row-even"><td><p>T &gt; 1.0</p></td>
<td><p>Flatter distribution, more random</p></td>
<td><p>Creative writing</p></td>
<td><p>Aggressive strategy: “Consider the unlikely – maybe a 3-stop works?”</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>What this means:</strong> Low temperature makes the model “confident” (picks high-probability tokens), while high temperature makes it “creative” (considers unlikely tokens).</p>
<p><strong>F1 analogy:</strong> Temperature is like the aggressiveness dial on a strategy computer. Low temperature (conservative) = the strategy sticks to the obvious call, like a safe one-stop. High temperature (aggressive) = the strategy considers wild alternatives, like an early pit under a virtual safety car or switching to inters on a drying track. Sometimes the aggressive call wins the race.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualization: Temperature effect on probability distribution</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Simulated logits for next character</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">])</span>
<span class="n">chars_viz</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;t&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">]</span>

<span class="n">temperatures</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">temp</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">temperatures</span><span class="p">):</span>
    <span class="c1"># Apply temperature and softmax</span>
    <span class="n">scaled</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">/</span> <span class="n">temp</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scaled</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scaled</span><span class="p">))</span>
    
    <span class="n">colors</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">(</span><span class="n">probs</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="n">probs</span><span class="p">))</span>
    <span class="n">bars</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">chars_viz</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Character&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span> <span class="k">if</span> <span class="n">temp</span> <span class="o">==</span> <span class="mf">0.3</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;T = </span><span class="si">{</span><span class="n">temp</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
    
    <span class="c1"># Annotate top prob</span>
    <span class="n">max_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">max_idx</span><span class="p">,</span> <span class="n">probs</span><span class="p">[</span><span class="n">max_idx</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.02</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">probs</span><span class="p">[</span><span class="n">max_idx</span><span class="p">]</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> 
           <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Effect of Temperature on Token Probability Distribution&#39;</span><span class="p">,</span> 
            <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.02</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;T=0.3: Very peaked (deterministic) -&gt; always picks &#39;a&#39;&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;T=2.0: Very flat (random) -&gt; might pick any character&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate at different temperatures</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;the cat &quot;</span>
<span class="n">prompt_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt</span><span class="p">)],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Generating from prompt: &#39;&quot;</span> <span class="o">+</span> <span class="n">prompt</span> <span class="o">+</span> <span class="s2">&quot;&#39;&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

<span class="k">for</span> <span class="n">temp</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Temperature = </span><span class="si">{</span><span class="n">temp</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
        <span class="n">generated</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt_tokens</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="n">temp</span><span class="p">)</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">decode</span><span class="p">(</span><span class="n">generated</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">. &#39;</span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="scaling-laws">
<h2>6. Scaling Laws<a class="headerlink" href="#scaling-laws" title="Link to this heading">#</a></h2>
<section id="id6">
<h3>Intuitive Explanation<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<p>One of the most remarkable discoveries in deep learning is that language model performance follows <strong>predictable scaling laws</strong>. As you increase model size, training data, or compute, performance improves in a smooth, predictable way.</p>
<p>The key insight from OpenAI and DeepMind research:</p>
<div class="math notranslate nohighlight">
\[L(N, D, C) \approx \left(\frac{N_c}{N}\right)^{\alpha_N} + \left(\frac{D_c}{D}\right)^{\alpha_D} + \left(\frac{C_c}{C}\right)^{\alpha_C}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L\)</span> = Loss (lower is better)</p></li>
<li><p><span class="math notranslate nohighlight">\(N\)</span> = Number of parameters</p></li>
<li><p><span class="math notranslate nohighlight">\(D\)</span> = Dataset size (tokens)</p></li>
<li><p><span class="math notranslate nohighlight">\(C\)</span> = Compute (FLOPs)</p></li>
</ul>
<p><strong>What this means:</strong> If you want a 10x better model, you can predict exactly how much bigger it needs to be, how much more data you need, or how much more compute you need.</p>
<p><strong>F1 analogy:</strong> Scaling laws in AI are like development budgets in F1. The relationship between spending and performance is remarkably predictable: more wind tunnel hours, more CFD simulations, more development tokens all improve lap time in a smooth, power-law fashion. Just as F1 teams can predict “X million in aero development yields Y tenths per lap,” AI labs can predict “X more compute yields Y reduction in loss.” The cost cap in F1 is essentially a compute budget constraint.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualization: Scaling laws</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Parameters scaling</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>  <span class="c1"># 1M to 1T</span>
<span class="n">loss_params</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="p">(</span><span class="n">params</span> <span class="o">/</span> <span class="mf">1e6</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.076</span><span class="p">)</span>  <span class="c1"># Approximate scaling</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">loss_params</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Parameters&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Scaling with Model Size&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">)</span>

<span class="c1"># Mark some famous models</span>
<span class="n">models_p</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;GPT-2&#39;</span><span class="p">,</span> <span class="mf">1.5e9</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;GPT-3&#39;</span><span class="p">,</span> <span class="mf">175e9</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;GPT-4&#39;</span><span class="p">,</span> <span class="mf">1.8e12</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">),</span>
<span class="p">]</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">loss</span> <span class="ow">in</span> <span class="n">models_p</span><span class="p">:</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">size</span><span class="p">],</span> <span class="p">[</span><span class="n">loss</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">loss</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>

<span class="c1"># Data scaling</span>
<span class="n">data_size</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>  <span class="c1"># 1B to 10T tokens</span>
<span class="n">loss_data</span> <span class="o">=</span> <span class="mi">8</span> <span class="o">*</span> <span class="p">(</span><span class="n">data_size</span> <span class="o">/</span> <span class="mf">1e9</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.095</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">data_size</span><span class="p">,</span> <span class="n">loss_data</span><span class="p">,</span> <span class="s1">&#39;g-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Training Tokens&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Scaling with Dataset Size&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">)</span>

<span class="c1"># Compute scaling</span>
<span class="n">compute</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">17</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>  <span class="c1"># FLOPs</span>
<span class="n">loss_compute</span> <span class="o">=</span> <span class="mi">20</span> <span class="o">*</span> <span class="p">(</span><span class="n">compute</span> <span class="o">/</span> <span class="mf">1e17</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">compute</span><span class="p">,</span> <span class="n">loss_compute</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Compute (FLOPs)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Scaling with Compute&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Neural Scaling Laws: Predictable Performance Improvements&#39;</span><span class="p">,</span> 
            <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.02</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Key insight: Performance improves predictably on log-log scale&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;This allows researchers to plan training runs and predict outcomes&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Timeline of model sizes</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">models</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="mf">2018.5</span><span class="p">,</span> <span class="s1">&#39;GPT-1&#39;</span><span class="p">,</span> <span class="mf">0.117</span><span class="p">,</span> <span class="s1">&#39;OpenAI&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="mf">2018.8</span><span class="p">,</span> <span class="s1">&#39;BERT&#39;</span><span class="p">,</span> <span class="mf">0.34</span><span class="p">,</span> <span class="s1">&#39;Google&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="mf">2019.2</span><span class="p">,</span> <span class="s1">&#39;GPT-2&#39;</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="s1">&#39;OpenAI&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="mf">2020.5</span><span class="p">,</span> <span class="s1">&#39;GPT-3&#39;</span><span class="p">,</span> <span class="mi">175</span><span class="p">,</span> <span class="s1">&#39;OpenAI&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="mf">2022.0</span><span class="p">,</span> <span class="s1">&#39;PaLM&#39;</span><span class="p">,</span> <span class="mi">540</span><span class="p">,</span> <span class="s1">&#39;Google&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="mf">2022.3</span><span class="p">,</span> <span class="s1">&#39;Chinchilla&#39;</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="s1">&#39;DeepMind&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="mf">2023.0</span><span class="p">,</span> <span class="s1">&#39;GPT-4&#39;</span><span class="p">,</span> <span class="mi">1800</span><span class="p">,</span> <span class="s1">&#39;OpenAI&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="mf">2023.2</span><span class="p">,</span> <span class="s1">&#39;LLaMA&#39;</span><span class="p">,</span> <span class="mi">65</span><span class="p">,</span> <span class="s1">&#39;Meta&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="mf">2023.7</span><span class="p">,</span> <span class="s1">&#39;Llama-2&#39;</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="s1">&#39;Meta&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="mf">2024.0</span><span class="p">,</span> <span class="s1">&#39;Gemini Ultra&#39;</span><span class="p">,</span> <span class="mi">1500</span><span class="p">,</span> <span class="s1">&#39;Google&#39;</span><span class="p">),</span>
<span class="p">]</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;OpenAI&#39;</span><span class="p">:</span> <span class="s1">&#39;#2ecc71&#39;</span><span class="p">,</span> <span class="s1">&#39;Google&#39;</span><span class="p">:</span> <span class="s1">&#39;#3498db&#39;</span><span class="p">,</span> <span class="s1">&#39;DeepMind&#39;</span><span class="p">:</span> <span class="s1">&#39;#9b59b6&#39;</span><span class="p">,</span> <span class="s1">&#39;Meta&#39;</span><span class="p">:</span> <span class="s1">&#39;#e74c3c&#39;</span><span class="p">}</span>

<span class="k">for</span> <span class="n">year</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">company</span> <span class="ow">in</span> <span class="n">models</span><span class="p">:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">year</span><span class="p">],</span> <span class="p">[</span><span class="n">params</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">company</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> 
              <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="mi">15</span> <span class="k">if</span> <span class="n">params</span> <span class="o">&lt;</span> <span class="mi">100</span> <span class="k">else</span> <span class="o">-</span><span class="mi">25</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="se">\n</span><span class="s1">(</span><span class="si">{</span><span class="n">params</span><span class="si">}</span><span class="s1">B)&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">year</span><span class="p">,</span> <span class="n">params</span><span class="p">),</span> 
               <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">offset</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">,</span>
               <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Year&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Parameters (Billions)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;The Race to Scale: Language Model Parameter Growth&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">2018</span><span class="p">,</span> <span class="mf">2024.5</span><span class="p">)</span>

<span class="c1"># Legend</span>
<span class="k">for</span> <span class="n">company</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="n">colors</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">([],</span> <span class="p">[],</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">company</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;From 117M (GPT-1) to 1.8T (GPT-4) in 5 years: 15,000x increase!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">But Chinchilla showed: it&#39;s not just about size, data matters too&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="key-scaling-insights">
<h3>Key Scaling Insights<a class="headerlink" href="#key-scaling-insights" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Discovery</p></th>
<th class="head"><p>Implication</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Power law scaling</strong></p></td>
<td><p>Can predict performance before training</p></td>
<td><p>Predict lap time improvement from development spend</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Compute-optimal training</strong></p></td>
<td><p>Balance model size and data (Chinchilla)</p></td>
<td><p>Balance aero and PU development budgets</p></td>
</tr>
<tr class="row-even"><td><p><strong>Emergent abilities</strong></p></td>
<td><p>Some capabilities appear suddenly at scale</p></td>
<td><p>Sudden breakthrough in car concept (e.g., double diffuser)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Diminishing returns</strong></p></td>
<td><p>Each 10x costs more for same improvement</p></td>
<td><p>Last tenth of a second costs more than the first</p></td>
</tr>
<tr class="row-even"><td><p><strong>Architecture matters less</strong></p></td>
<td><p>At scale, different architectures converge</p></td>
<td><p>At the front, all cars converge on similar concepts</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="the-chinchilla-finding">
<h3>The Chinchilla Finding<a class="headerlink" href="#the-chinchilla-finding" title="Link to this heading">#</a></h3>
<p>DeepMind’s Chinchilla paper (2022) showed that many models were <strong>undertrained</strong> – they had too many parameters for the amount of data they saw. The optimal ratio is roughly:</p>
<div class="math notranslate nohighlight">
\[\text{Tokens} \approx 20 \times \text{Parameters}\]</div>
<p><strong>What this means:</strong> A 70B parameter model trained on 1.4T tokens (Chinchilla) outperformed the 280B parameter Gopher trained on 300B tokens. <strong>More data can be better than more parameters.</strong></p>
<p><strong>F1 analogy:</strong> This is like discovering that a team with a smaller budget but more testing days beats a team with a bigger budget but fewer track sessions. The car is only as good as the data used to develop it. Chinchilla showed that “testing” (training data) matters at least as much as “car complexity” (parameters).</p>
</section>
</section>
<hr class="docutils" />
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<section id="exercise-1-extend-bpe-with-encoding">
<h3>Exercise 1: Extend BPE with Encoding<a class="headerlink" href="#exercise-1-extend-bpe-with-encoding" title="Link to this heading">#</a></h3>
<p>Add an <code class="docutils literal notranslate"><span class="pre">encode()</span></code> method to our SimpleBPE class that returns token indices.</p>
<p><strong>F1 framing:</strong> Build a radio message encoder. Given the BPE vocabulary learned from race communications, encode any new team radio message into its token sequence – turning “Box box box, switch to mediums” into a sequence of integer IDs the strategy computer can process.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># EXERCISE: Add encode method to SimpleBPE</span>
<span class="k">def</span><span class="w"> </span><span class="nf">bpe_encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Tokenize and convert to indices.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        text: Input string</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        List of token indices</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement this!</span>
    <span class="c1"># 1. Call self.tokenize(text) to get tokens</span>
    <span class="c1"># 2. Convert each token to its index using self.vocab</span>
    <span class="c1"># Hint: Handle unknown tokens gracefully</span>
    <span class="k">pass</span>

<span class="c1"># Test:</span>
<span class="c1"># SimpleBPE.encode = bpe_encode</span>
<span class="c1"># indices = bpe.encode(&quot;the cat sat&quot;)</span>
<span class="c1"># print(f&quot;Encoded: {indices}&quot;)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-2-top-p-nucleus-sampling">
<h3>Exercise 2: Top-p (Nucleus) Sampling<a class="headerlink" href="#exercise-2-top-p-nucleus-sampling" title="Link to this heading">#</a></h3>
<p>Implement top-p sampling, which samples from the smallest set of tokens whose cumulative probability exceeds p.</p>
<p><strong>F1 framing:</strong> Top-p sampling is like a strategy engineer who considers only the most likely scenarios until they cover, say, 90% of probable outcomes. With p=0.9, if “pit on this lap” (60%) and “pit next lap” (25%) and “stay out” (10%) cover 95%, the engineer ignores the remaining 5% of wild scenarios. Implement this “nucleus” of likely strategy options.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># EXERCISE: Implement top-p sampling</span>
<span class="k">def</span><span class="w"> </span><span class="nf">sample_top_p</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sample from the nucleus (top-p) of the distribution.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        logits: Raw model outputs (vocab_size,)</span>
<span class="sd">        p: Cumulative probability threshold</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        Sampled token index</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement this!</span>
    <span class="c1"># 1. Convert logits to probabilities with softmax</span>
    <span class="c1"># 2. Sort probabilities in descending order</span>
    <span class="c1"># 3. Compute cumulative sum</span>
    <span class="c1"># 4. Find cutoff where cumsum &gt; p</span>
    <span class="c1"># 5. Zero out probabilities below cutoff</span>
    <span class="c1"># 6. Renormalize and sample</span>
    <span class="k">pass</span>

<span class="c1"># Test:</span>
<span class="c1"># logits = torch.randn(100)</span>
<span class="c1"># sampled = sample_top_p(logits, p=0.9)</span>
<span class="c1"># print(f&quot;Sampled token: {sampled}&quot;)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-3-calculate-perplexity">
<h3>Exercise 3: Calculate Perplexity<a class="headerlink" href="#exercise-3-calculate-perplexity" title="Link to this heading">#</a></h3>
<p>Perplexity is the standard metric for language models: <span class="math notranslate nohighlight">\(PPL = \exp(\text{average cross-entropy loss})\)</span></p>
<p><strong>F1 framing:</strong> Perplexity measures how “surprised” the model is by the actual sequence of events. A race predictor with perplexity 10 on a race means it was, on average, as uncertain as choosing between 10 equally likely next events. A perplexity of 2 means the model is almost always choosing between just 2 options – much better calibrated. Calculate this metric for our trained model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># EXERCISE: Calculate perplexity</span>
<span class="k">def</span><span class="w"> </span><span class="nf">calculate_perplexity</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">encode_fn</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate perplexity of a model on given text.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        model: Trained language model</span>
<span class="sd">        text: Text string to evaluate</span>
<span class="sd">        encode_fn: Function to convert text to token indices</span>
<span class="sd">        block_size: Sequence length for evaluation</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        Perplexity value (lower is better)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement this!</span>
    <span class="c1"># 1. Encode the text</span>
    <span class="c1"># 2. Split into sequences of block_size</span>
    <span class="c1"># 3. Compute average loss over all sequences</span>
    <span class="c1"># 4. Return exp(average_loss)</span>
    <span class="k">pass</span>

<span class="c1"># Test:</span>
<span class="c1"># ppl = calculate_perplexity(model, &quot;the cat sat on the mat&quot;, encode)</span>
<span class="c1"># print(f&quot;Perplexity: {ppl:.2f}&quot;)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<section id="key-concepts">
<h3>Key Concepts<a class="headerlink" href="#key-concepts" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Language Model</strong>: Predicts <span class="math notranslate nohighlight">\(P(w_t | w_1, ..., w_{t-1})\)</span> – the probability of the next token given context</p></li>
<li><p><strong>Tokenization</strong>: Breaking text into discrete units; BPE is the dominant modern approach</p></li>
<li><p><strong>GPT (Decoder-Only)</strong>: Left-to-right generation with causal masking</p></li>
<li><p><strong>BERT (Encoder-Only)</strong>: Bidirectional understanding with masked language modeling</p></li>
<li><p><strong>Temperature</strong>: Controls randomness in generation (low = deterministic, high = creative)</p></li>
<li><p><strong>Scaling Laws</strong>: Performance improves predictably with parameters, data, and compute</p></li>
</ul>
</section>
<section id="connection-to-deep-learning">
<h3>Connection to Deep Learning<a class="headerlink" href="#connection-to-deep-learning" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Concept</p></th>
<th class="head"><p>Application</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Next-token prediction</p></td>
<td><p>Foundation of ChatGPT, Claude, and all modern LLMs</p></td>
<td><p>Predicting the next event in a race sequence</p></td>
</tr>
<tr class="row-odd"><td><p>BPE tokenization</p></td>
<td><p>Used by GPT-4, LLaMA, and most production models</p></td>
<td><p>Breaking radio messages into meaningful units</p></td>
</tr>
<tr class="row-even"><td><p>Causal masking</p></td>
<td><p>Enables autoregressive text generation</p></td>
<td><p>Real-time decisions based only on past events</p></td>
</tr>
<tr class="row-odd"><td><p>Bidirectional attention</p></td>
<td><p>Powers search engines and classification systems</p></td>
<td><p>Full race situation analysis (BERT-style understanding)</p></td>
</tr>
<tr class="row-even"><td><p>Temperature sampling</p></td>
<td><p>Controls creativity in AI writing assistants</p></td>
<td><p>Conservative vs. aggressive strategy dial</p></td>
</tr>
<tr class="row-odd"><td><p>Scaling laws</p></td>
<td><p>Guide billion-dollar training decisions</p></td>
<td><p>Development budget allocation in F1</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="checklist">
<h3>Checklist<a class="headerlink" href="#checklist" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>[ ] I can explain why next-token prediction leads to general intelligence</p></li>
<li><p>[ ] I understand how BPE tokenization works and why it’s used</p></li>
<li><p>[ ] I can implement a simple GPT model from scratch</p></li>
<li><p>[ ] I know when to use GPT vs BERT for different tasks</p></li>
<li><p>[ ] I understand how temperature affects generation</p></li>
<li><p>[ ] I can explain the key findings of scaling laws research</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="next-steps">
<h2>Next Steps<a class="headerlink" href="#next-steps" title="Link to this heading">#</a></h2>
<p>Now that you understand language model architectures, the next notebook covers <strong>Embeddings</strong> – how to represent words, sentences, and concepts as vectors that capture meaning, and how these representations power search, recommendation, and RAG systems.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="19_tokenization_lm_training.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Part 6.3: Tokenization &amp; Language Model Training</p>
      </div>
    </a>
    <a class="right-next"
       href="21_finetuning_and_peft.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Part 6.5: Fine-tuning &amp; PEFT</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-language-model">1. What Is a Language Model?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuitive-explanation">Intuitive Explanation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#breaking-down-the-formula">Breaking down the formula:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-this-is-useful">Why This Is Useful</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#brief-history-from-n-grams-to-transformers">Brief History: From N-grams to Transformers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">2. Tokenization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#byte-pair-encoding-bpe-the-dominant-tokenization-algorithm">Byte Pair Encoding (BPE): The Dominant Tokenization Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-why-bpe-works-so-well">Deep Dive: Why BPE Works So Well</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#common-misconceptions">Common Misconceptions</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-bpe-from-scratch">Implementing BPE from Scratch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#special-tokens">Special Tokens</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization-comparison-table">Tokenization Comparison Table</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-architecture-decoder-only">3. GPT Architecture (Decoder-Only)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-overview">Architecture Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-a-mini-gpt-model">Implementing a Mini-GPT Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-decoder-only-vs-encoder-decoder">Comparison: Decoder-Only vs Encoder-Decoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-gpt-scaling-history">Deep Dive: GPT Scaling History</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#key-insight">Key Insight</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bert-architecture-encoder-only">4. BERT Architecture (Encoder-Only)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bert-s-two-training-objectives">BERT’s Two Training Objectives</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mlm-details">MLM Details</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-vs-bert-complete-comparison">GPT vs BERT: Complete Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use-which">When to Use Which?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Key Insight</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-a-small-language-model">5. Training a Small Language Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#temperature-sampling">Temperature Sampling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-laws">6. Scaling Laws</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-scaling-insights">Key Scaling Insights</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-chinchilla-finding">The Chinchilla Finding</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-extend-bpe-with-encoding">Exercise 1: Extend BPE with Encoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-top-p-nucleus-sampling">Exercise 2: Top-p (Nucleus) Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-calculate-perplexity">Exercise 3: Calculate Perplexity</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts">Key Concepts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-deep-learning">Connection to Deep Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checklist">Checklist</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">Next Steps</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dan Shah
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>