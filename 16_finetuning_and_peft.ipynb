{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5.4: Fine-tuning & PEFT\n\nImagine you want to build a model that classifies medical images. Training from scratch would require millions of labeled medical images and weeks of compute. But what if you could start with a model that already understands edges, textures, shapes, and objects from training on millions of general images? You'd only need to teach it the *medical-specific* part.\n\nThis is **transfer learning** -- one of the most important practical techniques in modern deep learning. And with today's massive language models (billions of parameters), we can't even afford to fine-tune all parameters. That's where **Parameter-Efficient Fine-Tuning (PEFT)** methods like **LoRA** come in, letting us adapt huge models by modifying less than 1% of their parameters.\n\n## Learning Objectives\n\n- [ ] Understand transfer learning and when to use feature extraction vs fine-tuning\n- [ ] Implement full fine-tuning with discriminative learning rates\n- [ ] Explain why Parameter-Efficient Fine-Tuning (PEFT) is necessary for large models\n- [ ] Implement LoRA (Low-Rank Adaptation) from scratch and understand rank decomposition\n- [ ] Compare adapters, prompt tuning, and LoRA as PEFT methods\n- [ ] Build a complete fine-tuning pipeline with evaluation\n- [ ] Understand the RLHF pipeline that produces ChatGPT-like models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport math\nfrom copy import deepcopy\n\n%matplotlib inline\nplt.style.use('seaborn-v0_8-whitegrid')\ntorch.manual_seed(42)\nnp.random.seed(42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Transfer Learning\n\n### The Key Insight\n\nThink about how humans learn. When you learn to play tennis, you don't start from scratch -- you already know how to move your body, track objects with your eyes, and coordinate hand movements. You **transfer** these general skills to the new task.\n\nNeural networks work the same way:\n\n- **Early layers** learn general features (edges, textures, basic patterns)\n- **Middle layers** learn compositional features (shapes, parts, common structures)\n- **Later layers** learn task-specific features (specific objects, categories)\n\nA model trained on ImageNet has already learned a rich hierarchy of visual features. A language model trained on the internet already \"knows\" grammar, facts, and reasoning patterns. We can **reuse** this knowledge for new tasks.\n\n### Two Approaches\n\n| Approach | What You Do | When to Use |\n|----------|-------------|-------------|\n| **Feature Extraction** | Freeze pretrained layers, only train new head | Small dataset, similar domain |\n| **Fine-tuning** | Unfreeze some/all layers, train with small LR | Larger dataset, different domain |\n\n**What this means:** Feature extraction treats the pretrained model as a fixed feature extractor -- like using it as a sophisticated preprocessing step. Fine-tuning actually updates the pretrained weights to better fit your specific task."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualization: What different layers learn (general -> task-specific)\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\n\n# Simulate what each layer \"responds to\"\nnp.random.seed(42)\n\n# Layer 1: Edge detectors (general)\nx = np.linspace(0, 1, 50)\ny = np.linspace(0, 1, 50)\nX, Y = np.meshgrid(x, y)\n\n# Simple edge patterns\npatterns = [\n    (\"Layer 1:\\nEdges & Textures\", X > 0.5, \"General\\n(Transfer easily)\"),\n    (\"Layer 2:\\nShapes & Parts\", ((X-0.5)**2 + (Y-0.5)**2) < 0.15, \"Somewhat General\\n(Usually transfer)\"),\n    (\"Layer 3:\\nObject Parts\", ((X-0.3)**2 + (Y-0.3)**2 < 0.05) | ((X-0.7)**2 + (Y-0.7)**2 < 0.05), \"Task-Related\\n(May need updating)\"),\n    (\"Layer 4:\\nTask-Specific\", ((X-0.5)**2/0.08 + (Y-0.5)**2/0.2) < 1, \"Task-Specific\\n(Must retrain)\")\n]\n\ncolors = ['#2ecc71', '#27ae60', '#e67e22', '#e74c3c']\nfor ax, (title, pattern, transfer), color in zip(axes, patterns, colors):\n    ax.imshow(pattern.astype(float), cmap='gray', extent=[0,1,0,1])\n    ax.set_title(title, fontsize=12, fontweight='bold')\n    ax.set_xlabel(transfer, fontsize=10, color=color, fontweight='bold')\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.suptitle('What Neural Network Layers Learn (General to Specific)',\n             fontsize=14, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"Key insight: Earlier layers learn universal features that transfer across tasks\")\nprint(\"Later layers learn task-specific features that need retraining\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Which Approach\n\nThe right strategy depends on two factors: **dataset size** and **domain similarity**.\n\n| | Small Dataset | Large Dataset |\n|---|---|---|\n| **Similar Domain** | Feature extraction (freeze all, train head) | Fine-tune last few layers |\n| **Different Domain** | Feature extraction + augmentation (risky!) | Fine-tune entire network |\n\n**Examples:**\n- Medical X-rays with 500 images, pretrained on ImageNet: **Feature extraction** (similar visual domain, small data)\n- Satellite imagery with 100K images, pretrained on ImageNet: **Fine-tune all** (different domain, large data)\n- Product reviews with 1K examples, pretrained on Wikipedia: **Feature extraction** (similar text domain, small data)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualization: Feature extraction vs Fine-tuning decision flowchart\nfig, ax = plt.subplots(figsize=(12, 7))\nax.set_xlim(0, 10)\nax.set_ylim(0, 8)\nax.axis('off')\n\n# Draw boxes and arrows\nboxes = {\n    'start': (5, 7.2, 'Start: Have a\\npretrained model'),\n    'data': (5, 5.5, 'How much\\nlabeled data?'),\n    'sim_small': (2.5, 3.8, 'Domain\\nsimilar?'),\n    'sim_large': (7.5, 3.8, 'Domain\\nsimilar?'),\n    'fe': (1, 2, 'Feature\\nExtraction'),\n    'fe_aug': (4, 2, 'Feature Extraction\\n+ Augmentation'),\n    'ft_last': (6, 2, 'Fine-tune\\nLast Layers'),\n    'ft_all': (9, 2, 'Fine-tune\\nAll Layers'),\n}\n\ncolors_map = {\n    'start': '#3498db', 'data': '#f39c12',\n    'sim_small': '#f39c12', 'sim_large': '#f39c12',\n    'fe': '#2ecc71', 'fe_aug': '#e67e22',\n    'ft_last': '#2ecc71', 'ft_all': '#e74c3c'\n}\n\nfor key, (x, y, text) in boxes.items():\n    color = colors_map[key]\n    bbox = dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.3, edgecolor=color)\n    ax.text(x, y, text, ha='center', va='center', fontsize=11, fontweight='bold', bbox=bbox)\n\n# Arrows with labels\narrow_props = dict(arrowstyle='->', color='gray', lw=2)\nax.annotate('', xy=(5, 6.1), xytext=(5, 6.8), arrowprops=arrow_props)\nax.annotate('', xy=(2.5, 4.5), xytext=(4.2, 5.2), arrowprops=arrow_props)\nax.text(3, 5.1, 'Small', fontsize=10, color='gray')\nax.annotate('', xy=(7.5, 4.5), xytext=(5.8, 5.2), arrowprops=arrow_props)\nax.text(6.8, 5.1, 'Large', fontsize=10, color='gray')\n\nax.annotate('', xy=(1, 2.7), xytext=(2, 3.4), arrowprops=arrow_props)\nax.text(1.1, 3.2, 'Yes', fontsize=9, color='green')\nax.annotate('', xy=(4, 2.7), xytext=(3, 3.4), arrowprops=arrow_props)\nax.text(3.7, 3.2, 'No', fontsize=9, color='red')\nax.annotate('', xy=(6, 2.7), xytext=(7, 3.4), arrowprops=arrow_props)\nax.text(6.1, 3.2, 'Yes', fontsize=9, color='green')\nax.annotate('', xy=(9, 2.7), xytext=(8, 3.4), arrowprops=arrow_props)\nax.text(8.7, 3.2, 'No', fontsize=9, color='red')\n\nax.set_title('Transfer Learning Strategy Decision Guide', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: Why Transfer Learning Works\n\nTransfer learning works because of a fundamental property of neural networks: **hierarchical feature learning**.\n\nWhen a deep network is trained on a large dataset (like ImageNet with 1.2M images or a text corpus with billions of words), the early layers learn features that are remarkably universal:\n- In vision: edges, corners, color gradients, textures\n- In language: word meanings, grammar patterns, syntactic structures\n\nThese features are useful for almost any task in the same modality. The later layers combine these basic features into increasingly abstract and task-specific representations.\n\n#### Key Insight\n\nThe knowledge stored in pretrained weights represents a **prior** over the space of useful features. Instead of learning everything from random initialization, you start with a good initialization that already captures the structure of the data domain.\n\n#### Common Misconceptions\n\n| Misconception | Reality |\n|---------------|---------|\n| Transfer learning only works within the same domain | Cross-domain transfer works too (e.g., ImageNet to medical) |\n| You always need to fine-tune | Feature extraction alone often works well |\n| More fine-tuning is always better | Too much fine-tuning can cause catastrophic forgetting |\n| Transfer learning is optional for large datasets | Even with large datasets, pretrained models converge faster |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Full Fine-tuning\n\n### The Approach\n\nIn full fine-tuning, we take a pretrained model and continue training **all parameters** on our new task-specific dataset. This is the most straightforward form of transfer learning.\n\n**The process:**\n1. Load a pretrained model (e.g., BERT, ResNet, GPT)\n2. Replace the final classification head with one suited to your task\n3. Train the entire model on your new dataset with a **small learning rate**\n\n### The Challenge: Catastrophic Forgetting\n\nWhen you fine-tune too aggressively, the model can \"forget\" the useful features it learned during pretraining. This is called **catastrophic forgetting**.\n\n**What this means:** The pretrained weights encode valuable knowledge. If you update them too much with new task data (especially small datasets), you overwrite that knowledge and lose the benefit of pretraining.\n\n### Solution: Learning Rate Strategies\n\n| Strategy | Description | When to Use |\n|----------|-------------|-------------|\n| **Small uniform LR** | Same small LR for all layers (e.g., 1e-5) | Simple, works well in practice |\n| **Discriminative LR** | Lower LR for early layers, higher for later | Better performance, more complex |\n| **Gradual unfreezing** | Unfreeze layers one at a time during training | Most conservative, prevents forgetting |\n| **Warmup** | Start with tiny LR, ramp up | Stabilizes early training |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Implement a simple pretrained model and fine-tuning\n\nclass SimplePretrainedModel(nn.Module):\n    \"\"\"\n    A simple model to demonstrate fine-tuning concepts.\n    Pretrained on a 'source' task, fine-tuned on a 'target' task.\n    \"\"\"\n    def __init__(self, input_dim=20, hidden_dim=64, num_classes=5):\n        super().__init__()\n        # \"Pretrained\" feature extractor\n        self.feature_extractor = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 32),\n            nn.ReLU()\n        )\n        # Task-specific head\n        self.classifier = nn.Linear(32, num_classes)\n\n    def forward(self, x):\n        features = self.feature_extractor(x)\n        return self.classifier(features)\n\n\ndef create_synthetic_data(n_samples, input_dim=20, num_classes=5, task='source'):\n    \"\"\"Create synthetic classification data.\"\"\"\n    torch.manual_seed(42 if task == 'source' else 123)\n    X = torch.randn(n_samples, input_dim)\n    # Different tasks use different feature combinations\n    if task == 'source':\n        logits = X[:, :num_classes] * 2 + X[:, 5:5+num_classes]\n    else:  # target task uses related but different features\n        logits = X[:, 2:2+num_classes] * 1.5 + X[:, 7:7+num_classes] * 0.8\n    y = logits.argmax(dim=1)\n    return X, y\n\n\ndef train_model(model, X_train, y_train, X_val, y_val, epochs=50, lr=0.01, verbose=False):\n    \"\"\"Train a model and return loss/accuracy history.\"\"\"\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    train_losses, val_accs = [], []\n    for epoch in range(epochs):\n        model.train()\n        optimizer.zero_grad()\n        outputs = model(X_train)\n        loss = criterion(outputs, y_train)\n        loss.backward()\n        optimizer.step()\n\n        model.eval()\n        with torch.no_grad():\n            val_preds = model(X_val).argmax(dim=1)\n            val_acc = (val_preds == y_val).float().mean().item()\n\n        train_losses.append(loss.item())\n        val_accs.append(val_acc)\n\n    return train_losses, val_accs\n\n\n# Step 1: \"Pretrain\" on source task (large dataset)\nX_source, y_source = create_synthetic_data(2000, task='source')\npretrained_model = SimplePretrainedModel()\nsource_losses, source_accs = train_model(\n    pretrained_model, X_source[:1600], y_source[:1600],\n    X_source[1600:], y_source[1600:], epochs=100, lr=0.005\n)\nprint(f\"Source task final accuracy: {source_accs[-1]:.4f}\")\n\n# Step 2: Create target task (small dataset)\nX_target, y_target = create_synthetic_data(200, task='target')\nX_train_t, y_train_t = X_target[:150], y_target[:150]\nX_val_t, y_val_t = X_target[150:], y_target[150:]\n\n# Approach 1: Train from scratch\nscratch_model = SimplePretrainedModel()\nscratch_losses, scratch_accs = train_model(\n    scratch_model, X_train_t, y_train_t, X_val_t, y_val_t,\n    epochs=100, lr=0.005\n)\n\n# Approach 2: Feature extraction (freeze backbone)\nfe_model = deepcopy(pretrained_model)\nfor param in fe_model.feature_extractor.parameters():\n    param.requires_grad = False\nfe_model.classifier = nn.Linear(32, 5)  # New head\nfe_losses, fe_accs = train_model(\n    fe_model, X_train_t, y_train_t, X_val_t, y_val_t,\n    epochs=100, lr=0.01\n)\n\n# Approach 3: Full fine-tuning (small LR)\nft_model = deepcopy(pretrained_model)\nft_model.classifier = nn.Linear(32, 5)  # New head\nft_losses, ft_accs = train_model(\n    ft_model, X_train_t, y_train_t, X_val_t, y_val_t,\n    epochs=100, lr=0.001\n)\n\nprint(f\"\\nTarget task accuracy (200 samples):\")\nprint(f\"  From scratch:       {scratch_accs[-1]:.4f}\")\nprint(f\"  Feature extraction: {fe_accs[-1]:.4f}\")\nprint(f\"  Full fine-tuning:   {ft_accs[-1]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualization: Training curves comparison\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Loss curves\naxes[0].plot(scratch_losses, label='From Scratch', color='red', alpha=0.7)\naxes[0].plot(fe_losses, label='Feature Extraction', color='blue', alpha=0.7)\naxes[0].plot(ft_losses, label='Full Fine-tuning', color='green', alpha=0.7)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Training Loss')\naxes[0].set_title('Training Loss: Fine-tuning vs From Scratch', fontweight='bold')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Accuracy curves\naxes[1].plot(scratch_accs, label='From Scratch', color='red', alpha=0.7)\naxes[1].plot(fe_accs, label='Feature Extraction', color='blue', alpha=0.7)\naxes[1].plot(ft_accs, label='Full Fine-tuning', color='green', alpha=0.7)\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Validation Accuracy')\naxes[1].set_title('Validation Accuracy: Fine-tuning vs From Scratch', fontweight='bold')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Notice: Fine-tuning reaches higher accuracy faster than training from scratch\")\nprint(\"Feature extraction converges quickly but may plateau lower (frozen features)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Discriminative Learning Rates\n\nclass DiscriminativeLRModel(nn.Module):\n    \"\"\"Model with separate parameter groups for discriminative LR.\"\"\"\n    def __init__(self, pretrained_model, num_classes=5):\n        super().__init__()\n        self.feature_extractor = deepcopy(pretrained_model.feature_extractor)\n        self.classifier = nn.Linear(32, num_classes)\n\n    def get_param_groups(self, base_lr=0.001, lr_mult=0.1):\n        \"\"\"\n        Return parameter groups with discriminative learning rates.\n        Earlier layers get smaller LR (lr * lr_mult).\n        \"\"\"\n        # Get the layers from the sequential\n        layers = list(self.feature_extractor.children())\n\n        param_groups = []\n        # Early layers: very small LR\n        for i, layer in enumerate(layers[:2]):\n            if hasattr(layer, 'parameters'):\n                lr = base_lr * (lr_mult ** 2)  # Smallest LR\n                param_groups.append({'params': layer.parameters(), 'lr': lr})\n\n        # Middle layers: medium LR\n        for layer in layers[2:4]:\n            if hasattr(layer, 'parameters'):\n                lr = base_lr * lr_mult\n                param_groups.append({'params': layer.parameters(), 'lr': lr})\n\n        # Later layers: larger LR\n        for layer in layers[4:]:\n            if hasattr(layer, 'parameters'):\n                param_groups.append({'params': layer.parameters(), 'lr': base_lr})\n\n        # Classifier: highest LR\n        param_groups.append({'params': self.classifier.parameters(), 'lr': base_lr * 10})\n\n        return param_groups\n\n# Demonstrate discriminative LR\ndisc_model = DiscriminativeLRModel(pretrained_model)\nparam_groups = disc_model.get_param_groups(base_lr=0.001, lr_mult=0.1)\n\nprint(\"Discriminative Learning Rates:\")\nprint(\"=\" * 50)\nlayer_names = ['Early Linear 1', 'Early ReLU', 'Mid Linear 2', 'Mid ReLU',\n               'Late Linear 3', 'Late ReLU', 'Classifier']\nidx = 0\nfor pg in param_groups:\n    n_params = sum(p.numel() for p in pg['params'])\n    if n_params > 0:\n        print(f\"  Layer group {idx}: LR = {pg['lr']:.6f}, params = {n_params}\")\n        idx += 1\n\nprint(\"\\nKey idea: Earlier (more general) layers change slowly,\")\nprint(\"later (more task-specific) layers change more freely\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parameter-Efficient Fine-Tuning (PEFT)\n\n### The Problem\n\nFull fine-tuning works great for small models (millions of parameters). But consider modern language models:\n\n| Model | Parameters | Storage (fp16) | Full Fine-tuning Memory |\n|-------|-----------|-----------------|------------------------|\n| BERT-base | 110M | 220 MB | ~1 GB |\n| GPT-2 | 1.5B | 3 GB | ~12 GB |\n| LLaMA-7B | 7B | 14 GB | ~56 GB |\n| LLaMA-70B | 70B | 140 GB | ~560 GB |\n| GPT-4 (est.) | ~1.8T | ~3.6 TB | ~14 TB |\n\n**The challenges of full fine-tuning at scale:**\n1. **Memory:** Need to store model + optimizer states + gradients for ALL parameters\n2. **Storage:** Each fine-tuned version is a full copy of the model\n3. **Catastrophic forgetting:** More parameters to destabilize\n4. **Cost:** Training compute scales with parameter count\n\n### The Key Insight\n\nResearch has shown that when you fine-tune a model, the weight changes are typically **low-dimensional** -- you don't need to modify all parameters to adapt the model. PEFT methods exploit this by modifying only a tiny fraction of parameters.\n\n**What this means:** Think of it like adjusting a complex machine. Instead of rebuilding the entire machine for a new task, you just add a few small adapters or turn a few knobs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualization: Parameter counts comparison\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Bar chart: trainable parameters\nmethods = ['Full\\nFine-tuning', 'LoRA\\n(r=8)', 'LoRA\\n(r=4)', 'Adapters', 'Prompt\\nTuning']\n# For a 7B parameter model\ntotal_params = 7_000_000_000\ntrainable = [total_params, 33_500_000, 16_800_000, 28_000_000, 40_960]\npercentages = [100, 0.48, 0.24, 0.4, 0.0006]\ncolors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6']\n\nbars = axes[0].bar(methods, [t/1e6 for t in trainable], color=colors, alpha=0.8, edgecolor='black')\naxes[0].set_ylabel('Trainable Parameters (Millions)', fontsize=12)\naxes[0].set_title('Trainable Parameters by Method\\n(7B Parameter Model)', fontsize=13, fontweight='bold')\naxes[0].set_yscale('log')\naxes[0].grid(True, alpha=0.3, axis='y')\n\n# Add percentage labels\nfor bar, pct in zip(bars, percentages):\n    axes[0].text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n                f'{pct}%', ha='center', va='bottom', fontweight='bold', fontsize=10)\n\n# Pie chart: what gets trained in LoRA\nsizes = [total_params - 33_500_000, 33_500_000]\nlabels = [f'Frozen\\n{(total_params-33_500_000)/1e9:.2f}B', f'LoRA Trainable\\n{33_500_000/1e6:.1f}M']\nexplode = (0, 0.1)\naxes[1].pie(sizes, explode=explode, labels=labels, colors=['#95a5a6', '#3498db'],\n            autopct='%1.2f%%', shadow=True, startangle=90, textprops={'fontsize': 12})\naxes[1].set_title('LoRA (r=8): What Gets Trained?', fontsize=13, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key takeaway: PEFT methods train < 1% of parameters while achieving\")\nprint(\"comparable performance to full fine-tuning!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why This Matters in Machine Learning\n\n| Application | How PEFT Helps |\n|-------------|----------------|\n| **Multi-task serving** | Store one base model + tiny adapters per task |\n| **Personalization** | Each user gets their own small adapter |\n| **Edge deployment** | Adapt models on resource-constrained devices |\n| **Rapid experimentation** | Try many configurations cheaply |\n| **Democratization** | Fine-tune billion-parameter models on consumer GPUs |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LoRA (Low-Rank Adaptation)\n\n### The Key Insight\n\nWhen you fine-tune a pretrained model, the weight update matrix $\\Delta W$ has been shown to have **low intrinsic rank**. This means the changes can be represented by a much smaller matrix.\n\n**Intuition:** Imagine you have a 1000x1000 weight matrix (1 million parameters). During fine-tuning, the *change* to this matrix ($\\Delta W$) can be well-approximated by the product of a 1000x4 matrix and a 4x1000 matrix. That's only 8,000 parameters instead of 1,000,000!\n\n### The LoRA Formula\n\nInstead of learning $\\Delta W$ directly (which is huge), LoRA decomposes it:\n\n$$W_{new} = W_{pretrained} + \\Delta W = W_{pretrained} + BA$$\n\nWhere:\n- $W_{pretrained}$ is the original weight matrix ($d \\times d$), **frozen**\n- $B$ is a learned matrix ($d \\times r$), initialized to **zeros**\n- $A$ is a learned matrix ($r \\times d$), initialized **randomly**\n- $r$ is the **rank** (typically 4, 8, or 16) -- much smaller than $d$\n\n#### Breaking down the formula:\n\n| Component | Shape | Parameters | Role |\n|-----------|-------|-----------|------|\n| $W_{pretrained}$ | $d \\times d$ | $d^2$ (frozen) | Original pretrained knowledge |\n| $B$ | $d \\times r$ | $d \\times r$ (trainable) | \"Down-projection\" of adaptation |\n| $A$ | $r \\times d$ | $r \\times d$ (trainable) | \"Up-projection\" of adaptation |\n| $BA$ | $d \\times d$ | 0 (computed) | Low-rank approximation of $\\Delta W$ |\n| Total trainable | -- | $2 \\times d \\times r$ | Much less than $d^2$! |\n\n**What this means:** LoRA says \"the way this weight matrix needs to change for the new task can be captured by a narrow bottleneck of rank $r$.\" At $r=8$ and $d=4096$ (typical for LLMs), you're training $2 \\times 4096 \\times 8 = 65,536$ parameters instead of $4096^2 = 16,777,216$ -- a **256x reduction!**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualization: Rank decomposition explained step by step\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\n\nd = 8  # Use small dimension for visualization\nr = 2  # Rank\n\n# Create example matrices\ntorch.manual_seed(42)\nW_pretrained = torch.randn(d, d) * 0.5\n\n# LoRA matrices\nA = torch.randn(r, d) * 0.1  # Random init\nB = torch.zeros(d, r)          # Zero init (so initial delta W = 0)\n\n# After some \"training\"\nA_trained = torch.randn(r, d) * 0.3\nB_trained = torch.randn(d, r) * 0.3\ndelta_W = B_trained @ A_trained\n\n# Plot W_pretrained\nim0 = axes[0].imshow(W_pretrained.numpy(), cmap='RdBu', vmin=-1.5, vmax=1.5)\naxes[0].set_title(f'W_pretrained\\n({d}x{d} = {d*d} params)\\nFROZEN', fontweight='bold')\naxes[0].set_xlabel('Frozen (not updated)')\n\n# Plot B (d x r)\nim1 = axes[1].imshow(B_trained.numpy(), cmap='RdBu', vmin=-1, vmax=1)\naxes[1].set_title(f'B matrix\\n({d}x{r} = {d*r} params)\\nTRAINABLE', fontweight='bold', color='blue')\n\n# Plot A (r x d)\nim2 = axes[2].imshow(A_trained.numpy(), cmap='RdBu', vmin=-1, vmax=1)\naxes[2].set_title(f'A matrix\\n({r}x{d} = {r*d} params)\\nTRAINABLE', fontweight='bold', color='blue')\n\n# Plot BA = delta_W\nim3 = axes[3].imshow(delta_W.numpy(), cmap='RdBu', vmin=-1.5, vmax=1.5)\naxes[3].set_title(f'BA = delta_W\\n({d}x{d} but rank {r})\\nLow-rank update', fontweight='bold', color='green')\n\nplt.suptitle('LoRA: Low-Rank Decomposition of Weight Updates', fontsize=14, fontweight='bold', y=1.05)\n\n# Add equation annotation\nfig.text(0.5, -0.02, f'W_new = W_pretrained + B*A    |    Trainable params: {2*d*r} vs {d*d} (full)    |    Compression: {d*d/(2*d*r):.1f}x',\n         ha='center', fontsize=12, style='italic',\n         bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n\nplt.tight_layout()\nplt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step-by-step numerical example of LoRA\nprint(\"=\" * 65)\nprint(\"STEP-BY-STEP LoRA EXAMPLE\")\nprint(\"=\" * 65)\n\n# Dimensions\nd = 4  # Small for clarity\nr = 2  # Rank\nprint(f\"\\nWeight matrix dimension: {d}x{d} = {d*d} parameters\")\nprint(f\"LoRA rank: r = {r}\")\nprint(f\"LoRA parameters: 2 * {d} * {r} = {2*d*r} parameters\")\nprint(f\"Compression ratio: {d*d}/{2*d*r} = {d*d/(2*d*r):.1f}x\\n\")\n\n# Original weight matrix (frozen)\nW = torch.tensor([[1.0, 0.5, -0.3, 0.2],\n                   [0.4, -0.8, 0.6, 0.1],\n                   [-0.2, 0.3, 1.0, -0.5],\n                   [0.7, -0.1, 0.4, 0.9]])\nprint(\"Step 1: Original pretrained weight W (FROZEN):\")\nprint(W.numpy().round(2))\n\n# Initialize LoRA matrices\nB = torch.zeros(d, r)  # Zero init!\nA = torch.randn(d, r).T * 0.1  # Small random init, shape (r, d)\n\nprint(f\"\\nStep 2: Initialize LoRA matrices\")\nprint(f\"  B ({d}x{r}), initialized to ZEROS:\")\nprint(f\"  {B.numpy().round(3)}\")\nprint(f\"  A ({r}x{d}), initialized randomly:\")\nprint(f\"  {A.numpy().round(3)}\")\n\n# Initial output: BA = 0, so W_new = W (no change!)\ndelta_W_init = B @ A\nprint(f\"\\nStep 3: Initial delta_W = B @ A = all zeros\")\nprint(f\"  (This is key! At initialization, LoRA changes nothing)\")\n\n# After training (simulate learned values)\nB_learned = torch.tensor([[0.3, -0.1],\n                           [0.1, 0.4],\n                           [-0.2, 0.2],\n                           [0.0, -0.3]])\nA_learned = torch.tensor([[0.2, -0.3, 0.1, 0.4],\n                           [-0.1, 0.2, 0.3, -0.2]])\n\nprint(f\"\\nStep 4: After training, B and A are updated:\")\nprint(f\"  B_learned ({d}x{r}):\")\nprint(f\"  {B_learned.numpy().round(3)}\")\nprint(f\"  A_learned ({r}x{d}):\")\nprint(f\"  {A_learned.numpy().round(3)}\")\n\ndelta_W = B_learned @ A_learned\nW_new = W + delta_W\n\nprint(f\"\\nStep 5: Compute delta_W = B_learned @ A_learned:\")\nprint(delta_W.numpy().round(3))\n\nprint(f\"\\nStep 6: W_new = W + delta_W:\")\nprint(W_new.numpy().round(3))\n\nprint(f\"\\nNote: delta_W is rank-{r} (at most), but modifies all {d*d} entries!\")\nprint(f\"We only trained {2*d*r} = {2*d*r} parameters to get a {d}x{d} = {d*d} update!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Implement LoRA from scratch!\n\nclass LoRALinear(nn.Module):\n    \"\"\"\n    A Linear layer augmented with LoRA (Low-Rank Adaptation).\n\n    The forward pass computes: y = W_frozen @ x + (B @ A) @ x * (alpha/r)\n\n    Args:\n        original_layer: The pretrained nn.Linear layer to augment\n        rank: The rank of the LoRA decomposition\n        alpha: Scaling factor for LoRA (controls magnitude)\n    \"\"\"\n    def __init__(self, original_layer, rank=4, alpha=1.0):\n        super().__init__()\n        self.original_layer = original_layer\n        self.rank = rank\n        self.alpha = alpha\n\n        in_features = original_layer.in_features\n        out_features = original_layer.out_features\n\n        # Freeze original weights\n        for param in self.original_layer.parameters():\n            param.requires_grad = False\n\n        # LoRA matrices\n        # A: (rank, in_features) - initialized with small random values\n        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.01)\n        # B: (out_features, rank) - initialized to zeros\n        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n\n        # Scaling factor\n        self.scaling = alpha / rank\n\n    def forward(self, x):\n        # Original frozen computation\n        original_output = self.original_layer(x)\n\n        # LoRA adaptation: x @ A^T @ B^T * scaling\n        lora_output = F.linear(F.linear(x, self.lora_A), self.lora_B) * self.scaling\n\n        return original_output + lora_output\n\n    def get_merged_weight(self):\n        \"\"\"Merge LoRA weights into original for inference (no extra cost!).\"\"\"\n        return self.original_layer.weight + (self.lora_B @ self.lora_A) * self.scaling\n\n    def extra_repr(self):\n        return f'rank={self.rank}, alpha={self.alpha}, scaling={self.scaling:.4f}'\n\n\n# Demonstrate LoRA\nprint(\"LoRA Linear Layer Demo\")\nprint(\"=\" * 50)\n\n# Create original layer\noriginal = nn.Linear(64, 32)\ntotal_original = sum(p.numel() for p in original.parameters())\nprint(f\"Original layer: {64}x{32} + bias = {total_original} parameters\")\n\n# Wrap with LoRA\nfor rank in [1, 2, 4, 8, 16]:\n    lora_layer = LoRALinear(original, rank=rank, alpha=rank)\n    trainable = sum(p.numel() for p in lora_layer.parameters() if p.requires_grad)\n    total = sum(p.numel() for p in lora_layer.parameters())\n    print(f\"  LoRA rank={rank:2d}: {trainable:5d} trainable / {total} total \"\n          f\"({100*trainable/total:.1f}% trainable, {total_original/trainable:.1f}x compression)\")\n\n# Verify forward pass works\nx = torch.randn(4, 64)\nlora = LoRALinear(nn.Linear(64, 32), rank=4, alpha=4)\ny = lora(x)\nprint(f\"\\nForward pass: input {x.shape} -> output {y.shape}\")\n\n# Verify initialization preserves original output\noriginal_out = lora.original_layer(x)\nlora_out = lora(x)\nprint(f\"Output matches original at init: {torch.allclose(original_out, lora_out, atol=1e-6)}\")\nprint(\"(Because B is initialized to zeros, so B@A = 0)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualization: How LoRA modifies weight matrices for different ranks\n\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\n\ntorch.manual_seed(42)\nd_in, d_out = 32, 32\noriginal_weight = torch.randn(d_out, d_in) * 0.5\n\nranks = [1, 2, 4, 8]\n\nfor i, r in enumerate(ranks):\n    # Simulate trained LoRA\n    A = torch.randn(r, d_in) * 0.3\n    B = torch.randn(d_out, r) * 0.3\n    delta_W = B @ A\n    W_new = original_weight + delta_W\n\n    # Top row: delta_W\n    im = axes[0, i].imshow(delta_W.numpy(), cmap='RdBu', vmin=-2, vmax=2)\n    axes[0, i].set_title(f'delta_W (rank={r})\\nParams: {2*d_in*r}', fontweight='bold')\n    axes[0, i].set_ylabel('delta_W' if i == 0 else '')\n\n    # Bottom row: singular values showing rank\n    U, S, V = torch.svd(delta_W)\n    axes[1, i].bar(range(min(16, len(S))), S[:16].numpy(), color='steelblue', alpha=0.8)\n    axes[1, i].axhline(y=0.01, color='red', linestyle='--', alpha=0.5, label='Threshold')\n    axes[1, i].set_xlabel('Singular Value Index')\n    axes[1, i].set_ylabel('Value' if i == 0 else '')\n    axes[1, i].set_title(f'Singular Values\\n(Only {r} non-zero!)', fontweight='bold')\n    axes[1, i].grid(True, alpha=0.3)\n\nplt.suptitle('LoRA Weight Updates at Different Ranks', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"Key observation: delta_W = B@A has exactly r non-zero singular values\")\nprint(\"Higher rank = more expressive updates, but more parameters\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Interactive exploration: Choosing rank r\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Simulate accuracy vs rank tradeoff\nd = 768  # BERT hidden size\nranks = [1, 2, 4, 8, 16, 32, 64, 128, 256]\nparams_per_layer = [2 * d * r for r in ranks]\ntotal_params_pct = [2 * d * r / (d * d) * 100 for r in ranks]\n\n# Simulated accuracy (typical pattern: diminishing returns)\naccuracies = [85.2, 88.1, 90.5, 91.8, 92.3, 92.5, 92.6, 92.7, 92.7]\nfull_ft_accuracy = 92.8\n\n# Params vs rank\naxes[0].plot(ranks, total_params_pct, 'bo-', linewidth=2, markersize=8)\naxes[0].fill_between(ranks, total_params_pct, alpha=0.1, color='blue')\naxes[0].set_xlabel('LoRA Rank (r)', fontsize=12)\naxes[0].set_ylabel('Trainable Parameters\\n(% of full weight matrix)', fontsize=12)\naxes[0].set_title('Parameter Efficiency vs Rank', fontweight='bold', fontsize=13)\naxes[0].grid(True, alpha=0.3)\naxes[0].set_xscale('log', base=2)\n\n# Accuracy vs rank\naxes[1].plot(ranks, accuracies, 'go-', linewidth=2, markersize=8, label='LoRA')\naxes[1].axhline(y=full_ft_accuracy, color='red', linestyle='--', linewidth=2, label='Full fine-tuning')\naxes[1].fill_between(ranks, accuracies, alpha=0.1, color='green')\naxes[1].set_xlabel('LoRA Rank (r)', fontsize=12)\naxes[1].set_ylabel('Accuracy (%)', fontsize=12)\naxes[1].set_title('Accuracy vs Rank (Diminishing Returns)', fontweight='bold', fontsize=13)\naxes[1].legend(fontsize=11)\naxes[1].grid(True, alpha=0.3)\naxes[1].set_xscale('log', base=2)\naxes[1].annotate('Sweet spot\\n(r=8)', xy=(8, 91.8), xytext=(20, 88),\n                arrowprops=dict(arrowstyle='->', color='orange', lw=2),\n                fontsize=12, fontweight='bold', color='orange')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Practical guidance for choosing rank r:\")\nprint(\"  r=4:  Good for simple tasks, maximum efficiency\")\nprint(\"  r=8:  Best tradeoff for most tasks (recommended default)\")\nprint(\"  r=16: Use when r=8 underperforms\")\nprint(\"  r>16: Rarely needed, diminishing returns\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: Why Low-Rank Works\n\nThis is a profound finding: when you fine-tune a large model for a specific task, the weight changes lie in a **low-dimensional subspace**. But why?\n\n**Intuition 1: Task Simplicity.** Most downstream tasks are simpler than the pretraining task. Classifying sentiment (positive/negative) requires much less information than predicting the next word in all of Wikipedia. The \"direction\" of change in weight space is simple.\n\n**Intuition 2: Over-parameterization.** Large models have far more parameters than needed for any single task. The relevant \"task information\" can be compressed into a small subspace.\n\n**Intuition 3: Weight Matrix Structure.** Pretrained weight matrices already capture rich representations. Fine-tuning only needs to \"steer\" these representations slightly -- and small steering adjustments are inherently low-rank.\n\n#### Key Insight\n\nLoRA doesn't just save parameters -- it acts as a **regularizer**. By limiting the rank of the update, it prevents the model from making large, complex changes to the pretrained weights, which helps prevent catastrophic forgetting.\n\n#### Common Misconceptions\n\n| Misconception | Reality |\n|---------------|---------|\n| LoRA always matches full fine-tuning | For very different tasks, full FT can still win |\n| Lower rank is always better | Too low rank limits expressiveness |\n| LoRA adds inference cost | Weights can be merged: W_new = W + BA (zero overhead!) |\n| LoRA only works for attention layers | Works for any linear layer, but attention is most impactful |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adapters\n\n### The Idea\n\nAdapters take a different approach from LoRA: instead of decomposing weight updates, they **insert small bottleneck layers** into the existing model architecture.\n\nAn adapter module:\n1. **Down-projects** the hidden representation to a small dimension\n2. Applies a **non-linearity** (like ReLU)\n3. **Up-projects** back to the original dimension\n4. Adds a **residual connection** (so the adapter can be \"skipped\")\n\nThis is similar to LoRA in spirit (bottleneck = low rank), but differs in execution (sequential layers vs parallel decomposition).\n\n### Comparison: LoRA vs Adapters\n\n| Aspect | LoRA | Adapters |\n|--------|------|----------|\n| **Where** | Modifies existing weight matrices | Inserts new layers |\n| **Inference overhead** | None (can merge weights) | Small (extra computation) |\n| **Architecture change** | No | Yes (adds modules) |\n| **Training parameters** | $2 \\times d \\times r$ per layer | $2 \\times d \\times b + 2b$ per adapter |\n| **Non-linearity** | No | Yes (ReLU in bottleneck) |\n| **Popular for** | LLMs (GPT, LLaMA) | Earlier work (BERT, ViT) |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Implement an Adapter module\n\nclass AdapterLayer(nn.Module):\n    \"\"\"\n    Adapter module: down-project -> ReLU -> up-project + residual.\n\n    Args:\n        hidden_dim: Dimension of the input/output\n        bottleneck_dim: Dimension of the bottleneck (smaller = fewer params)\n    \"\"\"\n    def __init__(self, hidden_dim, bottleneck_dim=16):\n        super().__init__()\n        self.down_project = nn.Linear(hidden_dim, bottleneck_dim)\n        self.activation = nn.ReLU()\n        self.up_project = nn.Linear(bottleneck_dim, hidden_dim)\n\n        # Initialize up-project close to zero (minimal initial impact)\n        nn.init.zeros_(self.up_project.weight)\n        nn.init.zeros_(self.up_project.bias)\n\n    def forward(self, x):\n        residual = x\n        x = self.down_project(x)\n        x = self.activation(x)\n        x = self.up_project(x)\n        return x + residual  # Residual connection\n\n\n# Visualize adapter architecture\nfig, ax = plt.subplots(figsize=(8, 10))\nax.set_xlim(0, 10)\nax.set_ylim(0, 12)\nax.axis('off')\n\n# Draw transformer block with adapter\ncomponents = [\n    (5, 11, 'Input Embeddings', '#bdc3c7', 3),\n    (5, 9.5, 'Self-Attention\\n(Frozen)', '#3498db', 3),\n    (5, 8, 'Adapter', '#e74c3c', 2),\n    (5, 6.5, 'Layer Norm + Residual', '#bdc3c7', 3),\n    (5, 5, 'Feed-Forward\\n(Frozen)', '#3498db', 3),\n    (5, 3.5, 'Adapter', '#e74c3c', 2),\n    (5, 2, 'Layer Norm + Residual', '#bdc3c7', 3),\n    (5, 0.5, 'Output', '#bdc3c7', 3),\n]\n\nfor x, y, text, color, width in components:\n    bbox = dict(boxstyle=f'round,pad=0.3', facecolor=color, alpha=0.3, edgecolor=color)\n    fontsize = 11 if 'Adapter' not in text else 12\n    fw = 'bold' if 'Adapter' in text else 'normal'\n    ax.text(x, y, text, ha='center', va='center', fontsize=fontsize, fontweight=fw, bbox=bbox)\n\n# Arrows\nfor i in range(len(components)-1):\n    ax.annotate('', xy=(5, components[i+1][1]+0.5), xytext=(5, components[i][1]-0.5),\n               arrowprops=dict(arrowstyle='->', color='gray', lw=1.5))\n\n# Adapter detail box\ndetail_x = 8.5\nax.text(detail_x, 8, 'Adapter Detail:', ha='center', fontsize=11, fontweight='bold')\ndetail_parts = [\n    (detail_x, 7.3, 'Down: d->b', '#e74c3c'),\n    (detail_x, 6.8, 'ReLU', '#f39c12'),\n    (detail_x, 6.3, 'Up: b->d', '#e74c3c'),\n    (detail_x, 5.8, '+ Residual', '#2ecc71'),\n]\nfor x, y, t, c in detail_parts:\n    ax.text(x, y, t, ha='center', fontsize=10,\n            bbox=dict(boxstyle='round,pad=0.2', facecolor=c, alpha=0.2))\n\nax.set_title('Transformer Block with Adapters', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Show parameter comparison\nhidden_dim = 768  # BERT-base\nfor bottleneck in [8, 16, 32, 64]:\n    adapter = AdapterLayer(hidden_dim, bottleneck)\n    n_params = sum(p.numel() for p in adapter.parameters())\n    full_layer = hidden_dim * hidden_dim\n    print(f\"Adapter (bottleneck={bottleneck:2d}): {n_params:6d} params \"\n          f\"({100*n_params/full_layer:.2f}% of full layer {full_layer:,})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prompt Engineering & Prompt Tuning\n\n### Hard Prompts vs Soft Prompts\n\nThere are two fundamentally different ways to \"steer\" a language model:\n\n**Hard Prompts (Prompt Engineering):** Craft text instructions in natural language.\n```\n\"Classify the following review as positive or negative: {review}\"\n```\nNo model parameters change. You're just finding the right input words.\n\n**Soft Prompts (Prompt Tuning):** Prepend learned continuous vectors to the input.\nThese vectors aren't real words -- they're optimized embeddings that steer the model.\n\n### Comparison of Approaches\n\n| Method | What Changes | # Parameters | Human Readable? | Training Required? |\n|--------|-------------|-------------|-----------------|-------------------|\n| **Prompt Engineering** | Input text | 0 | Yes | No (manual craft) |\n| **In-Context Learning** | Input text (examples) | 0 | Yes | No (select examples) |\n| **Prompt Tuning** | Prepended embeddings | ~20K | No | Yes (gradient) |\n| **Prefix Tuning** | Key/Value prefixes per layer | ~200K | No | Yes (gradient) |\n| **LoRA** | Weight decomposition | ~1M-30M | N/A | Yes (gradient) |\n| **Full Fine-tuning** | All weights | All | N/A | Yes (gradient) |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualization: Hard prompts vs Soft prompts\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Hard prompts\nax = axes[0]\nax.set_xlim(0, 10)\nax.set_ylim(0, 8)\nax.axis('off')\nax.set_title('Hard Prompts (Prompt Engineering)', fontsize=13, fontweight='bold')\n\n# Token boxes for hard prompt\nhard_tokens = ['Classify', 'this', 'review', 'as', 'positive', 'or', 'negative', ':', '\"Great', 'movie!\"']\ny_pos = 6\nfor i, token in enumerate(hard_tokens):\n    x_pos = 0.5 + (i % 5) * 1.8\n    y = y_pos - (i // 5) * 1.5\n    color = '#3498db' if i < 8 else '#2ecc71'\n    label = 'Instruction' if i == 0 else ('Input' if i == 8 else '')\n    ax.text(x_pos, y, token, ha='center', va='center', fontsize=10,\n            bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.3))\n\nax.text(5, 2.5, 'All real words!\\nDesigned by humans', ha='center', fontsize=12,\n        style='italic', color='gray')\nax.text(5, 1.5, 'Model: \"positive\"', ha='center', fontsize=13, fontweight='bold',\n        bbox=dict(boxstyle='round', facecolor='#2ecc71', alpha=0.3))\n\n# Soft prompts\nax = axes[1]\nax.set_xlim(0, 10)\nax.set_ylim(0, 8)\nax.axis('off')\nax.set_title('Soft Prompts (Prompt Tuning)', fontsize=13, fontweight='bold')\n\n# Learned vectors\nfor i in range(5):\n    x_pos = 0.8 + i * 1.8\n    color = '#e74c3c'\n    ax.text(x_pos, 6, f'v_{i+1}', ha='center', va='center', fontsize=11, fontweight='bold',\n            bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.3))\n\n# Input tokens\ninput_tokens = ['\"Great', 'movie!\"']\nfor i, token in enumerate(input_tokens):\n    x_pos = 0.8 + i * 1.8\n    ax.text(x_pos, 4.5, token, ha='center', va='center', fontsize=10,\n            bbox=dict(boxstyle='round,pad=0.3', facecolor='#2ecc71', alpha=0.3))\n\nax.text(5, 3.2, 'v_1...v_5 are learned embeddings\\n(not real words!)',\n        ha='center', fontsize=11, style='italic', color='gray')\nax.text(5, 2, 'Optimized via gradient descent', ha='center', fontsize=11,\n        style='italic', color='#e74c3c')\nax.text(5, 1, 'Model: \"positive\"', ha='center', fontsize=13, fontweight='bold',\n        bbox=dict(boxstyle='round', facecolor='#2ecc71', alpha=0.3))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Hard prompts: Zero parameters, but requires human expertise to craft\")\nprint(\"Soft prompts: ~20K parameters, learned automatically, often outperform hard prompts\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Implement soft prompt tuning\n\nclass SoftPromptModel(nn.Module):\n    \"\"\"\n    Demonstrates soft prompt tuning.\n    Prepends learnable embeddings to the input sequence.\n\n    Args:\n        base_model: A frozen model that takes embeddings as input\n        embed_dim: Dimension of each embedding vector\n        n_prompt_tokens: Number of soft prompt tokens to prepend\n    \"\"\"\n    def __init__(self, embed_dim=64, n_prompt_tokens=10, vocab_size=100,\n                 hidden_dim=128, num_classes=3):\n        super().__init__()\n        # \"Base model\" components (frozen)\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.encoder = nn.Sequential(\n            nn.Linear(embed_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n\n        # Freeze base model\n        for param in self.embedding.parameters():\n            param.requires_grad = False\n        for param in self.encoder.parameters():\n            param.requires_grad = False\n\n        # Soft prompt tokens (TRAINABLE)\n        self.soft_prompt = nn.Parameter(torch.randn(n_prompt_tokens, embed_dim) * 0.01)\n\n        # Classification head (TRAINABLE)\n        self.classifier = nn.Linear(hidden_dim, num_classes)\n\n        self.n_prompt_tokens = n_prompt_tokens\n\n    def forward(self, input_ids):\n        # Get input embeddings (frozen)\n        input_embeds = self.embedding(input_ids)  # (batch, seq_len, embed_dim)\n\n        # Prepend soft prompt\n        batch_size = input_ids.size(0)\n        prompt = self.soft_prompt.unsqueeze(0).expand(batch_size, -1, -1)\n        combined = torch.cat([prompt, input_embeds], dim=1)  # (batch, n_prompt + seq_len, embed_dim)\n\n        # Encode (mean pooling)\n        encoded = self.encoder(combined)\n        pooled = encoded.mean(dim=1)  # (batch, hidden_dim)\n\n        return self.classifier(pooled)\n\n\n# Demo\nmodel = SoftPromptModel(n_prompt_tokens=10)\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(\"Soft Prompt Tuning Model:\")\nprint(f\"  Total parameters:     {total_params:,}\")\nprint(f\"  Trainable parameters: {trainable_params:,}\")\nprint(f\"  Frozen parameters:    {total_params - trainable_params:,}\")\nprint(f\"  Trainable fraction:   {100*trainable_params/total_params:.2f}%\")\nprint(f\"  Soft prompt shape:    {model.soft_prompt.shape}\")\n\n# Test forward pass\ndummy_input = torch.randint(0, 100, (4, 20))  # batch=4, seq_len=20\noutput = model(dummy_input)\nprint(f\"\\n  Input shape:  {dummy_input.shape}\")\nprint(f\"  Output shape: {output.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practical Fine-tuning Pipeline\n\nLet's put it all together with a complete example: fine-tuning a small model for text classification, comparing **full fine-tuning** vs **LoRA**.\n\n### The Setup\n\nWe'll:\n1. Create a pretrained \"mini language model\"\n2. Prepare a downstream classification dataset\n3. Fine-tune with both approaches\n4. Compare results (accuracy, parameter count, training speed)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 1: Create a \"pretrained\" base model\n\nclass MiniLanguageModel(nn.Module):\n    \"\"\"A small model pretending to be a pretrained language model.\"\"\"\n    def __init__(self, vocab_size=200, embed_dim=64, hidden_dim=128, num_layers=3):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n\n        # Transformer-like layers (simplified)\n        self.layers = nn.ModuleList()\n        for _ in range(num_layers):\n            self.layers.append(nn.ModuleDict({\n                'attention': nn.Linear(embed_dim, embed_dim),  # Simplified attention\n                'ff1': nn.Linear(embed_dim, hidden_dim),\n                'ff2': nn.Linear(hidden_dim, embed_dim),\n                'norm': nn.LayerNorm(embed_dim)\n            }))\n\n        self.final_norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x, return_features=True):\n        h = self.embedding(x)  # (batch, seq, embed)\n\n        for layer in self.layers:\n            # Simplified self-attention\n            attn_out = torch.tanh(layer['attention'](h))\n            h = h + attn_out\n\n            # Feed-forward\n            ff_out = F.gelu(layer['ff1'](h))\n            ff_out = layer['ff2'](ff_out)\n            h = layer['norm'](h + ff_out)\n\n        h = self.final_norm(h)\n\n        if return_features:\n            return h.mean(dim=1)  # Mean pooling -> (batch, embed)\n        return h\n\n\n# \"Pretrain\" the model (simulate by training on a pretext task)\ntorch.manual_seed(42)\nbase_model = MiniLanguageModel()\n\n# Simulate pretraining with a simple task\npretrain_optimizer = optim.Adam(base_model.parameters(), lr=0.001)\nfor step in range(200):\n    fake_input = torch.randint(0, 200, (32, 15))\n    features = base_model(fake_input)\n    # Simple pretraining objective: predict bag-of-words statistics\n    target = torch.zeros(32, 64)\n    for i in range(32):\n        for token in fake_input[i]:\n            target[i, token.item() % 64] += 1\n    target = target / target.sum(dim=1, keepdim=True)\n    loss = F.mse_loss(features, target)\n    pretrain_optimizer.zero_grad()\n    loss.backward()\n    pretrain_optimizer.step()\n\nprint(\"'Pretrained' base model created!\")\nprint(f\"Base model parameters: {sum(p.numel() for p in base_model.parameters()):,}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 2: Create downstream classification dataset\n\ndef create_classification_data(n_samples=500, seq_len=15, vocab_size=200, num_classes=4):\n    \"\"\"\n    Create synthetic text classification data.\n    Different classes use different vocabulary distributions.\n    \"\"\"\n    torch.manual_seed(42)\n    X = torch.zeros(n_samples, seq_len, dtype=torch.long)\n    y = torch.zeros(n_samples, dtype=torch.long)\n\n    samples_per_class = n_samples // num_classes\n    for c in range(num_classes):\n        start = c * samples_per_class\n        end = start + samples_per_class\n        # Each class favors different vocabulary ranges\n        base_vocab = c * (vocab_size // num_classes)\n        X[start:end] = torch.randint(base_vocab, base_vocab + vocab_size // num_classes,\n                                      (samples_per_class, seq_len))\n        y[start:end] = c\n\n    # Shuffle\n    perm = torch.randperm(n_samples)\n    return X[perm], y[perm]\n\n# Create data\nX_data, y_data = create_classification_data(n_samples=600)\nX_train, y_train = X_data[:400], y_data[:400]\nX_val, y_val = X_data[400:500], y_data[400:500]\nX_test, y_test = X_data[500:], y_data[500:]\n\nprint(f\"Dataset splits:\")\nprint(f\"  Train: {len(X_train)} samples\")\nprint(f\"  Val:   {len(X_val)} samples\")\nprint(f\"  Test:  {len(X_test)} samples\")\nprint(f\"  Classes: {y_data.unique().tolist()}\")\nprint(f\"  Sequence length: {X_train.shape[1]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 3: Define classifiers using different fine-tuning strategies\n\nclass FullFineTuneClassifier(nn.Module):\n    \"\"\"Full fine-tuning: all parameters are trainable.\"\"\"\n    def __init__(self, base_model, num_classes=4):\n        super().__init__()\n        self.base = deepcopy(base_model)\n        self.classifier = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        features = self.base(x, return_features=True)\n        return self.classifier(features)\n\n\nclass LoRAClassifier(nn.Module):\n    \"\"\"LoRA fine-tuning: only LoRA params + classifier are trainable.\"\"\"\n    def __init__(self, base_model, num_classes=4, lora_rank=4, lora_alpha=4):\n        super().__init__()\n        self.base = deepcopy(base_model)\n\n        # Freeze all base model parameters\n        for param in self.base.parameters():\n            param.requires_grad = False\n\n        # Add LoRA to attention and FF layers\n        self.lora_layers = nn.ModuleList()\n        for layer in self.base.layers:\n            # LoRA on attention\n            lora_attn = LoRALinear(layer['attention'], rank=lora_rank, alpha=lora_alpha)\n            layer['attention'] = lora_attn  # This won't freeze properly, handle below\n            self.lora_layers.append(lora_attn)\n\n            # LoRA on ff1\n            lora_ff = LoRALinear(layer['ff1'], rank=lora_rank, alpha=lora_alpha)\n            layer['ff1'] = lora_ff\n            self.lora_layers.append(lora_ff)\n\n        self.classifier = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        features = self.base(x, return_features=True)\n        return self.classifier(features)\n\n\ndef train_classifier(model, X_train, y_train, X_val, y_val, epochs=80, lr=0.001, batch_size=32):\n    \"\"\"Train classifier and track metrics.\"\"\"\n    import time\n\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    train_losses, val_accs = [], []\n    start_time = time.time()\n\n    for epoch in range(epochs):\n        model.train()\n        epoch_loss = 0\n        n_batches = 0\n\n        for i in range(0, len(X_train), batch_size):\n            batch_X = X_train[i:i+batch_size]\n            batch_y = y_train[i:i+batch_size]\n\n            optimizer.zero_grad()\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item()\n            n_batches += 1\n\n        train_losses.append(epoch_loss / n_batches)\n\n        model.eval()\n        with torch.no_grad():\n            val_preds = model(X_val).argmax(dim=1)\n            val_acc = (val_preds == y_val).float().mean().item()\n            val_accs.append(val_acc)\n\n    elapsed = time.time() - start_time\n    return train_losses, val_accs, elapsed\n\nprint(\"Classifier classes defined!\")\nprint(\"Ready to train Full Fine-tuning vs LoRA...\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 4: Train both approaches and compare\n\n# Full fine-tuning\nft_model = FullFineTuneClassifier(base_model)\nft_trainable = sum(p.numel() for p in ft_model.parameters() if p.requires_grad)\nft_total = sum(p.numel() for p in ft_model.parameters())\nft_losses, ft_accs, ft_time = train_classifier(ft_model, X_train, y_train, X_val, y_val)\n\n# LoRA fine-tuning\nlora_model = LoRAClassifier(base_model, lora_rank=4, lora_alpha=4)\nlora_trainable = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\nlora_total = sum(p.numel() for p in lora_model.parameters())\nlora_losses, lora_accs, lora_time = train_classifier(lora_model, X_train, y_train, X_val, y_val)\n\n# From scratch (no pretraining)\nscratch_base = MiniLanguageModel()\nscratch_model = FullFineTuneClassifier(scratch_base)\nscratch_losses, scratch_accs, scratch_time = train_classifier(\n    scratch_model, X_train, y_train, X_val, y_val\n)\n\n# Test set evaluation\nft_model.eval()\nlora_model.eval()\nscratch_model.eval()\nwith torch.no_grad():\n    ft_test_acc = (ft_model(X_test).argmax(1) == y_test).float().mean().item()\n    lora_test_acc = (lora_model(X_test).argmax(1) == y_test).float().mean().item()\n    scratch_test_acc = (scratch_model(X_test).argmax(1) == y_test).float().mean().item()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"RESULTS COMPARISON\")\nprint(\"=\" * 70)\nprint(f\"{'Method':<20} {'Test Acc':>10} {'Trainable':>12} {'% of Total':>12} {'Time':>8}\")\nprint(\"-\" * 70)\nprint(f\"{'From Scratch':<20} {scratch_test_acc:>10.4f} {ft_trainable:>12,} {'100.0%':>12} {scratch_time:>7.2f}s\")\nprint(f\"{'Full Fine-tuning':<20} {ft_test_acc:>10.4f} {ft_trainable:>12,} {'100.0%':>12} {ft_time:>7.2f}s\")\nprint(f\"{'LoRA (r=4)':<20} {lora_test_acc:>10.4f} {lora_trainable:>12,} {100*lora_trainable/lora_total:>11.1f}% {lora_time:>7.2f}s\")\nprint(\"=\" * 70)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 5: Visualization of results comparison\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# Training loss\naxes[0].plot(scratch_losses, label='From Scratch', color='red', alpha=0.7)\naxes[0].plot(ft_losses, label='Full Fine-tuning', color='blue', alpha=0.7)\naxes[0].plot(lora_losses, label='LoRA (r=4)', color='green', alpha=0.7)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Training Loss')\naxes[0].set_title('Training Loss', fontweight='bold')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Validation accuracy\naxes[1].plot(scratch_accs, label='From Scratch', color='red', alpha=0.7)\naxes[1].plot(ft_accs, label='Full Fine-tuning', color='blue', alpha=0.7)\naxes[1].plot(lora_accs, label='LoRA (r=4)', color='green', alpha=0.7)\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Validation Accuracy')\naxes[1].set_title('Validation Accuracy', fontweight='bold')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# Parameter comparison\nmethods = ['From\\nScratch', 'Full\\nFine-tune', 'LoRA\\n(r=4)']\ntrainable_counts = [ft_trainable, ft_trainable, lora_trainable]\ncolors = ['red', 'blue', 'green']\nbars = axes[2].bar(methods, trainable_counts, color=colors, alpha=0.7, edgecolor='black')\naxes[2].set_ylabel('Trainable Parameters')\naxes[2].set_title('Trainable Parameters', fontweight='bold')\naxes[2].grid(True, alpha=0.3, axis='y')\n\n# Add count labels on bars\nfor bar, count in zip(bars, trainable_counts):\n    axes[2].text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n                f'{count:,}', ha='center', va='bottom', fontweight='bold')\n\nplt.suptitle('Full Fine-tuning vs LoRA: Complete Comparison', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey takeaways:\")\nprint(\"1. LoRA achieves comparable accuracy with far fewer trainable parameters\")\nprint(\"2. Full fine-tuning may slightly outperform but at much higher cost\")\nprint(\"3. Both fine-tuning approaches beat training from scratch\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RLHF: Reinforcement Learning from Human Feedback\n\n### Why Fine-tuning on Data Isn't Enough\n\nA language model trained on internet text can generate fluent text, but it might:\n- Give harmful or toxic responses\n- Make up facts (hallucinate) confidently\n- Not follow instructions well\n- Be verbose when brevity is needed\n\n**The problem:** The model learned to predict the next word, not to be **helpful, harmless, and honest**.\n\n### The RLHF Pipeline\n\nRLHF adds human preferences to guide the model's behavior:\n\n| Stage | What Happens | Goal |\n|-------|-------------|------|\n| **1. Pretraining** | Train on massive text corpus | Learn language understanding |\n| **2. SFT (Supervised Fine-tuning)** | Fine-tune on demonstration data | Learn to follow instructions |\n| **3. Reward Model** | Train a model to predict human preferences | Learn what humans prefer |\n| **4. RL (PPO)** | Optimize policy against reward model | Maximize human preference |\n\n### How It Works\n\n1. **Collect comparison data:** Show humans two model responses, ask which is better\n2. **Train reward model:** Learn to score responses the way humans would\n3. **Optimize with PPO:** Use the reward model as a \"reward function\" in reinforcement learning to update the language model\n\n**What this means:** RLHF bridges the gap between \"predicting the next word\" and \"being a helpful assistant.\" This is the key technique that turned GPT-3 into ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualization: The RLHF Pipeline\n\nfig, ax = plt.subplots(figsize=(16, 8))\nax.set_xlim(0, 16)\nax.set_ylim(0, 9)\nax.axis('off')\n\n# Stage 1: Pretraining\nax.text(2, 7.5, 'Stage 1: Pretraining', ha='center', fontsize=13, fontweight='bold')\nboxes1 = [\n    (2, 6.5, 'Internet Text\\n(TB of data)', '#bdc3c7'),\n    (2, 5.3, 'Base LLM\\n(Next token prediction)', '#3498db'),\n]\nfor x, y, text, color in boxes1:\n    ax.text(x, y, text, ha='center', va='center', fontsize=10,\n            bbox=dict(boxstyle='round,pad=0.4', facecolor=color, alpha=0.3, edgecolor=color))\nax.annotate('', xy=(2, 5.8), xytext=(2, 6.2), arrowprops=dict(arrowstyle='->', lw=2, color='gray'))\n\n# Stage 2: SFT\nax.text(6, 7.5, 'Stage 2: SFT', ha='center', fontsize=13, fontweight='bold')\nboxes2 = [\n    (6, 6.5, 'Demonstration Data\\n(Human-written)', '#bdc3c7'),\n    (6, 5.3, 'SFT Model\\n(Follows instructions)', '#2ecc71'),\n]\nfor x, y, text, color in boxes2:\n    ax.text(x, y, text, ha='center', va='center', fontsize=10,\n            bbox=dict(boxstyle='round,pad=0.4', facecolor=color, alpha=0.3, edgecolor=color))\nax.annotate('', xy=(6, 5.8), xytext=(6, 6.2), arrowprops=dict(arrowstyle='->', lw=2, color='gray'))\nax.annotate('', xy=(4.5, 5.3), xytext=(3.5, 5.3), arrowprops=dict(arrowstyle='->', lw=2, color='gray'))\n\n# Stage 3: Reward Model\nax.text(10, 7.5, 'Stage 3: Reward Model', ha='center', fontsize=13, fontweight='bold')\nboxes3 = [\n    (10, 6.5, 'Comparison Data\\n(A > B preferences)', '#bdc3c7'),\n    (10, 5.3, 'Reward Model\\n(Scores responses)', '#f39c12'),\n]\nfor x, y, text, color in boxes3:\n    ax.text(x, y, text, ha='center', va='center', fontsize=10,\n            bbox=dict(boxstyle='round,pad=0.4', facecolor=color, alpha=0.3, edgecolor=color))\nax.annotate('', xy=(10, 5.8), xytext=(10, 6.2), arrowprops=dict(arrowstyle='->', lw=2, color='gray'))\n\n# Stage 4: PPO\nax.text(14, 7.5, 'Stage 4: RL (PPO)', ha='center', fontsize=13, fontweight='bold')\nax.text(14, 5.3, 'RLHF Model\\n(Aligned with humans!)', ha='center', va='center', fontsize=10,\n        bbox=dict(boxstyle='round,pad=0.4', facecolor='#e74c3c', alpha=0.3, edgecolor='#e74c3c'))\nax.annotate('', xy=(12.5, 5.3), xytext=(11.5, 5.3), arrowprops=dict(arrowstyle='->', lw=2, color='gray'))\n\n# PPO loop\nax.annotate('', xy=(14, 4.5), xytext=(14, 4.9), arrowprops=dict(arrowstyle='->', lw=1.5, color='#e74c3c'))\nax.text(14, 4, 'Generate\\nresponse', ha='center', fontsize=9,\n        bbox=dict(boxstyle='round,pad=0.2', facecolor='#e8e8e8', alpha=0.5))\nax.annotate('', xy=(14, 3.2), xytext=(14, 3.6), arrowprops=dict(arrowstyle='->', lw=1.5, color='#e74c3c'))\nax.text(14, 2.7, 'Score with\\nReward Model', ha='center', fontsize=9,\n        bbox=dict(boxstyle='round,pad=0.2', facecolor='#f39c12', alpha=0.3))\nax.annotate('', xy=(14, 2), xytext=(14, 2.4), arrowprops=dict(arrowstyle='->', lw=1.5, color='#e74c3c'))\nax.text(14, 1.5, 'Update policy\\n(PPO)', ha='center', fontsize=9,\n        bbox=dict(boxstyle='round,pad=0.2', facecolor='#e74c3c', alpha=0.3))\n\n# Curved arrow for loop\nfrom matplotlib.patches import FancyArrowPatch\nax.annotate('', xy=(15.2, 5.0), xytext=(15.2, 1.5),\n           arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=-0.3',\n                          color='#e74c3c', lw=1.5, linestyle='--'))\n\n# Bottom result\nax.text(8, 0.5, 'Result: A model that is helpful, harmless, and honest (like ChatGPT!)',\n        ha='center', fontsize=12, fontweight='bold', style='italic',\n        bbox=dict(boxstyle='round,pad=0.5', facecolor='#2ecc71', alpha=0.2))\n\nax.set_title('The RLHF Pipeline: From Base Model to AI Assistant',\n             fontsize=15, fontweight='bold', pad=10)\nplt.tight_layout()\nplt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Simplified demonstration of the reward model concept\n\nclass SimpleRewardModel(nn.Module):\n    \"\"\"\n    A reward model that scores text quality.\n    In practice, this is trained on human preference data.\n    \"\"\"\n    def __init__(self, input_dim=64, hidden_dim=32):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)  # Output: scalar reward\n        )\n\n    def forward(self, x):\n        return self.network(x).squeeze(-1)\n\n# Simulate the preference learning process\nprint(\"RLHF Reward Model Training (Simplified)\")\nprint(\"=\" * 55)\n\n# Create simulated \"response quality\" data\ntorch.manual_seed(42)\nn_comparisons = 200\n\n# For each comparison: feature vectors of response A and B\nfeatures_A = torch.randn(n_comparisons, 64)\nfeatures_B = torch.randn(n_comparisons, 64)\n\n# \"True\" quality scores (unknown to the model)\ntrue_quality_A = features_A[:, 0] + features_A[:, 1] * 0.5  # Simple quality function\ntrue_quality_B = features_B[:, 0] + features_B[:, 1] * 0.5\n\n# Human preference: 1 if A is preferred, 0 if B is preferred\nhuman_prefers_A = (true_quality_A > true_quality_B).float()\n\n# Train reward model\nreward_model = SimpleRewardModel()\noptimizer = optim.Adam(reward_model.parameters(), lr=0.01)\n\nlosses = []\nfor epoch in range(100):\n    score_A = reward_model(features_A)\n    score_B = reward_model(features_B)\n\n    # Bradley-Terry model: P(A > B) = sigmoid(score_A - score_B)\n    logits = score_A - score_B\n    loss = F.binary_cross_entropy_with_logits(logits, human_prefers_A)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n\n# Evaluate\nwith torch.no_grad():\n    pred_A_better = (reward_model(features_A) > reward_model(features_B)).float()\n    accuracy = (pred_A_better == human_prefers_A).float().mean()\n\nprint(f\"Reward model accuracy: {accuracy:.4f}\")\nprint(f\"Final loss: {losses[-1]:.4f}\")\nprint(f\"\\nThe reward model learned to predict which response humans prefer!\")\nprint(f\"In RLHF, this reward signal guides the LLM to generate better responses.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n\n### Exercise 1: Implement LoRA with Different Ranks\n\nCompare the parameter efficiency and expressiveness of LoRA at different ranks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# EXERCISE 1: Implement LoRA analysis at different ranks\ndef analyze_lora_efficiency(input_dim, output_dim, ranks):\n    \"\"\"\n    For each rank, compute:\n    - Number of LoRA parameters\n    - Compression ratio vs full matrix\n    - The rank of the resulting delta_W\n\n    Args:\n        input_dim: Input dimension of the weight matrix\n        output_dim: Output dimension of the weight matrix\n        ranks: List of LoRA ranks to analyze\n\n    Returns:\n        Dictionary with 'rank', 'params', 'compression', 'actual_rank' keys\n    \"\"\"\n    # TODO: Implement this!\n    # Hint: LoRA params = output_dim * r + r * input_dim\n    # Hint: Create random B (out x r) and A (r x in), compute rank of B@A\n\n    results = {'rank': [], 'params': [], 'compression': [], 'actual_rank': []}\n\n    full_params = input_dim * output_dim\n\n    for r in ranks:\n        # Calculate LoRA parameters\n        lora_params = output_dim * r + r * input_dim\n\n        # Calculate compression ratio\n        compression = full_params / lora_params\n\n        # Create random matrices and verify rank\n        B = torch.randn(output_dim, r)\n        A = torch.randn(r, input_dim)\n        delta_W = B @ A\n        actual_rank = torch.linalg.matrix_rank(delta_W).item()\n\n        results['rank'].append(r)\n        results['params'].append(lora_params)\n        results['compression'].append(compression)\n        results['actual_rank'].append(actual_rank)\n\n    return results\n\n# Test\nresults = analyze_lora_efficiency(768, 768, [1, 2, 4, 8, 16, 32, 64])\nprint(f\"{'Rank':>6} {'Params':>10} {'Compression':>12} {'Actual Rank':>12}\")\nprint(\"-\" * 45)\nfor i in range(len(results['rank'])):\n    print(f\"{results['rank'][i]:>6} {results['params'][i]:>10,} \"\n          f\"{results['compression'][i]:>11.1f}x {results['actual_rank'][i]:>12}\")\n\n# Verify\nassert results['params'][0] == 768 * 1 + 1 * 768, \"Params for rank 1 should be 1536\"\nassert results['actual_rank'][2] == 4, \"Rank of delta_W should equal LoRA rank\"\nprint(\"\\nAll checks passed!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Build a Custom LoRA Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# EXERCISE 2: Apply LoRA to a multi-layer model\nclass LoRAModel(nn.Module):\n    \"\"\"\n    Take a simple feedforward model and add LoRA to specified layers.\n\n    Args:\n        base_model: nn.Sequential with Linear layers\n        lora_rank: Rank for LoRA decomposition\n        lora_alpha: Scaling factor\n        target_layers: List of layer indices to apply LoRA to\n    \"\"\"\n    def __init__(self, base_model, lora_rank=4, lora_alpha=4, target_layers=None):\n        super().__init__()\n        # TODO: Implement this!\n        # Hint: Iterate through base_model layers\n        # Hint: Replace Linear layers at target indices with LoRALinear\n        # Hint: Freeze all base model parameters\n\n        self.layers = nn.ModuleList()\n        layer_idx = 0\n\n        for module in base_model:\n            if isinstance(module, nn.Linear):\n                if target_layers is None or layer_idx in target_layers:\n                    self.layers.append(LoRALinear(module, rank=lora_rank, alpha=lora_alpha))\n                else:\n                    # Freeze this layer\n                    for p in module.parameters():\n                        p.requires_grad = False\n                    self.layers.append(module)\n                layer_idx += 1\n            else:\n                self.layers.append(module)\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n# Test\nbase = nn.Sequential(\n    nn.Linear(128, 64),\n    nn.ReLU(),\n    nn.Linear(64, 64),\n    nn.ReLU(),\n    nn.Linear(64, 10)\n)\n\n# Apply LoRA only to first and last linear layers\nlora_model = LoRAModel(base, lora_rank=4, target_layers=[0, 2])\n\ntotal = sum(p.numel() for p in lora_model.parameters())\ntrainable = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n\nprint(f\"Total parameters:     {total:,}\")\nprint(f\"Trainable parameters: {trainable:,}\")\nprint(f\"Frozen parameters:    {total - trainable:,}\")\nprint(f\"Trainable fraction:   {100*trainable/total:.1f}%\")\n\n# Test forward pass\nx = torch.randn(8, 128)\ny = lora_model(x)\nprint(f\"\\nForward pass: {x.shape} -> {y.shape}\")\n\n# Verify only LoRA params and target layers are trainable\nexpected_trainable = (128*4 + 4*64) + (64*4 + 4*10)  # LoRA params for layers 0 and 2\nprint(f\"\\nExpected LoRA params: {expected_trainable}\")\nprint(f\"Actual trainable:     {trainable}\")\nprint(f\"Match: {trainable == expected_trainable}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Compare Fine-tuning Strategies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# EXERCISE 3: Run a systematic comparison of strategies\ndef compare_strategies(base_model, X_train, y_train, X_val, y_val, num_classes=4):\n    \"\"\"\n    Compare four strategies:\n    1. From scratch (random init)\n    2. Feature extraction (frozen base + trainable head)\n    3. Full fine-tuning (all trainable)\n    4. LoRA fine-tuning (LoRA + head trainable)\n\n    Returns dict with strategy names and their (trainable_params, final_val_acc)\n    \"\"\"\n    results = {}\n\n    # TODO: Implement each strategy, train for 50 epochs, record results\n    # Hint: Reuse the training functions from earlier\n\n    # Strategy 1: From scratch\n    scratch = FullFineTuneClassifier(MiniLanguageModel(), num_classes)\n    _, accs, _ = train_classifier(scratch, X_train, y_train, X_val, y_val, epochs=50)\n    trainable = sum(p.numel() for p in scratch.parameters() if p.requires_grad)\n    results['From Scratch'] = (trainable, accs[-1])\n\n    # Strategy 2: Feature extraction\n    fe = FullFineTuneClassifier(base_model, num_classes)\n    for p in fe.base.parameters():\n        p.requires_grad = False\n    _, accs, _ = train_classifier(fe, X_train, y_train, X_val, y_val, epochs=50)\n    trainable = sum(p.numel() for p in fe.parameters() if p.requires_grad)\n    results['Feature Extraction'] = (trainable, accs[-1])\n\n    # Strategy 3: Full fine-tuning\n    full = FullFineTuneClassifier(base_model, num_classes)\n    _, accs, _ = train_classifier(full, X_train, y_train, X_val, y_val, epochs=50)\n    trainable = sum(p.numel() for p in full.parameters() if p.requires_grad)\n    results['Full Fine-tuning'] = (trainable, accs[-1])\n\n    # Strategy 4: LoRA\n    lora = LoRAClassifier(base_model, num_classes, lora_rank=4)\n    _, accs, _ = train_classifier(lora, X_train, y_train, X_val, y_val, epochs=50)\n    trainable = sum(p.numel() for p in lora.parameters() if p.requires_grad)\n    results['LoRA (r=4)'] = (trainable, accs[-1])\n\n    return results\n\n# Run comparison\nresults = compare_strategies(base_model, X_train, y_train, X_val, y_val)\n\nprint(\"Strategy Comparison\")\nprint(\"=\" * 55)\nprint(f\"{'Strategy':<20} {'Trainable Params':>16} {'Val Accuracy':>14}\")\nprint(\"-\" * 55)\nfor name, (params, acc) in results.items():\n    print(f\"{name:<20} {params:>16,} {acc:>14.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n\n### Key Concepts\n\n- **Transfer Learning** reuses knowledge from pretrained models, dramatically reducing the data and compute needed for new tasks\n- **Feature Extraction** freezes the pretrained model and only trains a new classification head\n- **Full Fine-tuning** updates all parameters with a small learning rate; risk of catastrophic forgetting\n- **Discriminative Learning Rates** use smaller LR for earlier (more general) layers\n- **PEFT** methods modify only a small fraction of parameters (often < 1%) while achieving comparable performance\n- **LoRA** decomposes weight updates as $\\Delta W = BA$ where $B \\in \\mathbb{R}^{d \\times r}$ and $A \\in \\mathbb{R}^{r \\times d}$ with $r \\ll d$\n- **Adapters** insert small bottleneck layers into the network\n- **Prompt Tuning** prepends learnable embeddings to the input\n- **RLHF** uses human feedback to align language models with human preferences\n\n### Connection to Deep Learning\n\n| Concept | Where It's Used |\n|---------|----------------|\n| Transfer learning | Almost every practical DL application |\n| Full fine-tuning | When you have enough data and compute |\n| LoRA | Adapting LLMs (LLaMA, GPT, etc.) efficiently |\n| Adapters | Multi-task learning, modular AI systems |\n| Prompt tuning | When you can't modify model weights at all |\n| RLHF | Creating AI assistants (ChatGPT, Claude, etc.) |\n\n### Checklist\n\n- [ ] I can explain why transfer learning works (hierarchical features)\n- [ ] I know when to use feature extraction vs fine-tuning\n- [ ] I can implement discriminative learning rates\n- [ ] I understand why PEFT is necessary for large models\n- [ ] I can implement LoRA from scratch and explain the rank decomposition\n- [ ] I can calculate the parameter savings of LoRA for any given rank and dimension\n- [ ] I understand how adapters differ from LoRA\n- [ ] I know the difference between hard prompts and soft prompts\n- [ ] I can describe the four stages of the RLHF pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n\nIn the next notebook, we'll explore **Generative Models** -- including Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Diffusion Models. These are the techniques behind image generation (DALL-E, Stable Diffusion), text generation, and more. We'll see how some of the fine-tuning techniques from this notebook (especially LoRA) are used to customize generative models for specific styles and tasks."
   ]
  }
 ]
}