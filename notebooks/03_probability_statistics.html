
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Part 1.3: Probability &amp; Statistics for Deep Learning &#8212; Foundations of AI</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=9c3e77be" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=1ae7504c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/03_probability_statistics';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Part 2.1: Python OOP for Deep Learning" href="04_python_oop.html" />
    <link rel="prev" title="Part 1.2: Calculus for Deep Learning" href="02_calculus.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Foundations of AI</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 1: Mathematical Foundations</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_linear_algebra.html">Part 1.1: Linear Algebra for Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_calculus.html">Part 1.2: Calculus for Deep Learning</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Part 1.3: Probability &amp; Statistics for Deep Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 2: Programming Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_python_oop.html">Part 2.1: Python OOP for Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_numpy_deep_dive.html">Part 2.2: NumPy Deep Dive</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 3: Classical ML &amp; Optimization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="06_classical_ml.html">Part 3.1: Classical Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_optimization_linear_programming.html">Part 3.2: Optimization &amp; Linear Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_optimization_theory.html">Part 3.3: Optimization Theory for Machine Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 4: Neural Network Fundamentals</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="09_perceptrons_basic_networks.html">Part 4.1: Perceptrons &amp; Basic Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_backpropagation.html">Part 4.2: Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_pytorch_fundamentals.html">Part 4.3: PyTorch Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_training_deep_networks.html">Part 4.4: Training Deep Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 5: Neural Network Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="13_convolutional_neural_networks.html">Part 5.1: Convolutional Neural Networks (CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_computer_vision_depth.html">Part 5.2: Computer Vision — Beyond Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_recurrent_neural_networks.html">Part 5.3: Recurrent Neural Networks (RNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_attention_mechanisms.html">Part 5.4: Attention Mechanisms</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 6: Transformers &amp; LLMs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="17_transformer_architecture.html">Part 6.1: Transformer Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_embeddings.html">Part 6.2: Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="19_tokenization_lm_training.html">Part 6.3: Tokenization &amp; Language Model Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="20_language_models.html">Part 6.4: Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="21_finetuning_and_peft.html">Part 6.5: Fine-tuning &amp; PEFT</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 7: Reinforcement Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="22_rl_fundamentals.html">Part 7.1: Reinforcement Learning Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="23_q_learning_dqn.html">Part 7.2: Q-Learning and Deep Q-Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="24_policy_gradients.html">Part 7.3: Policy Gradient Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="25_ppo_modern_rl.html">Part 7.4: PPO and Modern RL</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 8: Applied AI Systems</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="26_rag.html">Part 8.1: Retrieval-Augmented Generation (RAG)</a></li>
<li class="toctree-l1"><a class="reference internal" href="27_ai_agents.html">Part 8.2: AI Agents and Tool Use</a></li>
<li class="toctree-l1"><a class="reference internal" href="28_ai_evals.html">Part 8.3: Evaluating AI Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="29_production_monitoring.html">Part 8.4: Production AI Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 9: Advanced Topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="30_inference_optimization.html">Part 9.1: LLM Inference Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="31_ml_systems.html">Part 9.2: ML Systems &amp; Experiment Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="32_multimodal_ai.html">Part 9.3: Multimodal AI</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/dan-shah/foundations-of-ai/blob/main/notebooks/03_probability_statistics.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/dan-shah/foundations-of-ai" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/dan-shah/foundations-of-ai/edit/main/notebooks/03_probability_statistics.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/dan-shah/foundations-of-ai/issues/new?title=Issue%20on%20page%20%2Fnotebooks/03_probability_statistics.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notebooks/03_probability_statistics.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Part 1.3: Probability & Statistics for Deep Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-basics">1. Probability Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-variables">Random Variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-distributions">Probability Distributions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-what-is-a-probability-distribution">Deep Dive: What is a Probability Distribution?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-vs-continuous-distributions">Discrete vs Continuous Distributions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-distributions">2. Common Distributions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bernoulli-distribution">2.1 Bernoulli Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binomial-distribution">2.2 Binomial Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-distribution">2.3 Categorical Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-normal-distribution">2.4 Gaussian (Normal) Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-gaussian">2.5 Multivariate Gaussian</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-the-right-distribution-a-decision-guide">Choosing the Right Distribution: A Decision Guide</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-value-and-variance">3. Expected Value and Variance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-value-mean">Expected Value (Mean)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variance">Variance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-understanding-each-term-in-bayes-theorem">Deep Dive: Understanding Each Term in Bayes’ Theorem</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem">4. Bayes’ Theorem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem-in-machine-learning">Bayes’ Theorem in Machine Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-the-intuition-behind-maximum-likelihood">Deep Dive: The Intuition Behind Maximum Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-understanding-entropy">Deep Dive: Understanding Entropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy-reference-table">Entropy Reference Table</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes-classifier">Naive Bayes Classifier</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-understanding-kl-divergence">Deep Dive: Understanding KL Divergence</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kl-divergence-in-machine-learning-applications">KL Divergence in Machine Learning Applications</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation-mle">5. Maximum Likelihood Estimation (MLE)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-for-bernoulli-coin-flip-race-finish">MLE for Bernoulli (Coin Flip / Race Finish)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#information-theory">6. Information Theory</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy">Entropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-entropy">Cross-Entropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kl-divergence">KL Divergence</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-bayesian-tire-degradation-inference">Exercise 1: Bayesian Tire Degradation Inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-implement-softmax-cross-entropy-loss-for-race-outcome-prediction">Exercise 2: Implement Softmax Cross-Entropy Loss for Race Outcome Prediction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-information-gain-for-pit-stop-strategy-decisions">Exercise 3: Information Gain for Pit Stop Strategy Decisions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts-and-their-f1-parallels">Key Concepts and Their F1 Parallels</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-deep-learning">Connection to Deep Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checklist">Checklist</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">Next Steps</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="part-1-3-probability-statistics-for-deep-learning">
<h1>Part 1.3: Probability &amp; Statistics for Deep Learning<a class="headerlink" href="#part-1-3-probability-statistics-for-deep-learning" title="Link to this heading">#</a></h1>
<p>Probability and statistics are essential for understanding:</p>
<ul class="simple">
<li><p>How models make predictions (probabilistic outputs)</p></li>
<li><p>How we train models (maximum likelihood)</p></li>
<li><p>How we measure uncertainty and information</p></li>
</ul>
<p><strong>The F1 Connection</strong>: Formula 1 is a sport drowning in probability. Will it rain at Spa? What’s the chance of a safety car at Monaco? How do lap times distribute around the mean? Every race strategy decision — when to pit, which tire compound to choose, whether to risk a one-stop — is a bet against a probability distribution. The teams that model these distributions best win championships.</p>
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>[ ] Work with common probability distributions</p></li>
<li><p>[ ] Apply Bayes’ theorem to update beliefs</p></li>
<li><p>[ ] Derive MLE estimators for simple distributions</p></li>
<li><p>[ ] Calculate entropy and KL divergence</p></li>
</ul>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.special</span><span class="w"> </span><span class="kn">import</span> <span class="n">comb</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn-v0_8-whitegrid&#39;</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="probability-basics">
<h2>1. Probability Basics<a class="headerlink" href="#probability-basics" title="Link to this heading">#</a></h2>
<section id="random-variables">
<h3>Random Variables<a class="headerlink" href="#random-variables" title="Link to this heading">#</a></h3>
<p>A <strong>random variable</strong> is a variable whose value is determined by a random process.</p>
<ul class="simple">
<li><p><strong>Discrete</strong>: Takes on countable values (e.g., coin flips, dice rolls)</p></li>
<li><p><strong>Continuous</strong>: Takes on any value in a range (e.g., height, temperature)</p></li>
</ul>
<p><strong>F1 analogy</strong>: A driver’s finishing position is a discrete random variable (1st, 2nd, …, DNF). Their lap time is a continuous random variable — it can be 1:31.204 or 1:31.205 or anything in between.</p>
</section>
<section id="probability-distributions">
<h3>Probability Distributions<a class="headerlink" href="#probability-distributions" title="Link to this heading">#</a></h3>
<p>A <strong>probability distribution</strong> describes the likelihood of each possible outcome.</p>
<ul class="simple">
<li><p><strong>PMF</strong> (Probability Mass Function): For discrete variables, <span class="math notranslate nohighlight">\(P(X = x)\)</span></p></li>
<li><p><strong>PDF</strong> (Probability Density Function): For continuous variables, <span class="math notranslate nohighlight">\(f(x)\)</span></p></li>
</ul>
<p><strong>F1 analogy</strong>: The PMF is like a grid of starting positions with the probability of each driver winning from that slot. The PDF is like the smooth curve of lap time variation — you can’t ask “what’s the probability of exactly 1:31.204?” but you can ask “what’s the probability of a lap between 1:31 and 1:32?”</p>
</section>
<section id="deep-dive-what-is-a-probability-distribution">
<h3>Deep Dive: What is a Probability Distribution?<a class="headerlink" href="#deep-dive-what-is-a-probability-distribution" title="Link to this heading">#</a></h3>
<p>A probability distribution answers a fundamental question: <strong>“What outcomes are possible, and how likely is each one?”</strong></p>
<p>Think of it as a complete recipe for uncertainty:</p>
<ul class="simple">
<li><p>It lists every possible outcome</p></li>
<li><p>It assigns a probability (or density) to each outcome</p></li>
<li><p>All probabilities sum to 1 (something must happen!)</p></li>
</ul>
<p><strong>The Key Insight</strong>: A distribution captures <em>everything</em> we know about a random process. Once you have the distribution, you can compute any probability, expectation, or uncertainty measure.</p>
<p><strong>F1 analogy</strong>: An F1 strategist’s entire job is building probability distributions. Before a race, they model: the distribution of possible lap times on each tire compound, the probability of rain in each 10-minute window, the likelihood of a safety car on each lap. The team with the best distributions makes the best pit stop calls.</p>
<section id="discrete-vs-continuous-distributions">
<h4>Discrete vs Continuous Distributions<a class="headerlink" href="#discrete-vs-continuous-distributions" title="Link to this heading">#</a></h4>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Aspect</p></th>
<th class="head"><p>Discrete</p></th>
<th class="head"><p>Continuous</p></th>
<th class="head"><p>F1 Example</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Possible values</strong></p></td>
<td><p>Countable (finite or infinite)</p></td>
<td><p>Uncountable (any value in a range)</p></td>
<td><p>Finishing position (1st-20th, DNF) vs. lap time (continuous)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Probability function</strong></p></td>
<td><p>PMF: P(X = x) gives exact probability</p></td>
<td><p>PDF: f(x) gives density, not probability</p></td>
<td><p>P(win from pole) = 0.45 vs. lap time density curve</p></td>
</tr>
<tr class="row-even"><td><p><strong>Finding probabilities</strong></p></td>
<td><p>Sum: P(a ≤ X ≤ b) = Σ P(X = x)</p></td>
<td><p>Integrate: P(a ≤ X ≤ b) = ∫f(x)dx</p></td>
<td><p>P(podium) = P(1st) + P(2nd) + P(3rd) vs. P(lap &lt; 1:32)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Examples</strong></p></td>
<td><p>Coin flips, dice, word counts</p></td>
<td><p>Height, temperature, neural network weights</p></td>
<td><p>Points scored, pit stops made vs. fuel load, tire degradation rate</p></td>
</tr>
<tr class="row-even"><td><p><strong>ML applications</strong></p></td>
<td><p>Classification labels, token IDs</p></td>
<td><p>Regression targets, latent variables</p></td>
<td><p>Predicting race winner vs. predicting lap time</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Important</strong>: For continuous distributions, P(X = x) = 0 for any specific value! We can only ask about ranges.</p>
</section>
</section>
</section>
<hr class="docutils" />
<section id="common-distributions">
<h2>2. Common Distributions<a class="headerlink" href="#common-distributions" title="Link to this heading">#</a></h2>
<section id="bernoulli-distribution">
<h3>2.1 Bernoulli Distribution<a class="headerlink" href="#bernoulli-distribution" title="Link to this heading">#</a></h3>
<p>Models a single binary outcome (success/failure, yes/no, 1/0).</p>
<div class="math notranslate nohighlight">
\[P(X = 1) = p, \quad P(X = 0) = 1 - p\]</div>
<p><strong>In ML</strong>: Binary classification outputs, dropout masks</p>
<p><strong>F1 analogy</strong>: Will the car finish the race? Every Grand Prix is a Bernoulli trial for each driver — they either finish (1) or DNF (0). A reliable car might have p = 0.95, while a fragile one has p = 0.70. Dropout in neural networks works the same way: each neuron is an “engine component” that randomly fails (is zeroed out) during training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Bernoulli distribution — Will the car finish the race?</span>
<span class="n">finish_probability</span> <span class="o">=</span> <span class="mf">0.7</span>  <span class="c1"># Probability of finishing (no DNF)</span>

<span class="c1"># Generate samples: 1000 race starts</span>
<span class="n">race_results</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">finish_probability</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bernoulli(p=</span><span class="si">{</span><span class="n">finish_probability</span><span class="si">}</span><span class="s2">) — Car Finish Probability&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean (theoretical): </span><span class="si">{</span><span class="n">finish_probability</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean (empirical): </span><span class="si">{</span><span class="n">race_results</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Variance (theoretical): </span><span class="si">{</span><span class="n">finish_probability</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">finish_probability</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Variance (empirical): </span><span class="si">{</span><span class="n">race_results</span><span class="o">.</span><span class="n">var</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Visualize</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="o">-</span><span class="n">finish_probability</span><span class="p">,</span> <span class="n">finish_probability</span><span class="p">],</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;DNF (0)&#39;</span><span class="p">,</span> <span class="s1">&#39;Finish (1)&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Bernoulli Distribution: Will the Car Finish? (p=</span><span class="si">{</span><span class="n">finish_probability</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="binomial-distribution">
<h3>2.2 Binomial Distribution<a class="headerlink" href="#binomial-distribution" title="Link to this heading">#</a></h3>
<p>Number of successes in <span class="math notranslate nohighlight">\(n\)</span> independent Bernoulli trials.</p>
<div class="math notranslate nohighlight">
\[P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}\]</div>
<p><strong>In ML</strong>: Counting successes in multiple trials</p>
<p><strong>F1 analogy</strong>: If a driver enters 20 races in a season and has a 30% chance of finishing on the podium at each race, the binomial distribution tells us the probability of getting exactly k podiums across the season. “How many points finishes will this driver collect over a 20-race calendar?”</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Binomial distribution — Podium finishes in a season</span>
<span class="n">n_races</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># Races in the season</span>
<span class="n">podium_prob</span> <span class="o">=</span> <span class="mf">0.3</span>  <span class="c1"># Probability of podium at each race</span>

<span class="c1"># PMF</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_races</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pmf</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n_races</span><span class="p">,</span> <span class="n">podium_prob</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">pmf</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Podium Finishes (k)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;P(X = k)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Binomial Distribution: Podiums in a </span><span class="si">{</span><span class="n">n_races</span><span class="si">}</span><span class="s1">-Race Season (p=</span><span class="si">{</span><span class="n">podium_prob</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">n_races</span><span class="o">*</span><span class="n">podium_prob</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Expected podiums = np = </span><span class="si">{</span><span class="n">n_races</span><span class="o">*</span><span class="n">podium_prob</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected podiums: E[X] = np = </span><span class="si">{</span><span class="n">n_races</span><span class="o">*</span><span class="n">podium_prob</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Variance: Var[X] = np(1-p) = </span><span class="si">{</span><span class="n">n_races</span><span class="o">*</span><span class="n">podium_prob</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">podium_prob</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="categorical-distribution">
<h3>2.3 Categorical Distribution<a class="headerlink" href="#categorical-distribution" title="Link to this heading">#</a></h3>
<p>Generalization of Bernoulli to <span class="math notranslate nohighlight">\(K\)</span> categories.</p>
<div class="math notranslate nohighlight">
\[P(X = k) = p_k, \quad \sum_{k=1}^K p_k = 1\]</div>
<p><strong>In ML</strong>: Multi-class classification (softmax output)</p>
<p><strong>F1 analogy</strong>: Predicting the race winner is a categorical distribution across all 20 drivers. The favorites might have P(Verstappen wins) = 0.40, P(Hamilton wins) = 0.25, and the remaining probability spread across the field. A softmax output in a neural network works exactly the same way — probabilities across categories that must sum to 1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Categorical distribution — Predicting the race winner</span>
<span class="n">teams</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Red Bull&#39;</span><span class="p">,</span> <span class="s1">&#39;Mercedes&#39;</span><span class="p">,</span> <span class="s1">&#39;Ferrari&#39;</span><span class="p">,</span> <span class="s1">&#39;McLaren&#39;</span><span class="p">]</span>
<span class="n">win_probabilities</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span>

<span class="c1"># Generate samples: simulate 1000 race outcomes</span>
<span class="n">race_outcomes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">teams</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">win_probabilities</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">teams</span><span class="p">,</span> <span class="n">win_probabilities</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Win Probability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Pre-Race Win Probabilities (True)&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">empirical_wins</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">race_outcomes</span> <span class="o">==</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">teams</span><span class="p">))]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">teams</span><span class="p">,</span> <span class="n">empirical_wins</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;coral&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Win Frequency&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Simulated Race Wins (1000 races)&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="gaussian-normal-distribution">
<h3>2.4 Gaussian (Normal) Distribution<a class="headerlink" href="#gaussian-normal-distribution" title="Link to this heading">#</a></h3>
<p>The most important continuous distribution.</p>
<div class="math notranslate nohighlight">
\[f(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\]</div>
<p><strong>In ML</strong>:</p>
<ul class="simple">
<li><p>Weight initialization</p></li>
<li><p>Noise in VAEs</p></li>
<li><p>Regression targets</p></li>
<li><p>Batch normalization</p></li>
</ul>
<p><strong>F1 analogy</strong>: Lap times follow an approximately normal distribution. A driver’s laps cluster around their mean pace (mu), with some natural variation (sigma). A consistent driver has small sigma (tight lap time window), while an erratic driver has large sigma. The same math that describes lap time scatter also describes how neural network weights are initialized — small random values drawn from a Gaussian.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Gaussian distribution — Lap time variation</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Different means = different drivers&#39; average pace</span>
<span class="n">lap_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">85</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>  <span class="c1"># Lap times in seconds</span>
<span class="k">for</span> <span class="n">mean_pace</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">89</span><span class="p">,</span> <span class="mi">91</span><span class="p">,</span> <span class="mi">93</span><span class="p">,</span> <span class="mi">95</span><span class="p">]:</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lap_time</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">lap_time</span><span class="p">,</span> <span class="n">mean_pace</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Mean pace=</span><span class="si">{</span><span class="n">mean_pace</span><span class="si">}</span><span class="s1">s, σ=1s&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Lap Time (seconds)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Effect of Mean Pace (μ) — Different Drivers&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Different standard deviations = different consistency levels</span>
<span class="n">lap_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="k">for</span> <span class="n">consistency</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]:</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lap_time</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">lap_time</span><span class="p">,</span> <span class="mi">92</span><span class="p">,</span> <span class="n">consistency</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Mean=92s, σ=</span><span class="si">{</span><span class="n">consistency</span><span class="si">}</span><span class="s1">s&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Lap Time (seconds)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Effect of Consistency (σ) — Same Driver, Different Conditions&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The 68-95-99.7 rule — Lap time consistency bands</span>
<span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Fill regions</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="o">-</span><span class="mi">3</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">3</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;99.7</span><span class="si">% o</span><span class="s1">f laps (±3σ)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">2</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;95</span><span class="si">% o</span><span class="s1">f laps (±2σ)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;68</span><span class="si">% o</span><span class="s1">f laps (±1σ)&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Deviation from Mean Lap Time (in standard deviations)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Lap Time Variation — The 68-95-99.7 Rule</span><span class="se">\n</span><span class="s1">&quot;68</span><span class="si">% o</span><span class="s1">f laps fall within ±1σ of the driver</span><span class="se">\&#39;</span><span class="s1">s average pace&quot;&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Verify with scipy</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Probability within:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ±1σ: </span><span class="si">{</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (68.27%)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ±2σ: </span><span class="si">{</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (95.45%)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ±3σ: </span><span class="si">{</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (99.73%)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">F1 insight: A lap outside ±3σ is almost certainly due to&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;traffic, an incident, or a mistake — not random variation.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="multivariate-gaussian">
<h3>2.5 Multivariate Gaussian<a class="headerlink" href="#multivariate-gaussian" title="Link to this heading">#</a></h3>
<p>Extension to multiple dimensions:</p>
<div class="math notranslate nohighlight">
\[f(\mathbf{x}) = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T \Sigma^{-1} (\mathbf{x}-\boldsymbol{\mu})\right)\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span>: Mean vector</p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma\)</span>: Covariance matrix</p></li>
</ul>
<p><strong>F1 analogy</strong>: A single lap time is univariate Gaussian, but a car’s full telemetry — speed, tire temperature, fuel load — follows a multivariate Gaussian. The covariance matrix captures how these variables relate: when tire temperature goes up, grip goes down (negative correlation). When fuel load drops, lap time improves (also correlated). Understanding these joint distributions is how teams optimize strategy across multiple interacting variables simultaneously.</p>
</section>
<section id="choosing-the-right-distribution-a-decision-guide">
<h3>Choosing the Right Distribution: A Decision Guide<a class="headerlink" href="#choosing-the-right-distribution-a-decision-guide" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Distribution</p></th>
<th class="head"><p>Use When</p></th>
<th class="head"><p>Parameters</p></th>
<th class="head"><p>Example in ML</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Bernoulli</strong></p></td>
<td><p>Single yes/no outcome</p></td>
<td><p>p (success probability)</p></td>
<td><p>Binary classification output, dropout mask</p></td>
<td><p>Will the car finish the race? (finish/DNF)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Binomial</strong></p></td>
<td><p>Count of successes in n trials</p></td>
<td><p>n (trials), p (success prob)</p></td>
<td><p>Number of correct predictions in batch</p></td>
<td><p>How many podiums in a 20-race season?</p></td>
</tr>
<tr class="row-even"><td><p><strong>Categorical</strong></p></td>
<td><p>Single choice from K options</p></td>
<td><p>p₁, p₂, …, pₖ (probabilities)</p></td>
<td><p>Softmax output, token prediction</p></td>
<td><p>Which of the 20 drivers wins this race?</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Multinomial</strong></p></td>
<td><p>Counts across K categories</p></td>
<td><p>n (trials), p₁…pₖ</p></td>
<td><p>Word counts in document (bag of words)</p></td>
<td><p>Finishing position counts across a season</p></td>
</tr>
<tr class="row-even"><td><p><strong>Gaussian</strong></p></td>
<td><p>Continuous value, symmetric uncertainty</p></td>
<td><p>μ (mean), σ (std dev)</p></td>
<td><p>Regression targets, weight initialization</p></td>
<td><p>Lap time variation around mean pace</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Multivariate Gaussian</strong></p></td>
<td><p>Multiple correlated continuous values</p></td>
<td><p>μ (mean vector), Σ (covariance)</p></td>
<td><p>VAE latent space, GP predictions</p></td>
<td><p>Joint distribution of speed, tire temp, fuel load</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>The Pattern</strong>:</p>
<ul class="simple">
<li><p>Bernoulli/Binomial are for binary outcomes (yes/no)</p></li>
<li><p>Categorical/Multinomial are for multi-class outcomes</p></li>
<li><p>Gaussian is for continuous outcomes with symmetric uncertainty</p></li>
</ul>
<p><strong>Key ML Connection</strong>: The distribution you choose for your model’s output determines your loss function:</p>
<ul class="simple">
<li><p>Categorical output → Cross-entropy loss</p></li>
<li><p>Gaussian output → MSE loss (equivalent to assuming Gaussian noise)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 2D Gaussian — Joint distributions of car telemetry variables</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Generate grid for contour plots</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">))</span>

<span class="c1"># Different covariance matrices representing different telemetry relationships</span>
<span class="n">covariances</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="s1">&#39;Independent Variables</span><span class="se">\n</span><span class="s1">(Speed vs. Fuel Load)&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]]),</span> <span class="s1">&#39;Different Variances</span><span class="se">\n</span><span class="s1">(Speed varies more than Tire Temp)&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="s1">&#39;Correlated Variables</span><span class="se">\n</span><span class="s1">(Tire Temp vs. Degradation Rate)&#39;</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">cov</span><span class="p">,</span> <span class="n">title</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">covariances</span><span class="p">):</span>
    <span class="n">rv</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">rv</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">pos</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
    
    <span class="c1"># Draw samples</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">rv</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Telemetry Variable 1&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Telemetry Variable 2&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">title</span><span class="si">}</span><span class="se">\n</span><span class="s1">Σ = </span><span class="si">{</span><span class="n">cov</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="expected-value-and-variance">
<h2>3. Expected Value and Variance<a class="headerlink" href="#expected-value-and-variance" title="Link to this heading">#</a></h2>
<section id="expected-value-mean">
<h3>Expected Value (Mean)<a class="headerlink" href="#expected-value-mean" title="Link to this heading">#</a></h3>
<p>The “average” outcome weighted by probability:</p>
<ul class="simple">
<li><p>Discrete: <span class="math notranslate nohighlight">\(E[X] = \sum_x x \cdot P(X = x)\)</span></p></li>
<li><p>Continuous: <span class="math notranslate nohighlight">\(E[X] = \int x \cdot f(x) dx\)</span></p></li>
</ul>
</section>
<section id="variance">
<h3>Variance<a class="headerlink" href="#variance" title="Link to this heading">#</a></h3>
<p>Measures spread around the mean:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2\]</div>
<p><strong>F1 analogy</strong>: Expected value is the average championship points a driver earns from a given starting position. Starting from pole, E[points] might be 20 (weighted by probability of each finishing position). Variance measures how much the actual result varies — a driver who always finishes where they qualify has low variance, while one who either wins or DNFs has high variance. Teams use expected points calculations to evaluate strategy decisions: “Does this pit stop gamble increase our expected points?”</p>
</section>
<section id="deep-dive-understanding-each-term-in-bayes-theorem">
<h3>Deep Dive: Understanding Each Term in Bayes’ Theorem<a class="headerlink" href="#deep-dive-understanding-each-term-in-bayes-theorem" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[P(\text{hypothesis}|\text{data}) = \frac{P(\text{data}|\text{hypothesis}) \cdot P(\text{hypothesis})}{P(\text{data})}\]</div>
<p>Let’s break down what each term really means:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Term</p></th>
<th class="head"><p>Name</p></th>
<th class="head"><p>Meaning</p></th>
<th class="head"><p>Medical Example</p></th>
<th class="head"><p>F1 Example</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>P(H)</strong></p></td>
<td><p>Prior</p></td>
<td><p>Your belief <em>before</em> seeing any evidence</p></td>
<td><p>1% of population has disease</p></td>
<td><p>30% chance of rain before the race</p></td>
</tr>
<tr class="row-odd"><td><p><strong>P(D|H)</strong></p></td>
<td><p>Likelihood</p></td>
<td><p>How probable is this evidence <em>if</em> hypothesis is true?</p></td>
<td><p>95% chance of positive test <em>if</em> you have disease</p></td>
<td><p>If it rains, 80% chance the track is wet by lap 10</p></td>
</tr>
<tr class="row-even"><td><p><strong>P(D)</strong></p></td>
<td><p>Evidence (Marginal)</p></td>
<td><p>Total probability of seeing this evidence</p></td>
<td><p>Overall rate of positive tests</p></td>
<td><p>Overall probability of a wet track by lap 10</p></td>
</tr>
<tr class="row-odd"><td><p><strong>P(H|D)</strong></p></td>
<td><p>Posterior</p></td>
<td><p>Updated belief <em>after</em> seeing evidence</p></td>
<td><p>Probability you have disease <em>given</em> positive test</p></td>
<td><p>P(rain) <em>given</em> the track is wet at lap 10</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>The Core Insight</strong>: Bayes’ theorem is a <em>belief update</em> mechanism:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">New</span> <span class="n">Belief</span> <span class="o">=</span> <span class="p">(</span><span class="n">How</span> <span class="n">well</span> <span class="n">evidence</span> <span class="n">supports</span> <span class="n">hypothesis</span><span class="p">)</span> <span class="n">x</span> <span class="p">(</span><span class="n">Old</span> <span class="n">Belief</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">How</span> <span class="n">common</span> <span class="ow">is</span> <span class="n">this</span> <span class="n">evidence</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>F1 analogy</strong>: Every lap, the strategy team is running Bayes’ theorem in their heads. Before the race: P(rain) = 30%. They see dark clouds forming: that’s new evidence. P(dark clouds | rain) is high, P(dark clouds | no rain) is low. Their posterior P(rain | dark clouds) shoots up. Now they’re preparing wet tires. This is exactly how Bayesian neural networks update their weight distributions given new training data.</p>
<p><strong>Why the denominator matters</strong>: P(D) normalizes everything. If positive tests are common (many false positives), a positive test is less informative.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visual: How Bayes&#39; Theorem Updates Beliefs</span>
<span class="c1"># F1 scenario: Predicting rain during a race based on weather radar data</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="c1"># Setup — Using the medical testing example (the math is identical)</span>
<span class="n">P_disease</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">P_positive_given_disease</span> <span class="o">=</span> <span class="mf">0.95</span>      <span class="c1"># True positive rate</span>
<span class="n">P_positive_given_no_disease</span> <span class="o">=</span> <span class="mf">0.05</span>   <span class="c1"># False positive rate</span>

<span class="c1"># Imagine 10,000 people</span>
<span class="n">n_people</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">n_sick</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_people</span> <span class="o">*</span> <span class="n">P_disease</span><span class="p">)</span>
<span class="n">n_healthy</span> <span class="o">=</span> <span class="n">n_people</span> <span class="o">-</span> <span class="n">n_sick</span>

<span class="c1"># Among sick people</span>
<span class="n">sick_test_positive</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_sick</span> <span class="o">*</span> <span class="n">P_positive_given_disease</span><span class="p">)</span>
<span class="n">sick_test_negative</span> <span class="o">=</span> <span class="n">n_sick</span> <span class="o">-</span> <span class="n">sick_test_positive</span>

<span class="c1"># Among healthy people  </span>
<span class="n">healthy_test_positive</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_healthy</span> <span class="o">*</span> <span class="n">P_positive_given_no_disease</span><span class="p">)</span>
<span class="n">healthy_test_negative</span> <span class="o">=</span> <span class="n">n_healthy</span> <span class="o">-</span> <span class="n">healthy_test_positive</span>

<span class="c1"># Plot 1: Prior - Population breakdown</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">([</span><span class="s1">&#39;Sick&#39;</span><span class="p">,</span> <span class="s1">&#39;Healthy&#39;</span><span class="p">],</span> <span class="p">[</span><span class="n">n_sick</span><span class="p">,</span> <span class="n">n_healthy</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Number of People&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Step 1: PRIOR</span><span class="se">\n</span><span class="si">{</span><span class="n">n_people</span><span class="si">:</span><span class="s1">,</span><span class="si">}</span><span class="s1"> people: </span><span class="si">{</span><span class="n">n_sick</span><span class="si">}</span><span class="s1"> sick (1%), </span><span class="si">{</span><span class="n">n_healthy</span><span class="si">}</span><span class="s1"> healthy (99%)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_people</span> <span class="o">*</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">n_sick</span><span class="p">,</span> <span class="n">n_healthy</span><span class="p">]):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="o">+</span> <span class="mi">200</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># Plot 2: Likelihood - Test results by group</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">width</span> <span class="o">=</span> <span class="mf">0.35</span>
<span class="n">bars1</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">width</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="n">sick_test_positive</span><span class="p">,</span> <span class="n">healthy_test_positive</span><span class="p">],</span> <span class="n">width</span><span class="p">,</span> 
               <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Test Positive&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">bars2</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">width</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="n">sick_test_negative</span><span class="p">,</span> <span class="n">healthy_test_negative</span><span class="p">],</span> <span class="n">width</span><span class="p">,</span>
               <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Test Negative&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;Sick (100)&#39;</span><span class="p">,</span> <span class="s1">&#39;Healthy (9900)&#39;</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Number of People&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Step 2: LIKELIHOOD</span><span class="se">\n</span><span class="s1">How the test performs on each group&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Plot 3: Evidence - All positive tests</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">([</span><span class="s1">&#39;True Positives</span><span class="se">\n</span><span class="s1">(Sick + Positive)&#39;</span><span class="p">,</span> <span class="s1">&#39;False Positives</span><span class="se">\n</span><span class="s1">(Healthy + Positive)&#39;</span><span class="p">],</span> 
       <span class="p">[</span><span class="n">sick_test_positive</span><span class="p">,</span> <span class="n">healthy_test_positive</span><span class="p">],</span> 
       <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">total_positive</span> <span class="o">=</span> <span class="n">sick_test_positive</span> <span class="o">+</span> <span class="n">healthy_test_positive</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Number of People&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Step 3: EVIDENCE</span><span class="se">\n</span><span class="s1">All positive tests: </span><span class="si">{</span><span class="n">total_positive</span><span class="si">}</span><span class="s1"> total</span><span class="se">\n</span><span class="s1">&#39;</span>
             <span class="sa">f</span><span class="s1">&#39;P(positive) = </span><span class="si">{</span><span class="n">total_positive</span><span class="o">/</span><span class="n">n_people</span><span class="si">:</span><span class="s1">.2%</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">sick_test_positive</span><span class="p">,</span> <span class="n">healthy_test_positive</span><span class="p">]):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="o">+</span> <span class="mi">10</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># Plot 4: Posterior - Among positive tests, who is actually sick?</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">sick_test_positive</span> <span class="o">/</span> <span class="n">total_positive</span>
<span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">([</span><span class="s1">&#39;Actually Sick&#39;</span><span class="p">,</span> <span class="s1">&#39;Actually Healthy&#39;</span><span class="p">],</span> 
       <span class="p">[</span><span class="n">sick_test_positive</span><span class="p">,</span> <span class="n">healthy_test_positive</span><span class="p">],</span>
       <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Number of People (with positive test)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Step 4: POSTERIOR</span><span class="se">\n</span><span class="s1">Among </span><span class="si">{</span><span class="n">total_positive</span><span class="si">}</span><span class="s1"> positive tests:</span><span class="se">\n</span><span class="s1">&#39;</span>
             <span class="sa">f</span><span class="s1">&#39;P(sick|positive) = </span><span class="si">{</span><span class="n">sick_test_positive</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">total_positive</span><span class="si">}</span><span class="s1"> = </span><span class="si">{</span><span class="n">posterior</span><span class="si">:</span><span class="s1">.1%</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">sick_test_positive</span><span class="p">,</span> <span class="n">healthy_test_positive</span><span class="p">]):</span>
    <span class="n">pct</span> <span class="o">=</span> <span class="n">v</span> <span class="o">/</span> <span class="n">total_positive</span> <span class="o">*</span> <span class="mi">100</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="o">+</span> <span class="mi">10</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">v</span><span class="si">}</span><span class="s1"> (</span><span class="si">{</span><span class="n">pct</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">%)&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Bayes Theorem: Why a 95% Accurate Test Gives Only 16% Confidence</span><span class="se">\n</span><span class="s1">&#39;</span>
             <span class="s1">&#39;(Same math applies: a 95</span><span class="si">% a</span><span class="s1">ccurate rain radar still misleads when rain is rare)&#39;</span><span class="p">,</span> 
             <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.02</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The Counterintuitive Result Explained:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Even though the test is 95% accurate:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  - Out of </span><span class="si">{</span><span class="n">n_sick</span><span class="si">}</span><span class="s2"> sick people: </span><span class="si">{</span><span class="n">sick_test_positive</span><span class="si">}</span><span class="s2"> test positive&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  - Out of </span><span class="si">{</span><span class="n">n_healthy</span><span class="si">}</span><span class="s2"> healthy people: </span><span class="si">{</span><span class="n">healthy_test_positive</span><span class="si">}</span><span class="s2"> ALSO test positive (false positives)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Total positive tests: </span><span class="si">{</span><span class="n">total_positive</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True positives: </span><span class="si">{</span><span class="n">sick_test_positive</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">sick_test_positive</span><span class="o">/</span><span class="n">total_positive</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;False positives: </span><span class="si">{</span><span class="n">healthy_test_positive</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">healthy_test_positive</span><span class="o">/</span><span class="n">total_positive</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The false positives OVERWHELM the true positives because&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;healthy people vastly outnumber sick people!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">F1 parallel: If your rain radar is 95% accurate but rain only happens&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;5% of the time, most &#39;rain detected&#39; alerts are false positives.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Computing expected value — Expected championship points from a grid position</span>
<span class="c1"># Example: Points distribution from starting P3</span>
<span class="n">finishing_positions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="n">points_scored</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">25</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>  <span class="c1"># F1 points system</span>
<span class="n">position_probabilities</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>  <span class="c1"># Biased toward lower positions</span>

<span class="c1"># Expected points</span>
<span class="n">expected_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">points_scored</span> <span class="o">*</span> <span class="n">position_probabilities</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;E[Points] = Σ points·P(position) = </span><span class="si">{</span><span class="n">expected_points</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Variance in points</span>
<span class="n">variance_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">points_scored</span> <span class="o">-</span> <span class="n">expected_points</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">position_probabilities</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Var(Points) = E[(Points - E[Points])²] = </span><span class="si">{</span><span class="n">variance_points</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Std(Points) = √Var(Points) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">variance_points</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Verify with sampling</span>
<span class="n">sampled_positions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">points_scored</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">position_probabilities</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Empirical mean points: </span><span class="si">{</span><span class="n">sampled_positions</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Empirical variance: </span><span class="si">{</span><span class="n">sampled_positions</span><span class="o">.</span><span class="n">var</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="bayes-theorem">
<h2>4. Bayes’ Theorem<a class="headerlink" href="#bayes-theorem" title="Link to this heading">#</a></h2>
<p>Bayes’ theorem tells us how to update beliefs given new evidence:</p>
<div class="math notranslate nohighlight">
\[P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}\]</div>
<p>In ML terms:</p>
<div class="math notranslate nohighlight">
\[P(\text{hypothesis}|\text{data}) = \frac{P(\text{data}|\text{hypothesis}) \cdot P(\text{hypothesis})}{P(\text{data})}\]</div>
<p>Or:</p>
<div class="math notranslate nohighlight">
\[\text{Posterior} = \frac{\text{Likelihood} \times \text{Prior}}{\text{Evidence}}\]</div>
<p><strong>F1 analogy</strong>: Bayes’ theorem is the mathematical backbone of real-time race strategy. Before the race, the team has a prior belief about tire degradation (say, 0.1s per lap). As laps unfold and actual lap times come in, they update this belief. If the driver’s times are dropping faster than expected, the posterior shifts toward higher degradation — and the team calls an earlier pit stop. Every lap is new evidence, and the strategy wall is constantly computing posteriors.</p>
<section id="bayes-theorem-in-machine-learning">
<h3>Bayes’ Theorem in Machine Learning<a class="headerlink" href="#bayes-theorem-in-machine-learning" title="Link to this heading">#</a></h3>
<p>Bayesian thinking is fundamental to many ML techniques:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Application</p></th>
<th class="head"><p>Prior P(H)</p></th>
<th class="head"><p>Likelihood P(D|H)</p></th>
<th class="head"><p>Posterior P(H|D)</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Naive Bayes Classifier</strong></p></td>
<td><p>Class frequencies in training data</p></td>
<td><p>P(features|class) assumed independent</p></td>
<td><p>P(class|features) for prediction</p></td>
<td><p>Predicting tire compound from telemetry features</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Bayesian Neural Networks</strong></p></td>
<td><p>Prior on weights (e.g., Gaussian)</p></td>
<td><p>P(data|weights) from network output</p></td>
<td><p>Distribution over weights given data</p></td>
<td><p>Uncertainty in lap time predictions</p></td>
</tr>
<tr class="row-even"><td><p><strong>Bayesian Optimization</strong></p></td>
<td><p>GP prior over objective function</p></td>
<td><p>Observations so far</p></td>
<td><p>Updated belief about function</p></td>
<td><p>Finding optimal car setup (test limited configs)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Spam Filtering</strong></p></td>
<td><p>Base rate of spam emails</p></td>
<td><p>P(words|spam) and P(words|ham)</p></td>
<td><p>P(spam|email content)</p></td>
<td><p>Filtering valid telemetry from sensor noise</p></td>
</tr>
<tr class="row-even"><td><p><strong>A/B Testing</strong></p></td>
<td><p>Prior belief about conversion rates</p></td>
<td><p>Observed clicks/conversions</p></td>
<td><p>Updated belief about which variant wins</p></td>
<td><p>Testing two setup configurations mid-weekend</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>The Bayesian vs Frequentist Perspective</strong>:</p>
<ul class="simple">
<li><p><strong>Frequentist</strong>: Parameters are fixed, unknown constants. We estimate them.</p></li>
<li><p><strong>Bayesian</strong>: Parameters have probability distributions. We update our beliefs.</p></li>
</ul>
<p>In deep learning, we’re usually frequentist (point estimates via SGD), but Bayesian methods give us uncertainty quantification.</p>
</section>
<section id="deep-dive-the-intuition-behind-maximum-likelihood">
<h3>Deep Dive: The Intuition Behind Maximum Likelihood<a class="headerlink" href="#deep-dive-the-intuition-behind-maximum-likelihood" title="Link to this heading">#</a></h3>
<p><strong>The Core Question</strong>: Given observed data, what parameters would have made this data <em>most probable</em>?</p>
<p>Imagine you flip a coin 10 times and get 7 heads. What’s the “most likely” value of p (probability of heads)?</p>
<p><strong>MLE answers</strong>: Find the p that maximizes P(7 heads in 10 flips | p)</p>
<p>The answer is p = 0.7, because:</p>
<ul class="simple">
<li><p>If p = 0.5, getting 7 heads is somewhat unlikely</p></li>
<li><p>If p = 0.9, getting only 7 heads (not 9) is unlikely</p></li>
<li><p>p = 0.7 makes our observed data most probable</p></li>
</ul>
<p><strong>F1 analogy</strong>: Imagine you’re an engineer trying to estimate the tire degradation rate from lap data. You observe lap times of 92.1, 92.3, 92.5, 92.8, 93.0 seconds over 5 laps. MLE asks: “What degradation rate makes these observed lap times most probable?” If you assume lap times increase linearly with degradation, MLE finds the slope that best fits the data — just like fitting a line through your lap time scatter plot.</p>
<p><strong>Why Log-Likelihood?</strong></p>
<ol class="arabic simple">
<li><p>Products become sums: log(a x b x c) = log(a) + log(b) + log(c)</p></li>
<li><p>Numerical stability: Avoids underflow when multiplying many small probabilities</p></li>
<li><p>Same maximum: log is monotonic, so argmax is preserved</p></li>
</ol>
<p><strong>The Profound Connection to Loss Functions</strong>:</p>
<p>For classification with softmax outputs:
$<span class="math notranslate nohighlight">\(\text{Minimize Cross-Entropy} = \text{Maximize Log-Likelihood}\)</span>$</p>
<p>They’re the same optimization! When you train with cross-entropy loss, you’re doing MLE.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Classic example: Safety car prediction</span>
<span class="c1"># Safety cars occur in ~5% of race laps</span>
<span class="c1"># Sensor detects incidents with 90% accuracy</span>

<span class="n">P_safety_car</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># Prior: probability of safety car on any given lap</span>
<span class="n">P_sensor_alert_given_incident</span> <span class="o">=</span> <span class="mf">0.95</span>  <span class="c1"># Sensitivity (true positive rate)</span>
<span class="n">P_sensor_alert_given_no_incident</span> <span class="o">=</span> <span class="mf">0.05</span>  <span class="c1"># False positive rate (1 - specificity)</span>

<span class="c1"># P(alert) = P(alert|incident)P(incident) + P(alert|no incident)P(no incident)</span>
<span class="n">P_alert</span> <span class="o">=</span> <span class="n">P_sensor_alert_given_incident</span> <span class="o">*</span> <span class="n">P_safety_car</span> <span class="o">+</span> <span class="n">P_sensor_alert_given_no_incident</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">P_safety_car</span><span class="p">)</span>

<span class="c1"># Bayes&#39; theorem: P(incident|alert)</span>
<span class="n">P_incident_given_alert</span> <span class="o">=</span> <span class="p">(</span><span class="n">P_sensor_alert_given_incident</span> <span class="o">*</span> <span class="n">P_safety_car</span><span class="p">)</span> <span class="o">/</span> <span class="n">P_alert</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;F1 Safety Car Prediction (Same Math as Medical Testing)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prior P(safety car this lap) = </span><span class="si">{</span><span class="n">P_safety_car</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sensor sensitivity = </span><span class="si">{</span><span class="n">P_sensor_alert_given_incident</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sensor specificity = </span><span class="si">{</span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">P_sensor_alert_given_no_incident</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(sensor alert) = </span><span class="si">{</span><span class="n">P_alert</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(actual incident | sensor alert) = </span><span class="si">{</span><span class="n">P_incident_given_alert</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Surprising! Even with an alert from a 95</span><span class="si">% a</span><span class="s2">ccurate sensor,&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;there&#39;s only a </span><span class="si">{</span><span class="n">P_incident_given_alert</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2"> chance of an actual safety car!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;This is because incidents are rare on any given lap (low prior).&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Demonstrating: Cross-Entropy Loss = Negative Log-Likelihood</span>
<span class="c1"># This shows they&#39;re mathematically equivalent!</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cross-Entropy Loss vs Negative Log-Likelihood&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

<span class="c1"># Imagine a 3-class tire compound prediction problem</span>
<span class="c1"># True label is Soft compound (class 0), model outputs these probabilities:</span>
<span class="n">true_compound</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">model_probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>  <span class="c1"># Model is fairly confident it&#39;s Soft</span>

<span class="c1"># Method 1: Cross-Entropy Loss (what we use in practice)</span>
<span class="c1"># CE = -sum(y_true * log(y_pred)) where y_true is one-hot</span>
<span class="n">one_hot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>  <span class="c1"># One-hot encoding of true compound (Soft)</span>
<span class="n">cross_entropy_val</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">one_hot</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">model_probs</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Cross-Entropy Loss: -sum(y_true * log(y_pred))&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  = -(</span><span class="si">{</span><span class="n">one_hot</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> * log(</span><span class="si">{</span><span class="n">model_probs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">) + </span><span class="si">{</span><span class="n">one_hot</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> * log(</span><span class="si">{</span><span class="n">model_probs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">) + </span><span class="si">{</span><span class="n">one_hot</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> * log(</span><span class="si">{</span><span class="n">model_probs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">))&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  = -</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">model_probs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  = </span><span class="si">{</span><span class="n">cross_entropy_val</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Method 2: Negative Log-Likelihood (MLE perspective)</span>
<span class="c1"># NLL = -log(P(true_class))</span>
<span class="n">neg_log_likelihood</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">model_probs</span><span class="p">[</span><span class="n">true_compound</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Negative Log-Likelihood: -log(P(true_compound))&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  = -log(</span><span class="si">{</span><span class="n">model_probs</span><span class="p">[</span><span class="n">true_compound</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  = </span><span class="si">{</span><span class="n">neg_log_likelihood</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">They&#39;re identical! CE = NLL = </span><span class="si">{</span><span class="n">cross_entropy_val</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">This means: Training with cross-entropy loss is doing MLE!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;We&#39;re finding network weights that maximize P(correct labels | inputs)&quot;</span><span class="p">)</span>

<span class="c1"># Show how loss changes with confidence</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;How loss varies with model confidence (predicting tire compound):&quot;</span><span class="p">)</span>
<span class="n">probs_for_true_compound</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;P(correct compound)&#39;</span><span class="si">:</span><span class="s2">&lt;22</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Cross-Entropy Loss&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">42</span><span class="p">)</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">probs_for_true_compound</span><span class="p">:</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s2">&lt;22.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">&lt;20.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize how posterior changes with prior</span>
<span class="c1"># F1 context: How P(rain | radar alert) changes with base rain probability</span>
<span class="n">priors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">sensitivity</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="n">specificity</span> <span class="o">=</span> <span class="mf">0.95</span>

<span class="n">posteriors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">prior</span> <span class="ow">in</span> <span class="n">priors</span><span class="p">:</span>
    <span class="n">p_positive</span> <span class="o">=</span> <span class="n">sensitivity</span> <span class="o">*</span> <span class="n">prior</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">specificity</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">prior</span><span class="p">)</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="p">(</span><span class="n">sensitivity</span> <span class="o">*</span> <span class="n">prior</span><span class="p">)</span> <span class="o">/</span> <span class="n">p_positive</span>
    <span class="n">posteriors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">posterior</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">priors</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">posteriors</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Prior Probability [%]</span><span class="se">\n</span><span class="s1">(e.g., base rate of rain at this circuit)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Posterior Probability [%]</span><span class="se">\n</span><span class="s1">(e.g., P(rain | radar alert))&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;How the Prior Affects the Posterior (95% Accurate Sensor)</span><span class="se">\n</span><span class="s1">&#39;</span>
          <span class="s1">&#39;F1: Low base-rate rain circuits give more false alarms&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Mark some key points</span>
<span class="k">for</span> <span class="n">prior</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]:</span>
    <span class="n">p_positive</span> <span class="o">=</span> <span class="n">sensitivity</span> <span class="o">*</span> <span class="n">prior</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">specificity</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">prior</span><span class="p">)</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="p">(</span><span class="n">sensitivity</span> <span class="o">*</span> <span class="n">prior</span><span class="p">)</span> <span class="o">/</span> <span class="n">p_positive</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">prior</span> <span class="o">*</span> <span class="mi">100</span><span class="p">],</span> <span class="p">[</span><span class="n">posterior</span> <span class="o">*</span> <span class="mi">100</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">prior</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.0f</span><span class="si">}</span><span class="s1">%, </span><span class="si">{</span><span class="n">posterior</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">%)&#39;</span><span class="p">,</span> 
                 <span class="p">(</span><span class="n">prior</span> <span class="o">*</span> <span class="mi">100</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">posterior</span> <span class="o">*</span> <span class="mi">100</span> <span class="o">-</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Interactive visualization: The Likelihood Surface</span>
<span class="c1"># F1 context: Estimating mean lap time (mu) and consistency (sigma) from observed laps</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Generate data from known distribution (simulated lap times)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">true_mean_lap</span> <span class="o">=</span> <span class="mf">3.0</span>  <span class="c1"># True mean (offset for math convenience)</span>
<span class="n">true_consistency</span> <span class="o">=</span> <span class="mf">1.5</span>  <span class="c1"># True sigma</span>
<span class="n">lap_times</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">true_mean_lap</span><span class="p">,</span> <span class="n">true_consistency</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="c1"># Plot 1: 1D likelihood for mu (sigma fixed at true value)</span>
<span class="n">mus</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">log_likelihoods_mu</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">lap_times</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">true_consistency</span><span class="p">))</span> <span class="k">for</span> <span class="n">mu</span> <span class="ow">in</span> <span class="n">mus</span><span class="p">]</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mus</span><span class="p">,</span> <span class="n">log_likelihoods_mu</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">lap_times</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;MLE: </span><span class="si">{</span><span class="n">lap_times</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">true_mean_lap</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;True: </span><span class="si">{</span><span class="n">true_mean_lap</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Mean Lap Time (mu)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Log-Likelihood&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Likelihood vs. Mean Pace</span><span class="se">\n</span><span class="s1">(consistency fixed)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Plot 2: 1D likelihood for sigma (mu fixed at true value)</span>
<span class="n">sigmas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">log_likelihoods_sigma</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">lap_times</span><span class="p">,</span> <span class="n">true_mean_lap</span><span class="p">,</span> <span class="n">sigma</span><span class="p">))</span> <span class="k">for</span> <span class="n">sigma</span> <span class="ow">in</span> <span class="n">sigmas</span><span class="p">]</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sigmas</span><span class="p">,</span> <span class="n">log_likelihoods_sigma</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">lap_times</span><span class="o">.</span><span class="n">std</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;MLE: </span><span class="si">{</span><span class="n">lap_times</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">true_consistency</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;True: </span><span class="si">{</span><span class="n">true_consistency</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Lap Time Consistency (sigma)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Log-Likelihood&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Likelihood vs. Consistency</span><span class="se">\n</span><span class="s1">(mean pace fixed)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Plot 3: 2D likelihood surface</span>
<span class="n">mus_2d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">sigmas_2d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">MU</span><span class="p">,</span> <span class="n">SIGMA</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">mus_2d</span><span class="p">,</span> <span class="n">sigmas_2d</span><span class="p">)</span>

<span class="n">LL</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">MU</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sigmas_2d</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mus_2d</span><span class="p">)):</span>
        <span class="n">LL</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">lap_times</span><span class="p">,</span> <span class="n">MU</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">SIGMA</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]))</span>

<span class="n">contour</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">MU</span><span class="p">,</span> <span class="n">SIGMA</span><span class="p">,</span> <span class="n">LL</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">lap_times</span><span class="o">.</span><span class="n">mean</span><span class="p">()],</span> <span class="p">[</span><span class="n">lap_times</span><span class="o">.</span><span class="n">std</span><span class="p">()],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> 
                <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;MLE&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">true_mean_lap</span><span class="p">],</span> <span class="p">[</span><span class="n">true_consistency</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span>
                <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;True&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Mean Lap Time (mu)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Consistency (sigma)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;2D Log-Likelihood Surface</span><span class="se">\n</span><span class="s1">(Finding best mu, sigma from lap data)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">contour</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Log-Likelihood&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Key Observations:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;1. The likelihood surface has a clear peak (the MLE)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;2. As we move away from the MLE, likelihood decreases&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;3. Gradient ascent on this surface finds the MLE&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;4. This is exactly what neural network training does!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">F1 insight: MLE finds the mean pace and consistency that best&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;explain the observed lap times — the same technique teams use&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;to estimate tire degradation rates from stint data.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="deep-dive-understanding-entropy">
<h3>Deep Dive: Understanding Entropy<a class="headerlink" href="#deep-dive-understanding-entropy" title="Link to this heading">#</a></h3>
<p>Entropy has several intuitive interpretations that all lead to the same formula:</p>
<p><strong>Interpretation 1: Average Surprise</strong></p>
<ul class="simple">
<li><p>“Surprise” of an event = -log P(event)</p></li>
<li><p>Rare events (low probability) are more surprising</p></li>
<li><p>Entropy = average surprise across all possible outcomes</p></li>
<li><p>H(X) = E[-log P(X)] = “How surprised will I be on average?”</p></li>
</ul>
<p><strong>Interpretation 2: Uncertainty</strong></p>
<ul class="simple">
<li><p>How uncertain are we about the outcome?</p></li>
<li><p>Maximum entropy = maximum uncertainty (uniform distribution)</p></li>
<li><p>Zero entropy = complete certainty (deterministic)</p></li>
</ul>
<p><strong>Interpretation 3: Information Content (Bits)</strong></p>
<ul class="simple">
<li><p>“How many yes/no questions do I need to identify the outcome?”</p></li>
<li><p>Fair coin: 1 bit (one yes/no question: “Was it heads?”)</p></li>
<li><p>Fair 4-sided die: 2 bits (“Is it 1 or 2?” then “Is it the first of those two?”)</p></li>
<li><p>Biased distributions need fewer questions on average (can ask about likely outcomes first)</p></li>
</ul>
<p><strong>F1 analogy</strong>: Entropy measures the <strong>unpredictability of race results</strong>. A season where one driver dominates (P(Verstappen wins) = 0.9) has low entropy — you’re rarely surprised by the winner. A season with 5 competitive drivers splitting wins equally has high entropy — every race is a genuine surprise. This is exactly why fans call competitive seasons “exciting” — high entropy = high unpredictability = better entertainment. In ML, softmax temperature controls this same trade-off: low temperature = peaked distribution (confident), high temperature = flat distribution (uncertain).</p>
<p><strong>Why log base 2?</strong></p>
<ul class="simple">
<li><p>Gives entropy in “bits” - the number of binary questions</p></li>
<li><p>log base e gives “nats” (natural units)</p></li>
<li><p>They’re proportional: 1 nat = 1.44 bits</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualizing Entropy as &quot;Average Surprise&quot;</span>
<span class="c1"># Surprise of an event = -log2(P(event))</span>
<span class="c1"># F1: How surprised are you when a particular driver wins?</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Plot 1: Surprise function</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">surprise</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">surprise</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;P(driver wins)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Surprise = -log2(P(win))&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Surprise Function</span><span class="se">\n</span><span class="s1">&quot;How shocked are you by the race winner?&quot;&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;Underdog wins!</span><span class="se">\n</span><span class="s1">(high surprise)&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;Favorite wins</span><span class="se">\n</span><span class="s1">(low surprise)&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Plot 2: Entropy for different championship scenarios</span>
<span class="n">distributions</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;Dominant era</span><span class="se">\n</span><span class="s1">[1,0,0,0]&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="s1">&#39;Clear favorite</span><span class="se">\n</span><span class="s1">[0.7,0.2,0.1,0]&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="s1">&#39;Competitive</span><span class="se">\n</span><span class="s1">[0.4,0.3,0.2,0.1]&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
    <span class="s1">&#39;Wide open</span><span class="se">\n</span><span class="s1">[0.25,0.25,0.25,0.25]&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">],</span>
<span class="p">}</span>

<span class="n">names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">distributions</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="n">entropies</span> <span class="o">=</span> <span class="p">[</span><span class="n">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">distributions</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>

<span class="n">bars</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">names</span><span class="p">)),</span> <span class="n">entropies</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;darkblue&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;steelblue&#39;</span><span class="p">,</span> <span class="s1">&#39;lightblue&#39;</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">names</span><span class="p">)))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Entropy (bits)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Season Competitiveness = Entropy</span><span class="se">\n</span><span class="s1">&quot;How unpredictable is each race?&quot;&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">entropies</span><span class="p">):</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">v</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># Plot 3: Why uniform has maximum entropy</span>
<span class="c1"># Show entropy vs &quot;peakedness&quot; of distribution (like softmax temperature)</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>  <span class="c1"># Base driver ratings</span>

<span class="n">entropy_values</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">alpha</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="o">/</span> <span class="mi">4</span>  <span class="c1"># Uniform — anyone can win</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">scores</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">logits</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">probs</span> <span class="o">/</span> <span class="n">probs</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">entropy_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">entropy</span><span class="p">(</span><span class="n">probs</span><span class="p">))</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">entropy_values</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Performance gap (lower = more equal)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Entropy (bits)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Entropy vs. Field Competitiveness</span><span class="se">\n</span><span class="s1">(Like softmax temperature in ML)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Max entropy (anyone can win)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="entropy-reference-table">
<h3>Entropy Reference Table<a class="headerlink" href="#entropy-reference-table" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Distribution</p></th>
<th class="head"><p>Formula</p></th>
<th class="head"><p>Entropy (bits)</p></th>
<th class="head"><p>Interpretation</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Fair coin</p></td>
<td><p>[0.5, 0.5]</p></td>
<td><p>1.00</p></td>
<td><p>1 yes/no question needed</p></td>
<td><p>“Will the car finish?” — pure coin flip</p></td>
</tr>
<tr class="row-odd"><td><p>Biased coin (90/10)</p></td>
<td><p>[0.9, 0.1]</p></td>
<td><p>0.47</p></td>
<td><p>Less than 1 question on average</p></td>
<td><p>Reliable car: almost certainly finishes</p></td>
</tr>
<tr class="row-even"><td><p>Certain outcome</p></td>
<td><p>[1, 0]</p></td>
<td><p>0.00</p></td>
<td><p>No uncertainty, no questions needed</p></td>
<td><p>Dominant driver: guaranteed win</p></td>
</tr>
<tr class="row-odd"><td><p>Fair 4-sided die</p></td>
<td><p>[0.25, 0.25, 0.25, 0.25]</p></td>
<td><p>2.00</p></td>
<td><p>2 yes/no questions needed</p></td>
<td><p>4-way title fight: any of them can win</p></td>
</tr>
<tr class="row-even"><td><p>Fair 8-sided die</p></td>
<td><p>[1/8] * 8</p></td>
<td><p>3.00</p></td>
<td><p>3 yes/no questions needed</p></td>
<td><p>8 competitive drivers: wide-open field</p></td>
</tr>
<tr class="row-odd"><td><p>Fair N-sided die</p></td>
<td><p>[1/N] * N</p></td>
<td><p>log2(N)</p></td>
<td><p>log2(N) questions needed</p></td>
<td><p>N equally-matched drivers</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Pattern</strong>: For a uniform distribution over N outcomes, entropy = log2(N) bits.</p>
<p><strong>Why Maximum Entropy = Uniform?</strong></p>
<ul class="simple">
<li><p>Mathematically: Proven via Lagrange multipliers (maximizing H subject to sum = 1)</p></li>
<li><p>Intuitively: Any preference toward one outcome reduces average surprise</p></li>
<li><p>Philosophically: Maximum entropy = maximum ignorance = all outcomes equally plausible</p></li>
<li><p>In F1 terms: The most unpredictable season is when every driver has equal chance of winning</p></li>
</ul>
</section>
<section id="naive-bayes-classifier">
<h3>Naive Bayes Classifier<a class="headerlink" href="#naive-bayes-classifier" title="Link to this heading">#</a></h3>
<p>A simple but effective classifier using Bayes’ theorem:</p>
<div class="math notranslate nohighlight">
\[P(y|x_1, ..., x_n) \propto P(y) \prod_{i=1}^n P(x_i|y)\]</div>
<p>The “naive” assumption is that features are conditionally independent given the class.</p>
<p><strong>F1 analogy</strong>: Imagine predicting which tire compound a driver is on (Soft/Medium/Hard) based on telemetry features: average speed, tire temperature, and lap time degradation rate. Naive Bayes assumes these features are independent given the compound — which isn’t perfectly true (speed and degradation correlate), but it works surprisingly well in practice, just as it does in spam filtering and text classification.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simple Naive Bayes from scratch</span>
<span class="c1"># F1 context: Classifying tire compound from telemetry features</span>
<span class="k">class</span><span class="w"> </span><span class="nc">NaiveBayesClassifier</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">class_priors</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature_params</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># (class, feature) -&gt; (mean, std)</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Fit Gaussian Naive Bayes.&quot;&quot;&quot;</span>
        <span class="n">classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">:</span>
            <span class="c1"># Class prior</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">class_priors</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">c</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_samples</span>
            
            <span class="c1"># Feature parameters (Gaussian)</span>
            <span class="n">X_c</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">c</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">feature_params</span><span class="p">[(</span><span class="n">c</span><span class="p">,</span> <span class="n">j</span><span class="p">)]</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_c</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">X_c</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>
                
    <span class="k">def</span><span class="w"> </span><span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute class probabilities.&quot;&quot;&quot;</span>
        <span class="n">classes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">class_priors</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">)))</span>
        
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">classes</span><span class="p">):</span>
            <span class="c1"># Start with log prior</span>
            <span class="n">log_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">class_priors</span><span class="p">[</span><span class="n">c</span><span class="p">])</span>
            
            <span class="c1"># Add log likelihood for each feature</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">mean</span><span class="p">,</span> <span class="n">std</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_params</span><span class="p">[(</span><span class="n">c</span><span class="p">,</span> <span class="n">j</span><span class="p">)]</span>
                <span class="n">log_prob</span> <span class="o">+=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="n">j</span><span class="p">],</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="p">)</span>
            
            <span class="n">probs</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">log_prob</span>
        
        <span class="c1"># Convert to probabilities (softmax of log probs)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">probs</span> <span class="o">-</span> <span class="n">probs</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">probs</span> <span class="o">/</span> <span class="n">probs</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">probs</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Predict class labels.&quot;&quot;&quot;</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">classes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">class_priors</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">classes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">probs</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)])</span>


<span class="c1"># Generate synthetic telemetry data (two tire compounds)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">200</span>

<span class="c1"># Compound 0 (Hard): lower avg speed, lower degradation</span>
<span class="n">hard_compound_telemetry</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="c1"># Compound 1 (Soft): higher avg speed, higher degradation</span>
<span class="n">soft_compound_telemetry</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="n">X_telemetry</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">hard_compound_telemetry</span><span class="p">,</span> <span class="n">soft_compound_telemetry</span><span class="p">])</span>
<span class="n">y_compound</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_samples</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_samples</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1"># Train</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">NaiveBayesClassifier</span><span class="p">()</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_telemetry</span><span class="p">,</span> <span class="n">y_compound</span><span class="p">)</span>

<span class="c1"># Predict</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_telemetry</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">==</span> <span class="n">y_compound</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Visualize decision boundary</span>
<span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X_telemetry</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X_telemetry</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X_telemetry</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X_telemetry</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
                     <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_telemetry</span><span class="p">[</span><span class="n">y_compound</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_telemetry</span><span class="p">[</span><span class="n">y_compound</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Hard Compound&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_telemetry</span><span class="p">[</span><span class="n">y_compound</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_telemetry</span><span class="p">[</span><span class="n">y_compound</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Soft Compound&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Average Speed (normalized)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Tire Degradation Rate (normalized)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Naive Bayes: Classifying Tire Compound from Telemetry&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="deep-dive-understanding-kl-divergence">
<h3>Deep Dive: Understanding KL Divergence<a class="headerlink" href="#deep-dive-understanding-kl-divergence" title="Link to this heading">#</a></h3>
<p>KL divergence measures how “different” one distribution is from another. Here are multiple ways to understand it:</p>
<p><strong>Interpretation 1: Extra Bits for Wrong Encoding</strong></p>
<ul class="simple">
<li><p>Suppose you design a code optimized for distribution Q</p></li>
<li><p>But the true data comes from distribution P</p></li>
<li><p>KL(P || Q) = extra bits needed because you used the wrong distribution</p></li>
<li><p>If P = Q, you need exactly H(P) bits (optimal)</p></li>
<li><p>If P != Q, you need H(P) + KL(P||Q) bits (suboptimal)</p></li>
</ul>
<p><strong>Interpretation 2: Information Lost</strong></p>
<ul class="simple">
<li><p>KL(P || Q) measures information lost when Q is used to approximate P</p></li>
<li><p>It’s the “distance” from Q to P (but not symmetric!)</p></li>
</ul>
<p><strong>Interpretation 3: The Fundamental Relationship</strong>
$<span class="math notranslate nohighlight">\(D_{KL}(P || Q) = H(P, Q) - H(P) = \text{Cross-Entropy} - \text{Entropy}\)</span>$</p>
<p>This tells us:</p>
<ul class="simple">
<li><p>Cross-entropy = cost of using Q to encode P</p></li>
<li><p>Entropy = minimum possible cost (using P itself)</p></li>
<li><p>KL divergence = the “wasted” bits from using Q instead of P</p></li>
</ul>
<p><strong>F1 analogy</strong>: KL divergence measures <strong>how different qualifying pace is from race pace</strong>. If a team qualifies brilliantly but fades in the race (different distributions), the KL divergence between their qualifying and race performance is large. A team whose race pace mirrors qualifying (like Red Bull in a dominant season) has low KL divergence. In ML, this exact concept powers knowledge distillation — measuring how well the student model’s distribution matches the teacher’s.</p>
<p><strong>Why Not Symmetric?</strong></p>
<ul class="simple">
<li><p>KL(P || Q): Cost of using Q when truth is P</p></li>
<li><p>KL(Q || P): Cost of using P when truth is Q</p></li>
<li><p>These are different questions!</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualizing the KL = CrossEntropy - Entropy relationship</span>
<span class="c1"># And demonstrating asymmetry</span>
<span class="c1"># F1: Qualifying pace (P) vs Race pace (Q)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Example distributions: Qualifying vs Race performance</span>
<span class="n">quali_pace</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>  <span class="c1"># Qualifying: strong at top positions</span>
<span class="n">race_pace</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.33</span><span class="p">,</span> <span class="mf">0.33</span><span class="p">,</span> <span class="mf">0.34</span><span class="p">])</span>  <span class="c1"># Race: much more spread out</span>

<span class="c1"># Calculate all quantities</span>
<span class="n">H_P</span> <span class="o">=</span> <span class="n">entropy</span><span class="p">(</span><span class="n">quali_pace</span><span class="p">)</span>
<span class="n">H_P_Q</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">quali_pace</span><span class="p">,</span> <span class="n">race_pace</span><span class="p">)</span>  <span class="c1"># Cross-entropy</span>
<span class="n">KL_P_Q</span> <span class="o">=</span> <span class="n">kl_divergence</span><span class="p">(</span><span class="n">quali_pace</span><span class="p">,</span> <span class="n">race_pace</span><span class="p">)</span>

<span class="c1"># Plot 1: Bar chart showing the relationship</span>
<span class="n">quantities</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;H(P)</span><span class="se">\n</span><span class="s1">Entropy&#39;</span><span class="p">,</span> <span class="s1">&#39;KL(P||Q)</span><span class="se">\n</span><span class="s1">Divergence&#39;</span><span class="p">,</span> <span class="s1">&#39;H(P,Q)</span><span class="se">\n</span><span class="s1">Cross-Entropy&#39;</span><span class="p">]</span>
<span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="n">H_P</span><span class="p">,</span> <span class="n">KL_P_Q</span><span class="p">,</span> <span class="n">H_P_Q</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">]</span>

<span class="n">bars</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">quantities</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Bits&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;H(P,Q) = H(P) + KL(P||Q)</span><span class="se">\n</span><span class="s1">Quali vs Race Pace Gap&#39;</span><span class="p">)</span>

<span class="c1"># Add value labels</span>
<span class="k">for</span> <span class="n">bar</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">bars</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">bar</span><span class="o">.</span><span class="n">get_x</span><span class="p">()</span> <span class="o">+</span> <span class="n">bar</span><span class="o">.</span><span class="n">get_width</span><span class="p">()</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">bar</span><span class="o">.</span><span class="n">get_height</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.05</span><span class="p">,</span> 
                 <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">val</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="c1"># Verify the relationship</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.85</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">H_P</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1"> + </span><span class="si">{</span><span class="n">KL_P_Q</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1"> = </span><span class="si">{</span><span class="n">H_P</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">KL_P_Q</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> 
             <span class="n">transform</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span>
             <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s1">&#39;round&#39;</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;yellow&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">))</span>

<span class="c1"># Plot 2: Asymmetry visualization</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>  <span class="c1"># Dominant driver: almost always wins</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>  <span class="c1"># Equal competition</span>

<span class="n">kl_pq</span> <span class="o">=</span> <span class="n">kl_divergence</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">Q</span><span class="p">)</span>
<span class="n">kl_qp</span> <span class="o">=</span> <span class="n">kl_divergence</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">P</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">width</span> <span class="o">=</span> <span class="mf">0.35</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">width</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;P (dominant era)&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">width</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Q (equal field)&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;Win&#39;</span><span class="p">,</span> <span class="s1">&#39;Lose&#39;</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;KL Asymmetry</span><span class="se">\n</span><span class="s1">KL(P||Q)=</span><span class="si">{</span><span class="n">kl_pq</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">, KL(Q||P)=</span><span class="si">{</span><span class="n">kl_qp</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Plot 3: Why asymmetry matters in practice</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.85</span><span class="p">,</span> <span class="s1">&#39;KL(P || Q) vs KL(Q || P)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span>
             <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">transAxes</span><span class="p">)</span>

<span class="n">explanation</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">KL(P || Q): &quot;How bad is Q as a model of P?&quot;</span>
<span class="s2">- Averages over P (true distribution)</span>
<span class="s2">- Catastrophic if Q gives 0 probability </span>
<span class="s2">  where P has probability (log(0) = -inf!)</span>
<span class="s2">- Used in: VAE loss, variational inference</span>

<span class="s2">KL(Q || P): &quot;How bad is P as a model of Q?&quot;  </span>
<span class="s2">- Averages over Q (approximate distribution)</span>
<span class="s2">- Catastrophic if P gives 0 probability</span>
<span class="s2">  where Q has probability</span>
<span class="s2">- Used in: Reverse KL for mode-seeking</span>

<span class="s2">In classification:</span>
<span class="s2">- P = true labels (one-hot), Q = model predictions</span>
<span class="s2">- Cross-entropy = H(P) + KL(P||Q) = KL(P||Q)</span>
<span class="s2">  (since H(P) = 0 for one-hot)</span>

<span class="s2">F1: KL(quali || race) != KL(race || quali)</span>
<span class="s2">&quot;How different is race pace from quali pace&quot;</span>
<span class="s2">is NOT the same as the reverse!</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="n">explanation</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span>
             <span class="n">verticalalignment</span><span class="o">=</span><span class="s1">&#39;top&#39;</span><span class="p">,</span> <span class="n">fontfamily</span><span class="o">=</span><span class="s1">&#39;monospace&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Key Insight for Classification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;When true labels are one-hot, H(P) = 0&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;So: Cross-Entropy Loss = KL(true || predicted)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Minimizing cross-entropy = minimizing KL divergence!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Demonstrating KL divergence in Knowledge Distillation</span>
<span class="c1"># F1 context: An experienced race engineer (teacher) training a junior (student)</span>

<span class="c1"># Imagine a 5-class race outcome prediction problem</span>
<span class="n">race_outcomes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Win&#39;</span><span class="p">,</span> <span class="s1">&#39;Podium&#39;</span><span class="p">,</span> <span class="s1">&#39;Points&#39;</span><span class="p">,</span> <span class="s1">&#39;No Points&#39;</span><span class="p">,</span> <span class="s1">&#39;DNF&#39;</span><span class="p">]</span>

<span class="c1"># Hard label (ground truth: the driver won)</span>
<span class="n">hard_label</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>  <span class="c1"># True outcome is &#39;Win&#39;</span>

<span class="c1"># Experienced engineer&#39;s prediction (teacher — soft, nuanced)</span>
<span class="c1"># Notice: teacher thinks podium was likely too (car was competitive)</span>
<span class="n">senior_engineer</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">])</span>

<span class="c1"># Junior engineer predictions at different training stages</span>
<span class="n">junior_untrained</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>  <span class="c1"># Uniform (no insight)</span>
<span class="n">junior_learning</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>  <span class="c1"># Developing intuition</span>
<span class="n">junior_trained</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.68</span><span class="p">,</span> <span class="mf">0.18</span><span class="p">,</span> <span class="mf">0.07</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">])</span>  <span class="c1"># Well-calibrated</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Plot 1: Compare distributions</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">race_outcomes</span><span class="p">))</span>
<span class="n">width</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mf">1.5</span><span class="o">*</span><span class="n">width</span><span class="p">,</span> <span class="n">hard_label</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Hard Label (result)&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">width</span><span class="p">,</span> <span class="n">senior_engineer</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Senior Engineer&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">width</span><span class="p">,</span> <span class="n">junior_learning</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Junior (learning)&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">1.5</span><span class="o">*</span><span class="n">width</span><span class="p">,</span> <span class="n">junior_trained</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Junior (trained)&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">race_outcomes</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Knowledge Distillation: Senior to Junior Engineer</span><span class="se">\n</span><span class="s1">&quot;Learning the nuance behind race outcomes&quot;&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Plot 2: KL divergences</span>
<span class="n">juniors</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;Untrained&#39;</span><span class="p">:</span> <span class="n">junior_untrained</span><span class="p">,</span>
    <span class="s1">&#39;Learning&#39;</span><span class="p">:</span> <span class="n">junior_learning</span><span class="p">,</span> 
    <span class="s1">&#39;Trained&#39;</span><span class="p">:</span> <span class="n">junior_trained</span>
<span class="p">}</span>

<span class="c1"># KL from hard labels (what standard cross-entropy uses)</span>
<span class="n">kl_hard</span> <span class="o">=</span> <span class="p">[</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">hard_label</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">juniors</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>

<span class="c1"># KL from senior engineer&#39;s soft labels (knowledge distillation)</span>
<span class="n">kl_soft</span> <span class="o">=</span> <span class="p">[</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">senior_engineer</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">juniors</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">juniors</span><span class="p">))</span>
<span class="n">width</span> <span class="o">=</span> <span class="mf">0.35</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">width</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">kl_hard</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;KL(Result || Junior)&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">width</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">kl_soft</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;KL(Senior || Junior)&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">juniors</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;KL Divergence (bits)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Loss: Hard Result vs Knowledge Distillation&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Why Soft Labels Help:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hard label only says: &#39;The driver won this race&#39;&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Soft label says: &#39;The driver won, but podium was likely too,&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;                  DNF was very unlikely given car reliability&#39;&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The relationships between outcomes (&#39;dark knowledge&#39;) help&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;the junior engineer develop better race intuition!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="kl-divergence-in-machine-learning-applications">
<h3>KL Divergence in Machine Learning Applications<a class="headerlink" href="#kl-divergence-in-machine-learning-applications" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Application</p></th>
<th class="head"><p>P (True/Target)</p></th>
<th class="head"><p>Q (Approximate/Model)</p></th>
<th class="head"><p>What KL Measures</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Classification Loss</strong></p></td>
<td><p>One-hot labels</p></td>
<td><p>Softmax predictions</p></td>
<td><p>How wrong are predictions</p></td>
<td><p>How far model’s race prediction is from actual result</p></td>
</tr>
<tr class="row-odd"><td><p><strong>VAE Loss</strong></p></td>
<td><p>Posterior q(z|x)</p></td>
<td><p>Prior p(z), usually N(0,1)</p></td>
<td><p>How far latent code is from prior</p></td>
<td><p>How far telemetry encoding is from baseline</p></td>
</tr>
<tr class="row-even"><td><p><strong>Knowledge Distillation</strong></p></td>
<td><p>Teacher softmax</p></td>
<td><p>Student softmax</p></td>
<td><p>How well student mimics teacher</p></td>
<td><p>Junior engineer matching senior’s race intuition</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Policy Gradient (PPO)</strong></p></td>
<td><p>Old policy</p></td>
<td><p>New policy</p></td>
<td><p>Prevents too-large policy updates</p></td>
<td><p>Gradual strategy updates between races</p></td>
</tr>
<tr class="row-even"><td><p><strong>Variational Inference</strong></p></td>
<td><p>True posterior</p></td>
<td><p>Variational approx</p></td>
<td><p>Quality of approximation</p></td>
<td><p>How well simplified model captures real tire behavior</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>The VAE Loss Decomposition</strong>:
$<span class="math notranslate nohighlight">\(\mathcal{L}_{VAE} = \underbrace{-\mathbb{E}_{q(z|x)}[\log p(x|z)]}_{\text{Reconstruction Loss}} + \underbrace{D_{KL}(q(z|x) || p(z))}_{\text{Regularization}}\)</span>$</p>
<p>The KL term pulls the encoder’s latent distribution toward the prior, enabling generation.</p>
<p><strong>Knowledge Distillation</strong>:</p>
<ul class="simple">
<li><p>Teacher: Large, accurate model with “soft” predictions</p></li>
<li><p>Student: Small model learning to match teacher</p></li>
<li><p>Loss = KL(Teacher || Student) on softmax outputs</p></li>
<li><p>Student learns teacher’s “dark knowledge” (relationships between classes)</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="maximum-likelihood-estimation-mle">
<h2>5. Maximum Likelihood Estimation (MLE)<a class="headerlink" href="#maximum-likelihood-estimation-mle" title="Link to this heading">#</a></h2>
<p>MLE finds parameters that maximize the probability of observing the data:</p>
<div class="math notranslate nohighlight">
\[\hat{\theta}_{MLE} = \arg\max_\theta P(\text{data}|\theta) = \arg\max_\theta \prod_i P(x_i|\theta)\]</div>
<p>In practice, we maximize the <strong>log-likelihood</strong> (easier to work with):</p>
<div class="math notranslate nohighlight">
\[\hat{\theta}_{MLE} = \arg\max_\theta \sum_i \log P(x_i|\theta)\]</div>
<p><strong>Key insight</strong>: Minimizing cross-entropy loss = maximizing log-likelihood!</p>
<p><strong>F1 analogy</strong>: MLE is how teams estimate tire degradation rate from lap data. You observe a stint of 15 laps with gradually increasing times. MLE asks: “What degradation rate per lap makes these observed times most probable?” The answer is the slope of best fit through the lap time data. This is identical to how neural networks learn: find the parameters (weights) that make the training data most probable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># MLE for Gaussian parameters — Estimating a driver&#39;s true pace</span>
<span class="c1"># True parameters (unknown to us in practice)</span>
<span class="n">true_mean_pace</span> <span class="o">=</span> <span class="mf">5.0</span>  <span class="c1"># True mean lap time offset</span>
<span class="n">true_pace_sigma</span> <span class="o">=</span> <span class="mf">2.0</span>  <span class="c1"># True consistency</span>

<span class="c1"># Generate observed lap times</span>
<span class="n">n_laps</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">observed_laps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">true_mean_pace</span><span class="p">,</span> <span class="n">true_pace_sigma</span><span class="p">,</span> <span class="n">n_laps</span><span class="p">)</span>

<span class="c1"># MLE estimates (can be derived analytically)</span>
<span class="n">pace_mle</span> <span class="o">=</span> <span class="n">observed_laps</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>  <span class="c1"># Sample mean</span>
<span class="n">sigma_mle</span> <span class="o">=</span> <span class="n">observed_laps</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>  <span class="c1"># Sample std (biased, but MLE)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True parameters: mean pace = </span><span class="si">{</span><span class="n">true_mean_pace</span><span class="si">}</span><span class="s2">, consistency = </span><span class="si">{</span><span class="n">true_pace_sigma</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MLE estimates:   mean pace = </span><span class="si">{</span><span class="n">pace_mle</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, consistency = </span><span class="si">{</span><span class="n">sigma_mle</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Visualize</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">true_mean_pace</span> <span class="o">-</span> <span class="mi">4</span><span class="o">*</span><span class="n">true_pace_sigma</span><span class="p">,</span> <span class="n">true_mean_pace</span> <span class="o">+</span> <span class="mi">4</span><span class="o">*</span><span class="n">true_pace_sigma</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">observed_laps</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Observed lap times&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">true_mean_pace</span><span class="p">,</span> <span class="n">true_pace_sigma</span><span class="p">),</span> <span class="s1">&#39;g-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
         <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;True: N(</span><span class="si">{</span><span class="n">true_mean_pace</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">true_pace_sigma</span><span class="si">}</span><span class="s1">²)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pace_mle</span><span class="p">,</span> <span class="n">sigma_mle</span><span class="p">),</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;MLE: N(</span><span class="si">{</span><span class="n">pace_mle</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">sigma_mle</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">²)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Lap Time (offset from baseline)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;MLE for Lap Time Distribution</span><span class="se">\n</span><span class="s1">&quot;Finding the driver</span><span class="se">\&#39;</span><span class="s1">s true pace from observed data&quot;&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize the likelihood function — Finding the best-fit pace parameters</span>
<span class="k">def</span><span class="w"> </span><span class="nf">log_likelihood</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute log-likelihood of data under N(mu, sigma^2).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">))</span>

<span class="c1"># Create grid of parameters</span>
<span class="n">mus</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">sigmas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">MU</span><span class="p">,</span> <span class="n">SIGMA</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">mus</span><span class="p">,</span> <span class="n">sigmas</span><span class="p">)</span>

<span class="c1"># Compute log-likelihood at each point</span>
<span class="n">LL</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">MU</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sigmas</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mus</span><span class="p">)):</span>
        <span class="n">LL</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">log_likelihood</span><span class="p">(</span><span class="n">MU</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">SIGMA</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">observed_laps</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">MU</span><span class="p">,</span> <span class="n">SIGMA</span><span class="p">,</span> <span class="n">LL</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Log-Likelihood&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">pace_mle</span><span class="p">],</span> <span class="p">[</span><span class="n">sigma_mle</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> 
            <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;MLE: (pace=</span><span class="si">{</span><span class="n">pace_mle</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">, consistency=</span><span class="si">{</span><span class="n">sigma_mle</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">true_mean_pace</span><span class="p">],</span> <span class="p">[</span><span class="n">true_pace_sigma</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;True: (pace=</span><span class="si">{</span><span class="n">true_mean_pace</span><span class="si">}</span><span class="s1">, consistency=</span><span class="si">{</span><span class="n">true_pace_sigma</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Mean Lap Time (mu)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Consistency / Sigma&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Log-Likelihood Surface for Driver Pace Estimation</span><span class="se">\n</span><span class="s1">&quot;The peak is the MLE — the best estimate from observed laps&quot;&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<section id="mle-for-bernoulli-coin-flip-race-finish">
<h3>MLE for Bernoulli (Coin Flip / Race Finish)<a class="headerlink" href="#mle-for-bernoulli-coin-flip-race-finish" title="Link to this heading">#</a></h3>
<p>If we observe <span class="math notranslate nohighlight">\(k\)</span> heads in <span class="math notranslate nohighlight">\(n\)</span> flips, the MLE estimate is simply:</p>
<div class="math notranslate nohighlight">
\[\hat{p}_{MLE} = \frac{k}{n}\]</div>
<p><strong>F1 analogy</strong>: If a car finishes 14 out of 20 races, the MLE estimate of its reliability is p = 14/20 = 0.70. Simple, elegant, and exactly how teams track reliability statistics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># MLE for Bernoulli — Estimating car reliability from race finishes</span>
<span class="n">reliability_true</span> <span class="o">=</span> <span class="mf">0.7</span>
<span class="n">n_races</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">race_finishes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">reliability_true</span><span class="p">,</span> <span class="n">n_races</span><span class="p">)</span>
<span class="n">n_finishes</span> <span class="o">=</span> <span class="n">race_finishes</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>  <span class="c1"># Number of races finished</span>

<span class="n">reliability_mle</span> <span class="o">=</span> <span class="n">n_finishes</span> <span class="o">/</span> <span class="n">n_races</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True reliability: </span><span class="si">{</span><span class="n">reliability_true</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Observed: </span><span class="si">{</span><span class="n">n_finishes</span><span class="si">}</span><span class="s2"> finishes in </span><span class="si">{</span><span class="n">n_races</span><span class="si">}</span><span class="s2"> races&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MLE estimate: reliability = </span><span class="si">{</span><span class="n">reliability_mle</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Visualize likelihood function</span>
<span class="n">p_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">likelihoods</span> <span class="o">=</span> <span class="p">[</span><span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">n_finishes</span><span class="p">,</span> <span class="n">n_races</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">p_values</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_values</span><span class="p">,</span> <span class="n">likelihoods</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">reliability_mle</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;MLE: reliability = </span><span class="si">{</span><span class="n">reliability_mle</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">reliability_true</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;True: reliability = </span><span class="si">{</span><span class="n">reliability_true</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Reliability (p)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Likelihood P(data|p)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Likelihood Function: </span><span class="si">{</span><span class="n">n_finishes</span><span class="si">}</span><span class="s1"> Finishes in </span><span class="si">{</span><span class="n">n_races</span><span class="si">}</span><span class="s1"> Race Starts</span><span class="se">\n</span><span class="s1">&#39;</span>
          <span class="sa">f</span><span class="s1">&#39;&quot;What reliability makes this season most probable?&quot;&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="information-theory">
<h2>6. Information Theory<a class="headerlink" href="#information-theory" title="Link to this heading">#</a></h2>
<p>Information theory quantifies information and uncertainty.</p>
<section id="entropy">
<h3>Entropy<a class="headerlink" href="#entropy" title="Link to this heading">#</a></h3>
<p>Entropy measures the “uncertainty” or “information content” of a distribution:</p>
<div class="math notranslate nohighlight">
\[H(X) = -\sum_x P(x) \log P(x) = -E[\log P(X)]\]</div>
<p><strong>Properties</strong>:</p>
<ul class="simple">
<li><p>Higher entropy = more uncertainty</p></li>
<li><p>Uniform distribution has maximum entropy</p></li>
<li><p>Deterministic variable has entropy 0</p></li>
</ul>
<p><strong>F1 analogy</strong>: Entropy is the <strong>excitement level of a championship</strong>. A season where one team dominates has low entropy (boring, predictable). A season with 5 teams in contention has high entropy (thrilling, unpredictable). The 2021 Hamilton-Verstappen title fight had much higher entropy than the 2023 Verstappen dominance. In ML, when your model’s softmax output has high entropy, it means the model is uncertain about its prediction — just like a pundit who says “anyone could win this race.”</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute entropy of a discrete distribution.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="n">p</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># Avoid log(0)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>

<span class="c1"># F1 season competitiveness examples (in bits)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Entropy examples — Race/Season Unpredictability (in bits):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Two equal rivals [0.5, 0.5]: H = </span><span class="si">{</span><span class="n">entropy</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span><span class="w"> </span><span class="mf">0.5</span><span class="p">])</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> bits&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dominant driver [0.9, 0.1]: H = </span><span class="si">{</span><span class="n">entropy</span><span class="p">([</span><span class="mf">0.9</span><span class="p">,</span><span class="w"> </span><span class="mf">0.1</span><span class="p">])</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> bits&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Certain winner [1.0, 0.0]: H = </span><span class="si">{</span><span class="n">entropy</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">])</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> bits&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Six equal drivers [1/6]*6: H = </span><span class="si">{</span><span class="n">entropy</span><span class="p">([</span><span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">]</span><span class="o">*</span><span class="mi">6</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> bits&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Eight-way fight [1/8]*8: H = </span><span class="si">{</span><span class="n">entropy</span><span class="p">([</span><span class="mi">1</span><span class="o">/</span><span class="mi">8</span><span class="p">]</span><span class="o">*</span><span class="mi">8</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> bits&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Entropy of binary distribution as function of p</span>
<span class="c1"># F1: How uncertain is a head-to-head title fight?</span>
<span class="n">p_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">entropies</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="n">p</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">p_values</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_values</span><span class="p">,</span> <span class="n">entropies</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;P(Driver A wins the championship)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Entropy (bits)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Binary Entropy: Excitement of a Two-Way Title Fight</span><span class="se">\n</span><span class="s1">&#39;</span>
          <span class="s1">&#39;&quot;Maximum drama when both drivers have equal chance&quot;&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Maximum = 1 bit (50/50 fight)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;p = 0.5 (equal)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Maximum entropy at p = 0.5 (maximum uncertainty / most exciting)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Entropy = 0 when p = 0 or p = 1 (championship already decided)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="cross-entropy">
<h3>Cross-Entropy<a class="headerlink" href="#cross-entropy" title="Link to this heading">#</a></h3>
<p>Cross-entropy measures the “cost” of using distribution <span class="math notranslate nohighlight">\(Q\)</span> to encode samples from distribution <span class="math notranslate nohighlight">\(P\)</span>:</p>
<div class="math notranslate nohighlight">
\[H(P, Q) = -\sum_x P(x) \log Q(x) = -E_P[\log Q(X)]\]</div>
<p><strong>In ML</strong>: Cross-entropy loss measures how well predicted probabilities <span class="math notranslate nohighlight">\(Q\)</span> match true labels <span class="math notranslate nohighlight">\(P\)</span>.</p>
<p><strong>F1 analogy</strong>: Imagine you’re a betting house using your model’s race predictions (Q) to set odds, but the actual outcomes follow distribution P. Cross-entropy measures how much money you lose because your model doesn’t perfectly match reality. The closer your predictions to truth, the lower the cross-entropy — and the less you lose.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute cross-entropy H(P, Q).&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
    <span class="c1"># Avoid log(0) by clipping</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">q</span><span class="p">))</span>

<span class="c1"># Example: True race winner vs model predictions</span>
<span class="c1"># True outcome: Driver A won (class 0)</span>
<span class="n">true_result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>  <span class="c1"># Driver A won</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">([</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">],</span> <span class="s2">&quot;Model confident in Driver A (correct!)&quot;</span><span class="p">),</span>
    <span class="p">([</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="s2">&quot;Model leans toward A but unsure&quot;</span><span class="p">),</span>
    <span class="p">([</span><span class="mf">0.33</span><span class="p">,</span> <span class="mf">0.33</span><span class="p">,</span> <span class="mf">0.34</span><span class="p">],</span> <span class="s2">&quot;Model has no idea (uniform)&quot;</span><span class="p">),</span>
    <span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.45</span><span class="p">,</span> <span class="mf">0.45</span><span class="p">],</span> <span class="s2">&quot;Model confident in wrong driver&quot;</span><span class="p">),</span>
<span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cross-entropy loss for different race predictions:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True result: Driver A wins (one-hot: </span><span class="si">{</span><span class="n">true_result</span><span class="si">}</span><span class="s2">)</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">q</span><span class="p">,</span> <span class="n">desc</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">:</span>
    <span class="n">ce</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">true_result</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">desc</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Prediction: </span><span class="si">{</span><span class="n">q</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Cross-entropy: </span><span class="si">{</span><span class="n">ce</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> bits</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="kl-divergence">
<h3>KL Divergence<a class="headerlink" href="#kl-divergence" title="Link to this heading">#</a></h3>
<p>KL divergence measures how different distribution <span class="math notranslate nohighlight">\(Q\)</span> is from <span class="math notranslate nohighlight">\(P\)</span>:</p>
<div class="math notranslate nohighlight">
\[D_{KL}(P || Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)} = H(P, Q) - H(P)\]</div>
<p><strong>Properties</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(D_{KL}(P || Q) \geq 0\)</span> (always non-negative)</p></li>
<li><p><span class="math notranslate nohighlight">\(D_{KL}(P || Q) = 0\)</span> if and only if <span class="math notranslate nohighlight">\(P = Q\)</span></p></li>
<li><p>Not symmetric: <span class="math notranslate nohighlight">\(D_{KL}(P || Q) \neq D_{KL}(Q || P)\)</span></p></li>
</ul>
<p><strong>In ML</strong>: Used in VAEs, knowledge distillation, regularization</p>
<p><strong>F1 analogy</strong>: KL divergence measures how different two performance distributions are. If P is a team’s qualifying pace distribution and Q is their race pace distribution, KL(P||Q) quantifies the “qualifying-to-race translation gap.” A team that’s a qualifying specialist (fast in quali, slow in race) has high KL divergence between these distributions. A team whose race pace reliably mirrors qualifying has low KL divergence — they “translate” Saturday pace to Sunday.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">kl_divergence</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute KL divergence D_KL(P || Q).&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
    <span class="c1"># Only sum where p &gt; 0</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">/</span> <span class="n">q</span><span class="p">[</span><span class="n">mask</span><span class="p">]))</span>

<span class="c1"># Compare qualifying pace vs race pace distributions</span>
<span class="n">quali_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>  <span class="c1"># Qualifying: tends to be at front</span>
<span class="n">race_similar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>  <span class="c1"># Race pace similar to quali</span>
<span class="n">race_reversed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">])</span>  <span class="c1"># Race pace: drops back (quali specialist)</span>
<span class="n">race_uniform</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">])</span>  <span class="c1"># Race pace: anything can happen</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Qualifying distribution = </span><span class="si">{</span><span class="n">quali_dist</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">KL divergences (how different is race pace from quali?):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Similar race pace    </span><span class="si">{</span><span class="n">race_similar</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">quali_dist</span><span class="p">,</span><span class="w"> </span><span class="n">race_similar</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> bits&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Reversed (drops back) </span><span class="si">{</span><span class="n">race_reversed</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">quali_dist</span><span class="p">,</span><span class="w"> </span><span class="n">race_reversed</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> bits&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Unpredictable race   </span><span class="si">{</span><span class="n">race_uniform</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">quali_dist</span><span class="p">,</span><span class="w"> </span><span class="n">race_uniform</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> bits&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Note asymmetry (direction matters!):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  KL(quali || reversed) = </span><span class="si">{</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">quali_dist</span><span class="p">,</span><span class="w"> </span><span class="n">race_reversed</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  KL(reversed || quali) = </span><span class="si">{</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">race_reversed</span><span class="p">,</span><span class="w"> </span><span class="n">quali_dist</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize KL divergence between two Gaussians</span>
<span class="c1"># F1: How different is one driver&#39;s lap time distribution from another&#39;s?</span>
<span class="k">def</span><span class="w"> </span><span class="nf">kl_gaussian</span><span class="p">(</span><span class="n">mu1</span><span class="p">,</span> <span class="n">sigma1</span><span class="p">,</span> <span class="n">mu2</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;KL divergence between two univariate Gaussians.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sigma2</span><span class="o">/</span><span class="n">sigma1</span><span class="p">)</span> <span class="o">+</span> 
            <span class="p">(</span><span class="n">sigma1</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">mu1</span> <span class="o">-</span> <span class="n">mu2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma2</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Driver A&#39;s lap time distribution: N(0, 1) (baseline)</span>
<span class="n">mu_driver_a</span><span class="p">,</span> <span class="n">sigma_driver_a</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Vary mean pace difference</span>
<span class="n">pace_offsets</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">kls</span> <span class="o">=</span> <span class="p">[</span><span class="n">kl_gaussian</span><span class="p">(</span><span class="n">mu_driver_a</span><span class="p">,</span> <span class="n">sigma_driver_a</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma_driver_a</span><span class="p">)</span> <span class="k">for</span> <span class="n">mu</span> <span class="ow">in</span> <span class="n">pace_offsets</span><span class="p">]</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">pace_offsets</span><span class="p">,</span> <span class="n">kls</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Driver B Mean Pace Difference (seconds)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;KL Divergence (nats)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;KL Divergence: Same Consistency, Different Pace</span><span class="se">\n</span><span class="s1">&#39;</span>
                   <span class="s1">&#39;&quot;How different is Driver B</span><span class="se">\&#39;</span><span class="s1">s pace from Driver A?&quot;&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Vary consistency</span>
<span class="n">consistencies</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">kls</span> <span class="o">=</span> <span class="p">[</span><span class="n">kl_gaussian</span><span class="p">(</span><span class="n">mu_driver_a</span><span class="p">,</span> <span class="n">sigma_driver_a</span><span class="p">,</span> <span class="n">mu_driver_a</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span> <span class="k">for</span> <span class="n">sigma</span> <span class="ow">in</span> <span class="n">consistencies</span><span class="p">]</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">consistencies</span><span class="p">,</span> <span class="n">kls</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Same consistency as A&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Driver B Consistency (sigma, seconds)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;KL Divergence (nats)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;KL Divergence: Same Pace, Different Consistency</span><span class="se">\n</span><span class="s1">&#39;</span>
                   <span class="s1">&#39;&quot;How different is Driver B</span><span class="se">\&#39;</span><span class="s1">s consistency?&quot;&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<section id="exercise-1-bayesian-tire-degradation-inference">
<h3>Exercise 1: Bayesian Tire Degradation Inference<a class="headerlink" href="#exercise-1-bayesian-tire-degradation-inference" title="Link to this heading">#</a></h3>
<p>Use Bayes’ theorem to update beliefs about a tire compound’s degradation rate after observing lap times. Just as we used Beta-Binomial conjugacy for coin flips, we’ll update our beliefs about the probability that tires are in “high degradation” mode based on observed performance drops.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Bayesian inference for tire degradation rate</span>
<span class="c1"># Model: Each lap, we observe if there was a &quot;performance drop&quot; (1) or not (0)</span>
<span class="c1"># Prior: Beta(a, b) distribution over the degradation probability p</span>
<span class="c1"># Posterior after k drops in n laps: Beta(a + k, b + n - k)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">plot_beta_posterior</span><span class="p">(</span><span class="n">a_prior</span><span class="p">,</span> <span class="n">b_prior</span><span class="p">,</span> <span class="n">n_drops</span><span class="p">,</span> <span class="n">n_clean_laps</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plot prior and posterior distributions for degradation rate.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    
    <span class="c1"># Prior</span>
    <span class="n">prior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">a_prior</span><span class="p">,</span> <span class="n">b_prior</span><span class="p">)</span>
    
    <span class="c1"># Posterior</span>
    <span class="n">a_post</span> <span class="o">=</span> <span class="n">a_prior</span> <span class="o">+</span> <span class="n">n_drops</span>
    <span class="n">b_post</span> <span class="o">=</span> <span class="n">b_prior</span> <span class="o">+</span> <span class="n">n_clean_laps</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">a_post</span><span class="p">,</span> <span class="n">b_post</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="s1">&#39;b--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Prior: Beta(</span><span class="si">{</span><span class="n">a_prior</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">b_prior</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
             <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Posterior: Beta(</span><span class="si">{</span><span class="n">a_post</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">b_post</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">n_drops</span><span class="o">/</span><span class="p">(</span><span class="n">n_drops</span> <span class="o">+</span> <span class="n">n_clean_laps</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="n">n_drops</span> <span class="o">+</span> <span class="n">n_clean_laps</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">0.5</span><span class="p">,</span> 
                <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;MLE: </span><span class="si">{</span><span class="n">n_drops</span><span class="o">/</span><span class="p">(</span><span class="n">n_drops</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">n_clean_laps</span><span class="p">)</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Degradation Rate (p = probability of performance drop per lap)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Bayesian Tire Degradation Inference: </span><span class="si">{</span><span class="n">n_drops</span><span class="si">}</span><span class="s1"> drops in </span><span class="si">{</span><span class="n">n_drops</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">n_clean_laps</span><span class="si">}</span><span class="s1"> laps&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
    <span class="c1"># Posterior statistics</span>
    <span class="n">post_mean</span> <span class="o">=</span> <span class="n">a_post</span> <span class="o">/</span> <span class="p">(</span><span class="n">a_post</span> <span class="o">+</span> <span class="n">b_post</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Posterior mean degradation rate: </span><span class="si">{</span><span class="n">post_mean</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% credible interval: [</span><span class="si">{</span><span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.025</span><span class="p">,</span><span class="w"> </span><span class="n">a_post</span><span class="p">,</span><span class="w"> </span><span class="n">b_post</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.975</span><span class="p">,</span><span class="w"> </span><span class="n">a_post</span><span class="p">,</span><span class="w"> </span><span class="n">b_post</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>

<span class="c1"># Start with uniform prior (no prior knowledge about this tire compound)</span>
<span class="c1"># Observe 7 laps with performance drops, 3 clean laps</span>
<span class="n">plot_beta_posterior</span><span class="p">(</span><span class="n">a_prior</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">b_prior</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_drops</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">n_clean_laps</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># TODO: Experiment with different priors and data</span>
<span class="c1"># What happens with:</span>
<span class="c1"># 1. Strong prior from testing data that tires are durable: Beta(10, 10)</span>
<span class="c1"># 2. More race laps observed: 70 drops, 30 clean laps</span>
<span class="c1"># 3. Prior from testing conflicts with race data (e.g., testing says durable but race says fragile)</span>

<span class="c1"># Your experiments here:</span>
<span class="c1"># Try: What if the team tested extensively and believed degradation was moderate?</span>
<span class="n">plot_beta_posterior</span><span class="p">(</span><span class="n">a_prior</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">b_prior</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_drops</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">n_clean_laps</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-2-implement-softmax-cross-entropy-loss-for-race-outcome-prediction">
<h3>Exercise 2: Implement Softmax Cross-Entropy Loss for Race Outcome Prediction<a class="headerlink" href="#exercise-2-implement-softmax-cross-entropy-loss-for-race-outcome-prediction" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute softmax.&quot;&quot;&quot;</span>
    <span class="n">x_shifted</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">exp_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x_shifted</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">exp_x</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">cross_entropy_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute cross-entropy loss for race outcome prediction.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        logits: Raw model outputs (before softmax), shape (batch_size, num_classes)</span>
<span class="sd">               e.g., scores for [Win, Podium, Points, DNF]</span>
<span class="sd">        labels: True outcome indices, shape (batch_size,)</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        Scalar loss value</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement</span>
    <span class="c1"># 1. Apply softmax to get probabilities</span>
    <span class="c1"># 2. Extract probability of true outcome</span>
    <span class="c1"># 3. Return negative log probability (averaged over batch)</span>
    
    <span class="n">probs</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
    <span class="c1"># Get probability assigned to correct outcome for each race</span>
    <span class="n">correct_probs</span> <span class="o">=</span> <span class="n">probs</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">labels</span><span class="p">]</span>
    <span class="c1"># Negative log likelihood</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">correct_probs</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># Test: Predicting race outcomes for 3 different race weekends</span>
<span class="c1"># Classes: [Win, Podium, Points finish]</span>
<span class="n">race_logits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>   <span class="c1"># Model thinks Win is likely</span>
                        <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>   <span class="c1"># Model thinks Podium is likely</span>
                        <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span>  <span class="c1"># Model thinks Points finish</span>
<span class="n">true_outcomes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>  <span class="c1"># Actual results: Win, Podium, Points</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">race_logits</span><span class="p">,</span> <span class="n">true_outcomes</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Race outcome logits:</span><span class="se">\n</span><span class="si">{</span><span class="n">race_logits</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Softmax probabilities:</span><span class="se">\n</span><span class="si">{</span><span class="n">softmax</span><span class="p">(</span><span class="n">race_logits</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True outcomes: </span><span class="si">{</span><span class="n">true_outcomes</span><span class="si">}</span><span class="s2"> (Win=0, Podium=1, Points=2)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cross-entropy loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-3-information-gain-for-pit-stop-strategy-decisions">
<h3>Exercise 3: Information Gain for Pit Stop Strategy Decisions<a class="headerlink" href="#exercise-3-information-gain-for-pit-stop-strategy-decisions" title="Link to this heading">#</a></h3>
<p>In decision trees, we split data to maximize information gain (reduction in entropy). Here, imagine you’re deciding whether to split race laps into groups based on a feature (e.g., “is it raining?”) to better predict outcomes. The split that gives the highest information gain is the most useful for the decision.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">information_gain</span><span class="p">(</span><span class="n">parent_labels</span><span class="p">,</span> <span class="n">left_labels</span><span class="p">,</span> <span class="n">right_labels</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute information gain from a split.</span>
<span class="sd">    </span>
<span class="sd">    IG = H(parent) - weighted_avg(H(left), H(right))</span>
<span class="sd">    </span>
<span class="sd">    F1 context: Splitting race data by a condition (e.g., wet vs dry)</span>
<span class="sd">    to better predict outcome (e.g., podium vs no podium).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">label_entropy</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute entropy of label distribution.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">0</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">entropy</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
    
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">parent_labels</span><span class="p">)</span>
    <span class="n">n_left</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">left_labels</span><span class="p">)</span>
    <span class="n">n_right</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">right_labels</span><span class="p">)</span>
    
    <span class="n">h_parent</span> <span class="o">=</span> <span class="n">label_entropy</span><span class="p">(</span><span class="n">parent_labels</span><span class="p">)</span>
    <span class="n">h_left</span> <span class="o">=</span> <span class="n">label_entropy</span><span class="p">(</span><span class="n">left_labels</span><span class="p">)</span>
    <span class="n">h_right</span> <span class="o">=</span> <span class="n">label_entropy</span><span class="p">(</span><span class="n">right_labels</span><span class="p">)</span>
    
    <span class="n">weighted_child</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_left</span><span class="o">/</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">h_left</span> <span class="o">+</span> <span class="p">(</span><span class="n">n_right</span><span class="o">/</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">h_right</span>
    
    <span class="k">return</span> <span class="n">h_parent</span> <span class="o">-</span> <span class="n">weighted_child</span>

<span class="c1"># Example: Splitting race results by track condition</span>
<span class="c1"># Parent: mixed results (4 podiums, 6 no-podiums)</span>
<span class="n">race_results</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c1"># 0=no podium, 1=podium</span>

<span class="c1"># Good split: &quot;Is it a dry race?&quot; separates podiums from non-podiums</span>
<span class="n">dry_races</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>        <span class="c1"># Dry: all no-podiums (clear pattern)</span>
<span class="n">wet_races</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c1"># Wet: all podiums (rain specialist!)</span>

<span class="c1"># Bad split: Random grouping that doesn&#39;t separate outcomes</span>
<span class="n">group_a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c1"># Mixed outcomes</span>
<span class="n">group_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c1"># Also mixed</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parent entropy (all races): </span><span class="si">{</span><span class="n">entropy</span><span class="p">([</span><span class="mf">0.4</span><span class="p">,</span><span class="w"> </span><span class="mf">0.6</span><span class="p">])</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> bits&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Good split (dry vs wet): IG = </span><span class="si">{</span><span class="n">information_gain</span><span class="p">(</span><span class="n">race_results</span><span class="p">,</span><span class="w"> </span><span class="n">dry_races</span><span class="p">,</span><span class="w"> </span><span class="n">wet_races</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> bits&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  -&gt; Track condition perfectly separates outcomes!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Bad split (random groups): IG = </span><span class="si">{</span><span class="n">information_gain</span><span class="p">(</span><span class="n">race_results</span><span class="p">,</span><span class="w"> </span><span class="n">group_a</span><span class="p">,</span><span class="w"> </span><span class="n">group_b</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> bits&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  -&gt; No useful separation of outcomes&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<section id="key-concepts-and-their-f1-parallels">
<h3>Key Concepts and Their F1 Parallels<a class="headerlink" href="#key-concepts-and-their-f1-parallels" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Concept</p></th>
<th class="head"><p>What It Does</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Probability Distributions</strong></p></td>
<td><p>Describe likelihood of all outcomes</p></td>
<td><p>Lap time distributions, race outcome probabilities</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Bernoulli</strong></p></td>
<td><p>Models binary yes/no outcomes</p></td>
<td><p>Will the car finish this race?</p></td>
</tr>
<tr class="row-even"><td><p><strong>Gaussian</strong></p></td>
<td><p>Models continuous symmetric uncertainty</p></td>
<td><p>Lap time variation around mean pace</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Bayes’ Theorem</strong></p></td>
<td><p>Updates beliefs given evidence (prior x likelihood = posterior)</p></td>
<td><p>Updating rain probability as clouds form during a race</p></td>
</tr>
<tr class="row-even"><td><p><strong>Maximum Likelihood</strong></p></td>
<td><p>Finds parameters that maximize P(data|params)</p></td>
<td><p>Estimating tire degradation rate from observed lap times</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Entropy</strong></p></td>
<td><p>Measures uncertainty in a distribution</p></td>
<td><p>Season competitiveness — high entropy = anyone can win</p></td>
</tr>
<tr class="row-even"><td><p><strong>Cross-Entropy</strong></p></td>
<td><p>The loss function for classification</p></td>
<td><p>How wrong is your race prediction model?</p></td>
</tr>
<tr class="row-odd"><td><p><strong>KL Divergence</strong></p></td>
<td><p>Measures difference between distributions</p></td>
<td><p>Gap between qualifying pace and race pace</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="connection-to-deep-learning">
<h3>Connection to Deep Learning<a class="headerlink" href="#connection-to-deep-learning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Classification</strong>: Softmax outputs a categorical distribution, trained with cross-entropy</p></li>
<li><p><strong>Regression</strong>: Often assumes Gaussian noise, uses MSE (= MLE for Gaussian)</p></li>
<li><p><strong>VAEs</strong>: Use KL divergence to regularize latent distributions</p></li>
<li><p><strong>Dropout</strong>: Samples from Bernoulli to create masks</p></li>
<li><p><strong>Bayesian NN</strong>: Treat weights as distributions, use Bayes’ theorem</p></li>
</ul>
</section>
<section id="checklist">
<h3>Checklist<a class="headerlink" href="#checklist" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>[ ] I understand common probability distributions (and can map them to race scenarios)</p></li>
<li><p>[ ] I can apply Bayes’ theorem (like updating rain predictions mid-race)</p></li>
<li><p>[ ] I understand MLE and its connection to loss functions (like fitting tire degradation curves)</p></li>
<li><p>[ ] I can compute entropy and KL divergence (like measuring season unpredictability)</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="next-steps">
<h2>Next Steps<a class="headerlink" href="#next-steps" title="Link to this heading">#</a></h2>
<p>Continue to <strong>Part 2.1: Python OOP for ML</strong> (Notebook 04), where we’ll build the programming foundations needed for everything ahead:</p>
<ul class="simple">
<li><p>Classes, inheritance, and magic methods for ML frameworks</p></li>
<li><p>Decorators and context managers</p></li>
<li><p>Building a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> class with operator overloading</p></li>
<li><p>The OOP patterns behind PyTorch’s <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> and scikit-learn’s API</p></li>
</ul>
<p><strong>Looking ahead with F1</strong>: The probability foundations from this notebook power everything ahead. When we build neural networks, softmax + cross-entropy (from this notebook) becomes our classification loss. When we explore VAEs, KL divergence becomes the regularizer. When we study reinforcement learning, Bayes’ theorem helps the agent update its beliefs about the world.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="02_calculus.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Part 1.2: Calculus for Deep Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="04_python_oop.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Part 2.1: Python OOP for Deep Learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-basics">1. Probability Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-variables">Random Variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-distributions">Probability Distributions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-what-is-a-probability-distribution">Deep Dive: What is a Probability Distribution?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-vs-continuous-distributions">Discrete vs Continuous Distributions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-distributions">2. Common Distributions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bernoulli-distribution">2.1 Bernoulli Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binomial-distribution">2.2 Binomial Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-distribution">2.3 Categorical Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-normal-distribution">2.4 Gaussian (Normal) Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-gaussian">2.5 Multivariate Gaussian</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-the-right-distribution-a-decision-guide">Choosing the Right Distribution: A Decision Guide</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-value-and-variance">3. Expected Value and Variance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-value-mean">Expected Value (Mean)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variance">Variance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-understanding-each-term-in-bayes-theorem">Deep Dive: Understanding Each Term in Bayes’ Theorem</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem">4. Bayes’ Theorem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem-in-machine-learning">Bayes’ Theorem in Machine Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-the-intuition-behind-maximum-likelihood">Deep Dive: The Intuition Behind Maximum Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-understanding-entropy">Deep Dive: Understanding Entropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy-reference-table">Entropy Reference Table</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes-classifier">Naive Bayes Classifier</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-understanding-kl-divergence">Deep Dive: Understanding KL Divergence</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kl-divergence-in-machine-learning-applications">KL Divergence in Machine Learning Applications</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation-mle">5. Maximum Likelihood Estimation (MLE)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-for-bernoulli-coin-flip-race-finish">MLE for Bernoulli (Coin Flip / Race Finish)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#information-theory">6. Information Theory</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy">Entropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-entropy">Cross-Entropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kl-divergence">KL Divergence</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-bayesian-tire-degradation-inference">Exercise 1: Bayesian Tire Degradation Inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-implement-softmax-cross-entropy-loss-for-race-outcome-prediction">Exercise 2: Implement Softmax Cross-Entropy Loss for Race Outcome Prediction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-information-gain-for-pit-stop-strategy-decisions">Exercise 3: Information Gain for Pit Stop Strategy Decisions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts-and-their-f1-parallels">Key Concepts and Their F1 Parallels</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-deep-learning">Connection to Deep Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checklist">Checklist</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">Next Steps</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dan Shah
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>