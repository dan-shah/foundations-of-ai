
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Part 3.3: Optimization Theory for Machine Learning &#8212; Foundations of AI</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=9c3e77be" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=1ae7504c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/08_optimization_theory';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Part 4.1: Perceptrons &amp; Basic Networks" href="09_perceptrons_basic_networks.html" />
    <link rel="prev" title="Part 3.2: Optimization &amp; Linear Programming" href="07_optimization_linear_programming.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Foundations of AI</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 1: Mathematical Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_linear_algebra.html">Part 1.1: Linear Algebra for Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_calculus.html">Part 1.2: Calculus for Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_probability_statistics.html">Part 1.3: Probability &amp; Statistics for Deep Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 2: Programming Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_python_oop.html">Part 2.1: Python OOP for Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_numpy_deep_dive.html">Part 2.2: NumPy Deep Dive</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 3: Classical ML &amp; Optimization</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="06_classical_ml.html">Part 3.1: Classical Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_optimization_linear_programming.html">Part 3.2: Optimization &amp; Linear Programming</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Part 3.3: Optimization Theory for Machine Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 4: Neural Network Fundamentals</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="09_perceptrons_basic_networks.html">Part 4.1: Perceptrons &amp; Basic Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_backpropagation.html">Part 4.2: Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_pytorch_fundamentals.html">Part 4.3: PyTorch Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_training_deep_networks.html">Part 4.4: Training Deep Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 5: Neural Network Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="13_convolutional_neural_networks.html">Part 5.1: Convolutional Neural Networks (CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_computer_vision_depth.html">Part 5.2: Computer Vision — Beyond Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_recurrent_neural_networks.html">Part 5.3: Recurrent Neural Networks (RNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_attention_mechanisms.html">Part 5.4: Attention Mechanisms</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 6: Transformers &amp; LLMs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="17_transformer_architecture.html">Part 6.1: Transformer Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_embeddings.html">Part 6.2: Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="19_tokenization_lm_training.html">Part 6.3: Tokenization &amp; Language Model Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="20_language_models.html">Part 6.4: Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="21_finetuning_and_peft.html">Part 6.5: Fine-tuning &amp; PEFT</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 7: Reinforcement Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="22_rl_fundamentals.html">Part 7.1: Reinforcement Learning Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="23_q_learning_dqn.html">Part 7.2: Q-Learning and Deep Q-Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="24_policy_gradients.html">Part 7.3: Policy Gradient Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="25_ppo_modern_rl.html">Part 7.4: PPO and Modern RL</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 8: Applied AI Systems</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="26_rag.html">Part 8.1: Retrieval-Augmented Generation (RAG)</a></li>
<li class="toctree-l1"><a class="reference internal" href="27_ai_agents.html">Part 8.2: AI Agents and Tool Use</a></li>
<li class="toctree-l1"><a class="reference internal" href="28_ai_evals.html">Part 8.3: Evaluating AI Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="29_production_monitoring.html">Part 8.4: Production AI Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 9: Advanced Topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="30_inference_optimization.html">Part 9.1: LLM Inference Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="31_ml_systems.html">Part 9.2: ML Systems &amp; Experiment Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="32_multimodal_ai.html">Part 9.3: Multimodal AI</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/dan-shah/foundations-of-ai/blob/main/notebooks/08_optimization_theory.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/dan-shah/foundations-of-ai" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/dan-shah/foundations-of-ai/edit/main/notebooks/08_optimization_theory.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/dan-shah/foundations-of-ai/issues/new?title=Issue%20on%20page%20%2Fnotebooks/08_optimization_theory.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notebooks/08_optimization_theory.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Part 3.3: Optimization Theory for Machine Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-convergence">1. Gradient Descent Convergence</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuitive-explanation">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-f1-connection">The F1 Connection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-choosing-the-right-setup-change-rate">Deep Dive: Choosing the Right Setup Change Rate</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convexity-and-guarantees">2. Convexity and Guarantees</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">The F1 Connection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-condition-number">Deep Dive: Condition Number</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent">3. Stochastic Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">The F1 Connection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-why-sgd-works-so-well-in-practice">Deep Dive: Why SGD Works So Well in Practice</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bias-variance-tradeoff">4. The Bias-Variance Tradeoff</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">The F1 Connection</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vc-dimension-and-generalization">5. VC Dimension and Generalization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">The F1 Connection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-the-vc-generalization-bound">Deep Dive: The VC Generalization Bound</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#common-misconceptions">Common Misconceptions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pac-learning">6. PAC Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">The F1 Connection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-what-pac-learning-tells-us-about-ml">Deep Dive: What PAC Learning Tells Us About ML</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-theory">7. Regularization Theory</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">The F1 Connection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-regularization-as-bayesian-priors">Deep Dive: Regularization as Bayesian Priors</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#key-insight">Key Insight</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-overparameterized-networks-generalize">8. Why Overparameterized Networks Generalize</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">The F1 Connection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-the-lottery-ticket-hypothesis">Deep Dive: The Lottery Ticket Hypothesis</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#open-questions-honest-assessment">Open Questions (Honest Assessment)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-implement-setup-optimization-with-momentum">Exercise 1: Implement Setup Optimization with Momentum</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-cross-validation-for-lap-time-model-selection">Exercise 2: Cross-Validation for Lap Time Model Selection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-compare-setup-regularization-strategies">Exercise 3: Compare Setup Regularization Strategies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-4-bonus-setup-change-rate-schedules">Exercise 4 (Bonus): Setup Change Rate Schedules</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts">Key Concepts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-machine-learning">Connection to Machine Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checklist">Checklist</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">Next Steps</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="part-3-3-optimization-theory-for-machine-learning">
<h1>Part 3.3: Optimization Theory for Machine Learning<a class="headerlink" href="#part-3-3-optimization-theory-for-machine-learning" title="Link to this heading">#</a></h1>
<p>Every machine learning model learns by optimizing an objective function. Understanding optimization theory
is what separates practitioners who <em>use</em> ML from those who <em>understand</em> it. When your model fails to
converge, when training is painfully slow, or when your network mysteriously generalizes despite having
millions of parameters — optimization theory tells you why.</p>
<p>But let’s make this concrete: <strong>F1 is an optimization sport</strong>. Every race weekend, engineers run through a convergence process — adjusting wing angles, suspension geometry, tire pressures, and differential settings across FP1, FP2, FP3, and qualifying. They’re navigating a high-dimensional performance surface, hunting for the setup that minimizes lap time. Too aggressive a change between sessions and the car becomes undriveable. Too conservative and you leave tenths on the table. That tension — how fast and how far to push — is exactly what optimization theory formalizes.</p>
<p>This notebook covers the theoretical foundations: convergence guarantees, the bias-variance tradeoff,
generalization theory, and regularization. These ideas form the bridge between the math foundations
of Part 1 and the practical deep learning of Parts 3+. Throughout, we’ll use F1 analogies to ground every concept.</p>
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>[ ] Analyze gradient descent convergence rates for convex and strongly convex functions</p></li>
<li><p>[ ] Explain why convexity guarantees global optima and how condition number affects convergence</p></li>
<li><p>[ ] Compare gradient descent vs. stochastic gradient descent and understand mini-batch tradeoffs</p></li>
<li><p>[ ] Derive the bias-variance decomposition and connect it to model selection</p></li>
<li><p>[ ] Understand VC dimension as a measure of model capacity</p></li>
<li><p>[ ] Explain the PAC learning framework and sample complexity</p></li>
<li><p>[ ] Visualize why L1 regularization produces sparsity and L2 produces small weights</p></li>
<li><p>[ ] Discuss why overparameterized networks generalize (double descent, implicit regularization)</p></li>
</ul>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib.patches</span><span class="w"> </span><span class="kn">import</span> <span class="n">Circle</span><span class="p">,</span> <span class="n">FancyArrowPatch</span><span class="p">,</span> <span class="n">Ellipse</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib.colors</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogNorm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">,</span> <span class="n">Lasso</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn-v0_8-whitegrid&#39;</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="gradient-descent-convergence">
<h2>1. Gradient Descent Convergence<a class="headerlink" href="#gradient-descent-convergence" title="Link to this heading">#</a></h2>
<section id="intuitive-explanation">
<h3>Intuitive Explanation<a class="headerlink" href="#intuitive-explanation" title="Link to this heading">#</a></h3>
<p>Gradient descent is the workhorse of machine learning optimization. At each step, we move in the
direction of steepest descent:</p>
<div class="math notranslate nohighlight">
\[x_{t+1} = x_t - \eta \nabla f(x_t)\]</div>
<p>But how fast does it converge? The answer depends on two things:</p>
<ol class="arabic simple">
<li><p><strong>The step size</strong> <span class="math notranslate nohighlight">\(\eta\)</span> (learning rate) — too large and we overshoot, too small and we crawl</p></li>
<li><p><strong>The function’s properties</strong> — convex functions converge faster than non-convex ones</p></li>
</ol>
</section>
<section id="the-f1-connection">
<h3>The F1 Connection<a class="headerlink" href="#the-f1-connection" title="Link to this heading">#</a></h3>
<p>Think of gradient descent as the <strong>setup convergence process across practice sessions</strong>. After FP1, the engineers look at the data (the gradient) and adjust the car. The learning rate is how aggressively they change the setup — too aggressive and the car becomes unpredictable (oscillation/divergence), too conservative and they run out of sessions before finding the sweet spot.</p>
<p>A well-conditioned car (balanced aero, predictable tires) converges to its optimal setup quickly — like a strongly convex function. A poorly conditioned car (unpredictable balance, tire degradation issues) zigzags through setup space like an ill-conditioned optimization problem.</p>
<p><strong>Key convergence rates:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Function Type</p></th>
<th class="head"><p>Rate</p></th>
<th class="head"><p>What It Means</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Convex</p></td>
<td><p><span class="math notranslate nohighlight">\(O(1/t)\)</span></p></td>
<td><p>Error halves every time you double iterations</p></td>
<td><p>Slow but steady setup improvement</p></td>
</tr>
<tr class="row-odd"><td><p>Strongly convex</p></td>
<td><p><span class="math notranslate nohighlight">\(O(e^{-ct})\)</span></p></td>
<td><p>Error decreases exponentially (linear convergence)</p></td>
<td><p>Dialing in a well-understood car</p></td>
</tr>
<tr class="row-even"><td><p>Non-convex</p></td>
<td><p>No guarantee</p></td>
<td><p>May get stuck at local minima or saddle points</p></td>
<td><p>Setup feels “stuck” — fast in some sectors, slow in others</p></td>
</tr>
</tbody>
</table>
</div>
<p>The difference is dramatic: for a strongly convex function, 100 iterations might suffice where a merely
convex function needs 10,000.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">gradient_descent</span><span class="p">(</span><span class="n">grad_f</span><span class="p">,</span> <span class="n">setup0</span><span class="p">,</span> <span class="n">setup_change_rate</span><span class="p">,</span> <span class="n">n_sessions</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gradient descent with convergence tracking.</span>
<span class="sd">    (F1: Iterative setup optimization across practice sessions.)</span>

<span class="sd">    Args:</span>
<span class="sd">        grad_f: Function that returns the gradient at current setup</span>
<span class="sd">        setup0: Initial car setup (numpy array)</span>
<span class="sd">        setup_change_rate: Learning rate — how aggressively engineers change setup</span>
<span class="sd">        n_sessions: Number of practice iterations</span>
<span class="sd">        f: Optional objective function (lap time error) for tracking</span>

<span class="sd">    Returns:</span>
<span class="sd">        history: dict with &#39;x&#39; (setup trajectory), &#39;f&#39; (lap time errors), &#39;grad_norm&#39; (gradient norms)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">setup</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">setup0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">history</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">setup</span><span class="o">.</span><span class="n">copy</span><span class="p">()],</span> <span class="s1">&#39;f&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;grad_norm&#39;</span><span class="p">:</span> <span class="p">[]}</span>

    <span class="k">if</span> <span class="n">f</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">history</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">setup</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sessions</span><span class="p">):</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">grad_f</span><span class="p">(</span><span class="n">setup</span><span class="p">)</span>
        <span class="n">history</span><span class="p">[</span><span class="s1">&#39;grad_norm&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">g</span><span class="p">))</span>
        <span class="n">setup</span> <span class="o">=</span> <span class="n">setup</span> <span class="o">-</span> <span class="n">setup_change_rate</span> <span class="o">*</span> <span class="n">g</span>
        <span class="n">history</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">setup</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">f</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">history</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">setup</span><span class="p">))</span>

    <span class="n">history</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">history</span>

<span class="c1"># Example: quadratic lap time surface f(x) = 0.5 * x^T A x</span>
<span class="c1"># Strongly convex when all eigenvalues of A are positive</span>
<span class="c1"># Think of A as how the car&#39;s lap time responds to setup parameter changes</span>
<span class="n">A_balanced</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]])</span>   <span class="c1"># condition number = 1 (balanced car)</span>
<span class="n">A_tricky</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>    <span class="c1"># condition number = 10 (tricky car)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">make_quadratic</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span> <span class="o">@</span> <span class="n">A</span> <span class="o">@</span> <span class="n">x</span>
    <span class="n">grad_f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">A</span> <span class="o">@</span> <span class="n">x</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">,</span> <span class="n">grad_f</span>

<span class="n">setup0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">])</span>  <span class="c1"># Initial setup (far from optimal)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Balanced car (kappa=1): setup sensitivities =&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvalsh</span><span class="p">(</span><span class="n">A_balanced</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tricky car (kappa=10): setup sensitivities =&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvalsh</span><span class="p">(</span><span class="n">A_tricky</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Balanced car (kappa=1): setup sensitivities = [2. 2.]
Tricky car (kappa=10): setup sensitivities = [ 1. 10.]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize setup convergence for different adjustment aggressiveness</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">lap_time_surface</span><span class="p">,</span> <span class="n">lap_time_gradient</span> <span class="o">=</span> <span class="n">make_quadratic</span><span class="p">(</span><span class="n">A_balanced</span><span class="p">)</span>

<span class="n">setup_change_rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.45</span><span class="p">]</span>
<span class="n">titles</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Too Conservative (rate=0.01)&#39;</span><span class="p">,</span> <span class="s1">&#39;Just Right (rate=0.1)&#39;</span><span class="p">,</span> <span class="s1">&#39;Too Aggressive (rate=0.45)&#39;</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">setup_change_rates</span><span class="p">,</span> <span class="n">titles</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
    <span class="n">hist</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">lap_time_gradient</span><span class="p">,</span> <span class="n">setup0</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">lap_time_surface</span><span class="p">)</span>

    <span class="c1"># Plot lap time contours (performance surface)</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
    <span class="n">zz</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">lap_time_surface</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">xi</span><span class="p">,</span> <span class="n">yi</span><span class="p">]))</span> <span class="k">for</span> <span class="n">xi</span><span class="p">,</span> <span class="n">yi</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">())])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">zz</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">)</span>

    <span class="c1"># Plot setup trajectory across sessions</span>
    <span class="n">traj</span> <span class="o">=</span> <span class="n">hist</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">traj</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">traj</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">traj</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">traj</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;ko&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;FP1 Setup&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;r*&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Optimal Setup&#39;</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Wing Angle Offset&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Suspension Stiffness Offset&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Setup Convergence: How Aggressively Should Engineers Change the Car?&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.02</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7869429e38dfc801f7aa817bd0384732ba9081d2064d8c1e20ec038468710b66.png" src="../_images/7869429e38dfc801f7aa817bd0384732ba9081d2064d8c1e20ec038468710b66.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compare convergence rates: predictable car vs unpredictable car</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Strongly convex: predictable car — setup changes map cleanly to lap time improvement</span>
<span class="n">f_predictable</span><span class="p">,</span> <span class="n">grad_predictable</span> <span class="o">=</span> <span class="n">make_quadratic</span><span class="p">(</span><span class="n">A_balanced</span><span class="p">)</span>
<span class="n">hist_predictable</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">grad_predictable</span><span class="p">,</span> <span class="n">setup0</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">f_predictable</span><span class="p">)</span>

<span class="c1"># Convex but not strongly convex: car with a flat performance plateau</span>
<span class="c1"># Use Huber-like: f(x) = sqrt(x1^2 + 0.01) + 0.5*x2^2</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f_plateau</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.01</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span><span class="w"> </span><span class="nf">grad_plateau</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">g1</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.01</span><span class="p">)</span>
    <span class="n">g2</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">])</span>

<span class="n">hist_plateau</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">grad_plateau</span><span class="p">,</span> <span class="n">setup0</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">f_plateau</span><span class="p">)</span>

<span class="c1"># Plot lap time error convergence</span>
<span class="n">f_star_predictable</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">f_star_plateau</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>  <span class="c1"># minimum value</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">errors_predictable</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">hist_predictable</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">])</span> <span class="o">-</span> <span class="n">f_star_predictable</span>
<span class="n">errors_plateau</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">hist_plateau</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">])</span> <span class="o">-</span> <span class="n">f_star_plateau</span>
<span class="n">ax</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">errors_predictable</span> <span class="o">+</span> <span class="mf">1e-16</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predictable Car ($O(e^{-ct})$)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">errors_plateau</span> <span class="o">+</span> <span class="mf">1e-16</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Plateau Car ($O(1/t)$)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Practice Session&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Lap Time Error $f(x_t) - f^*$ (log scale)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Setup Convergence Rate Comparison&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Plot theoretical rates</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">t</span><span class="p">,</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$O(1/t)$ — Plateau (convex)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">t</span><span class="p">),</span> <span class="s1">&#39;b--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$O(e^{-ct})$ — Predictable (strongly convex)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Practice Session&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Lap Time Error (log scale)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Theoretical Convergence Rates&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After 100 practice sessions:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Predictable car (strongly convex) error: </span><span class="si">{</span><span class="n">errors_predictable</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Plateau car (convex) error:              </span><span class="si">{</span><span class="n">errors_plateau</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/941ae019b3f67a6787b6957cb110dbd17199f8289b093a6a10fdcbd4246b6396.png" src="../_images/941ae019b3f67a6787b6957cb110dbd17199f8289b093a6a10fdcbd4246b6396.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>After 100 practice sessions:
  Predictable car (strongly convex) error: 1.29e-78
  Plateau car (convex) error:              5.00e-02
</pre></div>
</div>
</div>
</div>
</section>
<section id="deep-dive-choosing-the-right-setup-change-rate">
<h3>Deep Dive: Choosing the Right Setup Change Rate<a class="headerlink" href="#deep-dive-choosing-the-right-setup-change-rate" title="Link to this heading">#</a></h3>
<p>The optimal step size depends on the function’s <strong>Lipschitz constant</strong> <span class="math notranslate nohighlight">\(L\)</span> — the maximum curvature of the gradient:</p>
<div class="math notranslate nohighlight">
\[\|\nabla f(x) - \nabla f(y)\| \leq L \|x - y\|\]</div>
<p><strong>The rule:</strong> Set <span class="math notranslate nohighlight">\(\eta \leq 1/L\)</span> for guaranteed convergence.</p>
<p>For a quadratic <span class="math notranslate nohighlight">\(f(x) = \frac{1}{2}x^T A x\)</span>, the Lipschitz constant is <span class="math notranslate nohighlight">\(L = \lambda_{\max}(A)\)</span> (the largest eigenvalue).</p>
<p><strong>F1 analogy:</strong> The Lipschitz constant is like the car’s <strong>setup sensitivity</strong> — how dramatically lap time changes when you adjust a parameter. A car with high aero sensitivity (high <span class="math notranslate nohighlight">\(L\)</span>) requires smaller, more cautious changes. Push too hard and the car snaps into oversteer. A car with low sensitivity allows bolder adjustments.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Setup Change Rate</p></th>
<th class="head"><p>Behavior</p></th>
<th class="head"><p>Convergence</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\eta \ll 1/L\)</span></p></td>
<td><p>Very slow, safe</p></td>
<td><p>Guaranteed but wastes sessions</p></td>
<td><p>Conservative team — solid results but leaves time on the table</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\eta = 1/L\)</span></p></td>
<td><p>Optimal for GD</p></td>
<td><p>Fastest guaranteed convergence</p></td>
<td><p>Top team — pushes right to the limit of what the data says</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\eta &gt; 2/L\)</span></p></td>
<td><p>Oscillates then diverges</p></td>
<td><p>No convergence</p></td>
<td><p>Over-aggressive — car goes from understeer to oversteer every session</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>What this means:</strong> In practice, learning rate is the most important hyperparameter. Too large = training explodes. Too small = training takes forever. Learning rate schedulers (starting large, decreasing over time) try to get the best of both worlds — like an F1 team making big changes in FP1, then fine-tuning through FP3 and qualifying.</p>
</section>
</section>
<hr class="docutils" />
<section id="convexity-and-guarantees">
<h2>2. Convexity and Guarantees<a class="headerlink" href="#convexity-and-guarantees" title="Link to this heading">#</a></h2>
<section id="id1">
<h3>Intuitive Explanation<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>A function is <strong>convex</strong> if every chord lies above the curve — there are no “valleys” to get trapped in. This is the single most important property in optimization because it guarantees that any local minimum is also the <em>global</em> minimum.</p>
<p><strong>Three levels of “niceness”:</strong></p>
<ol class="arabic simple">
<li><p><strong>Non-convex</strong>: Multiple local minima, saddle points. No guarantees. (Most neural network losses)</p></li>
<li><p><strong>Convex</strong>: One global minimum (or a flat region of minima). GD converges at <span class="math notranslate nohighlight">\(O(1/t)\)</span>.</p></li>
<li><p><strong>Strongly convex</strong>: A unique global minimum with curvature bounded away from zero. GD converges exponentially.</p></li>
</ol>
</section>
<section id="id2">
<h3>The F1 Connection<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>Think of the <strong>performance surface</strong> — lap time as a function of all setup parameters. A convex performance surface means there’s one clear optimal setup: every direction you improve in genuinely gets you closer to the best. A non-convex surface means there are <strong>local minima</strong> — setups that feel optimal because small changes make things worse, but a completely different setup philosophy could be faster.</p>
<p>This is exactly what happens at circuits like Monaco vs Monza. At Monza (low downforce, straightforward optimization), the performance surface is nearly convex — there’s one obvious setup direction. At Monaco, the tradeoffs between mechanical grip, low-speed aero, and ride height create a much more complex, non-convex landscape with multiple local optima.</p>
<p><strong>What this means:</strong> Strongly convex is the “easy mode” of optimization. Convex is “medium.” Non-convex is “hard mode” — and it’s where deep learning lives.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize three types of performance surfaces</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>

<span class="c1"># Strongly convex: f(x) = x^2 — like Monza setup optimization</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="c1"># Show chord above curve</span>
<span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mf">1.5</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">],</span> <span class="p">[</span><span class="n">x1</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">x2</span><span class="o">**</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Chord (always above)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;g*&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Optimal setup&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Strongly Convex: $f(x) = x^2$</span><span class="se">\n</span><span class="s1">(Monza — one clear optimum)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Setup Parameter&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Lap Time Penalty&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Convex but not strongly: f(x) = |x| — flat plateau near optimum</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mf">1.5</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">],</span> <span class="p">[</span><span class="nb">abs</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x2</span><span class="p">)],</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Chord (always above)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;g*&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Optimal setup&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Convex: $f(x) = |x|$</span><span class="se">\n</span><span class="s1">(Flat near optimum — hard to fine-tune)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Setup Parameter&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Lap Time Penalty&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Non-convex: f(x) = x^4 - 3x^2 + x — like Monaco with multiple local optima</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">x</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="c1"># Find and mark local minima</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.optimize</span><span class="w"> </span><span class="kn">import</span> <span class="n">minimize_scalar</span>
<span class="n">res1</span> <span class="o">=</span> <span class="n">minimize_scalar</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">**</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">3</span><span class="o">*</span><span class="n">t</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">t</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;bounded&#39;</span><span class="p">)</span>
<span class="n">res2</span> <span class="o">=</span> <span class="n">minimize_scalar</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">**</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">3</span><span class="o">*</span><span class="n">t</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">t</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;bounded&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">res1</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">res1</span><span class="o">.</span><span class="n">fun</span><span class="p">,</span> <span class="s1">&#39;g*&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Global optimum&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">res2</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">res2</span><span class="o">.</span><span class="n">fun</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Local optimum (trap!)&#39;</span><span class="p">)</span>
<span class="c1"># Show chord below curve (non-convex!)</span>
<span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span>
<span class="n">fx1</span> <span class="o">=</span> <span class="n">x1</span><span class="o">**</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">3</span><span class="o">*</span><span class="n">x1</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">x1</span>
<span class="n">fx2</span> <span class="o">=</span> <span class="n">x2</span><span class="o">**</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">3</span><span class="o">*</span><span class="n">x2</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">x2</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">],</span> <span class="p">[</span><span class="n">fx1</span><span class="p">,</span> <span class="n">fx2</span><span class="p">],</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Chord (goes below!)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Non-convex: $f(x) = x^4 - 3x^2 + 0.5x$</span><span class="se">\n</span><span class="s1">(Monaco — multiple setup philosophies)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Setup Parameter&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Lap Time Penalty&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper center&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Performance Surfaces: Why Convexity Matters for Setup Optimization&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.02</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4f9e3e1adcec93232cf2e0ecb8d3a7202a4778ae96a34e4c272a4a0801de66a7.png" src="../_images/4f9e3e1adcec93232cf2e0ecb8d3a7202a4778ae96a34e4c272a4a0801de66a7.png" />
</div>
</div>
</section>
<section id="deep-dive-condition-number">
<h3>Deep Dive: Condition Number<a class="headerlink" href="#deep-dive-condition-number" title="Link to this heading">#</a></h3>
<p>The <strong>condition number</strong> <span class="math notranslate nohighlight">\(\kappa = \lambda_{\max} / \lambda_{\min}\)</span> measures how “stretched” a function’s contours are.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\kappa = 1\)</span>: Perfect circles — GD goes straight to the minimum</p></li>
<li><p><span class="math notranslate nohighlight">\(\kappa \gg 1\)</span>: Elongated ellipses — GD zigzags painfully</p></li>
</ul>
<p><strong>F1 analogy:</strong> The condition number measures how <strong>unbalanced</strong> the car’s sensitivity is across setup dimensions. A well-balanced car (<span class="math notranslate nohighlight">\(\kappa \approx 1\)</span>) responds equally to wing angle and suspension changes — engineers can optimize both simultaneously. A poorly balanced car (<span class="math notranslate nohighlight">\(\kappa \gg 1\)</span>) is extremely sensitive to one parameter (say, ride height) but barely responds to another (say, toe angle). This forces the engineers into a frustrating zigzag: they fix ride height, which throws off the now-sensitive balance, then fix balance, which throws off ride height.</p>
<p><strong>Why it matters in ML:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Situation</p></th>
<th class="head"><p>Effect</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Poorly scaled features</p></td>
<td><p>High condition number, slow training</p></td>
<td><p>Unbalanced car — one parameter dominates</p></td>
</tr>
<tr class="row-odd"><td><p>Batch normalization</p></td>
<td><p>Reduces effective condition number</p></td>
<td><p>Re-balancing the car mid-session</p></td>
</tr>
<tr class="row-even"><td><p>Adam optimizer</p></td>
<td><p>Adapts per-parameter, handles ill-conditioning</p></td>
<td><p>Separate engineers tuning each subsystem</p></td>
</tr>
<tr class="row-odd"><td><p>Feature normalization</p></td>
<td><p>Reduces condition number before training</p></td>
<td><p>Baseline setup that starts balanced</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Key insight:</strong> The convergence rate of GD on strongly convex functions is <span class="math notranslate nohighlight">\(O\left(\left(\frac{\kappa - 1}{\kappa + 1}\right)^t\right)\)</span>. When <span class="math notranslate nohighlight">\(\kappa = 1\)</span>, this is <span class="math notranslate nohighlight">\(0^t\)</span> — instant convergence. When <span class="math notranslate nohighlight">\(\kappa = 100\)</span>, convergence is painfully slow.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize effect of car balance (condition number) on setup convergence paths</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">condition_numbers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">]</span>
<span class="n">titles</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;$\kappa = 1$ (perfectly balanced car)&#39;</span><span class="p">,</span> <span class="s1">&#39;$\kappa = 5$ (moderately unbalanced)&#39;</span><span class="p">,</span> <span class="s1">&#39;$\kappa = 20$ (badly unbalanced)&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">kappa</span><span class="p">,</span> <span class="n">title</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">condition_numbers</span><span class="p">,</span> <span class="n">titles</span><span class="p">):</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">kappa</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>
    <span class="n">f</span><span class="p">,</span> <span class="n">grad_f</span> <span class="o">=</span> <span class="n">make_quadratic</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

    <span class="c1"># Optimal step size: 2 / (lambda_max + lambda_min)</span>
    <span class="n">setup_rate</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">kappa</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="n">hist</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">grad_f</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]),</span> <span class="n">setup_rate</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">f</span><span class="p">)</span>

    <span class="c1"># Performance surface contours</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
    <span class="n">zz</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">f</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">xi</span><span class="p">,</span> <span class="n">yi</span><span class="p">]))</span> <span class="k">for</span> <span class="n">xi</span><span class="p">,</span> <span class="n">yi</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">())])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">zz</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">)</span>

    <span class="c1"># Setup trajectory</span>
    <span class="n">traj</span> <span class="o">=</span> <span class="n">hist</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">traj</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">traj</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;b.-&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">traj</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">traj</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;ko&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;r*&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Wing Angle&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Suspension Stiffness&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Car Balance (Condition Number): The Hidden Cost of Unbalanced Sensitivity&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.02</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sessions to reach lap time error &lt; 0.01:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">kappa</span> <span class="ow">in</span> <span class="n">condition_numbers</span><span class="p">:</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">kappa</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>
    <span class="n">f</span><span class="p">,</span> <span class="n">grad_f</span> <span class="o">=</span> <span class="n">make_quadratic</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="n">setup_rate</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">kappa</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="n">hist</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">grad_f</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]),</span> <span class="n">setup_rate</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">f</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">fval</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">hist</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">fval</span> <span class="o">&lt;</span> <span class="mf">0.01</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  kappa=</span><span class="si">{</span><span class="n">kappa</span><span class="si">:</span><span class="s2">2d</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> sessions&quot;</span><span class="p">)</span>
            <span class="k">break</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  kappa=</span><span class="si">{</span><span class="n">kappa</span><span class="si">:</span><span class="s2">2d</span><span class="si">}</span><span class="s2">: &gt;500 sessions&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;&gt;:5: SyntaxWarning: invalid escape sequence &#39;\k&#39;
&lt;&gt;:5: SyntaxWarning: invalid escape sequence &#39;\k&#39;
&lt;&gt;:5: SyntaxWarning: invalid escape sequence &#39;\k&#39;
&lt;&gt;:5: SyntaxWarning: invalid escape sequence &#39;\k&#39;
&lt;&gt;:5: SyntaxWarning: invalid escape sequence &#39;\k&#39;
&lt;&gt;:5: SyntaxWarning: invalid escape sequence &#39;\k&#39;
/var/folders/l3/qp293kmd0ps5v659jn3njrpc0000gn/T/ipykernel_7266/2788462551.py:5: SyntaxWarning: invalid escape sequence &#39;\k&#39;
  titles = [&#39;$\kappa = 1$ (perfectly balanced car)&#39;, &#39;$\kappa = 5$ (moderately unbalanced)&#39;, &#39;$\kappa = 20$ (badly unbalanced)&#39;]
/var/folders/l3/qp293kmd0ps5v659jn3njrpc0000gn/T/ipykernel_7266/2788462551.py:5: SyntaxWarning: invalid escape sequence &#39;\k&#39;
  titles = [&#39;$\kappa = 1$ (perfectly balanced car)&#39;, &#39;$\kappa = 5$ (moderately unbalanced)&#39;, &#39;$\kappa = 20$ (badly unbalanced)&#39;]
/var/folders/l3/qp293kmd0ps5v659jn3njrpc0000gn/T/ipykernel_7266/2788462551.py:5: SyntaxWarning: invalid escape sequence &#39;\k&#39;
  titles = [&#39;$\kappa = 1$ (perfectly balanced car)&#39;, &#39;$\kappa = 5$ (moderately unbalanced)&#39;, &#39;$\kappa = 20$ (badly unbalanced)&#39;]
</pre></div>
</div>
<img alt="../_images/92c0f2d369d9aed9e8ab2b8c4b91263d31f7282b522111c3af14684c2557f4d5.png" src="../_images/92c0f2d369d9aed9e8ab2b8c4b91263d31f7282b522111c3af14684c2557f4d5.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sessions to reach lap time error &lt; 0.01:
  kappa= 1: 1 sessions
  kappa= 5: 11 sessions
  kappa=20: 49 sessions
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="stochastic-gradient-descent">
<h2>3. Stochastic Gradient Descent<a class="headerlink" href="#stochastic-gradient-descent" title="Link to this heading">#</a></h2>
<section id="id3">
<h3>Intuitive Explanation<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>In machine learning, the objective is usually a sum over data points:</p>
<div class="math notranslate nohighlight">
\[f(w) = \frac{1}{N}\sum_{i=1}^{N} \ell(w; x_i, y_i)\]</div>
<p><strong>Full gradient descent</strong> computes the gradient using <em>all</em> <span class="math notranslate nohighlight">\(N\)</span> data points — expensive when <span class="math notranslate nohighlight">\(N\)</span> is millions.</p>
<p><strong>Stochastic gradient descent (SGD)</strong> approximates the gradient using a single random data point (or a small mini-batch). The gradient estimate is noisy but <em>unbiased</em>:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\nabla \ell(w; x_i, y_i)] = \nabla f(w)\]</div>
</section>
<section id="id4">
<h3>The F1 Connection<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<p>Think of the difference between <strong>learning from individual laps vs. averaging over a full session</strong>:</p>
<ul class="simple">
<li><p><strong>Full GD</strong> = waiting until the end of a full practice session, averaging telemetry across all laps, then making one big setup change. Accurate but slow — you only get 3 practice sessions before qualifying.</p></li>
<li><p><strong>SGD</strong> = making small adjustments after <em>every single lap</em> based on that lap’s data alone. Noisy (one lap might have traffic or a mistake) but you iterate much faster.</p></li>
<li><p><strong>Mini-batch SGD</strong> = analyzing a <strong>stint</strong> (a group of laps on the same tires) — a practical middle ground between one lap and the full session.</p></li>
</ul>
<p><strong>Why stochasticity helps:</strong></p>
<ol class="arabic simple">
<li><p><strong>Computational efficiency</strong>: Each step is <span class="math notranslate nohighlight">\(O(1)\)</span> instead of <span class="math notranslate nohighlight">\(O(N)\)</span></p></li>
<li><p><strong>Escaping saddle points</strong>: Noise helps SGD escape flat regions where GD gets stuck</p></li>
<li><p><strong>Implicit regularization</strong>: The noise acts as a regularizer, improving generalization</p></li>
</ol>
<p><strong>The tradeoff:</strong> SGD steps are cheaper but noisier. Mini-batches balance this — larger batches reduce variance but cost more compute.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">sgd</span><span class="p">(</span><span class="n">lap_grad</span><span class="p">,</span> <span class="n">setup0</span><span class="p">,</span> <span class="n">setup_rate</span><span class="p">,</span> <span class="n">n_sessions</span><span class="p">,</span> <span class="n">n_laps</span><span class="p">,</span> <span class="n">stint_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stochastic gradient descent with mini-batches.</span>
<span class="sd">    (F1: Learning from individual laps or stints instead of full sessions.)</span>

<span class="sd">    Args:</span>
<span class="sd">        lap_grad: Function(setup, lap_indices) returning gradient estimate from a subset of laps</span>
<span class="sd">        setup0: Initial car setup</span>
<span class="sd">        setup_rate: Learning rate (can be a function of session t)</span>
<span class="sd">        n_sessions: Number of iterations</span>
<span class="sd">        n_laps: Total number of laps in the dataset</span>
<span class="sd">        stint_size: Mini-batch size (number of laps per stint)</span>
<span class="sd">        f: Optional full objective function for tracking</span>

<span class="sd">    Returns:</span>
<span class="sd">        history dict with setup trajectory and lap time errors</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">setup</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">setup0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">history</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">setup</span><span class="o">.</span><span class="n">copy</span><span class="p">()],</span> <span class="s1">&#39;f&#39;</span><span class="p">:</span> <span class="p">[]}</span>

    <span class="k">if</span> <span class="n">f</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">history</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">setup</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sessions</span><span class="p">):</span>
        <span class="c1"># Sample a stint (mini-batch of laps)</span>
        <span class="n">lap_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_laps</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">stint_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Setup change rate schedule</span>
        <span class="n">current_rate</span> <span class="o">=</span> <span class="n">setup_rate</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">if</span> <span class="nb">callable</span><span class="p">(</span><span class="n">setup_rate</span><span class="p">)</span> <span class="k">else</span> <span class="n">setup_rate</span>

        <span class="n">g</span> <span class="o">=</span> <span class="n">lap_grad</span><span class="p">(</span><span class="n">setup</span><span class="p">,</span> <span class="n">lap_indices</span><span class="p">)</span>
        <span class="n">setup</span> <span class="o">=</span> <span class="n">setup</span> <span class="o">-</span> <span class="n">current_rate</span> <span class="o">*</span> <span class="n">g</span>
        <span class="n">history</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">setup</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">f</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">history</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">setup</span><span class="p">))</span>

    <span class="n">history</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">history</span>

<span class="c1"># Create a synthetic lap time prediction problem: f(w) = (1/N) sum_i (w^T x_i - y_i)^2</span>
<span class="c1"># Each data point is a lap with telemetry features, and we&#39;re predicting lap time</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">200</span>  <span class="c1"># Total laps in dataset</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">2</span>    <span class="c1"># Setup dimensions (e.g., wing angle, suspension stiffness)</span>
<span class="n">X_telemetry</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">setup_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">])</span>  <span class="c1"># True optimal setup mapping</span>
<span class="n">y_laptimes</span> <span class="o">=</span> <span class="n">X_telemetry</span> <span class="o">@</span> <span class="n">setup_true</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">full_lap_error</span><span class="p">(</span><span class="n">setup</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Lap time error averaged over all laps (full session analysis).&quot;&quot;&quot;</span>
    <span class="n">residuals</span> <span class="o">=</span> <span class="n">X_telemetry</span> <span class="o">@</span> <span class="n">setup</span> <span class="o">-</span> <span class="n">y_laptimes</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">residuals</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">full_grad</span><span class="p">(</span><span class="n">setup</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gradient from all laps (full session).&quot;&quot;&quot;</span>
    <span class="n">residuals</span> <span class="o">=</span> <span class="n">X_telemetry</span> <span class="o">@</span> <span class="n">setup</span> <span class="o">-</span> <span class="n">y_laptimes</span>
    <span class="k">return</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">X_telemetry</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">residuals</span> <span class="o">/</span> <span class="n">N</span>

<span class="k">def</span><span class="w"> </span><span class="nf">stint_grad</span><span class="p">(</span><span class="n">setup</span><span class="p">,</span> <span class="n">lap_indices</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gradient from a stint (subset of laps).&quot;&quot;&quot;</span>
    <span class="n">X_stint</span> <span class="o">=</span> <span class="n">X_telemetry</span><span class="p">[</span><span class="n">lap_indices</span><span class="p">]</span>
    <span class="n">y_stint</span> <span class="o">=</span> <span class="n">y_laptimes</span><span class="p">[</span><span class="n">lap_indices</span><span class="p">]</span>
    <span class="n">residuals</span> <span class="o">=</span> <span class="n">X_stint</span> <span class="o">@</span> <span class="n">setup</span> <span class="o">-</span> <span class="n">y_stint</span>
    <span class="k">return</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">X_stint</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">residuals</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">lap_indices</span><span class="p">)</span>

<span class="n">setup_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>  <span class="c1"># Starting from baseline setup</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True optimal setup: </span><span class="si">{</span><span class="n">setup_true</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Initial lap time error: </span><span class="si">{</span><span class="n">full_lap_error</span><span class="p">(</span><span class="n">setup_init</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True optimal setup: [ 3. -1.]
Initial lap time error: 9.4693
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compare Full Session Analysis vs Single-Lap vs Stint-Based Learning</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_sessions</span> <span class="o">=</span> <span class="mi">200</span>

<span class="c1"># Full session GD (analyze all laps, then adjust)</span>
<span class="n">hist_full</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">full_grad</span><span class="p">,</span> <span class="n">setup_init</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">n_sessions</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">full_lap_error</span><span class="p">)</span>

<span class="c1"># Single-lap SGD (adjust after each lap — noisy but fast)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">hist_single_lap</span> <span class="o">=</span> <span class="n">sgd</span><span class="p">(</span><span class="n">stint_grad</span><span class="p">,</span> <span class="n">setup_init</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">n_sessions</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">stint_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">full_lap_error</span><span class="p">)</span>

<span class="c1"># Stint-based SGD (analyze a stint of 32 laps, then adjust)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">hist_stint</span> <span class="o">=</span> <span class="n">sgd</span><span class="p">(</span><span class="n">stint_grad</span><span class="p">,</span> <span class="n">setup_init</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="n">n_sessions</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">stint_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">full_lap_error</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Lap time error curves</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">hist_full</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">],</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Full Session GD&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">hist_single_lap</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">],</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Single-Lap SGD (stint=1)&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">hist_stint</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">],</span> <span class="s1">&#39;g-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Stint-Based SGD (stint=32)&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Lap Time Error (log scale)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Setup Convergence: Full Session vs Stint vs Single Lap&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Trajectories in setup space</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hist_full</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">hist_full</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Full Session GD&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hist_single_lap</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">hist_single_lap</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Single-Lap SGD&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hist_stint</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">hist_stint</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;g-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Stint-Based SGD&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">setup_true</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">setup_true</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;k*&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True optimal setup&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">setup_init</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">setup_init</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;ko&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Baseline setup&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Wing Angle&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Suspension Stiffness&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Setup Trajectories: Navigating the Performance Surface&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Final setups found:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Full Session:   [</span><span class="si">{</span><span class="n">hist_full</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">hist_full</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Single-Lap:     [</span><span class="si">{</span><span class="n">hist_single_lap</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">hist_single_lap</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Stint (B=32):   [</span><span class="si">{</span><span class="n">hist_stint</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">hist_stint</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  True optimal:   [</span><span class="si">{</span><span class="n">setup_true</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">setup_true</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/5128414c35513409a647b12acc567e71bdfbac25277b8d265132a1b17d934eaf.png" src="../_images/5128414c35513409a647b12acc567e71bdfbac25277b8d265132a1b17d934eaf.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Final setups found:
  Full Session:   [3.0196, -0.9871]
  Single-Lap:     [2.9759, -0.9666]
  Stint (B=32):   [3.0187, -0.9944]
  True optimal:   [3.0000, -1.0000]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Stint size tradeoff: how many laps to analyze before adjusting setup</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">stint_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">N</span><span class="p">]</span>
<span class="n">n_sessions_per</span> <span class="o">=</span> <span class="mi">300</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Adjust setup change rates for each stint size</span>
<span class="n">rate_map</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="mf">0.005</span><span class="p">,</span> <span class="mi">4</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mi">16</span><span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mi">64</span><span class="p">:</span> <span class="mf">0.04</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">}</span>

<span class="k">for</span> <span class="n">bs</span> <span class="ow">in</span> <span class="n">stint_sizes</span><span class="p">:</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">rate</span> <span class="o">=</span> <span class="n">rate_map</span><span class="p">[</span><span class="n">bs</span><span class="p">]</span>
    <span class="n">label</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Stint=</span><span class="si">{</span><span class="n">bs</span><span class="si">}</span><span class="s1"> laps&#39;</span> <span class="k">if</span> <span class="n">bs</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="k">else</span> <span class="sa">f</span><span class="s1">&#39;Stint=</span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1"> (Full Session GD)&#39;</span>
    <span class="k">if</span> <span class="n">bs</span> <span class="o">==</span> <span class="n">N</span><span class="p">:</span>
        <span class="n">hist</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">full_grad</span><span class="p">,</span> <span class="n">setup_init</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">n_sessions_per</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">full_lap_error</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">hist</span> <span class="o">=</span> <span class="n">sgd</span><span class="p">(</span><span class="n">stint_grad</span><span class="p">,</span> <span class="n">setup_init</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">n_sessions_per</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">stint_size</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">full_lap_error</span><span class="p">)</span>

    <span class="c1"># Smooth the SGD curves for visibility</span>
    <span class="n">fvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">hist</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">bs</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">:</span>
        <span class="c1"># Moving average</span>
        <span class="n">window</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="n">fvals_smooth</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">fvals</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">window</span><span class="p">)</span><span class="o">/</span><span class="n">window</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">fvals_smooth</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">fvals</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Lap Time Error (log scale, smoothed)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Convergence vs Stint Size (Laps Analyzed Per Update)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Variance of gradient estimates across stints</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">stint_range</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span>
<span class="n">grad_variances</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">setup_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>

<span class="k">for</span> <span class="n">bs</span> <span class="ow">in</span> <span class="n">stint_range</span><span class="p">:</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">stint_grad</span><span class="p">(</span><span class="n">setup_test</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
        <span class="n">grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
    <span class="n">grad_variances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">stint_range</span><span class="p">,</span> <span class="n">grad_variances</span><span class="p">,</span> <span class="s1">&#39;bo-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">stint_range</span><span class="p">,</span> <span class="n">grad_variances</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">stint_range</span><span class="p">),</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$O(1/B)$ scaling&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Stint Size (Laps per Update)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Gradient Variance&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Noise vs Stint Size: Longer Stints = Less Noise&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Key insight: Gradient variance scales as O(1/B)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Doubling stint size halves the noise but doubles compute per update&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/eef803f24a2ea69fd04d3190c53ecfaf8228b34c03720ea569e9ed597527e021.png" src="../_images/eef803f24a2ea69fd04d3190c53ecfaf8228b34c03720ea569e9ed597527e021.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Key insight: Gradient variance scales as O(1/B)
Doubling stint size halves the noise but doubles compute per update
</pre></div>
</div>
</div>
</div>
</section>
<section id="deep-dive-why-sgd-works-so-well-in-practice">
<h3>Deep Dive: Why SGD Works So Well in Practice<a class="headerlink" href="#deep-dive-why-sgd-works-so-well-in-practice" title="Link to this heading">#</a></h3>
<p>SGD’s noise is not just a computational shortcut — it’s a <em>feature</em>:</p>
<ol class="arabic simple">
<li><p><strong>Escaping sharp minima</strong>: SGD’s noise bounces it out of sharp, narrow minima and into flatter ones. Flat minima tend to generalize better (the loss doesn’t change much if weights shift slightly).</p></li>
<li><p><strong>Implicit regularization</strong>: SGD with a finite learning rate implicitly prefers solutions with smaller norms. This acts like free regularization.</p></li>
<li><p><strong>Exploration</strong>: Early in training, large noise helps explore the loss landscape. Late in training, we want less noise — hence learning rate decay.</p></li>
</ol>
<p><strong>F1 analogy:</strong> Think of SGD’s noise as <strong>changing track conditions</strong> — wind gusts, rubber buildup, fuel load changes. A setup that works under all those perturbations (flat minimum) will be more robust on race day than a setup that’s perfect in one specific condition but falls apart otherwise. Teams that learn from noisy lap data build more robust setups than teams that only look at their one perfect qualifying simulation lap.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Technique</p></th>
<th class="head"><p>Purpose</p></th>
<th class="head"><p>Effect</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Learning rate warmup</p></td>
<td><p>Avoid early instability</p></td>
<td><p>Start small, increase to target LR</p></td>
<td><p>Installation laps before pushing</p></td>
</tr>
<tr class="row-odd"><td><p>Learning rate decay</p></td>
<td><p>Converge precisely</p></td>
<td><p>Reduce noise late in training</p></td>
<td><p>Fine-tuning in FP3 after big FP1 changes</p></td>
</tr>
<tr class="row-even"><td><p>Momentum</p></td>
<td><p>Smooth out noise</p></td>
<td><p>Average gradients over time</p></td>
<td><p>Keeping changes in the direction that’s been working</p></td>
</tr>
<tr class="row-odd"><td><p>Adam</p></td>
<td><p>Adapt per-parameter</p></td>
<td><p>Handle different scales automatically</p></td>
<td><p>Different engineers tuning each subsystem independently</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Common misconception:</strong> “SGD is just a noisy version of GD.” <em>Reality:</em> SGD finds qualitatively different (often better) solutions than GD because the noise structure matters.</p>
</section>
</section>
<hr class="docutils" />
<section id="the-bias-variance-tradeoff">
<h2>4. The Bias-Variance Tradeoff<a class="headerlink" href="#the-bias-variance-tradeoff" title="Link to this heading">#</a></h2>
<section id="id5">
<h3>Intuitive Explanation<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<p>When we train a model, we want it to generalize — to perform well on <em>new</em> data, not just the training data. The <strong>bias-variance decomposition</strong> tells us exactly what can go wrong:</p>
<div class="math notranslate nohighlight">
\[\text{Expected Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Noise}\]</div>
</section>
<section id="id6">
<h3>The F1 Connection<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<p>This tradeoff shows up directly in car setup philosophy:</p>
<ul class="simple">
<li><p><strong>High bias (conservative setup)</strong>: A setup that works reasonably everywhere — decent in corners, decent on straights, decent in the wet. It never embarrasses you, but it never excels either. This is the midfield “safe” approach.</p></li>
<li><p><strong>High variance (specialized setup)</strong>: A setup optimized for one specific sector or condition. Blazing fast through Maggotts-Becketts at Silverstone, but a handful through Copse. Different practice sessions give wildly different results.</p></li>
<li><p><strong>Irreducible noise</strong>: Track conditions you can’t control — wind, temperature, safety car timing. No setup eliminates this.</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Meaning</p></th>
<th class="head"><p>Caused By</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Bias</strong><span class="math notranslate nohighlight">\(^2\)</span></p></td>
<td><p>How far off the average prediction is from truth</p></td>
<td><p>Model too simple (underfitting)</p></td>
<td><p>Conservative setup — safe but slow</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Variance</strong></p></td>
<td><p>How much predictions change across different training sets</p></td>
<td><p>Model too complex (overfitting)</p></td>
<td><p>Specialized setup — fast in practice, unpredictable in race</p></td>
</tr>
<tr class="row-even"><td><p><strong>Irreducible noise</strong></p></td>
<td><p>Inherent randomness in the data</p></td>
<td><p>Nothing — this is the floor</p></td>
<td><p>Weather, safety cars, track evolution</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>What this means:</strong> You can’t minimize both bias and variance simultaneously. Simple models have high bias but low variance. Complex models have low bias but high variance. The sweet spot is in the middle — like a setup that’s optimized but still robust.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Demonstrate bias-variance tradeoff with polynomial lap time models</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># True lap time function (what we&#39;re trying to predict)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">true_laptime</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Generate multiple race weekends and fit models of different complexity</span>
<span class="n">n_laps_train</span> <span class="o">=</span> <span class="mi">25</span>
<span class="n">n_weekends</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">noise_std</span> <span class="o">=</span> <span class="mf">0.4</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">true_laptime</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>

<span class="c1"># Model complexity: from simple (linear) to complex (high-degree polynomial)</span>
<span class="n">complexities</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">15</span><span class="p">]</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="p">{</span><span class="n">d</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">complexities</span><span class="p">}</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_weekends</span><span class="p">):</span>
    <span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">n_laps_train</span><span class="p">)</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">true_laptime</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span> <span class="o">+</span> <span class="n">noise_std</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_laps_train</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">complexities</span><span class="p">:</span>
        <span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
        <span class="n">X_train_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">X_test_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_poly</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_poly</span><span class="p">)</span>
        <span class="n">predictions</span><span class="p">[</span><span class="n">d</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># Compute bias^2 and variance</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">complexities</span><span class="p">[:</span><span class="mi">5</span><span class="p">]):</span>
    <span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="nb">divmod</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="n">col</span><span class="p">]</span>

    <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="n">d</span><span class="p">])</span>
    <span class="n">mean_pred</span> <span class="o">=</span> <span class="n">preds</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Plot predictions from individual weekends (sample of 20)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_weekends</span><span class="p">)):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">preds</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True lap time curve&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">mean_pred</span><span class="p">,</span> <span class="s1">&#39;g--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Average prediction&#39;</span><span class="p">)</span>

    <span class="n">bias_sq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">mean_pred</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Complexity </span><span class="si">{</span><span class="n">d</span><span class="si">}</span><span class="s1">: Bias²=</span><span class="si">{</span><span class="n">bias_sq</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">, Var=</span><span class="si">{</span><span class="n">variance</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Track Position&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Lap Time Delta&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Use last subplot for the decomposition summary</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span>
<span class="n">biases_sq</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">variances</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">complexities</span><span class="p">:</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="n">d</span><span class="p">])</span>
    <span class="n">mean_pred</span> <span class="o">=</span> <span class="n">preds</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">biases_sq</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">mean_pred</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">variances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span>

<span class="n">total</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">biases_sq</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">variances</span><span class="p">)</span> <span class="o">+</span> <span class="n">noise_std</span><span class="o">**</span><span class="mi">2</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">complexities</span><span class="p">,</span> <span class="n">biases_sq</span><span class="p">,</span> <span class="s1">&#39;bo-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Bias² (underfitting)&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">complexities</span><span class="p">,</span> <span class="n">variances</span><span class="p">,</span> <span class="s1">&#39;rs-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Variance (overfitting)&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">complexities</span><span class="p">,</span> <span class="n">total</span><span class="p">,</span> <span class="s1">&#39;g^-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Total Error&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">noise_std</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Track noise floor (</span><span class="si">{</span><span class="n">noise_std</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Model Complexity (Polynomial Degree)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Prediction Error&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Bias-Variance Decomposition&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Bias-Variance Tradeoff: Setup Complexity vs Race-Day Consistency&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.02</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7658be14b0d87436b293509df5d99e40b0b182d32e32e3565a16c6d315bb7c12.png" src="../_images/7658be14b0d87436b293509df5d99e40b0b182d32e32e3565a16c6d315bb7c12.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The classic U-shaped race performance curve</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_practice_laps</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">n_race_laps</span> <span class="o">=</span> <span class="mi">200</span>

<span class="n">x_practice</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">n_practice_laps</span><span class="p">)</span>
<span class="n">y_practice</span> <span class="o">=</span> <span class="n">true_laptime</span><span class="p">(</span><span class="n">x_practice</span><span class="p">)</span> <span class="o">+</span> <span class="n">noise_std</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_practice_laps</span><span class="p">)</span>
<span class="n">x_race</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">n_race_laps</span><span class="p">)</span>
<span class="n">y_race</span> <span class="o">=</span> <span class="n">true_laptime</span><span class="p">(</span><span class="n">x_race</span><span class="p">)</span> <span class="o">+</span> <span class="n">noise_std</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_race_laps</span><span class="p">)</span>

<span class="n">complexities_range</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">practice_errors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">race_errors</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">complexities_range</span><span class="p">:</span>
    <span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">X_pr</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_practice</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">X_rc</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x_race</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_pr</span><span class="p">,</span> <span class="n">y_practice</span><span class="p">)</span>

    <span class="n">practice_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_pr</span><span class="p">)</span> <span class="o">-</span> <span class="n">y_practice</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">race_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_rc</span><span class="p">)</span> <span class="o">-</span> <span class="n">y_race</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">complexities_range</span><span class="p">),</span> <span class="n">practice_errors</span><span class="p">,</span> <span class="s1">&#39;b-o&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Practice Error (training)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">complexities_range</span><span class="p">),</span> <span class="n">race_errors</span><span class="p">,</span> <span class="s1">&#39;r-s&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Race Error (test)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">noise_std</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Irreducible track noise (</span><span class="si">{</span><span class="n">noise_std</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>

<span class="c1"># Mark the sweet spot</span>
<span class="n">best_complexity</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">complexities_range</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">race_errors</span><span class="p">)]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">best_complexity</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Best complexity (degree=</span><span class="si">{</span><span class="n">best_complexity</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>

<span class="c1"># Annotations</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;Underfitting</span><span class="se">\n</span><span class="s1">(too conservative)&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">race_errors</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span>
            <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">3.5</span><span class="p">,</span> <span class="n">race_errors</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mf">0.3</span><span class="p">),</span> <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;Overfitting</span><span class="se">\n</span><span class="s1">(over-specialized)&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">race_errors</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">:])</span><span class="o">+</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span>
            <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">race_errors</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">:])</span><span class="o">+</span><span class="mf">1.0</span><span class="p">),</span> <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Model Complexity (Setup Parameters)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Squared Error&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;The U-Shaped Curve: Practice Performance vs Race-Day Results&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">race_errors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best model complexity: degree </span><span class="si">{</span><span class="n">best_complexity</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Practice error at best: </span><span class="si">{</span><span class="n">practice_errors</span><span class="p">[</span><span class="n">best_complexity</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Race error at best:     </span><span class="si">{</span><span class="n">race_errors</span><span class="p">[</span><span class="n">best_complexity</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/91b30e86c67ced416d2e7ec9435e08c2fe055d3ddb41f5c5035adaea759ed119.png" src="../_images/91b30e86c67ced416d2e7ec9435e08c2fe055d3ddb41f5c5035adaea759ed119.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best model complexity: degree 10
Practice error at best: 0.0618
Race error at best:     0.2496
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="vc-dimension-and-generalization">
<h2>5. VC Dimension and Generalization<a class="headerlink" href="#vc-dimension-and-generalization" title="Link to this heading">#</a></h2>
<section id="id7">
<h3>Intuitive Explanation<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<p>How do we measure a model’s <strong>capacity</strong> — its ability to fit arbitrary patterns? The <strong>Vapnik-Chervonenkis (VC) dimension</strong> provides a formal answer.</p>
<p><strong>Definition:</strong> The VC dimension of a model class is the largest number of points it can <strong>shatter</strong> — i.e., classify correctly for <em>every possible</em> labeling of those points.</p>
</section>
<section id="id8">
<h3>The F1 Connection<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<p>Think of VC dimension as the <strong>complexity needed to predict lap times</strong>:</p>
<ul class="simple">
<li><p><strong>Low VC dimension (simple model)</strong>: Predict lap time using only track length. This “model” can rank Monza vs Monaco, but can’t capture the detail of individual corners.</p></li>
<li><p><strong>High VC dimension (complex model)</strong>: Predict lap time using every corner’s radius, camber, elevation change, surface roughness, and ambient temperature. This model can memorize any set of lap times perfectly, but it might not generalize to a new circuit.</p></li>
</ul>
<p><strong>Examples:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>VC Dimension</p></th>
<th class="head"><p>Intuition</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Linear classifier in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(d + 1\)</span></p></td>
<td><p>Can shatter <span class="math notranslate nohighlight">\(d+1\)</span> points in general position</p></td>
<td><p>Predicting from <span class="math notranslate nohighlight">\(d+1\)</span> track features</p></td>
</tr>
<tr class="row-odd"><td><p>Linear classifier in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span></p></td>
<td><p>3</p></td>
<td><p>Can shatter any 3 non-collinear points</p></td>
<td><p>Two-feature model (track length + avg speed)</p></td>
</tr>
<tr class="row-even"><td><p>Polynomial of degree <span class="math notranslate nohighlight">\(k\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(k + 1\)</span></p></td>
<td><p>Can memorize <span class="math notranslate nohighlight">\(k+1\)</span> points exactly</p></td>
<td><p>Adding more telemetry channels</p></td>
</tr>
<tr class="row-odd"><td><p>Neural net with <span class="math notranslate nohighlight">\(W\)</span> weights</p></td>
<td><p><span class="math notranslate nohighlight">\(O(W \log W)\)</span></p></td>
<td><p>Roughly proportional to parameter count</p></td>
<td><p>Full simulation model</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(k\)</span>-nearest neighbors</p></td>
<td><p><span class="math notranslate nohighlight">\(\infty\)</span></p></td>
<td><p>Can memorize any dataset (but doesn’t generalize!)</p></td>
<td><p>“Just copy last year’s setup” — perfect memory, poor transfer</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>What this means:</strong> Higher VC dimension = more expressive model = needs more data to generalize reliably.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize shattering: can a simple model classify any labeling of circuits?</span>
<span class="c1"># A linear classifier in R^2 can shatter 3 points (circuits) but not 4</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># 3 circuits in 2D feature space (e.g., track length vs avg corner speed)</span>
<span class="n">circuits_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="n">labelings_3</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">labelings_3</span><span class="p">[:</span><span class="mi">4</span><span class="p">])):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">pt</span><span class="p">,</span> <span class="n">lab</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">circuits_3</span><span class="p">,</span> <span class="n">labels</span><span class="p">)):</span>
        <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span> <span class="k">if</span> <span class="n">lab</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="s1">&#39;red&#39;</span>
        <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span> <span class="k">if</span> <span class="n">lab</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="s1">&#39;x&#39;</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pt</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pt</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Labeling </span><span class="si">{</span><span class="n">labels</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;VC dim = 3</span><span class="se">\n</span><span class="s1">(3 circuits shattered)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">][:</span><span class="mi">4</span><span class="p">],</span> <span class="n">labelings_3</span><span class="p">[</span><span class="mi">4</span><span class="p">:])):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">pt</span><span class="p">,</span> <span class="n">lab</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">circuits_3</span><span class="p">,</span> <span class="n">labels</span><span class="p">)):</span>
        <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span> <span class="k">if</span> <span class="n">lab</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="s1">&#39;red&#39;</span>
        <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span> <span class="k">if</span> <span class="n">lab</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="s1">&#39;x&#39;</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pt</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pt</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Labeling </span><span class="si">{</span><span class="n">labels</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;All 8 labelings</span><span class="se">\n</span><span class="s1">separable by a line!&#39;</span><span class="p">,</span>
                      <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span>
                      <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;VC Dimension: Shattering 3 Circuits with a Linear Classifier&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.02</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;A linear classifier in R^2 has VC dimension = 3&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;It can shatter (perfectly classify) any 3 circuits in general position&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;But there exist 4 circuits (e.g., XOR-like pattern) that NO line can shatter&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/var/folders/l3/qp293kmd0ps5v659jn3njrpc0000gn/T/ipykernel_7266/2715062849.py:17: UserWarning: You passed a edgecolor/edgecolors (&#39;black&#39;) for an unfilled marker (&#39;x&#39;).  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
  ax.scatter(pt[0], pt[1], c=color, marker=marker, s=150, zorder=5, edgecolors=&#39;black&#39;, linewidths=1)
/var/folders/l3/qp293kmd0ps5v659jn3njrpc0000gn/T/ipykernel_7266/2715062849.py:31: UserWarning: You passed a edgecolor/edgecolors (&#39;black&#39;) for an unfilled marker (&#39;x&#39;).  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
  ax.scatter(pt[0], pt[1], c=color, marker=marker, s=150, zorder=5, edgecolors=&#39;black&#39;, linewidths=1)
</pre></div>
</div>
<img alt="../_images/e56f83a8b91a42e2dbfd6cc019f74700e610630d472f0a81025806bcc1b7ddd6.png" src="../_images/e56f83a8b91a42e2dbfd6cc019f74700e610630d472f0a81025806bcc1b7ddd6.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A linear classifier in R^2 has VC dimension = 3
It can shatter (perfectly classify) any 3 circuits in general position
But there exist 4 circuits (e.g., XOR-like pattern) that NO line can shatter
</pre></div>
</div>
</div>
</div>
</section>
<section id="deep-dive-the-vc-generalization-bound">
<h3>Deep Dive: The VC Generalization Bound<a class="headerlink" href="#deep-dive-the-vc-generalization-bound" title="Link to this heading">#</a></h3>
<p>The VC dimension connects model complexity to generalization through a fundamental bound:</p>
<div class="math notranslate nohighlight">
\[\text{Test Error} \leq \text{Training Error} + O\left(\sqrt{\frac{d_{VC} \log(N/d_{VC})}{N}}\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(d_{VC}\)</span> is the VC dimension and <span class="math notranslate nohighlight">\(N\)</span> is the number of training samples.</p>
<p><strong>What this tells us:</strong></p>
<ul class="simple">
<li><p>The gap between training and test error grows with <span class="math notranslate nohighlight">\(d_{VC}\)</span> (model complexity)</p></li>
<li><p>The gap shrinks with <span class="math notranslate nohighlight">\(N\)</span> (more data helps)</p></li>
<li><p>To keep the gap small: need <span class="math notranslate nohighlight">\(N \gg d_{VC}\)</span></p></li>
</ul>
<p><strong>F1 analogy:</strong> This bound tells you how much data you need to trust your lap time model. If your model uses 50 setup parameters (<span class="math notranslate nohighlight">\(d_{VC} \approx 50\)</span>), you need hundreds of laps before you can trust that practice performance predicts race performance. If you use a simple 3-parameter model, a short stint might suffice — but it won’t capture the full picture.</p>
<p><strong>Key insight:</strong> This is the theoretical justification for the rule of thumb “you need at least 10x more data points than parameters.”</p>
<section id="common-misconceptions">
<h4>Common Misconceptions<a class="headerlink" href="#common-misconceptions" title="Link to this heading">#</a></h4>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Misconception</p></th>
<th class="head"><p>Reality</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>“Low VC dim = good model”</p></td>
<td><p>Low VC dim means simple model — might underfit</p></td>
</tr>
<tr class="row-odd"><td><p>“VC bounds are tight”</p></td>
<td><p>VC bounds are usually very loose; they give qualitative, not quantitative guidance</p></td>
</tr>
<tr class="row-even"><td><p>“More parameters = higher VC dim always”</p></td>
<td><p>Regularization can effectively reduce VC dimension</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
</section>
<hr class="docutils" />
<section id="pac-learning">
<h2>6. PAC Learning<a class="headerlink" href="#pac-learning" title="Link to this heading">#</a></h2>
<section id="id9">
<h3>Intuitive Explanation<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<p><strong>Probably Approximately Correct (PAC)</strong> learning asks: “How much data do we need to <em>probably</em> find a model that is <em>approximately</em> correct?”</p>
<p>More precisely, for given:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> = accuracy tolerance (“approximately correct” — error &lt; <span class="math notranslate nohighlight">\(\epsilon\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta\)</span> = confidence tolerance (“probably” — succeed with probability <span class="math notranslate nohighlight">\(\geq 1 - \delta\)</span>)</p></li>
</ul>
<p>PAC learning tells us the <strong>sample complexity</strong>: the minimum number of training examples <span class="math notranslate nohighlight">\(N\)</span> needed.</p>
</section>
<section id="id10">
<h3>The F1 Connection<a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<p>PAC learning answers the question every race engineer asks: <strong>“How many laps do we need to be confident in this setup?”</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> = how close to the true optimal lap time we need to get (say, within 0.1s)</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta\)</span> = the probability we’re wrong (say, 5% chance the setup is actually bad)</p></li>
<li><p><span class="math notranslate nohighlight">\(|\mathcal{H}|\)</span> = how many different setup configurations we’re considering</p></li>
</ul>
<p>If you’re choosing between 10 wing angles, you need fewer laps than if you’re choosing among 1000 full setup permutations. PAC theory quantifies this precisely.</p>
<p><strong>For a finite hypothesis class</strong> <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> with <span class="math notranslate nohighlight">\(|\mathcal{H}|\)</span> hypotheses:</p>
<div class="math notranslate nohighlight">
\[N \geq \frac{1}{\epsilon}\left(\ln |\mathcal{H}| + \ln \frac{1}{\delta}\right)\]</div>
<p><strong>What this means in plain English:</strong></p>
<ul class="simple">
<li><p>Want more accuracy (smaller <span class="math notranslate nohighlight">\(\epsilon\)</span>)? Need more data.</p></li>
<li><p>Want more confidence (smaller <span class="math notranslate nohighlight">\(\delta\)</span>)? Need more data (but only logarithmically).</p></li>
<li><p>More complex model (larger <span class="math notranslate nohighlight">\(|\mathcal{H}|\)</span>)? Need more data (also logarithmically).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize PAC sample complexity — &quot;How many laps do we need?&quot;</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Plot 1: Laps needed vs accuracy tolerance</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="k">for</span> <span class="n">H_size</span><span class="p">,</span> <span class="n">color</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="p">[(</span><span class="mi">10</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;10 setups&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;100 setups&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;1000 setups&#39;</span><span class="p">)]:</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="mf">0.05</span>
    <span class="n">N_pac</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">epsilon</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">H_size</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">delta</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">N_pac</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$\epsilon$ (lap time tolerance)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Laps Needed $N$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Tighter tolerance = more laps needed&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Plot 2: Laps needed vs confidence level</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">delta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="k">for</span> <span class="n">eps</span><span class="p">,</span> <span class="n">color</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="p">[(</span><span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;0.1s tolerance&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;0.05s tolerance&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;0.01s tolerance&#39;</span><span class="p">)]:</span>
    <span class="n">H_size</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">N_pac</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">eps</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">H_size</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">delta</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">N_pac</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$\delta$ (failure probability)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Laps Needed $N$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;More confidence = more laps (log scaling)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Plot 3: Laps needed vs number of setup configurations</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">H_sizes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="k">for</span> <span class="n">eps</span><span class="p">,</span> <span class="n">color</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="p">[(</span><span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;0.1s tolerance&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;0.05s tolerance&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;0.01s tolerance&#39;</span><span class="p">)]:</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="mf">0.05</span>
    <span class="n">N_pac</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">eps</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">H_sizes</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">delta</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">H_sizes</span><span class="p">,</span> <span class="n">N_pac</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Setup Configurations $|\mathcal</span><span class="si">{H}</span><span class="s1">|$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Laps Needed $N$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;More configurations = more laps (log scaling)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;PAC Learning: How Many Laps Before You Can Trust the Setup?&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.02</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Concrete F1 example</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">delta</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">H_size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">N_required</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="mi">1</span><span class="o">/</span><span class="n">epsilon</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">H_size</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">delta</span><span class="p">))))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Example: To get within </span><span class="si">{</span><span class="n">epsilon</span><span class="si">:</span><span class="s2">.0%</span><span class="si">}</span><span class="s2"> of optimal with </span><span class="si">{</span><span class="mi">1</span><span class="o">-</span><span class="n">delta</span><span class="si">:</span><span class="s2">.0%</span><span class="si">}</span><span class="s2"> confidence&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  choosing among </span><span class="si">{</span><span class="n">H_size</span><span class="si">}</span><span class="s2"> setup configurations, you need N &gt;= </span><span class="si">{</span><span class="n">N_required</span><span class="si">}</span><span class="s2"> laps&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;&gt;:11: SyntaxWarning: invalid escape sequence &#39;\e&#39;
&lt;&gt;:24: SyntaxWarning: invalid escape sequence &#39;\d&#39;
&lt;&gt;:37: SyntaxWarning: invalid escape sequence &#39;\m&#39;
&lt;&gt;:11: SyntaxWarning: invalid escape sequence &#39;\e&#39;
&lt;&gt;:24: SyntaxWarning: invalid escape sequence &#39;\d&#39;
&lt;&gt;:37: SyntaxWarning: invalid escape sequence &#39;\m&#39;
/var/folders/l3/qp293kmd0ps5v659jn3njrpc0000gn/T/ipykernel_7266/70424169.py:11: SyntaxWarning: invalid escape sequence &#39;\e&#39;
  ax.set_xlabel(&#39;$\epsilon$ (lap time tolerance)&#39;, fontsize=12)
/var/folders/l3/qp293kmd0ps5v659jn3njrpc0000gn/T/ipykernel_7266/70424169.py:24: SyntaxWarning: invalid escape sequence &#39;\d&#39;
  ax.set_xlabel(&#39;$\delta$ (failure probability)&#39;, fontsize=12)
/var/folders/l3/qp293kmd0ps5v659jn3njrpc0000gn/T/ipykernel_7266/70424169.py:37: SyntaxWarning: invalid escape sequence &#39;\m&#39;
  ax.set_xlabel(&#39;Number of Setup Configurations $|\mathcal{H}|$&#39;, fontsize=12)
</pre></div>
</div>
<img alt="../_images/36fc9c9c90be317826b7ef50c67532221426a607bb5fe3870d042e78cf0b6950.png" src="../_images/36fc9c9c90be317826b7ef50c67532221426a607bb5fe3870d042e78cf0b6950.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Example: To get within 5% of optimal with 95% confidence
  choosing among 1000 setup configurations, you need N &gt;= 199 laps
</pre></div>
</div>
</div>
</div>
</section>
<section id="deep-dive-what-pac-learning-tells-us-about-ml">
<h3>Deep Dive: What PAC Learning Tells Us About ML<a class="headerlink" href="#deep-dive-what-pac-learning-tells-us-about-ml" title="Link to this heading">#</a></h3>
<p>PAC learning provides the theoretical foundation for understanding <em>learnability</em>:</p>
<p><strong>A concept class is PAC-learnable</strong> if there exists an algorithm that, for any <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span> and <span class="math notranslate nohighlight">\(\delta &gt; 0\)</span>, can learn a hypothesis with error <span class="math notranslate nohighlight">\(\leq \epsilon\)</span> with probability <span class="math notranslate nohighlight">\(\geq 1 - \delta\)</span>, using a number of samples polynomial in <span class="math notranslate nohighlight">\(1/\epsilon\)</span>, <span class="math notranslate nohighlight">\(1/\delta\)</span>, and the model size.</p>
<p><strong>Key takeaways for practitioners:</strong></p>
<ol class="arabic simple">
<li><p><strong>Logarithmic dependence on <span class="math notranslate nohighlight">\(|\mathcal{H}|\)</span> and <span class="math notranslate nohighlight">\(1/\delta\)</span></strong> — making your model 10x more complex only increases data needs by <span class="math notranslate nohighlight">\(\ln(10) \approx 2.3\)</span>. That’s surprisingly cheap!</p></li>
<li><p><strong>Linear dependence on <span class="math notranslate nohighlight">\(1/\epsilon\)</span></strong> — going from 90% to 99% accuracy is much harder than 50% to 90%.</p></li>
<li><p><strong>Connection to VC dimension</strong>: For infinite hypothesis classes (like neural networks), replace <span class="math notranslate nohighlight">\(\ln|\mathcal{H}|\)</span> with <span class="math notranslate nohighlight">\(d_{VC}\)</span> to get similar bounds.</p></li>
</ol>
<p><strong>F1 insight:</strong> This explains why teams can test relatively few setup configurations and still find a good one. The logarithmic dependence on <span class="math notranslate nohighlight">\(|\mathcal{H}|\)</span> means that even with thousands of possible configurations, you only need modestly more data than if you had ten. The hard part is the <em>accuracy</em> requirement — getting within a tenth of a second of optimal requires dramatically more laps than getting within half a second.</p>
<p><strong>Why this matters:</strong> PAC theory tells us that learning <em>is</em> possible with finite data — not something obvious from first principles. It also tells us the fundamental resource tradeoffs.</p>
</section>
</section>
<hr class="docutils" />
<section id="regularization-theory">
<h2>7. Regularization Theory<a class="headerlink" href="#regularization-theory" title="Link to this heading">#</a></h2>
<section id="id11">
<h3>Intuitive Explanation<a class="headerlink" href="#id11" title="Link to this heading">#</a></h3>
<p>Regularization adds a penalty to the loss function to prevent overfitting:</p>
<div class="math notranslate nohighlight">
\[\text{Total Loss} = \text{Data Loss} + \lambda \cdot \text{Penalty}(w)\]</div>
<p>The two most common penalties:</p>
<ul class="simple">
<li><p><strong>L1 (Lasso):</strong> <span class="math notranslate nohighlight">\(\|w\|_1 = \sum |w_i|\)</span> — produces <strong>sparse</strong> solutions (many weights exactly zero)</p></li>
<li><p><strong>L2 (Ridge):</strong> <span class="math notranslate nohighlight">\(\|w\|_2^2 = \sum w_i^2\)</span> — produces <strong>small</strong> weights (but rarely exactly zero)</p></li>
</ul>
</section>
<section id="id12">
<h3>The F1 Connection<a class="headerlink" href="#id12" title="Link to this heading">#</a></h3>
<p>Regularization is <strong>preventing setup over-specialization</strong> — penalizing extreme settings that might work in one specific condition but fail everywhere else.</p>
<ul class="simple">
<li><p><strong>L1 regularization</strong> = zeroing out unnecessary setup parameters. If a parameter isn’t contributing meaningfully, set it to its default and stop wasting engineering time on it. This is like a team saying “we don’t need to touch the differential setting for this track — leave it at baseline.”</p></li>
<li><p><strong>L2 regularization</strong> = keeping all parameters in play but preventing any from being extreme. No single setting dominates — like a regulation that says “you can adjust everything, but nothing too aggressively.” This produces balanced, predictable cars.</p></li>
</ul>
<p><strong>Why does L1 give sparsity?</strong> It’s all about geometry. The L1 constraint region is a <em>diamond</em>, and the loss contours typically hit the diamond at a corner — where some coordinates are exactly zero. The L2 constraint region is a <em>sphere</em>, which has no corners, so the intersection point rarely has exact zeros.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The geometry of L1 vs L2 regularization — why sparsity happens</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Create elliptical contours (represent lap time error surface)</span>
<span class="n">theta_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>

<span class="c1"># Shifted center for the lap time error (unconstrained optimum)</span>
<span class="n">center</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>

<span class="c1"># L1 constraint: |w1| + |w2| &lt;= t (diamond boundary)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">l1_boundary</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="n">pts</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">th</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">200</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">t</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">th</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">t</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">th</span><span class="p">)</span>
        <span class="c1"># Project onto L1 ball</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">scale</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">t</span> <span class="o">/</span> <span class="n">scale</span><span class="p">,</span> <span class="n">y</span> <span class="o">*</span> <span class="n">t</span> <span class="o">/</span> <span class="n">scale</span>
        <span class="n">pts</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pts</span><span class="p">)</span>

<span class="c1"># L2 constraint: w1^2 + w2^2 &lt;= t^2 (circle boundary)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">l2_boundary</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">t</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta_range</span><span class="p">),</span> <span class="n">t</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta_range</span><span class="p">)])</span>

<span class="c1"># Plot L1: Diamond forces setup params to corners (zeros!)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">A_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]])</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">200</span><span class="p">))</span>
<span class="n">zz</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">xx</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">xx</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">],</span> <span class="n">yy</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]])</span> <span class="o">-</span> <span class="n">center</span>
        <span class="n">zz</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">d</span> <span class="o">@</span> <span class="n">A_loss</span> <span class="o">@</span> <span class="n">d</span>

<span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">zz</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdYlBu_r&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>

<span class="c1"># Diamond (L1 ball) — &quot;only adjust a few things&quot;</span>
<span class="n">t</span> <span class="o">=</span> <span class="mf">1.5</span>
<span class="n">diamond</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">t</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">t</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="n">t</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">t</span><span class="p">],</span> <span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">diamond</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">diamond</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">diamond</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">diamond</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Mark the solution (corner of diamond — one param zeroed!)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="s1">&#39;g*&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;L1 solution (sparse!)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">center</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">center</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;r*&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Unconstrained optimum&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;L1 (Lasso): Diamond Constraint</span><span class="se">\n</span><span class="s1">Zeros out setup params&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Setup Param 1 (wing angle)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Setup Param 2 (ride height)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Plot L2: Circle keeps all params but small</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">zz</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdYlBu_r&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>

<span class="c1"># Circle (L2 ball) — &quot;keep everything moderate&quot;</span>
<span class="n">circle_pts</span> <span class="o">=</span> <span class="n">l2_boundary</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">circle_pts</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">circle_pts</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">circle_pts</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">circle_pts</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Mark the solution (on circle, not at corner)</span>
<span class="n">setup_l2</span> <span class="o">=</span> <span class="n">center</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">center</span><span class="p">)</span> <span class="o">*</span> <span class="n">t</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">setup_l2</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">setup_l2</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;g*&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;L2 solution (small, balanced)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">center</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">center</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;r*&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Unconstrained optimum&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;L2 (Ridge): Circle Constraint</span><span class="se">\n</span><span class="s1">Keeps all params moderate&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Setup Param 1 (wing angle)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Setup Param 2 (ride height)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Plot comparison of setup parameter magnitudes</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_setup_params</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">X_reg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_setup_params</span><span class="p">)</span>
<span class="n">setup_true_reg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_setup_params</span><span class="p">)</span>
<span class="n">setup_true_reg</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>  <span class="c1"># Only 5 params actually matter</span>
<span class="n">y_reg</span> <span class="o">=</span> <span class="n">X_reg</span> <span class="o">@</span> <span class="n">setup_true_reg</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

<span class="c1"># Fit models</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">Lasso</span><span class="p">,</span> <span class="n">Ridge</span>
<span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_reg</span><span class="p">,</span> <span class="n">y_reg</span><span class="p">)</span>
<span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_reg</span><span class="p">,</span> <span class="n">y_reg</span><span class="p">)</span>

<span class="n">x_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_setup_params</span><span class="p">)</span>
<span class="n">width</span> <span class="o">=</span> <span class="mf">0.35</span>
<span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x_pos</span> <span class="o">-</span> <span class="n">width</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">),</span> <span class="n">width</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;L1 (Lasso) — sparse&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x_pos</span> <span class="o">+</span> <span class="n">width</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">),</span> <span class="n">width</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;L2 (Ridge) — balanced&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">4.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True important params boundary&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Setup Parameter Index&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;|Parameter Value|&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Setup Parameter Magnitudes: L1 vs L2&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Why L1 Simplifies Setups and L2 Keeps Them Balanced&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.02</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;L1 (Lasso) non-zero setup params: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mf">0.01</span><span class="p">)</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_setup_params</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;L2 (Ridge) non-zero setup params: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mf">0.01</span><span class="p">)</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_setup_params</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/9cad06a147e32ef251b94445c8c4638e49da1d580679039b90bb7a8d420dc59b.png" src="../_images/9cad06a147e32ef251b94445c8c4638e49da1d580679039b90bb7a8d420dc59b.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>L1 (Lasso) non-zero setup params: 5/20
L2 (Ridge) non-zero setup params: 15/20
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Regularization paths: how setup parameters change with penalty strength</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Lasso path — parameters get zeroed out as penalty increases</span>
<span class="n">lasso_coefs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_reg</span><span class="p">,</span> <span class="n">y_reg</span><span class="p">)</span>
    <span class="n">lasso_coefs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
<span class="n">lasso_coefs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">lasso_coefs</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_setup_params</span><span class="p">):</span>
    <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span> <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="mi">5</span> <span class="k">else</span> <span class="s1">&#39;gray&#39;</span>
    <span class="n">alpha_val</span> <span class="o">=</span> <span class="mf">0.9</span> <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="mi">5</span> <span class="k">else</span> <span class="mf">0.3</span>
    <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="mi">5</span> <span class="k">else</span> <span class="mf">0.5</span>
    <span class="n">label</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Param </span><span class="si">{</span><span class="n">j</span><span class="si">}</span><span class="s1"> (important)&#39;</span> <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="mi">5</span> <span class="k">else</span> <span class="p">(</span><span class="kc">None</span> <span class="k">if</span> <span class="n">j</span> <span class="o">&gt;</span> <span class="mi">5</span> <span class="k">else</span> <span class="s1">&#39;Noise params&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">lasso_coefs</span><span class="p">[:,</span> <span class="n">j</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha_val</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="n">linewidth</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$\lambda$ (regularization strength)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Setup Parameter Value&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;L1 Path: Parameters Drop to Zero (Simplified Setup)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Ridge path — parameters shrink but never reach zero</span>
<span class="n">ridge_coefs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_reg</span><span class="p">,</span> <span class="n">y_reg</span><span class="p">)</span>
    <span class="n">ridge_coefs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
<span class="n">ridge_coefs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ridge_coefs</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_setup_params</span><span class="p">):</span>
    <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span> <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="mi">5</span> <span class="k">else</span> <span class="s1">&#39;gray&#39;</span>
    <span class="n">alpha_val</span> <span class="o">=</span> <span class="mf">0.9</span> <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="mi">5</span> <span class="k">else</span> <span class="mf">0.3</span>
    <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="mi">5</span> <span class="k">else</span> <span class="mf">0.5</span>
    <span class="n">label</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Param </span><span class="si">{</span><span class="n">j</span><span class="si">}</span><span class="s1"> (important)&#39;</span> <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="mi">5</span> <span class="k">else</span> <span class="p">(</span><span class="kc">None</span> <span class="k">if</span> <span class="n">j</span> <span class="o">&gt;</span> <span class="mi">5</span> <span class="k">else</span> <span class="s1">&#39;Noise params&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">ridge_coefs</span><span class="p">[:,</span> <span class="n">j</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha_val</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="n">linewidth</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$\lambda$ (regularization strength)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Setup Parameter Value&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;L2 Path: Parameters Shrink But Stay Active&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Key observation:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  L1: Setup params drop to EXACTLY zero as penalty increases (parameter selection!)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  L2: Setup params shrink smoothly toward zero but never reach it&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;&gt;:21: SyntaxWarning: invalid escape sequence &#39;\l&#39;
&lt;&gt;:42: SyntaxWarning: invalid escape sequence &#39;\l&#39;
&lt;&gt;:21: SyntaxWarning: invalid escape sequence &#39;\l&#39;
&lt;&gt;:42: SyntaxWarning: invalid escape sequence &#39;\l&#39;
/var/folders/l3/qp293kmd0ps5v659jn3njrpc0000gn/T/ipykernel_7266/587251588.py:21: SyntaxWarning: invalid escape sequence &#39;\l&#39;
  ax.set_xlabel(&#39;$\lambda$ (regularization strength)&#39;, fontsize=12)
/var/folders/l3/qp293kmd0ps5v659jn3njrpc0000gn/T/ipykernel_7266/587251588.py:42: SyntaxWarning: invalid escape sequence &#39;\l&#39;
  ax.set_xlabel(&#39;$\lambda$ (regularization strength)&#39;, fontsize=12)
</pre></div>
</div>
<img alt="../_images/59a84f51f6b54300af5268ea610b2d82ac107bb519e8efbb20a7379bc45dd6c0.png" src="../_images/59a84f51f6b54300af5268ea610b2d82ac107bb519e8efbb20a7379bc45dd6c0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Key observation:
  L1: Setup params drop to EXACTLY zero as penalty increases (parameter selection!)
  L2: Setup params shrink smoothly toward zero but never reach it
</pre></div>
</div>
</div>
</div>
</section>
<section id="deep-dive-regularization-as-bayesian-priors">
<h3>Deep Dive: Regularization as Bayesian Priors<a class="headerlink" href="#deep-dive-regularization-as-bayesian-priors" title="Link to this heading">#</a></h3>
<p>There’s a beautiful connection between regularization and Bayesian statistics:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Regularizer</p></th>
<th class="head"><p>Equivalent Prior</p></th>
<th class="head"><p>Distribution</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>L2 (Ridge)</p></td>
<td><p>Gaussian prior: <span class="math notranslate nohighlight">\(w_i \sim \mathcal{N}(0, 1/\lambda)\)</span></p></td>
<td><p>Weights cluster near zero</p></td>
<td><p>“All setup params should be near baseline”</p></td>
</tr>
<tr class="row-odd"><td><p>L1 (Lasso)</p></td>
<td><p>Laplace prior: <span class="math notranslate nohighlight">\(w_i \sim \text{Laplace}(0, 1/\lambda)\)</span></p></td>
<td><p>Weights sparse (peak at zero)</p></td>
<td><p>“Most setup params don’t matter — find the few that do”</p></td>
</tr>
<tr class="row-even"><td><p>Elastic Net</p></td>
<td><p>Mix of Gaussian + Laplace</p></td>
<td><p>Best of both worlds</p></td>
<td><p>“Find important params, keep them moderate”</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>What this means:</strong> When you add L2 regularization, you’re saying “I believe the weights are probably small.” When you add L1, you’re saying “I believe most weights are probably zero.” The strength <span class="math notranslate nohighlight">\(\lambda\)</span> controls how strongly you hold this belief.</p>
<p><strong>F1 analogy:</strong> An experienced race engineer has strong priors — they know that for most circuits, ride height and wing angle matter most, while obscure differential settings rarely make a big difference. A rookie engineer with no priors might waste time optimizing parameters that don’t matter. Regularization encodes this experience mathematically.</p>
<p><strong>Elastic Net</strong> combines both:
$<span class="math notranslate nohighlight">\(\text{Penalty} = \alpha \|w\|_1 + (1-\alpha) \|w\|_2^2\)</span>$</p>
<p>This gives sparsity (from L1) while handling correlated features better (from L2).</p>
<section id="key-insight">
<h4>Key Insight<a class="headerlink" href="#key-insight" title="Link to this heading">#</a></h4>
<p>Regularization is not a “trick” — it’s a principled way of encoding prior knowledge about what good solutions look like. Every regularizer implicitly answers the question: “What kind of models do I expect to work well?”</p>
</section>
</section>
</section>
<hr class="docutils" />
<section id="why-overparameterized-networks-generalize">
<h2>8. Why Overparameterized Networks Generalize<a class="headerlink" href="#why-overparameterized-networks-generalize" title="Link to this heading">#</a></h2>
<section id="id13">
<h3>Intuitive Explanation<a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
<p>Classical learning theory says: more parameters → more overfitting. But modern neural networks have <em>millions</em> of parameters (far more than training examples) and still generalize beautifully. This contradicts classical theory and is one of the deepest open questions in ML.</p>
<p><strong>The double descent phenomenon:</strong> As model complexity increases, test error follows a U-shape (classical regime), but then <em>decreases again</em> after the interpolation threshold (where the model can perfectly fit the training data).</p>
</section>
<section id="id14">
<h3>The F1 Connection<a class="headerlink" href="#id14" title="Link to this heading">#</a></h3>
<p>Think of this as the <strong>paradox of large teams</strong>. Classical theory says a 3-person pit crew should be more “efficient” than a 20-person crew — fewer people to coordinate, fewer things to go wrong. But in practice, the 20-person Red Bull pit crew does 2-second stops. Why? Because having more people means each person can specialize, and the crew self-organizes into an efficient sub-structure. The “extra” crew members aren’t wasted — they enable the specialization that makes the whole operation faster.</p>
<p>Similarly, overparameterized networks have far more parameters than they “need” in theory, but the extra capacity allows SGD to find elegant, simple solutions within the vast solution space.</p>
<p>Three key ideas explain why overparameterization works:</p>
<ol class="arabic simple">
<li><p><strong>Double descent</strong>: The classical bias-variance tradeoff is incomplete — it misses the “modern” regime</p></li>
<li><p><strong>Implicit regularization</strong>: SGD, by its nature, finds “simple” solutions among the many that fit the data</p></li>
<li><p><strong>Lottery ticket hypothesis</strong>: Large networks contain small subnetworks that do most of the work</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simulate the double descent curve — the surprise of overparameterization</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Generate lap time data from a moderately complex relationship</span>
<span class="n">n_total</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">x_all</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_total</span><span class="p">)</span>
<span class="n">laptime_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">6</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
<span class="n">track_noise</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="n">n_practice</span> <span class="o">=</span> <span class="mi">25</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_total</span><span class="p">,</span> <span class="n">n_practice</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">x_practice_dd</span> <span class="o">=</span> <span class="n">x_all</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="n">y_practice_dd</span> <span class="o">=</span> <span class="n">laptime_fn</span><span class="p">(</span><span class="n">x_practice_dd</span><span class="p">)</span> <span class="o">+</span> <span class="n">track_noise</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_practice</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_total</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
<span class="n">mask</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">x_race_dd</span> <span class="o">=</span> <span class="n">x_all</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
<span class="n">y_race_dd</span> <span class="o">=</span> <span class="n">laptime_fn</span><span class="p">(</span><span class="n">x_race_dd</span><span class="p">)</span> <span class="o">+</span> <span class="n">track_noise</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_total</span> <span class="o">-</span> <span class="n">n_practice</span><span class="p">)</span>

<span class="c1"># Fit models of increasing complexity, including overparameterized</span>
<span class="n">complexities_dd</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">35</span><span class="p">))</span>
<span class="n">practice_errors_dd</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">race_errors_dd</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">complexities_dd</span><span class="p">:</span>
    <span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">X_pr</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_practice_dd</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">X_rc</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x_race_dd</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Use Ridge with tiny regularization for numerical stability past interpolation</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">Ridge</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_pr</span><span class="p">,</span> <span class="n">y_practice_dd</span><span class="p">)</span>

    <span class="n">y_pred_pr</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_pr</span><span class="p">)</span>
    <span class="n">y_pred_rc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_rc</span><span class="p">)</span>

    <span class="n">practice_errors_dd</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_pred_pr</span> <span class="o">-</span> <span class="n">y_practice_dd</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">race_errors_dd</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_pred_rc</span> <span class="o">-</span> <span class="n">y_race_dd</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">complexities_dd</span><span class="p">,</span> <span class="n">practice_errors_dd</span><span class="p">,</span> <span class="s1">&#39;b-o&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Practice Error (training)&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">complexities_dd</span><span class="p">,</span> <span class="n">race_errors_dd</span><span class="p">,</span> <span class="s1">&#39;r-s&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Race Error (test)&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">track_noise</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Track noise floor (</span><span class="si">{</span><span class="n">track_noise</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">n_practice</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
           <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Interpolation threshold (d=</span><span class="si">{</span><span class="n">n_practice</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>

<span class="c1"># Annotate regions</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;Classical</span><span class="se">\n</span><span class="s1">regime&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span>
            <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;Modern</span><span class="se">\n</span><span class="s1">(overparameterized)</span><span class="se">\n</span><span class="s1">regime&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span>
            <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;Interpolation</span><span class="se">\n</span><span class="s1">peak&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">n_practice</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">race_errors_dd</span><span class="p">)),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span>
            <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;bottom&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Model Complexity (Setup Parameters)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Squared Error&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Double Descent: Beyond the Classical Bias-Variance Tradeoff&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">race_errors_dd</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.1</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Double descent: race error peaks at the interpolation threshold,&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;then DECREASES as the model becomes even more overparameterized!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dbe3d02f55b465b4a1177fa1bfc5d2a593c8acf442f86ed1aba9c8dcfc189ef1.png" src="../_images/dbe3d02f55b465b4a1177fa1bfc5d2a593c8acf442f86ed1aba9c8dcfc189ef1.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Double descent: race error peaks at the interpolation threshold,
then DECREASES as the model becomes even more overparameterized!
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Demonstrate implicit regularization: SGD finds the simplest setup among many valid ones</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Underdetermined system: more setup parameters than data points</span>
<span class="n">n_laps_imp</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">n_params_imp</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># Way more parameters than laps — overparameterized!</span>

<span class="n">X_imp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_laps_imp</span><span class="p">,</span> <span class="n">n_params_imp</span><span class="p">)</span>
<span class="n">setup_target</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_params_imp</span><span class="p">)</span>
<span class="n">setup_target</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8</span><span class="p">])</span>  <span class="c1"># Sparse true setup</span>
<span class="n">y_imp</span> <span class="o">=</span> <span class="n">X_imp</span> <span class="o">@</span> <span class="n">setup_target</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_laps_imp</span><span class="p">)</span>

<span class="c1"># Many solutions exist! Let&#39;s see what different methods find.</span>

<span class="c1"># Method 1: Minimum-norm solution (pseudoinverse) — simplest mathematically</span>
<span class="n">setup_minnorm</span> <span class="o">=</span> <span class="n">X_imp</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">X_imp</span> <span class="o">@</span> <span class="n">X_imp</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y_imp</span><span class="p">)</span>

<span class="c1"># Method 2: SGD from baseline (zero initialization) — implicit regularization at work</span>
<span class="n">setup_sgd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_params_imp</span><span class="p">)</span>
<span class="n">sgd_rate</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="k">for</span> <span class="n">session</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">n_laps_imp</span><span class="p">):</span>
        <span class="n">xi</span> <span class="o">=</span> <span class="n">X_imp</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">yi</span> <span class="o">=</span> <span class="n">y_imp</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">xi</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">xi</span> <span class="o">@</span> <span class="n">setup_sgd</span> <span class="o">-</span> <span class="n">yi</span><span class="p">)</span>
        <span class="n">setup_sgd</span> <span class="o">=</span> <span class="n">setup_sgd</span> <span class="o">-</span> <span class="n">sgd_rate</span> <span class="o">*</span> <span class="n">grad</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="c1"># Method 3: Random valid solution (fits data but is unnecessarily complex)</span>
<span class="n">setup_particular</span> <span class="o">=</span> <span class="n">setup_minnorm</span>
<span class="n">null_component</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_params_imp</span><span class="p">)</span>
<span class="c1"># Project onto null space of X</span>
<span class="n">proj</span> <span class="o">=</span> <span class="n">X_imp</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">X_imp</span> <span class="o">@</span> <span class="n">X_imp</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_imp</span><span class="p">)</span>
<span class="n">null_component</span> <span class="o">=</span> <span class="n">null_component</span> <span class="o">-</span> <span class="n">proj</span> <span class="o">@</span> <span class="n">null_component</span>
<span class="n">setup_random</span> <span class="o">=</span> <span class="n">setup_particular</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">null_component</span>  <span class="c1"># Add large null space component</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">methods</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;Min-Norm (Pseudoinverse)&#39;</span><span class="p">,</span> <span class="n">setup_minnorm</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;SGD from Baseline&#39;</span><span class="p">,</span> <span class="n">setup_sgd</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Random Valid Setup&#39;</span><span class="p">,</span> <span class="n">setup_random</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">setup</span><span class="p">,</span> <span class="n">color</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">methods</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_params_imp</span><span class="p">),</span> <span class="n">setup</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Setup Parameter Index&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Parameter Value&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
    <span class="n">lap_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">X_imp</span> <span class="o">@</span> <span class="n">setup</span> <span class="o">-</span> <span class="n">y_imp</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">setup_complexity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">setup</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="se">\n</span><span class="s1">Lap Error=</span><span class="si">{</span><span class="n">lap_error</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">, Complexity=</span><span class="si">{</span><span class="n">setup_complexity</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Implicit Regularization: All Setups Fit the Data, But SGD Finds the Simplest&#39;</span><span class="p">,</span>
             <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.02</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Setup complexity (L2 norm):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Min-norm:  ||setup|| = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">setup_minnorm</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  SGD:       ||setup|| = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">setup_sgd</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Random:    ||setup|| = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">setup_random</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">All have near-zero lap errors — but the complexities are very different!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SGD implicitly finds a setup close to the minimum-norm solution.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/5612a536b3601c2633db6cb6c8d0f420d86ade4b30bb6a8ce64ef9432e578607.png" src="../_images/5612a536b3601c2633db6cb6c8d0f420d86ade4b30bb6a8ce64ef9432e578607.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Setup complexity (L2 norm):
  Min-norm:  ||setup|| = 1.3063
  SGD:       ||setup|| = 1.3063
  Random:    ||setup|| = 28.9465

All have near-zero lap errors — but the complexities are very different!
SGD implicitly finds a setup close to the minimum-norm solution.
</pre></div>
</div>
</div>
</div>
</section>
<section id="deep-dive-the-lottery-ticket-hypothesis">
<h3>Deep Dive: The Lottery Ticket Hypothesis<a class="headerlink" href="#deep-dive-the-lottery-ticket-hypothesis" title="Link to this heading">#</a></h3>
<p>The <strong>Lottery Ticket Hypothesis</strong> (Frankle &amp; Carlin, 2019) proposes:</p>
<blockquote>
<div><p>A randomly initialized, dense neural network contains a subnetwork (a “winning ticket”) that — when trained in isolation — reaches comparable accuracy to the full network in a similar number of training steps.</p>
</div></blockquote>
<p><strong>F1 analogy:</strong> Think of a large F1 team’s engineering department. Out of 800+ employees, there’s a core group — the “winning ticket” — whose contributions account for most of the car’s performance gains. You could theoretically achieve 90%+ of the performance with a much smaller team, but you need the large team to <em>discover</em> who those key contributors are.</p>
<p><strong>What this means:</strong></p>
<ul class="simple">
<li><p>Large networks work well not because all parameters are needed, but because having many parameters increases the chance of containing a good subnetwork</p></li>
<li><p>You can prune 90%+ of weights after training with minimal accuracy loss</p></li>
<li><p>The initial random values of the winning ticket matter — it’s the specific initialization that makes it trainable</p></li>
</ul>
<p><strong>Why this is profound:</strong></p>
<ol class="arabic simple">
<li><p>It suggests most parameters in large networks are “wasted” at inference time</p></li>
<li><p>It explains why overparameterization helps <em>training</em> even if not needed for <em>representation</em></p></li>
<li><p>It motivates network pruning and efficient inference</p></li>
</ol>
<section id="open-questions-honest-assessment">
<h4>Open Questions (Honest Assessment)<a class="headerlink" href="#open-questions-honest-assessment" title="Link to this heading">#</a></h4>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Question</p></th>
<th class="head"><p>Current Understanding</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Why does SGD find flat minima?</p></td>
<td><p>Partially understood — noise scale matters</p></td>
</tr>
<tr class="row-odd"><td><p>Is double descent universal?</p></td>
<td><p>Observed broadly, but theory is incomplete</p></td>
</tr>
<tr class="row-even"><td><p>Can we find winning tickets cheaply?</p></td>
<td><p>Active research; training is still needed to identify them</p></td>
</tr>
<tr class="row-odd"><td><p>Why do large models generalize at all?</p></td>
<td><p>Multiple competing theories; no consensus yet</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>The honest truth:</strong> We don’t fully understand why deep learning works as well as it does. The theory is catching up to the practice, and that’s what makes this field exciting.</p>
</section>
</section>
</section>
<hr class="docutils" />
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<section id="exercise-1-implement-setup-optimization-with-momentum">
<h3>Exercise 1: Implement Setup Optimization with Momentum<a class="headerlink" href="#exercise-1-implement-setup-optimization-with-momentum" title="Link to this heading">#</a></h3>
<p>In F1, momentum in setup changes means <strong>keeping adjustments going in the direction that’s been working</strong>. If lowering the front wing improved the car in FP1 and FP2, momentum says “keep going that direction” even if one noisy lap suggests otherwise.</p>
<p>Mathematically, momentum accelerates GD by accumulating a velocity vector:</p>
<div class="math notranslate nohighlight">
\[v_{t+1} = \beta v_t + \nabla f(x_t)\]</div>
<div class="math notranslate nohighlight">
\[x_{t+1} = x_t - \eta v_{t+1}\]</div>
<p>Implement this and compare convergence with vanilla GD on an ill-conditioned setup surface (where the car is much more sensitive to one parameter than another).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># EXERCISE 1: Implement setup optimization with momentum</span>
<span class="k">def</span><span class="w"> </span><span class="nf">setup_optimization_momentum</span><span class="p">(</span><span class="n">grad_f</span><span class="p">,</span> <span class="n">setup0</span><span class="p">,</span> <span class="n">setup_rate</span><span class="p">,</span> <span class="n">n_sessions</span><span class="p">,</span> <span class="n">momentum_coeff</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gradient descent with momentum for car setup optimization.</span>

<span class="sd">    Args:</span>
<span class="sd">        grad_f: Gradient function (telemetry feedback)</span>
<span class="sd">        setup0: Initial car setup</span>
<span class="sd">        setup_rate: How aggressively to change setup</span>
<span class="sd">        n_sessions: Number of practice iterations</span>
<span class="sd">        momentum_coeff: Momentum coefficient — how much to trust the previous direction (default 0.9)</span>
<span class="sd">        f: Optional objective function (lap time error) for tracking</span>

<span class="sd">    Returns:</span>
<span class="sd">        history dict with setup trajectory and lap time errors</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">setup</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">setup0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">velocity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">setup</span><span class="p">)</span>  <span class="c1"># Initialize momentum velocity</span>
    <span class="n">history</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">setup</span><span class="o">.</span><span class="n">copy</span><span class="p">()],</span> <span class="s1">&#39;f&#39;</span><span class="p">:</span> <span class="p">[]}</span>

    <span class="k">if</span> <span class="n">f</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">history</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">setup</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sessions</span><span class="p">):</span>
        <span class="c1"># TODO: Implement momentum update</span>
        <span class="c1"># Hint: First update velocity, then update setup</span>
        <span class="c1"># velocity = momentum_coeff * velocity + grad_f(setup)</span>
        <span class="c1"># setup = setup - setup_rate * velocity</span>

        <span class="k">pass</span>  <span class="c1"># Replace with your implementation</span>

        <span class="n">history</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">setup</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">f</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">history</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">setup</span><span class="p">))</span>

    <span class="n">history</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">history</span>

<span class="c1"># Test on an ill-conditioned setup surface (car very sensitive to one param)</span>
<span class="n">A_unbalanced</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">20.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>  <span class="c1"># condition number = 20</span>
<span class="n">f_unbalanced</span><span class="p">,</span> <span class="n">grad_unbalanced</span> <span class="o">=</span> <span class="n">make_quadratic</span><span class="p">(</span><span class="n">A_unbalanced</span><span class="p">)</span>
<span class="n">setup0_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">])</span>

<span class="c1"># Test your implementation</span>
<span class="n">hist_momentum</span> <span class="o">=</span> <span class="n">setup_optimization_momentum</span><span class="p">(</span><span class="n">grad_unbalanced</span><span class="p">,</span> <span class="n">setup0_test</span><span class="p">,</span> <span class="n">setup_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">n_sessions</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">momentum_coeff</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">f_unbalanced</span><span class="p">)</span>
<span class="n">hist_vanilla</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">grad_unbalanced</span><span class="p">,</span> <span class="n">setup0_test</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">f_unbalanced</span><span class="p">)</span>

<span class="c1"># Verify</span>
<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">hist_momentum</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">hist_momentum</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">hist_momentum</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Momentum final lap time error:  </span><span class="si">{</span><span class="n">hist_momentum</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Vanilla final lap time error:   </span><span class="si">{</span><span class="n">hist_vanilla</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Momentum converges faster: </span><span class="si">{</span><span class="n">hist_momentum</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hist_vanilla</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TODO: Implement the momentum update in the loop above!&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Expected: Momentum should converge significantly faster than vanilla GD&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Vanilla GD final lap time error: </span><span class="si">{</span><span class="n">hist_vanilla</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TODO: Implement the momentum update in the loop above!
Expected: Momentum should converge significantly faster than vanilla GD
Vanilla GD final lap time error: 0.000438
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-2-cross-validation-for-lap-time-model-selection">
<h3>Exercise 2: Cross-Validation for Lap Time Model Selection<a class="headerlink" href="#exercise-2-cross-validation-for-lap-time-model-selection" title="Link to this heading">#</a></h3>
<p>An F1 team needs to choose the right complexity for their lap time prediction model. Too simple and it misses important patterns (underfitting). Too complex and it memorizes noise from one specific session (overfitting).</p>
<p>Use cross-validation to find the best polynomial degree for predicting lap times from a noisy dataset. This connects the bias-variance tradeoff to practical model selection — exactly the decision teams face when choosing between a simple “track-type” model vs. a detailed telemetry-based simulator.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># EXERCISE 2: Find the best lap time model complexity using cross-validation</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Generate lap time data (e.g., lap time vs fuel load or track position)</span>
<span class="n">n_laps_cv</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x_cv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">n_laps_cv</span><span class="p">)</span>
<span class="n">y_cv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x_cv</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">3</span><span class="o">*</span><span class="n">x_cv</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.4</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_laps_cv</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">evaluate_laptime_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">complexity</span><span class="p">,</span> <span class="n">cv_folds</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluate a polynomial lap time model using cross-validation.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Input features (1D array — e.g., fuel load or track position)</span>
<span class="sd">        y: Target lap times</span>
<span class="sd">        complexity: Polynomial degree to evaluate</span>
<span class="sd">        cv_folds: Number of cross-validation folds (like testing across different sessions)</span>

<span class="sd">    Returns:</span>
<span class="sd">        mean_cv_score: Mean cross-validation score (negative MSE)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement this!</span>
    <span class="c1"># Hint:</span>
    <span class="c1"># 1. Create polynomial features: PolynomialFeatures(complexity)</span>
    <span class="c1"># 2. Transform x: poly.fit_transform(x.reshape(-1, 1))</span>
    <span class="c1"># 3. Use cross_val_score with LinearRegression and scoring=&#39;neg_mean_squared_error&#39;</span>
    <span class="c1"># 4. Return the mean score</span>

    <span class="k">pass</span>  <span class="c1"># Replace with your implementation</span>

<span class="c1"># Test all complexities from 1 to 15</span>
<span class="n">complexities_cv</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">cv_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">complexities_cv</span><span class="p">:</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">evaluate_laptime_model</span><span class="p">(</span><span class="n">x_cv</span><span class="p">,</span> <span class="n">y_cv</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">score</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">cv_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span><span class="n">score</span><span class="p">)</span>  <span class="c1"># Negate because sklearn returns negative MSE</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cv_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>

<span class="k">if</span> <span class="n">cv_scores</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">best_complexity</span> <span class="o">=</span> <span class="n">complexities_cv</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">cv_scores</span><span class="p">)]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best lap time model complexity by CV: degree </span><span class="si">{</span><span class="n">best_complexity</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CV MSE scores: </span><span class="si">{</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">s</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">cv_scores</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Verify</span>
    <span class="n">expected_best</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># Approximate expected best (sin + cos terms)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Expected best degree: around </span><span class="si">{</span><span class="n">expected_best</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Your best degree: </span><span class="si">{</span><span class="n">best_complexity</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Reasonable: </span><span class="si">{</span><span class="mi">1</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">best_complexity</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="mi">7</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TODO: Implement evaluate_laptime_model above!&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Expected: Best degree should be around 3-5 (matching the true function complexity)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TODO: Implement evaluate_laptime_model above!
Expected: Best degree should be around 3-5 (matching the true function complexity)
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-3-compare-setup-regularization-strategies">
<h3>Exercise 3: Compare Setup Regularization Strategies<a class="headerlink" href="#exercise-3-compare-setup-regularization-strategies" title="Link to this heading">#</a></h3>
<p>An F1 car has 30 adjustable setup parameters, but only about 8 truly matter for lap time at a given circuit. The rest are noise or have minimal impact.</p>
<p>Compare L1 (Lasso), L2 (Ridge), and Elastic Net regularization to see which best recovers the true sparse structure — i.e., which method correctly identifies the important setup parameters and ignores the irrelevant ones. This is the setup engineering equivalent of “which parameters should we spend time optimizing?”</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># EXERCISE 3: Setup regularization comparison</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Create dataset: 30 setup parameters, only 8 actually affect lap time</span>
<span class="n">n_laps_ex</span><span class="p">,</span> <span class="n">n_setup_ex</span> <span class="o">=</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">30</span>
<span class="n">X_ex</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_laps_ex</span><span class="p">,</span> <span class="n">n_setup_ex</span><span class="p">)</span>
<span class="n">setup_true_ex</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_setup_ex</span><span class="p">)</span>
<span class="n">setup_true_ex</span><span class="p">[:</span><span class="mi">8</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]</span>  <span class="c1"># Only 8 params matter</span>
<span class="n">y_ex</span> <span class="o">=</span> <span class="n">X_ex</span> <span class="o">@</span> <span class="n">setup_true_ex</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_laps_ex</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">compare_setup_regularizers</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">setup_true</span><span class="p">,</span> <span class="n">alpha_l1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha_l2</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compare L1, L2, and Elastic Net regularization for setup parameter recovery.</span>

<span class="sd">    Args:</span>
<span class="sd">        X: Telemetry feature matrix</span>
<span class="sd">        y: Lap time targets</span>
<span class="sd">        setup_true: True setup parameter values (for comparison)</span>
<span class="sd">        alpha_l1: Regularization strength for Lasso</span>
<span class="sd">        alpha_l2: Regularization strength for Ridge</span>
<span class="sd">        l1_ratio: L1 ratio for Elastic Net</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict with &#39;lasso&#39;, &#39;ridge&#39;, &#39;elastic_net&#39; entries, each containing:</span>
<span class="sd">            &#39;coef&#39;: fitted coefficients (recovered setup params)</span>
<span class="sd">            &#39;n_nonzero&#39;: number of active parameters (|w| &gt; 0.01)</span>
<span class="sd">            &#39;mse&#39;: mean squared error of parameter recovery</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># TODO: Implement this!</span>
    <span class="c1"># Hint:</span>
    <span class="c1"># 1. Fit Lasso(alpha=alpha_l1), Ridge(alpha=alpha_l2),</span>
    <span class="c1">#    and ElasticNet (from sklearn.linear_model import ElasticNet)</span>
    <span class="c1"># 2. For each, compute:</span>
    <span class="c1">#    - coef: model.coef_</span>
    <span class="c1">#    - n_nonzero: np.sum(np.abs(model.coef_) &gt; 0.01)</span>
    <span class="c1">#    - mse: np.mean((model.coef_ - setup_true)**2)</span>

    <span class="k">pass</span>  <span class="c1"># Replace with your implementation</span>

    <span class="k">return</span> <span class="n">results</span>

<span class="c1"># Test</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">compare_setup_regularizers</span><span class="p">(</span><span class="n">X_ex</span><span class="p">,</span> <span class="n">y_ex</span><span class="p">,</span> <span class="n">setup_true_ex</span><span class="p">)</span>

<span class="k">if</span> <span class="n">results</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Setup Regularization Comparison:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Method&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Active Params&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Recovery MSE&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;True Active&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
    <span class="n">true_active</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">setup_true_ex</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.01</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">res</span> <span class="ow">in</span> <span class="n">results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">res</span><span class="p">[</span><span class="s1">&#39;n_nonzero&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">res</span><span class="p">[</span><span class="s1">&#39;mse&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">true_active</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Verify</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Expected: Lasso should identify closest to </span><span class="si">{</span><span class="n">true_active</span><span class="si">}</span><span class="s2"> active parameters&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected: Ridge should keep ALL parameters active&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected: Elastic Net should be in between&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TODO: Implement compare_setup_regularizers above!&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True active setup parameters: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">setup_true_ex</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mf">0.01</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Expected: L1 finds the important params, L2 keeps all, Elastic Net balances both&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TODO: Implement compare_setup_regularizers above!
True active setup parameters: 8
Expected: L1 finds the important params, L2 keeps all, Elastic Net balances both
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-4-bonus-setup-change-rate-schedules">
<h3>Exercise 4 (Bonus): Setup Change Rate Schedules<a class="headerlink" href="#exercise-4-bonus-setup-change-rate-schedules" title="Link to this heading">#</a></h3>
<p>Just like F1 teams make <strong>big setup changes in FP1</strong> and progressively <strong>fine-tune through FP3</strong>, learning rate schedules reduce the step size over training.</p>
<p>Implement and compare three setup change rate schedules for SGD:</p>
<ol class="arabic simple">
<li><p><strong>Constant</strong>: <span class="math notranslate nohighlight">\(\eta_t = \eta_0\)</span> (same aggressiveness every session)</p></li>
<li><p><strong>Step decay</strong>: <span class="math notranslate nohighlight">\(\eta_t = \eta_0 \cdot 0.5^{\lfloor t/50 \rfloor}\)</span> (halve aggressiveness periodically)</p></li>
<li><p><strong>Cosine annealing</strong>: <span class="math notranslate nohighlight">\(\eta_t = \eta_0 \cdot \frac{1}{2}\left(1 + \cos\left(\frac{\pi t}{T}\right)\right)\)</span> (smooth transition from aggressive to cautious)</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># EXERCISE 4 (BONUS): Setup change rate schedules</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">rate_constant</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">rate0</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Constant rate — same aggressiveness every session.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">rate0</span>

<span class="k">def</span><span class="w"> </span><span class="nf">rate_step_decay</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">rate0</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">drop_every</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Step decay: halve aggressiveness every drop_every sessions.&quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement this</span>
    <span class="c1"># Hint: return rate0 * drop_rate ** (t // drop_every)</span>
    <span class="k">pass</span>

<span class="k">def</span><span class="w"> </span><span class="nf">rate_cosine</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">rate0</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Cosine annealing: smooth transition from aggressive to cautious.&quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement this</span>
    <span class="c1"># Hint: return rate0 * 0.5 * (1 + np.cos(np.pi * t / T))</span>
    <span class="k">pass</span>

<span class="c1"># Compare on the lap time prediction problem from Section 3</span>
<span class="n">schedules</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;Constant&#39;</span><span class="p">:</span> <span class="n">rate_constant</span><span class="p">,</span>
    <span class="s1">&#39;Step Decay&#39;</span><span class="p">:</span> <span class="n">rate_step_decay</span><span class="p">,</span>
    <span class="s1">&#39;Cosine Annealing&#39;</span><span class="p">:</span> <span class="n">rate_cosine</span>
<span class="p">}</span>

<span class="n">n_sessions_sched</span> <span class="o">=</span> <span class="mi">200</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">schedule</span> <span class="ow">in</span> <span class="n">schedules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">schedule</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">schedule</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;TODO: Implement </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> schedule&quot;</span><span class="p">)</span>
        <span class="k">continue</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">hist</span> <span class="o">=</span> <span class="n">sgd</span><span class="p">(</span><span class="n">stint_grad</span><span class="p">,</span> <span class="n">setup_init</span><span class="p">,</span> <span class="n">schedule</span><span class="p">,</span> <span class="n">n_sessions_sched</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">stint_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">full_lap_error</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">: final lap time error = </span><span class="si">{</span><span class="n">hist</span><span class="p">[</span><span class="s1">&#39;f&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Expected: Cosine annealing or step decay should outperform constant rate&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Constant: final lap time error = 0.091308
TODO: Implement Step Decay schedule
TODO: Implement Cosine Annealing schedule

Expected: Cosine annealing or step decay should outperform constant rate
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<section id="key-concepts">
<h3>Key Concepts<a class="headerlink" href="#key-concepts" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Gradient descent convergence</strong> depends on convexity and the condition number. Strongly convex functions converge exponentially; convex functions converge as <span class="math notranslate nohighlight">\(O(1/t)\)</span>. In F1 terms: a well-understood, balanced car converges to optimal setup much faster than a car with unpredictable behavior.</p></li>
<li><p><strong>Step size selection</strong> is critical: too large causes divergence, too small wastes compute. The optimal step size is <span class="math notranslate nohighlight">\(\eta = 1/L\)</span> where <span class="math notranslate nohighlight">\(L\)</span> is the Lipschitz constant. Engineers who change setup too aggressively oscillate; too conservatively, they run out of sessions.</p></li>
<li><p><strong>SGD</strong> trades exact gradients for computational efficiency, and the noise provides implicit regularization that often improves generalization. Learning from individual laps (noisy but fast) often builds more robust setups than waiting for full session averages.</p></li>
<li><p><strong>Bias-variance tradeoff</strong> decomposes test error into underfitting (bias) and overfitting (variance). A conservative “works everywhere” setup has high bias; an over-specialized setup has high variance.</p></li>
<li><p><strong>VC dimension</strong> measures model capacity — the largest dataset a model class can perfectly memorize for any labeling. It quantifies how much complexity you need to predict lap times.</p></li>
<li><p><strong>PAC learning</strong> provides sample complexity bounds: how many laps you need for a given accuracy and confidence level.</p></li>
<li><p><strong>L1 regularization</strong> produces sparse solutions (zeroing out unimportant setup parameters) while <strong>L2 regularization</strong> produces small, balanced parameters. This follows from the geometry of their constraint regions.</p></li>
<li><p><strong>Overparameterized networks</strong> generalize better than classical theory predicts, due to implicit regularization, the double descent phenomenon, and the lottery ticket hypothesis.</p></li>
</ul>
</section>
<section id="connection-to-machine-learning">
<h3>Connection to Machine Learning<a class="headerlink" href="#connection-to-machine-learning" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Concept</p></th>
<th class="head"><p>ML Application</p></th>
<th class="head"><p>Why It Matters</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Convergence rates</p></td>
<td><p>Learning rate tuning</p></td>
<td><p>Knowing the theory prevents trial-and-error</p></td>
<td><p>How fast the car converges to optimal setup across FP1-FP3</p></td>
</tr>
<tr class="row-odd"><td><p>Condition number</p></td>
<td><p>Feature normalization, BatchNorm</p></td>
<td><p>Explains why preprocessing helps</p></td>
<td><p>Car balance — unequal parameter sensitivity causes zigzag convergence</p></td>
</tr>
<tr class="row-even"><td><p>SGD noise</p></td>
<td><p>Generalization, escaping local minima</p></td>
<td><p>Why SGD often beats full-batch GD</p></td>
<td><p>Learning from individual laps builds more robust setups</p></td>
</tr>
<tr class="row-odd"><td><p>Bias-variance</p></td>
<td><p>Model selection, hyperparameter tuning</p></td>
<td><p>The fundamental tradeoff in all of ML</p></td>
<td><p>Conservative vs specialized setup — safe but slow vs fast but fragile</p></td>
</tr>
<tr class="row-even"><td><p>VC dimension</p></td>
<td><p>Choosing model architecture</p></td>
<td><p>Capacity must match data complexity</p></td>
<td><p>How many telemetry channels you need to predict lap times</p></td>
</tr>
<tr class="row-odd"><td><p>PAC learning</p></td>
<td><p>Dataset size requirements</p></td>
<td><p>How much data you actually need</p></td>
<td><p>How many laps before you can trust your setup</p></td>
</tr>
<tr class="row-even"><td><p>L1/L2 regularization</p></td>
<td><p>Dropout, weight decay, pruning</p></td>
<td><p>Most common tools against overfitting</p></td>
<td><p>Simplifying setup (L1) vs keeping it balanced (L2)</p></td>
</tr>
<tr class="row-odd"><td><p>Double descent</p></td>
<td><p>Why large models work</p></td>
<td><p>Challenges classical “simpler is better” wisdom</p></td>
<td><p>Why large teams with “redundant” engineers outperform small ones</p></td>
</tr>
<tr class="row-even"><td><p>Lottery tickets</p></td>
<td><p>Network pruning, efficient inference</p></td>
<td><p>90%+ of weights may be unnecessary</p></td>
<td><p>The core subteam doing most of the engineering work</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="checklist">
<h3>Checklist<a class="headerlink" href="#checklist" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>[ ] I can explain why strongly convex functions converge faster than convex ones</p></li>
<li><p>[ ] I can choose an appropriate learning rate given a function’s properties</p></li>
<li><p>[ ] I understand why SGD’s noise is a feature, not a bug</p></li>
<li><p>[ ] I can decompose test error into bias, variance, and irreducible noise</p></li>
<li><p>[ ] I can explain what VC dimension measures and why it matters</p></li>
<li><p>[ ] I can use PAC bounds to estimate required dataset sizes</p></li>
<li><p>[ ] I understand geometrically why L1 gives sparsity (diamond vs sphere)</p></li>
<li><p>[ ] I can explain the double descent curve and why overparameterization helps</p></li>
<li><p>[ ] I know what the lottery ticket hypothesis claims and its implications</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="next-steps">
<h2>Next Steps<a class="headerlink" href="#next-steps" title="Link to this heading">#</a></h2>
<p>In <strong>Part 4.1: Perceptrons &amp; Basic Networks</strong> (Notebook 09), we shift from theory to building. You’ll implement your first neural network from scratch — a single perceptron, then multi-layer networks with forward propagation and loss functions. The optimization theory from this notebook will come alive as you see gradient descent actually training a model.</p>
<p><strong>Key connections to look forward to:</strong></p>
<ul class="simple">
<li><p>Gradient descent (Sections 1-3) becomes the engine that trains every neural network — like the setup convergence process that runs every race weekend</p></li>
<li><p>Regularization shows up as <code class="docutils literal notranslate"><span class="pre">weight_decay</span></code> parameters in optimizers — preventing the network from over-specializing, just like preventing extreme car setups</p></li>
<li><p>Bias-variance tradeoff drives model architecture and hyperparameter choices — the same tension between conservative and aggressive setup philosophies</p></li>
<li><p>SGD vs Adam (Section 3) becomes a practical choice you’ll make in every training loop — full session analysis vs adaptive per-parameter optimization</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="07_optimization_linear_programming.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Part 3.2: Optimization &amp; Linear Programming</p>
      </div>
    </a>
    <a class="right-next"
       href="09_perceptrons_basic_networks.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Part 4.1: Perceptrons &amp; Basic Networks</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-convergence">1. Gradient Descent Convergence</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuitive-explanation">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-f1-connection">The F1 Connection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-choosing-the-right-setup-change-rate">Deep Dive: Choosing the Right Setup Change Rate</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convexity-and-guarantees">2. Convexity and Guarantees</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">The F1 Connection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-condition-number">Deep Dive: Condition Number</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent">3. Stochastic Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">The F1 Connection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-why-sgd-works-so-well-in-practice">Deep Dive: Why SGD Works So Well in Practice</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bias-variance-tradeoff">4. The Bias-Variance Tradeoff</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">The F1 Connection</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vc-dimension-and-generalization">5. VC Dimension and Generalization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">The F1 Connection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-the-vc-generalization-bound">Deep Dive: The VC Generalization Bound</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#common-misconceptions">Common Misconceptions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pac-learning">6. PAC Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">The F1 Connection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-what-pac-learning-tells-us-about-ml">Deep Dive: What PAC Learning Tells Us About ML</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-theory">7. Regularization Theory</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">The F1 Connection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-regularization-as-bayesian-priors">Deep Dive: Regularization as Bayesian Priors</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#key-insight">Key Insight</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-overparameterized-networks-generalize">8. Why Overparameterized Networks Generalize</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">The F1 Connection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-the-lottery-ticket-hypothesis">Deep Dive: The Lottery Ticket Hypothesis</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#open-questions-honest-assessment">Open Questions (Honest Assessment)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-implement-setup-optimization-with-momentum">Exercise 1: Implement Setup Optimization with Momentum</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-cross-validation-for-lap-time-model-selection">Exercise 2: Cross-Validation for Lap Time Model Selection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-compare-setup-regularization-strategies">Exercise 3: Compare Setup Regularization Strategies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-4-bonus-setup-change-rate-schedules">Exercise 4 (Bonus): Setup Change Rate Schedules</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts">Key Concepts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-machine-learning">Connection to Machine Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checklist">Checklist</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">Next Steps</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dan Shah
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>