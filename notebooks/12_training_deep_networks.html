
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Part 4.4: Training Deep Networks &#8212; Foundations of AI</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=9c3e77be" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=1ae7504c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/12_training_deep_networks';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Part 5.1: Convolutional Neural Networks (CNNs)" href="13_convolutional_neural_networks.html" />
    <link rel="prev" title="Part 4.3: PyTorch Fundamentals" href="11_pytorch_fundamentals.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Foundations of AI</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 1: Mathematical Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_linear_algebra.html">Part 1.1: Linear Algebra for Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_calculus.html">Part 1.2: Calculus for Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_probability_statistics.html">Part 1.3: Probability &amp; Statistics for Deep Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 2: Programming Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_python_oop.html">Part 2.1: Python OOP for Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_numpy_deep_dive.html">Part 2.2: NumPy Deep Dive</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 3: Classical ML &amp; Optimization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="06_classical_ml.html">Part 3.1: Classical Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_optimization_linear_programming.html">Part 3.2: Optimization &amp; Linear Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_optimization_theory.html">Part 3.3: Optimization Theory for Machine Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 4: Neural Network Fundamentals</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="09_perceptrons_basic_networks.html">Part 4.1: Perceptrons &amp; Basic Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_backpropagation.html">Part 4.2: Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_pytorch_fundamentals.html">Part 4.3: PyTorch Fundamentals</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Part 4.4: Training Deep Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 5: Neural Network Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="13_convolutional_neural_networks.html">Part 5.1: Convolutional Neural Networks (CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_computer_vision_depth.html">Part 5.2: Computer Vision — Beyond Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_recurrent_neural_networks.html">Part 5.3: Recurrent Neural Networks (RNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_attention_mechanisms.html">Part 5.4: Attention Mechanisms</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 6: Transformers &amp; LLMs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="17_transformer_architecture.html">Part 6.1: Transformer Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_embeddings.html">Part 6.2: Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="19_tokenization_lm_training.html">Part 6.3: Tokenization &amp; Language Model Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="20_language_models.html">Part 6.4: Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="21_finetuning_and_peft.html">Part 6.5: Fine-tuning &amp; PEFT</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 7: Reinforcement Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="22_rl_fundamentals.html">Part 7.1: Reinforcement Learning Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="23_q_learning_dqn.html">Part 7.2: Q-Learning and Deep Q-Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="24_policy_gradients.html">Part 7.3: Policy Gradient Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="25_ppo_modern_rl.html">Part 7.4: PPO and Modern RL</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 8: Applied AI Systems</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="26_rag.html">Part 8.1: Retrieval-Augmented Generation (RAG)</a></li>
<li class="toctree-l1"><a class="reference internal" href="27_ai_agents.html">Part 8.2: AI Agents and Tool Use</a></li>
<li class="toctree-l1"><a class="reference internal" href="28_ai_evals.html">Part 8.3: Evaluating AI Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="29_production_monitoring.html">Part 8.4: Production AI Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 9: Advanced Topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="30_inference_optimization.html">Part 9.1: LLM Inference Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="31_ml_systems.html">Part 9.2: ML Systems &amp; Experiment Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="32_multimodal_ai.html">Part 9.3: Multimodal AI</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/dan-shah/foundations-of-ai/blob/main/notebooks/12_training_deep_networks.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/dan-shah/foundations-of-ai" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/dan-shah/foundations-of-ai/edit/main/notebooks/12_training_deep_networks.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/dan-shah/foundations-of-ai/issues/new?title=Issue%20on%20page%20%2Fnotebooks/12_training_deep_networks.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notebooks/12_training_deep_networks.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Part 4.4: Training Deep Networks</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizers">1. Optimizers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuitive-explanation">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sgd-stochastic-gradient-descent">1.1 SGD (Stochastic Gradient Descent)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#breaking-down-the-formula">Breaking down the formula:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#momentum">1.2 Momentum</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rmsprop">1.3 RMSprop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adam-adaptive-moment-estimation">1.4 Adam (Adaptive Moment Estimation)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-why-adam-is-usually-the-default">Deep Dive: Why Adam is Usually the Default</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#key-insight">Key Insight</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#common-misconceptions">Common Misconceptions</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-optimizer-paths-on-loss-surface">Visualization: Optimizer Paths on Loss Surface</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizer-comparison-table">Optimizer Comparison Table</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-this-matters-in-machine-learning">Why This Matters in Machine Learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate">2. Learning Rate</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-too-high-vs-too-low">Visualization: Too High vs Too Low</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-schedulers">2.1 Learning Rate Schedulers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warmup-explained">2.2 Warmup Explained</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-finder-brief-mention">2.3 Learning Rate Finder (Brief Mention)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization">3. Regularization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-the-overfitting-problem">Visualization: The Overfitting Problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#l2-regularization-weight-decay">3.1 L2 Regularization (Weight Decay)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#l1-regularization-sparsity">3.2 L1 Regularization (Sparsity)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout">3.3 Dropout</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#early-stopping">3.4 Early Stopping</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-comparison-table">Regularization Comparison Table</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Why This Matters in Machine Learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-normalization">4. Batch Normalization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuitive-explanation-the-problem">Intuitive Explanation: The Problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-batchnorm-does">What BatchNorm Does</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-vs-eval-mode">Training vs Eval Mode</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-why-batchnorm-helps">Deep Dive: Why BatchNorm Helps</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Key Insight</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Common Misconceptions</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layernorm-for-transformers">LayerNorm: For Transformers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-initialization">5. Weight Initialization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-why-initialization-matters">Visualization: Why Initialization Matters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#xavier-glorot-initialization">5.1 Xavier/Glorot Initialization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kaiming-he-initialization">5.2 Kaiming/He Initialization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#which-initialization-to-use">Which Initialization to Use?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together">6. Putting It All Together</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-optimizer-comparison">Exercise 1: Optimizer Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-implement-early-stopping">Exercise 2: Implement Early Stopping</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-dropout-rate-experiment">Exercise 3: Dropout Rate Experiment</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts">Key Concepts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-deep-learning">Connection to Deep Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checklist">Checklist</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">Next Steps</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="part-4-4-training-deep-networks">
<h1>Part 4.4: Training Deep Networks<a class="headerlink" href="#part-4-4-training-deep-networks" title="Link to this heading">#</a></h1>
<p>Building a neural network architecture is only half the battle. The real challenge lies in <strong>training</strong> it effectively. In this notebook, we explore the techniques that make deep learning work in practice: optimizers that navigate complex loss landscapes, regularization methods that prevent overfitting, and normalization techniques that stabilize training.</p>
<p><strong>F1 analogy:</strong> Designing the car (architecture) is one thing. Getting it to go fast on race day is another. This notebook is about the engineering science of car setup: how to tune the setup parameters (optimizers), how to ensure the car works on all tracks and not just one (regularization), and how to keep the car stable through varying conditions (normalization). The difference between a championship-winning team and a backmarker is not the car concept – it is the quality of the development process.</p>
<hr class="docutils" />
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<p>By the end of this notebook, you should be able to:</p>
<ul class="simple">
<li><p>[ ] Explain the intuition behind SGD, Momentum, RMSprop, and Adam optimizers</p></li>
<li><p>[ ] Choose appropriate learning rate schedules for different training scenarios</p></li>
<li><p>[ ] Apply regularization techniques (L1, L2, Dropout, Early Stopping) to prevent overfitting</p></li>
<li><p>[ ] Understand when and why to use Batch Normalization vs LayerNorm</p></li>
<li><p>[ ] Select proper weight initialization for different activation functions</p></li>
<li><p>[ ] Build a complete training pipeline with best practices</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">TensorDataset</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn-v0_8-whitegrid&#39;</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="optimizers">
<h2>1. Optimizers<a class="headerlink" href="#optimizers" title="Link to this heading">#</a></h2>
<section id="intuitive-explanation">
<h3>Intuitive Explanation<a class="headerlink" href="#intuitive-explanation" title="Link to this heading">#</a></h3>
<p>Imagine you’re trying to find the lowest point in a mountainous landscape while blindfolded. All you can feel is the slope beneath your feet. <strong>Gradient descent</strong> is your basic strategy: always step downhill. But this simple approach has problems:</p>
<ol class="arabic simple">
<li><p><strong>Getting stuck in valleys</strong>: You might oscillate back and forth across a narrow valley instead of moving along it</p></li>
<li><p><strong>Slow progress on plateaus</strong>: Flat regions give tiny gradients, leading to tiny steps</p></li>
<li><p><strong>Different terrain scales</strong>: Some directions might be steep, others gentle</p></li>
</ol>
<p>Different optimizers address these challenges in different ways.</p>
<p><strong>F1 analogy:</strong> Optimizers are setup tuning strategies. SGD is the conservative engineer who makes one small change at a time and re-tests. Momentum is the engineer who notices “we have been improving by adding downforce for three sessions, so keep pushing in that direction.” Adam is the veteran engineer who adapts their approach to each parameter – making big changes where the car is clearly off and small refinements where it is already close.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Optimizer</p></th>
<th class="head"><p>Strategy</p></th>
<th class="head"><p>F1 Analogy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>SGD</strong></p></td>
<td><p>Follow the gradient</p></td>
<td><p>Conservative: one small setup change at a time</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Momentum</strong></p></td>
<td><p>Build up velocity</p></td>
<td><p>Aggressive: if the last 3 changes all went the same way, commit harder</p></td>
</tr>
<tr class="row-even"><td><p><strong>RMSprop</strong></p></td>
<td><p>Adapt step size per dimension</p></td>
<td><p>Smart: big changes for insensitive parameters, small for sensitive ones</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Adam</strong></p></td>
<td><p>Momentum + adaptive step sizes</p></td>
<td><p>Veteran: momentum awareness + parameter-specific tuning</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="sgd-stochastic-gradient-descent">
<h3>1.1 SGD (Stochastic Gradient Descent)<a class="headerlink" href="#sgd-stochastic-gradient-descent" title="Link to this heading">#</a></h3>
<p>The simplest optimizer. Update rule:</p>
<div class="math notranslate nohighlight">
\[\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t)\]</div>
<section id="breaking-down-the-formula">
<h4>Breaking down the formula:<a class="headerlink" href="#breaking-down-the-formula" title="Link to this heading">#</a></h4>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Meaning</p></th>
<th class="head"><p>Typical Values</p></th>
<th class="head"><p>F1 Analogy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\theta_t\)</span></p></td>
<td><p>Current weights</p></td>
<td><p>-</p></td>
<td><p>Current car setup</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\eta\)</span></p></td>
<td><p>Learning rate</p></td>
<td><p>0.001 to 0.1</p></td>
<td><p>How big each setup adjustment is</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\nabla L\)</span></p></td>
<td><p>Gradient of loss</p></td>
<td><p>Computed via backprop</p></td>
<td><p>Which direction to adjust each parameter</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>What this means:</strong> Move in the opposite direction of the gradient, scaled by learning rate. Simple but can be slow and get stuck oscillating.</p>
<p><strong>F1 analogy:</strong> This is the most conservative setup strategy. After each session, you look at the data, identify which parameter is most responsible for the time loss, and adjust it by a small fixed amount. It works, but it is slow – especially when one parameter needs a big change and another needs a tiny one, since SGD uses the same step size for everything.</p>
</section>
</section>
<section id="momentum">
<h3>1.2 Momentum<a class="headerlink" href="#momentum" title="Link to this heading">#</a></h3>
<p><strong>Intuition: A ball rolling downhill</strong></p>
<p>Instead of just following the current gradient, momentum keeps track of which direction we’ve been moving. Like a ball rolling downhill, we build up velocity and can push through small bumps.</p>
<div class="math notranslate nohighlight">
\[v_{t+1} = \beta v_t + \nabla L(\theta_t)\]</div>
<div class="math notranslate nohighlight">
\[\theta_{t+1} = \theta_t - \eta v_{t+1}\]</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Meaning</p></th>
<th class="head"><p>Typical Values</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(v_t\)</span></p></td>
<td><p>Velocity (accumulated gradient)</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\beta\)</span></p></td>
<td><p>Momentum coefficient</p></td>
<td><p>0.9</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>What this means:</strong> We blend the current gradient with our previous direction. This helps us:</p>
<ul class="simple">
<li><p>Move faster in consistent directions</p></li>
<li><p>Dampen oscillations in inconsistent directions</p></li>
<li><p>Escape shallow local minima</p></li>
</ul>
<p><strong>F1 analogy:</strong> This is the engineer who tracks trends across sessions. If adding downforce has improved lap time for the last three practice sessions, momentum says “keep going in that direction with confidence” rather than starting fresh each time. If one session says “add downforce” and the next says “remove downforce,” momentum dampens that oscillation. The <span class="math notranslate nohighlight">\(\beta = 0.9\)</span> means 90% of the previous direction is retained – a strong memory of recent trends.</p>
</section>
<section id="rmsprop">
<h3>1.3 RMSprop<a class="headerlink" href="#rmsprop" title="Link to this heading">#</a></h3>
<p><strong>Intuition: Adaptive step sizes</strong></p>
<p>Some parameters need big updates, others need small ones. RMSprop tracks how variable each gradient has been and adjusts accordingly.</p>
<div class="math notranslate nohighlight">
\[s_{t+1} = \beta s_t + (1-\beta)(\nabla L)^2\]</div>
<div class="math notranslate nohighlight">
\[\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{s_{t+1} + \epsilon}} \nabla L\]</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Meaning</p></th>
<th class="head"><p>Typical Values</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(s_t\)</span></p></td>
<td><p>Running average of squared gradients</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\epsilon\)</span></p></td>
<td><p>Small constant for stability</p></td>
<td><p><span class="math notranslate nohighlight">\(10^{-8}\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>What this means:</strong></p>
<ul class="simple">
<li><p>Parameters with large, variable gradients get smaller updates</p></li>
<li><p>Parameters with small, consistent gradients get larger updates</p></li>
<li><p>This “evens out” the optimization across all parameters</p></li>
</ul>
</section>
<section id="adam-adaptive-moment-estimation">
<h3>1.4 Adam (Adaptive Moment Estimation)<a class="headerlink" href="#adam-adaptive-moment-estimation" title="Link to this heading">#</a></h3>
<p><strong>Intuition: The best of both worlds</strong></p>
<p>Adam combines momentum (first moment) with RMSprop’s adaptive learning rates (second moment).</p>
<div class="math notranslate nohighlight">
\[m_{t+1} = \beta_1 m_t + (1-\beta_1) \nabla L \quad \text{(momentum)}\]</div>
<div class="math notranslate nohighlight">
\[v_{t+1} = \beta_2 v_t + (1-\beta_2) (\nabla L)^2 \quad \text{(adaptive rates)}\]</div>
<div class="math notranslate nohighlight">
\[\hat{m} = \frac{m_{t+1}}{1-\beta_1^t}, \quad \hat{v} = \frac{v_{t+1}}{1-\beta_2^t} \quad \text{(bias correction)}\]</div>
<div class="math notranslate nohighlight">
\[\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}} + \epsilon} \hat{m}\]</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Meaning</p></th>
<th class="head"><p>Default Value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\beta_1\)</span></p></td>
<td><p>Momentum decay</p></td>
<td><p>0.9</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\beta_2\)</span></p></td>
<td><p>RMSprop decay</p></td>
<td><p>0.999</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\epsilon\)</span></p></td>
<td><p>Stability constant</p></td>
<td><p><span class="math notranslate nohighlight">\(10^{-8}\)</span></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="deep-dive-why-adam-is-usually-the-default">
<h3>Deep Dive: Why Adam is Usually the Default<a class="headerlink" href="#deep-dive-why-adam-is-usually-the-default" title="Link to this heading">#</a></h3>
<p>Adam has become the go-to optimizer for most deep learning tasks. Here’s why:</p>
<ol class="arabic simple">
<li><p><strong>Works well out of the box</strong>: The default hyperparameters (<span class="math notranslate nohighlight">\(\beta_1=0.9\)</span>, <span class="math notranslate nohighlight">\(\beta_2=0.999\)</span>, <span class="math notranslate nohighlight">\(\eta=0.001\)</span>) work well for most problems</p></li>
<li><p><strong>Handles sparse gradients</strong>: The adaptive learning rates help when some features appear rarely</p></li>
<li><p><strong>Robust to hyperparameter choices</strong>: Less sensitive to learning rate than SGD</p></li>
<li><p><strong>Fast convergence</strong>: Combines the speed benefits of momentum with adaptive rates</p></li>
</ol>
<p><strong>F1 analogy:</strong> Adam is like the veteran race engineer who has tuned hundreds of cars across dozens of circuits. They use momentum (remembering what worked in recent sessions) combined with adaptive step sizes (making big changes to the wing but tiny adjustments to the differential). They know the default starting point that works 90% of the time, and they adapt from there. A rookie engineer (SGD) might find a better setup given enough time, but the veteran (Adam) gets you competitive much faster.</p>
<section id="key-insight">
<h4>Key Insight<a class="headerlink" href="#key-insight" title="Link to this heading">#</a></h4>
<p>Adam is like having an experienced hiker guide you through the mountains. They know when to speed up on clear paths and slow down on tricky terrain.</p>
</section>
<section id="common-misconceptions">
<h4>Common Misconceptions<a class="headerlink" href="#common-misconceptions" title="Link to this heading">#</a></h4>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Misconception</p></th>
<th class="head"><p>Reality</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>“Adam always beats SGD”</p></td>
<td><p>SGD+momentum often achieves better final accuracy on vision tasks</p></td>
</tr>
<tr class="row-odd"><td><p>“Adam doesn’t need LR tuning”</p></td>
<td><p>Still benefits from LR scheduling</p></td>
</tr>
<tr class="row-even"><td><p>“Use Adam for everything”</p></td>
<td><p>For transformers yes, but try SGD for CNNs</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="visualization-optimizer-paths-on-loss-surface">
<h3>Visualization: Optimizer Paths on Loss Surface<a class="headerlink" href="#visualization-optimizer-paths-on-loss-surface" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Rosenbrock function - a classic optimization test function.</span>
<span class="sd">    </span>
<span class="sd">    Has a narrow curved valley that&#39;s easy to find but hard to follow.</span>
<span class="sd">    Minimum at (1, 1).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span><span class="w"> </span><span class="nf">rosenbrock_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gradient of Rosenbrock function.&quot;&quot;&quot;</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mi">400</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">dy</span> <span class="o">=</span> <span class="mi">200</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">])</span>

<span class="c1"># Optimizer implementations from scratch</span>
<span class="k">class</span><span class="w"> </span><span class="nc">SGDOptimizer</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">params</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MomentumOptimizer</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="kc">None</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">+</span> <span class="n">grad</span>
        <span class="k">return</span> <span class="n">params</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span>

<span class="k">class</span><span class="w"> </span><span class="nc">RMSpropOptimizer</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">s</span> <span class="o">=</span> <span class="kc">None</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">s</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">s</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span><span class="o">**</span><span class="mi">2</span>
        <span class="k">return</span> <span class="n">params</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">AdamOptimizer</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span> <span class="o">=</span> <span class="n">beta1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span> <span class="o">=</span> <span class="n">beta2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">m_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">)</span>
        <span class="n">v_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">params</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">m_hat</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v_hat</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">optimize_path</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">grad_fn</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="mi">500</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Run optimization and record the path.&quot;&quot;&quot;</span>
    <span class="n">path</span> <span class="o">=</span> <span class="p">[</span><span class="n">start</span><span class="o">.</span><span class="n">copy</span><span class="p">()]</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">start</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
        <span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        
        <span class="c1"># Stop if converged or diverged</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">params</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]))</span> <span class="o">&lt;</span> <span class="mf">0.01</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">params</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">):</span>
            <span class="k">break</span>
            
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

<span class="c1"># Starting point</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>

<span class="c1"># Run each optimizer with tuned learning rates</span>
<span class="n">optimizers</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;SGD&#39;</span><span class="p">:</span> <span class="n">SGDOptimizer</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0002</span><span class="p">),</span>
    <span class="s1">&#39;Momentum&#39;</span><span class="p">:</span> <span class="n">MomentumOptimizer</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0002</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.9</span><span class="p">),</span>
    <span class="s1">&#39;RMSprop&#39;</span><span class="p">:</span> <span class="n">RMSpropOptimizer</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span>
    <span class="s1">&#39;Adam&#39;</span><span class="p">:</span> <span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">paths</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">opt</span> <span class="ow">in</span> <span class="n">optimizers</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">paths</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">optimize_path</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">rosenbrock_grad</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize the loss surface and optimization paths</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Create meshgrid for contour plot</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">rosenbrock</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

<span class="c1"># Left plot: All paths on contour</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;r*&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Minimum (1,1)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">start</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">start</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;ko&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Start&#39;</span><span class="p">)</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;SGD&#39;</span><span class="p">:</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;Momentum&#39;</span><span class="p">:</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;RMSprop&#39;</span><span class="p">:</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;Adam&#39;</span><span class="p">:</span> <span class="s1">&#39;orange&#39;</span><span class="p">}</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">paths</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">path</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">path</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">name</span><span class="p">],</span> 
            <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1"> (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">path</span><span class="p">)</span><span class="si">}</span><span class="s1"> steps)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Optimizer Paths on Rosenbrock Function&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1"># Right plot: Loss over steps</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">paths</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[</span><span class="n">rosenbrock</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">path</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">name</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Step&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Loss (log scale)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Loss Convergence&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Print final positions</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final positions and loss:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">paths</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">final</span> <span class="o">=</span> <span class="n">path</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">final_loss</span> <span class="o">=</span> <span class="n">rosenbrock</span><span class="p">(</span><span class="n">final</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">final</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">name</span><span class="si">:</span><span class="s2">10s</span><span class="si">}</span><span class="s2">: (</span><span class="si">{</span><span class="n">final</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">final</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">), loss=</span><span class="si">{</span><span class="n">final_loss</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="optimizer-comparison-table">
<h3>Optimizer Comparison Table<a class="headerlink" href="#optimizer-comparison-table" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Optimizer</p></th>
<th class="head"><p>Pros</p></th>
<th class="head"><p>Cons</p></th>
<th class="head"><p>When to Use</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>SGD</strong></p></td>
<td><p>Simple, less memory</p></td>
<td><p>Slow, sensitive to LR</p></td>
<td><p>Final fine-tuning, simple problems</p></td>
<td><p>Conservative approach: one change at a time</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Momentum</strong></p></td>
<td><p>Faster than SGD, escapes local minima</p></td>
<td><p>Still sensitive to LR</p></td>
<td><p>CNNs, when SGD is too slow</p></td>
<td><p>Trend-following: commit to directions that keep working</p></td>
</tr>
<tr class="row-even"><td><p><strong>RMSprop</strong></p></td>
<td><p>Adaptive LR per parameter</p></td>
<td><p>Can diverge with wrong settings</p></td>
<td><p>RNNs, non-stationary problems</p></td>
<td><p>Parameter-specific tuning intensity</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Adam</strong></p></td>
<td><p>Fast, robust, adaptive</p></td>
<td><p>Higher memory, can overfit</p></td>
<td><p>Default choice for most tasks</p></td>
<td><p>Veteran engineer: adapts to each parameter</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="why-this-matters-in-machine-learning">
<h3>Why This Matters in Machine Learning<a class="headerlink" href="#why-this-matters-in-machine-learning" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Application</p></th>
<th class="head"><p>Recommended Optimizer</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Computer vision (CNNs)</p></td>
<td><p>SGD + Momentum (best accuracy) or Adam (faster)</p></td>
<td><p>Fine-tuning downforce: patient, iterative</p></td>
</tr>
<tr class="row-odd"><td><p>NLP (Transformers)</p></td>
<td><p>Adam or AdamW</p></td>
<td><p>Real-time strategy adaptation</p></td>
</tr>
<tr class="row-even"><td><p>GANs</p></td>
<td><p>Adam with low beta1 (0.5)</p></td>
<td><p>Aggressive, exploratory setup changes</p></td>
</tr>
<tr class="row-odd"><td><p>Fine-tuning pretrained</p></td>
<td><p>Adam with small LR, or SGD</p></td>
<td><p>Gentle refinements to a known-good baseline</p></td>
</tr>
<tr class="row-even"><td><p>Quick prototyping</p></td>
<td><p>Adam (works out of the box)</p></td>
<td><p>Friday practice: get up to speed fast</p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorch optimizer examples</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># SGD with momentum</span>
<span class="n">sgd_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="c1"># Adam (most common default)</span>
<span class="n">adam_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="c1"># AdamW (Adam with proper weight decay - often better)</span>
<span class="n">adamw_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PyTorch optimizer examples:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  SGD: lr=</span><span class="si">{</span><span class="n">sgd_optimizer</span><span class="o">.</span><span class="n">defaults</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">, momentum=</span><span class="si">{</span><span class="n">sgd_optimizer</span><span class="o">.</span><span class="n">defaults</span><span class="p">[</span><span class="s1">&#39;momentum&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Adam: lr=</span><span class="si">{</span><span class="n">adam_optimizer</span><span class="o">.</span><span class="n">defaults</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">, betas=</span><span class="si">{</span><span class="n">adam_optimizer</span><span class="o">.</span><span class="n">defaults</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  AdamW: lr=</span><span class="si">{</span><span class="n">adamw_optimizer</span><span class="o">.</span><span class="n">defaults</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">, weight_decay=</span><span class="si">{</span><span class="n">adamw_optimizer</span><span class="o">.</span><span class="n">defaults</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="learning-rate">
<h2>2. Learning Rate<a class="headerlink" href="#learning-rate" title="Link to this heading">#</a></h2>
<section id="id1">
<h3>Intuitive Explanation<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>The <strong>learning rate</strong> is perhaps the single most important hyperparameter. It controls how big of a step you take with each update:</p>
<ul class="simple">
<li><p><strong>Too high</strong>: You overshoot the minimum, bouncing around wildly or even diverging</p></li>
<li><p><strong>Too low</strong>: Training takes forever and may get stuck in poor solutions</p></li>
<li><p><strong>Just right</strong>: Steady progress toward the minimum</p></li>
</ul>
<p><strong>F1 analogy:</strong> The learning rate is how big each setup adjustment is between sessions. If you change the front wing by 5 degrees at a time (too high), you will overshoot the sweet spot and oscillate wildly between understeer and oversteer. If you change it by 0.01 degrees (too low), you will never converge on the optimal setting before the weekend is over. The art is finding the right adjustment size – big enough to make progress, small enough not to overshoot. And as you get closer to the optimal setup, you should make smaller and smaller adjustments (learning rate scheduling).</p>
</section>
<section id="visualization-too-high-vs-too-low">
<h3>Visualization: Too High vs Too Low<a class="headerlink" href="#visualization-too-high-vs-too-low" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">simple_loss</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Simple 1D loss function: x^2&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span><span class="w"> </span><span class="nf">simple_grad</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gradient: 2x&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>

<span class="k">def</span><span class="w"> </span><span class="nf">gradient_descent_1d</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Run 1D gradient descent.&quot;&quot;&quot;</span>
    <span class="n">path</span> <span class="o">=</span> <span class="p">[</span><span class="n">start</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">start</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">simple_grad</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">100</span><span class="p">:</span>  <span class="c1"># Diverged</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">path</span>

<span class="c1"># Test different learning rates</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">start</span> <span class="o">=</span> <span class="mf">5.0</span>
<span class="n">n_steps</span> <span class="o">=</span> <span class="mi">20</span>

<span class="n">configs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;Too High (lr=1.1)&#39;</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Just Right (lr=0.3)&#39;</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Too Low (lr=0.01)&#39;</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">color</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">configs</span><span class="p">):</span>
    <span class="n">path</span> <span class="o">=</span> <span class="n">gradient_descent_1d</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">)</span>
    
    <span class="c1"># Plot x value over steps</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Step&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;x value&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">lr</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="learning-rate-schedulers">
<h3>2.1 Learning Rate Schedulers<a class="headerlink" href="#learning-rate-schedulers" title="Link to this heading">#</a></h3>
<p>A common strategy is to <strong>start with a larger learning rate</strong> for fast initial progress, then <strong>gradually reduce it</strong> to fine-tune the solution.</p>
<p><strong>F1 analogy:</strong> This is exactly how F1 teams approach a race weekend. In FP1 (Friday practice), you make bold setup changes to explore the performance landscape. In FP2, you narrow the range. By qualifying, you are making tiny refinements. Learning rate schedulers automate this progression.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Scheduler</p></th>
<th class="head"><p>Strategy</p></th>
<th class="head"><p>Use Case</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>StepLR</strong></p></td>
<td><p>Multiply by gamma every N epochs</p></td>
<td><p>Simple, predictable decay</p></td>
<td><p>Cut adjustment size at scheduled points (FP1 -&gt; FP2 -&gt; Quali)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>ExponentialLR</strong></p></td>
<td><p>Multiply by gamma each epoch</p></td>
<td><p>Smooth continuous decay</p></td>
<td><p>Gradually smaller changes every session</p></td>
</tr>
<tr class="row-even"><td><p><strong>CosineAnnealingLR</strong></p></td>
<td><p>Smooth cosine curve decay</p></td>
<td><p>Transformers, good generalization</p></td>
<td><p>Smooth transition from exploration to refinement</p></td>
</tr>
<tr class="row-odd"><td><p><strong>ReduceLROnPlateau</strong></p></td>
<td><p>Reduce when metric stops improving</p></td>
<td><p>Adaptive, data-driven</p></td>
<td><p>“If lap time stops improving, try smaller changes”</p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize different learning rate schedules</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">base_lr</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="k">def</span><span class="w"> </span><span class="nf">get_scheduler_lrs</span><span class="p">(</span><span class="n">scheduler_class</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get learning rates for a scheduler over epochs.&quot;&quot;&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">base_lr</span><span class="p">)</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">scheduler_class</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    
    <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">lrs</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="c1"># StepLR</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">lrs</span> <span class="o">=</span> <span class="n">get_scheduler_lrs</span><span class="p">(</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Learning Rate&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;StepLR (step=30, gamma=0.1)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># ExponentialLR</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">lrs</span> <span class="o">=</span> <span class="n">get_scheduler_lrs</span><span class="p">(</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ExponentialLR</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs</span><span class="p">,</span> <span class="s1">&#39;g-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Learning Rate&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;ExponentialLR (gamma=0.95)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># CosineAnnealingLR</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">lrs</span> <span class="o">=</span> <span class="n">get_scheduler_lrs</span><span class="p">(</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="n">epochs</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Learning Rate&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;CosineAnnealingLR&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># MultiStepLR</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">lrs</span> <span class="o">=</span> <span class="n">get_scheduler_lrs</span><span class="p">(</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">MultiStepLR</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">milestones</span><span class="o">=</span><span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="mi">80</span><span class="p">],</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs</span><span class="p">,</span> <span class="s1">&#39;purple&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Learning Rate&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;MultiStepLR (milestones=[30,60,80])&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Learning Rate Schedulers&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="warmup-explained">
<h3>2.2 Warmup Explained<a class="headerlink" href="#warmup-explained" title="Link to this heading">#</a></h3>
<p><strong>Warmup</strong> starts training with a very small learning rate and gradually increases it. This helps because:</p>
<ol class="arabic simple">
<li><p><strong>Early gradients are unreliable</strong>: Before the model has learned anything, gradients point in somewhat random directions</p></li>
<li><p><strong>Batch normalization needs time</strong>: BatchNorm statistics aren’t accurate initially</p></li>
<li><p><strong>Prevents early divergence</strong>: Large initial updates can push weights to bad regions</p></li>
</ol>
<p>Warmup is especially important for:</p>
<ul class="simple">
<li><p>Very deep networks</p></li>
<li><p>Large batch sizes</p></li>
<li><p>Transformer architectures</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">warmup_cosine_schedule</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">warmup_epochs</span><span class="p">,</span> <span class="n">total_epochs</span><span class="p">,</span> <span class="n">max_lr</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Linear warmup followed by cosine decay.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="n">warmup_epochs</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">max_lr</span> <span class="o">*</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">/</span> <span class="n">warmup_epochs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">progress</span> <span class="o">=</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">-</span> <span class="n">warmup_epochs</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">total_epochs</span> <span class="o">-</span> <span class="n">warmup_epochs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">max_lr</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">progress</span><span class="p">))</span>

<span class="c1"># Visualize warmup</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">warmup_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">max_lr</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Different schedules</span>
<span class="n">warmup_cosine</span> <span class="o">=</span> <span class="p">[</span><span class="n">warmup_cosine_schedule</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">warmup_epochs</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">max_lr</span><span class="p">)</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)]</span>
<span class="n">no_warmup</span> <span class="o">=</span> <span class="p">[</span><span class="n">max_lr</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">e</span> <span class="o">/</span> <span class="n">epochs</span><span class="p">))</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)]</span>
<span class="n">constant</span> <span class="o">=</span> <span class="p">[</span><span class="n">max_lr</span><span class="p">]</span> <span class="o">*</span> <span class="n">epochs</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">warmup_cosine</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Warmup + Cosine&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">no_warmup</span><span class="p">,</span> <span class="s1">&#39;g--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Cosine (no warmup)&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">constant</span><span class="p">,</span> <span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Constant&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">warmup_epochs</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;End warmup (epoch </span><span class="si">{</span><span class="n">warmup_epochs</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Learning Rate&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Learning Rate Warmup&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="learning-rate-finder-brief-mention">
<h3>2.3 Learning Rate Finder (Brief Mention)<a class="headerlink" href="#learning-rate-finder-brief-mention" title="Link to this heading">#</a></h3>
<p>The <strong>learning rate finder</strong> is a technique to automatically find a good learning rate:</p>
<ol class="arabic simple">
<li><p>Start with a very small learning rate</p></li>
<li><p>Train for a few iterations, gradually increasing the LR</p></li>
<li><p>Plot loss vs learning rate</p></li>
<li><p>Choose the LR where loss is decreasing fastest (steepest slope)</p></li>
</ol>
<p><strong>Rule of thumb:</strong> Pick a learning rate where the loss is clearly decreasing but hasn’t started to explode. Often about 10x smaller than where loss starts increasing.</p>
<p>Libraries like <code class="docutils literal notranslate"><span class="pre">pytorch-lightning</span></code> and <code class="docutils literal notranslate"><span class="pre">fastai</span></code> implement this automatically.</p>
</section>
</section>
<hr class="docutils" />
<section id="regularization">
<h2>3. Regularization<a class="headerlink" href="#regularization" title="Link to this heading">#</a></h2>
<section id="id2">
<h3>Intuitive Explanation<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p><strong>Overfitting</strong> happens when your model memorizes the training data instead of learning general patterns. It’s like a student who memorizes answers to practice problems but can’t solve new ones.</p>
<p><strong>Regularization</strong> techniques prevent overfitting by constraining the model’s capacity.</p>
<p><strong>F1 analogy:</strong> Overfitting is when the car is perfectly tuned for one specific track but falls apart everywhere else. A car that is overfit to Monaco (tight, slow corners) will have massive downforce and soft springs – but it will be hopeless on Monza (long straights, fast corners). Regularization is the engineering discipline of building a car that works well across the entire calendar, not just the track you tested on. It is the difference between winning one race and winning a championship.</p>
</section>
<section id="visualization-the-overfitting-problem">
<h3>Visualization: The Overfitting Problem<a class="headerlink" href="#visualization-the-overfitting-problem" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate data with noise</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">X_train</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

<span class="c1"># Fit polynomials of different degrees</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">degrees</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">15</span><span class="p">]</span>
<span class="n">titles</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Underfitting (degree=1)&#39;</span><span class="p">,</span> <span class="s1">&#39;Good Fit (degree=4)&#39;</span><span class="p">,</span> <span class="s1">&#39;Overfitting (degree=15)&#39;</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">degree</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">degrees</span><span class="p">,</span> <span class="n">titles</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
    <span class="c1"># Fit polynomial</span>
    <span class="n">coeffs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">degree</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyval</span><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
    
    <span class="c1"># Calculate errors</span>
    <span class="n">train_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyval</span><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
    <span class="n">train_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_train</span> <span class="o">-</span> <span class="n">train_pred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">test_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">X_test</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="c1"># Plot</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training data&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">X_test</span><span class="p">),</span> <span class="s1">&#39;g--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True function&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Model&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">title</span><span class="si">}</span><span class="se">\n</span><span class="s1">Train MSE: </span><span class="si">{</span><span class="n">train_mse</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">, Test MSE: </span><span class="si">{</span><span class="n">test_mse</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;The Bias-Variance Tradeoff&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="l2-regularization-weight-decay">
<h3>3.1 L2 Regularization (Weight Decay)<a class="headerlink" href="#l2-regularization-weight-decay" title="Link to this heading">#</a></h3>
<p><strong>Intuition:</strong> Penalize large weights to keep the model simple.</p>
<div class="math notranslate nohighlight">
\[L_{total} = L_{data} + \lambda \sum_i w_i^2\]</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Meaning</p></th>
<th class="head"><p>F1 Analogy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(L_{data}\)</span></p></td>
<td><p>Original loss (e.g., cross-entropy)</p></td>
<td><p>Lap time on the current track</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\lambda\)</span></p></td>
<td><p>Regularization strength (weight_decay)</p></td>
<td><p>How much you care about all-track performance</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\sum w_i^2\)</span></p></td>
<td><p>Sum of squared weights</p></td>
<td><p>How “extreme” your setup is from baseline</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>What this means:</strong> Large weights are costly, so the model prefers smaller weights. This leads to:</p>
<ul class="simple">
<li><p>Smoother decision boundaries</p></li>
<li><p>Less sensitivity to individual features</p></li>
<li><p>Better generalization</p></li>
</ul>
<p><strong>F1 analogy:</strong> L2 regularization is like penalizing extreme setup deviations. A car with wing angle at max, springs at minimum, and differential locked tight might be fast at one track, but it is “overfit” to those conditions. L2 says “there is a cost to being extreme.” The model (car) is incentivized to find a balanced setup that performs well broadly, rather than an extreme setup that only works in one specific condition.</p>
</section>
<section id="l1-regularization-sparsity">
<h3>3.2 L1 Regularization (Sparsity)<a class="headerlink" href="#l1-regularization-sparsity" title="Link to this heading">#</a></h3>
<p><strong>Intuition:</strong> Encourage weights to be exactly zero.</p>
<div class="math notranslate nohighlight">
\[L_{total} = L_{data} + \lambda \sum_i |w_i|\]</div>
<p><strong>What this means:</strong> Unlike L2 which makes weights small, L1 pushes weights all the way to zero. This creates <strong>sparse</strong> models where many weights are exactly 0.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Regularization</p></th>
<th class="head"><p>Effect on Weights</p></th>
<th class="head"><p>Use Case</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>L2 (Ridge)</p></td>
<td><p>Small but non-zero</p></td>
<td><p>General regularization</p></td>
</tr>
<tr class="row-odd"><td><p>L1 (Lasso)</p></td>
<td><p>Many exactly zero</p></td>
<td><p>Feature selection</p></td>
</tr>
<tr class="row-even"><td><p>L1 + L2 (Elastic Net)</p></td>
<td><p>Some zero, others small</p></td>
<td><p>Best of both</p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize L1 vs L2 regularization effect on weights</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Simulate weight distributions after regularization</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_weights</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># No regularization - weights can be large</span>
<span class="n">no_reg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_weights</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.5</span>

<span class="c1"># L2 regularization - weights are small but non-zero</span>
<span class="n">l2_reg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_weights</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.3</span>

<span class="c1"># L1 regularization - many weights exactly zero (Laplace distribution approximation)</span>
<span class="n">l1_reg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">laplace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">n_weights</span><span class="p">)</span>
<span class="n">l1_reg</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">l1_reg</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># More weights become exactly 0</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">no_reg</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Weight Value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Count&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;No Regularization&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">l2_reg</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Weight Value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Count&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;L2 Regularization</span><span class="se">\n</span><span class="s1">(small but non-zero)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">l1_reg</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Weight Value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Count&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;L1 Regularization</span><span class="se">\n</span><span class="s1">(</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">l1_reg</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="s1"> weights exactly 0)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="dropout">
<h3>3.3 Dropout<a class="headerlink" href="#dropout" title="Link to this heading">#</a></h3>
<p><strong>Intuition: Training an ensemble of networks</strong></p>
<p>During training, dropout randomly “turns off” neurons with probability <span class="math notranslate nohighlight">\(p\)</span>. This is like training many different smaller networks simultaneously.</p>
<p><strong>Why it works:</strong></p>
<ol class="arabic simple">
<li><p><strong>Prevents co-adaptation</strong>: Neurons can’t rely on specific other neurons always being there</p></li>
<li><p><strong>Ensemble effect</strong>: Like training many different networks and averaging them</p></li>
<li><p><strong>Forces redundancy</strong>: The network must learn robust features</p></li>
</ol>
<p><strong>Key insight:</strong> Dropout forces neurons to learn features that are useful on their own, not just in combination with specific other neurons.</p>
<p><strong>F1 analogy:</strong> Dropout is like randomly disabling sensors during testing to build robustness. Imagine running practice sessions where you randomly turn off the tire temperature sensor, or the brake temperature sensor, or the wind speed anemometer. The telemetry system cannot rely on any single sensor always being available – it must learn to make good predictions even with missing data. When race day comes and all sensors are active, the system is more robust because it never became dependent on any one input. This is exactly what dropout does to neural network layers.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize dropout as creating sub-networks</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">draw_network</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Draw a simple neural network with dropout visualization.&quot;&quot;&quot;</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span> <span class="o">+</span> <span class="nb">int</span><span class="p">(</span><span class="n">dropout_rate</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>  <span class="c1"># Different seeds for variety</span>
    
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
    <span class="n">positions</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">n_neurons</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layers</span><span class="p">):</span>
        <span class="n">layer_pos</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="p">(</span><span class="n">n_neurons</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.15</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer_idx</span> <span class="o">*</span> <span class="mf">0.3</span>
            
            <span class="c1"># Determine if neuron is dropped (not for input/output layers)</span>
            <span class="n">is_dropped</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="n">dropout_rate</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">layer_idx</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
                <span class="n">is_dropped</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">dropout_rate</span>
            
            <span class="n">layer_pos</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">is_dropped</span><span class="p">))</span>
            
            <span class="c1"># Draw neuron</span>
            <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;lightgray&#39;</span> <span class="k">if</span> <span class="n">is_dropped</span> <span class="k">else</span> <span class="s1">&#39;steelblue&#39;</span>
            <span class="n">edge</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span> <span class="k">if</span> <span class="n">is_dropped</span> <span class="k">else</span> <span class="s1">&#39;darkblue&#39;</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="n">edge</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        
        <span class="n">positions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer_pos</span><span class="p">)</span>
    
    <span class="c1"># Draw connections</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">positions</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">d1</span> <span class="ow">in</span> <span class="n">positions</span><span class="p">[</span><span class="n">l</span><span class="p">]:</span>
            <span class="k">for</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">d2</span> <span class="ow">in</span> <span class="n">positions</span><span class="p">[</span><span class="n">l</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">d1</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">d2</span><span class="p">:</span>
                    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">],</span> <span class="p">[</span><span class="n">y1</span><span class="p">,</span> <span class="n">y2</span><span class="p">],</span> <span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">draw_network</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;No Dropout&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">draw_network</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;Dropout p=0.3</span><span class="se">\n</span><span class="s1">(training step 1)&#39;</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">99</span><span class="p">)</span>
<span class="n">draw_network</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;Dropout p=0.3</span><span class="se">\n</span><span class="s1">(training step 2)&#39;</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Dropout Creates Different Sub-Networks Each Step&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Gray neurons are &#39;dropped&#39; - they don&#39;t participate in this forward/backward pass.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Each training step uses a different random sub-network!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dropout in PyTorch: Training vs Eval mode</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="c1"># Training mode (dropout active)</span>
<span class="n">dropout</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training mode (dropout ACTIVE):&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">out</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Evaluation mode (dropout disabled)</span>
<span class="n">dropout</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Evaluation mode (dropout DISABLED):&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">out</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Note: In training mode, surviving values are scaled by 1/(1-p)=2 to maintain expected value.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="early-stopping">
<h3>3.4 Early Stopping<a class="headerlink" href="#early-stopping" title="Link to this heading">#</a></h3>
<p><strong>Intuition:</strong> Stop training when validation performance starts to degrade.</p>
<p>Early stopping is a simple but effective form of regularization:</p>
<ol class="arabic simple">
<li><p>Monitor validation loss during training</p></li>
<li><p>Save the model when validation loss improves</p></li>
<li><p>Stop if validation loss hasn’t improved for N epochs (patience)</p></li>
<li><p>Restore the best saved model</p></li>
</ol>
<p><strong>F1 analogy:</strong> Early stopping is knowing when to stop chasing setup changes. There is a point in every race weekend where additional setup tweaks start making the car worse, not better – you have passed the optimum and are now overfitting to noise in the data (track temperature variations, wind gusts, tire batch differences). The experienced engineer knows when to say “the car is as good as it is going to get, lock it in.” The patience parameter is like giving yourself 3 more sessions to beat the current best before accepting it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simulate training with/without early stopping</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1"># Training loss keeps decreasing</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">epoch</span><span class="o">/</span><span class="mi">30</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.05</span> <span class="o">+</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span>
    
    <span class="c1"># Validation loss increases after epoch 40 (overfitting)</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="mi">40</span><span class="p">:</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">epoch</span><span class="o">/</span><span class="mi">30</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">+</span> <span class="mf">0.02</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="o">+</span> <span class="mf">0.004</span> <span class="o">*</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">-</span> <span class="mi">40</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.02</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span>
    
    <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">))</span>
    <span class="n">val_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">))</span>

<span class="n">best_epoch</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">val_losses</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training Loss&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_losses</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation Loss&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">best_epoch</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
           <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Best model (epoch </span><span class="si">{</span><span class="n">best_epoch</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvspan</span><span class="p">(</span><span class="n">best_epoch</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Overfitting region&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Early Stopping Prevents Overfitting&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best epoch: </span><span class="si">{</span><span class="n">best_epoch</span><span class="si">}</span><span class="s2"> with validation loss: </span><span class="si">{</span><span class="n">val_losses</span><span class="p">[</span><span class="n">best_epoch</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final epoch: </span><span class="si">{</span><span class="n">epochs</span><span class="o">-</span><span class="mi">1</span><span class="si">}</span><span class="s2"> with validation loss: </span><span class="si">{</span><span class="n">val_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Early stopping saves </span><span class="si">{</span><span class="n">val_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">val_losses</span><span class="p">[</span><span class="n">best_epoch</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> in validation loss!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="regularization-comparison-table">
<h3>Regularization Comparison Table<a class="headerlink" href="#regularization-comparison-table" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Technique</p></th>
<th class="head"><p>How It Works</p></th>
<th class="head"><p>Hyperparameter</p></th>
<th class="head"><p>When to Use</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>L2 (Weight Decay)</strong></p></td>
<td><p>Penalizes large weights</p></td>
<td><p>weight_decay (0.01-0.1)</p></td>
<td><p>Always (default choice)</p></td>
<td><p>Penalizing extreme setup deviations</p></td>
</tr>
<tr class="row-odd"><td><p><strong>L1</strong></p></td>
<td><p>Encourages sparse weights</p></td>
<td><p>lambda</p></td>
<td><p>Feature selection needed</p></td>
<td><p>Identifying which sensors actually matter</p></td>
</tr>
<tr class="row-even"><td><p><strong>Dropout</strong></p></td>
<td><p>Randomly drops neurons</p></td>
<td><p>p (0.1-0.5)</p></td>
<td><p>Deep networks, overfitting</p></td>
<td><p>Randomly disabling sensors to build robustness</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Early Stopping</strong></p></td>
<td><p>Stop when val loss increases</p></td>
<td><p>patience (5-20 epochs)</p></td>
<td><p>Always monitor</p></td>
<td><p>Knowing when to lock in the setup</p></td>
</tr>
<tr class="row-even"><td><p><strong>Data Augmentation</strong></p></td>
<td><p>Artificially expand dataset</p></td>
<td><p>Aug. parameters</p></td>
<td><p>Images, audio, text</p></td>
<td><p>Simulating varied track conditions</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="id3">
<h3>Why This Matters in Machine Learning<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Scenario</p></th>
<th class="head"><p>Recommended Regularization</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Small dataset</p></td>
<td><p>Dropout + weight decay + data augmentation</p></td>
<td><p>Limited testing: maximize learning from few sessions</p></td>
</tr>
<tr class="row-odd"><td><p>Large dataset</p></td>
<td><p>Lighter regularization, early stopping</p></td>
<td><p>Full test schedule: data speaks for itself</p></td>
</tr>
<tr class="row-even"><td><p>Very deep network</p></td>
<td><p>Dropout (0.2-0.5) between dense layers</p></td>
<td><p>Complex telemetry pipeline: prevent over-specialization</p></td>
</tr>
<tr class="row-odd"><td><p>Transformers</p></td>
<td><p>Dropout in attention + AdamW weight decay</p></td>
<td><p>Modern F1 analytics with many interacting systems</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<hr class="docutils" />
<section id="batch-normalization">
<h2>4. Batch Normalization<a class="headerlink" href="#batch-normalization" title="Link to this heading">#</a></h2>
<section id="intuitive-explanation-the-problem">
<h3>Intuitive Explanation: The Problem<a class="headerlink" href="#intuitive-explanation-the-problem" title="Link to this heading">#</a></h3>
<p>As data flows through a deep network, the distribution of activations can shift dramatically between layers. This is called <strong>internal covariate shift</strong>.</p>
<p><strong>Analogy:</strong> Imagine trying to learn to catch balls, but the thrower keeps changing how they throw - sometimes fast, sometimes slow, sometimes high, sometimes low. It would be much easier if every throw was similar.</p>
<p><strong>Batch Normalization</strong> normalizes the activations within each mini-batch, making training faster and more stable.</p>
<p><strong>F1 analogy:</strong> Batch normalization is normalizing telemetry data across different track conditions. Tire temperature readings at Bahrain (50C ambient) look completely different from Finland testing (-5C ambient). If the downstream analysis system expects a certain range of inputs, these wild variations cause instability. BatchNorm standardizes the inputs at each processing stage so that the downstream systems always see data in a consistent range, regardless of whether it was collected at Bahrain or Spa. The learnable scale (<span class="math notranslate nohighlight">\(\gamma\)</span>) and shift (<span class="math notranslate nohighlight">\(\beta\)</span>) parameters let each layer find its own optimal operating range.</p>
</section>
<section id="what-batchnorm-does">
<h3>What BatchNorm Does<a class="headerlink" href="#what-batchnorm-does" title="Link to this heading">#</a></h3>
<p>For each mini-batch, BatchNorm:</p>
<ol class="arabic simple">
<li><p><strong>Normalize</strong>: Subtract mean, divide by standard deviation
$<span class="math notranslate nohighlight">\(\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}\)</span>$</p></li>
<li><p><strong>Scale and Shift</strong>: Apply learnable parameters <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>
$<span class="math notranslate nohighlight">\(y = \gamma \hat{x} + \beta\)</span>$</p></li>
</ol>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Meaning</p></th>
<th class="head"><p>F1 Analogy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mu_B\)</span></p></td>
<td><p>Mean of the batch</p></td>
<td><p>Average sensor reading across current conditions</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\sigma_B^2\)</span></p></td>
<td><p>Variance of the batch</p></td>
<td><p>How spread out the readings are</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\gamma\)</span></p></td>
<td><p>Learnable scale</p></td>
<td><p>Optimal sensitivity range for this processing stage</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\beta\)</span></p></td>
<td><p>Learnable shift</p></td>
<td><p>Optimal baseline for this processing stage</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Why the learnable parameters?</strong> They let the network undo the normalization if needed. The network learns the optimal distribution for each layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize what BatchNorm does</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Simulate activations before BatchNorm (shifted and scaled)</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">before_bn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">+</span> <span class="mi">5</span>

<span class="c1"># Apply BatchNorm manually</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">before_bn</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">before_bn</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
<span class="n">normalized</span> <span class="o">=</span> <span class="p">(</span><span class="n">before_bn</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">sigma</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>

<span class="c1"># Learnable scale and shift</span>
<span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.5</span>
<span class="n">after_bn</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">normalized</span> <span class="o">+</span> <span class="n">beta</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">before_bn</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Mean: </span><span class="si">{</span><span class="n">mu</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Activation Value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Count&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Before BatchNorm</span><span class="se">\n</span><span class="s1">(shifted, varying scale)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">normalized</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;yellow&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Mean: 0, Std: 1&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Activation Value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Count&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;After Normalization</span><span class="se">\n</span><span class="s1">(zero mean, unit variance)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">after_bn</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Mean: </span><span class="si">{</span><span class="n">after_bn</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Activation Value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Count&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;After Scale/Shift</span><span class="se">\n</span><span class="s1">(gamma=</span><span class="si">{</span><span class="n">gamma</span><span class="si">}</span><span class="s1">, beta=</span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-vs-eval-mode">
<h3>Training vs Eval Mode<a class="headerlink" href="#training-vs-eval-mode" title="Link to this heading">#</a></h3>
<p>BatchNorm behaves differently during training and evaluation:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Mode</p></th>
<th class="head"><p>Mean/Variance</p></th>
<th class="head"><p>Why</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Training</strong></p></td>
<td><p>Computed from current batch</p></td>
<td><p>Different each batch, adds noise</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Evaluation</strong></p></td>
<td><p>Running averages from training</p></td>
<td><p>Consistent, deterministic predictions</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Critical:</strong> Always call <code class="docutils literal notranslate"><span class="pre">model.train()</span></code> before training and <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> before inference!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># BatchNorm: Training vs Eval mode</span>
<span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Training mode - uses batch statistics</span>
<span class="n">bn</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">x_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">+</span> <span class="mi">10</span>  <span class="c1"># batch of 32</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training mode:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Input mean: </span><span class="si">{</span><span class="n">x_batch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Input std:  </span><span class="si">{</span><span class="n">x_batch</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">y_batch</span> <span class="o">=</span> <span class="n">bn</span><span class="p">(</span><span class="n">x_batch</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Output mean: </span><span class="si">{</span><span class="n">y_batch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2"> (approx 0)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Output std:  </span><span class="si">{</span><span class="n">y_batch</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2"> (approx 1)&quot;</span><span class="p">)</span>

<span class="c1"># After some training, running statistics are updated</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">  Running mean: </span><span class="si">{</span><span class="n">bn</span><span class="o">.</span><span class="n">running_mean</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Running var:  </span><span class="si">{</span><span class="n">bn</span><span class="o">.</span><span class="n">running_var</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Evaluation mode - uses running statistics</span>
<span class="n">bn</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">x_single</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">+</span> <span class="mi">10</span>  <span class="c1"># Single sample</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Evaluation mode (single sample):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Input: </span><span class="si">{</span><span class="n">x_single</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">y_single</span> <span class="o">=</span> <span class="n">bn</span><span class="p">(</span><span class="n">x_single</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Output: </span><span class="si">{</span><span class="n">y_single</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  (Uses running statistics, not batch statistics)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="deep-dive-why-batchnorm-helps">
<h3>Deep Dive: Why BatchNorm Helps<a class="headerlink" href="#deep-dive-why-batchnorm-helps" title="Link to this heading">#</a></h3>
<p>BatchNorm provides several benefits:</p>
<ol class="arabic simple">
<li><p><strong>Faster training</strong>: Allows higher learning rates without diverging</p></li>
<li><p><strong>Regularization effect</strong>: Batch statistics add noise, like dropout</p></li>
<li><p><strong>Reduces initialization sensitivity</strong>: Normalization prevents extreme activations</p></li>
<li><p><strong>Smoother loss landscape</strong>: Makes optimization easier</p></li>
</ol>
<section id="id4">
<h4>Key Insight<a class="headerlink" href="#id4" title="Link to this heading">#</a></h4>
<p>BatchNorm doesn’t just normalize - it gives each layer a “fresh start” at each training step.</p>
<p><strong>F1 analogy:</strong> BatchNorm is like recalibrating every sensor at the start of each session. Without recalibration, the tire temperature sensor that read 90C in Bahrain and 40C in Barcelona gives the downstream systems wildly different inputs. With recalibration (normalization), the system always sees “this tire is 1.5 standard deviations above the session mean” – a consistent, comparable signal regardless of absolute conditions.</p>
</section>
<section id="id5">
<h4>Common Misconceptions<a class="headerlink" href="#id5" title="Link to this heading">#</a></h4>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Misconception</p></th>
<th class="head"><p>Reality</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Always put BatchNorm after activation</p></td>
<td><p>Before activation is more common and often better</p></td>
</tr>
<tr class="row-odd"><td><p>BatchNorm eliminates need for good init</p></td>
<td><p>Still helps to initialize properly</p></td>
</tr>
<tr class="row-even"><td><p>BatchNorm always helps</p></td>
<td><p>Can hurt with very small batches</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="layernorm-for-transformers">
<h3>LayerNorm: For Transformers<a class="headerlink" href="#layernorm-for-transformers" title="Link to this heading">#</a></h3>
<p><strong>Layer Normalization</strong> normalizes across features instead of across the batch.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Normalization</p></th>
<th class="head"><p>Normalizes Across</p></th>
<th class="head"><p>Use Case</p></th>
<th class="head"><p>F1 Analogy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>BatchNorm</strong></p></td>
<td><p>Batch dimension</p></td>
<td><p>CNNs, large batches</p></td>
<td><p>Normalize each sensor across all laps in a session</p></td>
</tr>
<tr class="row-odd"><td><p><strong>LayerNorm</strong></p></td>
<td><p>Feature dimension</p></td>
<td><p>Transformers, RNNs</p></td>
<td><p>Normalize all sensors within a single lap</p></td>
</tr>
<tr class="row-even"><td><p><strong>InstanceNorm</strong></p></td>
<td><p>Spatial dimensions</p></td>
<td><p>Style transfer</p></td>
<td><p>Normalize within a single corner trace</p></td>
</tr>
<tr class="row-odd"><td><p><strong>GroupNorm</strong></p></td>
<td><p>Groups of channels</p></td>
<td><p>Small batches, detection</p></td>
<td><p>Normalize groups of related sensors together</p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize BatchNorm vs LayerNorm</span>
<span class="n">batch_size</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">features</span><span class="p">)</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="mi">2</span>

<span class="c1"># BatchNorm normalizes each column (feature) across the batch</span>
<span class="n">batch_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
<span class="n">batch_norm</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">bn_out</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># LayerNorm normalizes each row (sample) across features</span>
<span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
<span class="n">ln_out</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">im0</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdBu&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">6</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Features&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Batch samples&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Original Input&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im0</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">im1</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">bn_out</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdBu&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">3</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Features&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Batch samples&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;BatchNorm</span><span class="se">\n</span><span class="s1">(normalizes columns)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">im2</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">ln_out</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdBu&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">3</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Features&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Batch samples&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;LayerNorm</span><span class="se">\n</span><span class="s1">(normalizes rows)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;BatchNorm: Each COLUMN has mean~0, std~1&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;LayerNorm: Each ROW has mean~0, std~1&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="weight-initialization">
<h2>5. Weight Initialization<a class="headerlink" href="#weight-initialization" title="Link to this heading">#</a></h2>
<section id="id6">
<h3>Intuitive Explanation<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<p>How we initialize weights dramatically affects whether training succeeds. Poor initialization can cause:</p>
<ul class="simple">
<li><p><strong>Vanishing activations</strong>: All outputs become near-zero, gradients vanish</p></li>
<li><p><strong>Exploding activations</strong>: Outputs grow without bound, gradients explode</p></li>
<li><p><strong>Dead neurons</strong>: Some neurons never activate (especially with ReLU)</p></li>
</ul>
<p>The goal is to keep activations and gradients at reasonable scales throughout the network.</p>
<p><strong>F1 analogy:</strong> Weight initialization is choosing your baseline setup before you start tuning. If you start with a completely random setup – wing at max, springs at minimum, ride height at maximum – the car might not even be drivable (exploding activations) or it might be so slow it provides no useful data (vanishing activations). A good starting point, like using last year’s setup at a similar track, gives you a drivable car from which you can tune effectively. Xavier and Kaiming initialization are the engineering equivalent of “start from a known-good baseline.”</p>
</section>
<section id="visualization-why-initialization-matters">
<h3>Visualization: Why Initialization Matters<a class="headerlink" href="#visualization-why-initialization-matters" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward_activations</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Track activation magnitudes through layers.&quot;&quot;&quot;</span>
    <span class="n">magnitudes</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>
    
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">magnitudes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    
    <span class="k">return</span> <span class="n">magnitudes</span>

<span class="k">def</span><span class="w"> </span><span class="nf">create_layers</span><span class="p">(</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">init_scale</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create linear layers with specified initialization scale.&quot;&quot;&quot;</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">):</span>
        <span class="n">layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">init_scale</span><span class="p">)</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">layers</span>

<span class="c1"># Test different initialization scales</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_layers</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">scales</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Too Small (std=0.01)&#39;</span><span class="p">,</span> <span class="s1">&#39;Better (std=0.1)&#39;</span><span class="p">,</span> <span class="s1">&#39;Too Large (std=1.0)&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">scale</span><span class="p">,</span> <span class="n">color</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">scales</span><span class="p">,</span> <span class="n">colors</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="n">create_layers</span><span class="p">(</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
    <span class="n">mags</span> <span class="o">=</span> <span class="n">forward_activations</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mags</span><span class="p">,</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Layer&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Activation Magnitude&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Effect of Weight Initialization Scale&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Too small: Activations vanish (shrink to 0)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Too large: Activations explode (grow unbounded)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Just right: Activations stay roughly constant&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="xavier-glorot-initialization">
<h3>5.1 Xavier/Glorot Initialization<a class="headerlink" href="#xavier-glorot-initialization" title="Link to this heading">#</a></h3>
<p><strong>For tanh and sigmoid activations.</strong></p>
<div class="math notranslate nohighlight">
\[W \sim \mathcal{N}\left(0, \frac{2}{n_{in} + n_{out}}\right)\]</div>
<p><strong>Intuition:</strong> Balance the variance of inputs and outputs to maintain signal magnitude through layers. Works well when activations are symmetric around zero.</p>
</section>
<section id="kaiming-he-initialization">
<h3>5.2 Kaiming/He Initialization<a class="headerlink" href="#kaiming-he-initialization" title="Link to this heading">#</a></h3>
<p><strong>For ReLU activations.</strong></p>
<div class="math notranslate nohighlight">
\[W \sim \mathcal{N}\left(0, \frac{2}{n_{in}}\right)\]</div>
<p><strong>Intuition:</strong> ReLU zeros out half the activations (negative values become 0), so we need larger initial weights to compensate. The factor of 2 accounts for this.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compare Xavier vs Kaiming with ReLU</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">n_layers</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">create_initialized_layers</span><span class="p">(</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">init_type</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create layers with proper initialization.&quot;&quot;&quot;</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">):</span>
        <span class="n">layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">init_type</span> <span class="o">==</span> <span class="s1">&#39;xavier&#39;</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">init_type</span> <span class="o">==</span> <span class="s1">&#39;kaiming&#39;</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_in&#39;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">init_type</span> <span class="o">==</span> <span class="s1">&#39;random&#39;</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">layers</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># With ReLU activation</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">for</span> <span class="n">init_type</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="p">[(</span><span class="s1">&#39;random&#39;</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;xavier&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;kaiming&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">)]:</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="n">create_initialized_layers</span><span class="p">(</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">init_type</span><span class="p">)</span>
    <span class="n">mags</span> <span class="o">=</span> <span class="n">forward_activations</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mags</span><span class="p">,</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">init_type</span><span class="o">.</span><span class="n">capitalize</span><span class="p">(),</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Layer&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Activation Magnitude&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;ReLU Activation</span><span class="se">\n</span><span class="s1">(Kaiming is designed for ReLU)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">1e-6</span><span class="p">,</span> <span class="mf">1e2</span><span class="p">)</span>

<span class="c1"># With Tanh activation</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="k">for</span> <span class="n">init_type</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="p">[(</span><span class="s1">&#39;random&#39;</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;xavier&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;kaiming&#39;</span><span class="p">,</span> <span class="s1">&#39;orange&#39;</span><span class="p">)]:</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="n">create_initialized_layers</span><span class="p">(</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">init_type</span><span class="p">)</span>
    <span class="n">mags</span> <span class="o">=</span> <span class="n">forward_activations</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mags</span><span class="p">,</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">init_type</span><span class="o">.</span><span class="n">capitalize</span><span class="p">(),</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Layer&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Activation Magnitude&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Tanh Activation</span><span class="se">\n</span><span class="s1">(Xavier is designed for symmetric activations)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">1e-6</span><span class="p">,</span> <span class="mf">1e2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="which-initialization-to-use">
<h3>Which Initialization to Use?<a class="headerlink" href="#which-initialization-to-use" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Activation</p></th>
<th class="head"><p>Recommended Init</p></th>
<th class="head"><p>PyTorch Function</p></th>
<th class="head"><p>F1 Analogy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ReLU, LeakyReLU</p></td>
<td><p>Kaiming (He)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nn.init.kaiming_normal_</span></code></p></td>
<td><p>Baseline for threshold-based systems</p></td>
</tr>
<tr class="row-odd"><td><p>Tanh, Sigmoid</p></td>
<td><p>Xavier (Glorot)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nn.init.xavier_normal_</span></code></p></td>
<td><p>Baseline for smooth-response systems</p></td>
</tr>
<tr class="row-even"><td><p>GELU, SiLU</p></td>
<td><p>Kaiming</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nn.init.kaiming_normal_</span></code></p></td>
<td><p>Baseline for modern nonlinear responses</p></td>
</tr>
<tr class="row-odd"><td><p>Linear (no activation)</p></td>
<td><p>Xavier</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nn.init.xavier_normal_</span></code></p></td>
<td><p>Baseline for linear processing stages</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Good news:</strong> PyTorch’s default initialization works well for most cases!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorch initialization examples</span>
<span class="n">layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>

<span class="c1"># Xavier initialization (for tanh/sigmoid)</span>
<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Xavier: mean=</span><span class="si">{</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">, std=</span><span class="si">{</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Kaiming initialization (for ReLU)</span>
<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_in&#39;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Kaiming: mean=</span><span class="si">{</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">, std=</span><span class="si">{</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># For biases, usually zero</span>
<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bias: all zeros = </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="putting-it-all-together">
<h2>6. Putting It All Together<a class="headerlink" href="#putting-it-all-together" title="Link to this heading">#</a></h2>
<p>Now let’s build a complete training pipeline that incorporates all the techniques:</p>
<ul class="simple">
<li><p>Adam optimizer with learning rate scheduling</p></li>
<li><p>BatchNorm and Dropout for regularization</p></li>
<li><p>Proper Kaiming initialization</p></li>
<li><p>Training with validation monitoring</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create synthetic classification dataset</span>
<span class="k">def</span><span class="w"> </span><span class="nf">make_moons_data</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate two interleaving half circles (moons).&quot;&quot;&quot;</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">n_samples</span> <span class="o">//</span> <span class="mi">2</span>
    
    <span class="c1"># First moon</span>
    <span class="n">theta1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta1</span><span class="p">)</span>
    <span class="n">y1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta1</span><span class="p">)</span>
    
    <span class="c1"># Second moon (shifted and flipped)</span>
    <span class="n">theta2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta2</span><span class="p">)</span>
    <span class="n">y2</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta2</span><span class="p">)</span>
    
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span>
        <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">]),</span>
        <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">])</span>
    <span class="p">])</span>
    <span class="n">X</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">noise</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>
    
    <span class="c1"># Shuffle</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Create train and validation data</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">make_moons_data</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">make_moons_data</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># Convert to PyTorch</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> 
    <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X_val</span><span class="p">),</span> 
    <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y_val</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">val_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>

<span class="c1"># Visualize</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Class 0&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Class 1&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;x2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training Data (Two Moons)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">BestPracticesNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Network with all best practices:</span>
<span class="sd">    - BatchNorm after linear layers</span>
<span class="sd">    - Dropout for regularization</span>
<span class="sd">    - ReLU activation</span>
<span class="sd">    - Kaiming initialization</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">],</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.3</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">prev_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
        
        <span class="k">for</span> <span class="n">hidden_dim</span> <span class="ow">in</span> <span class="n">hidden_dims</span><span class="p">:</span>
            <span class="c1"># Linear layer with Kaiming init</span>
            <span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">prev_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_in&#39;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">linear</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">linear</span><span class="p">)</span>
            
            <span class="c1"># BatchNorm</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">))</span>
            
            <span class="c1"># Activation</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
            
            <span class="c1"># Dropout</span>
            <span class="k">if</span> <span class="n">dropout_rate</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">))</span>
            
            <span class="n">prev_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        
        <span class="c1"># Output layer (no BatchNorm, no Dropout)</span>
        <span class="n">output_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">prev_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">output_layer</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">output_layer</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output_layer</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">BestPracticesNetwork</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">],</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Total parameters: </span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Complete training pipeline with:</span>
<span class="sd">    - AdamW optimizer with weight decay</span>
<span class="sd">    - Cosine annealing learning rate schedule</span>
<span class="sd">    - Training and validation tracking</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="n">epochs</span><span class="p">)</span>
    
    <span class="n">history</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;train_loss&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;val_loss&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;train_acc&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[]}</span>
    
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="c1"># Training</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_correct</span><span class="p">,</span> <span class="n">train_total</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        
        <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_batch</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            
            <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">X_batch</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="n">train_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">y_batch</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">train_total</span> <span class="o">+=</span> <span class="n">X_batch</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># Validation</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">val_loss</span><span class="p">,</span> <span class="n">val_correct</span><span class="p">,</span> <span class="n">val_total</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_batch</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">)</span>
                
                <span class="n">val_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">X_batch</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">predictions</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                <span class="n">val_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">y_batch</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">val_total</span> <span class="o">+=</span> <span class="n">X_batch</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># Record and update</span>
        <span class="n">history</span><span class="p">[</span><span class="s1">&#39;train_loss&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss</span> <span class="o">/</span> <span class="n">train_total</span><span class="p">)</span>
        <span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_loss</span> <span class="o">/</span> <span class="n">val_total</span><span class="p">)</span>
        <span class="n">history</span><span class="p">[</span><span class="s1">&#39;train_acc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_correct</span> <span class="o">/</span> <span class="n">train_total</span><span class="p">)</span>
        <span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_acc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_correct</span> <span class="o">/</span> <span class="n">val_total</span><span class="p">)</span>
        <span class="n">history</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>
        
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">20</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="s2">3d</span><span class="si">}</span><span class="s2">: train_loss=</span><span class="si">{</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;train_loss&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, &quot;</span>
                  <span class="sa">f</span><span class="s2">&quot;val_loss=</span><span class="si">{</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, &quot;</span>
                  <span class="sa">f</span><span class="s2">&quot;val_acc=</span><span class="si">{</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_acc&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">history</span>

<span class="c1"># Train the model</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BestPracticesNetwork</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">],</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot training history</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Loss</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;train_loss&#39;</span><span class="p">],</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Train&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">],</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Accuracy</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;train_acc&#39;</span><span class="p">],</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Train&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_acc&#39;</span><span class="p">],</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Learning Rate</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">],</span> <span class="s1">&#39;g-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Learning Rate&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Learning Rate (Cosine Annealing)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize decision boundary</span>
<span class="k">def</span><span class="w"> </span><span class="nf">plot_decision_boundary</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plot the decision boundary.&quot;&quot;&quot;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    
    <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.5</span>
    <span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.5</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span>
                         <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="mi">200</span><span class="p">))</span>
    
    <span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">grid</span><span class="p">)))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdBu&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;P(Class 1)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Class 0&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Class 1&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x1&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;x2&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Trained Model Decision Boundary&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Final accuracy</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Final validation accuracy: </span><span class="si">{</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_acc&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<section id="exercise-1-optimizer-comparison">
<h3>Exercise 1: Optimizer Comparison<a class="headerlink" href="#exercise-1-optimizer-comparison" title="Link to this heading">#</a></h3>
<p>Compare the performance of different optimizers on the moons dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># EXERCISE 1: Compare optimizers</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train_with_optimizer</span><span class="p">(</span><span class="n">optimizer_name</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Train a model with a specific optimizer and return validation accuracy.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        optimizer_name: &#39;sgd&#39;, &#39;momentum&#39;, &#39;rmsprop&#39;, or &#39;adam&#39;</span>
<span class="sd">        lr: learning rate</span>
<span class="sd">        epochs: number of epochs</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        List of validation accuracies per epoch</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement this!</span>
    <span class="c1"># Hint: Create a fresh model</span>
    <span class="c1"># Hint: Choose optimizer based on optimizer_name:</span>
    <span class="c1">#   - &#39;sgd&#39;: optim.SGD(params, lr=lr)</span>
    <span class="c1">#   - &#39;momentum&#39;: optim.SGD(params, lr=lr, momentum=0.9)</span>
    <span class="c1">#   - &#39;rmsprop&#39;: optim.RMSprop(params, lr=lr)</span>
    <span class="c1">#   - &#39;adam&#39;: optim.Adam(params, lr=lr)</span>
    <span class="c1"># Hint: Train and record validation accuracy each epoch</span>
    
    <span class="k">pass</span>  <span class="c1"># Replace with your implementation</span>

<span class="c1"># Test your implementation</span>
<span class="c1"># val_accs = train_with_optimizer(&#39;adam&#39;, lr=0.01, epochs=50)</span>
<span class="c1"># print(f&quot;Adam final accuracy: {val_accs[-1]:.4f}&quot;)</span>

<span class="c1"># Expected output: Compare all optimizers</span>
<span class="c1"># optimizers = [&#39;sgd&#39;, &#39;momentum&#39;, &#39;adam&#39;]</span>
<span class="c1"># for opt in optimizers:</span>
<span class="c1">#     accs = train_with_optimizer(opt, lr=0.01)</span>
<span class="c1">#     plt.plot(accs, label=opt)</span>
<span class="c1"># plt.legend()</span>
<span class="c1"># plt.show()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-2-implement-early-stopping">
<h3>Exercise 2: Implement Early Stopping<a class="headerlink" href="#exercise-2-implement-early-stopping" title="Link to this heading">#</a></h3>
<p>Implement an early stopping class that stops training when validation loss stops improving.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># EXERCISE 2: Implement early stopping</span>
<span class="k">class</span><span class="w"> </span><span class="nc">EarlyStopping</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Early stopping to prevent overfitting.</span>
<span class="sd">    </span>
<span class="sd">    Usage:</span>
<span class="sd">        early_stop = EarlyStopping(patience=5)</span>
<span class="sd">        for epoch in range(epochs):</span>
<span class="sd">            # ... training code ...</span>
<span class="sd">            if early_stop(val_loss):</span>
<span class="sd">                print(&quot;Early stopping!&quot;)</span>
<span class="sd">                break</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_delta</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            patience: Number of epochs to wait before stopping</span>
<span class="sd">            min_delta: Minimum improvement to count as improvement</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patience</span> <span class="o">=</span> <span class="n">patience</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_delta</span> <span class="o">=</span> <span class="n">min_delta</span>
        <span class="c1"># TODO: Initialize tracking variables</span>
        <span class="c1"># Hint: Track best_loss and counter</span>
        <span class="k">pass</span>
        
    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check if training should stop.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            val_loss: Current validation loss</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            True if training should stop, False otherwise</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO: Implement this!</span>
        <span class="c1"># Hint: If val_loss improved by min_delta, reset counter</span>
        <span class="c1"># Hint: Otherwise, increment counter</span>
        <span class="c1"># Hint: Return True if counter &gt;= patience</span>
        
        <span class="k">pass</span>  <span class="c1"># Replace with your implementation</span>

<span class="c1"># Test your implementation</span>
<span class="n">early_stop</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">patience</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.85</span><span class="p">,</span> <span class="mf">0.83</span><span class="p">,</span> <span class="mf">0.84</span><span class="p">,</span> <span class="mf">0.86</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing Early Stopping:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">loss</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">losses</span><span class="p">):</span>
    <span class="n">should_stop</span> <span class="o">=</span> <span class="n">early_stop</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">: loss=</span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s2">, stop=</span><span class="si">{</span><span class="n">should_stop</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">should_stop</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training stopped at epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">break</span>

<span class="c1"># Expected: Should stop at epoch 6 (after 3 epochs without improvement from 0.8)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-3-dropout-rate-experiment">
<h3>Exercise 3: Dropout Rate Experiment<a class="headerlink" href="#exercise-3-dropout-rate-experiment" title="Link to this heading">#</a></h3>
<p>Find the optimal dropout rate for the moons classification problem.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># EXERCISE 3: Find optimal dropout rate</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_dropout_rate</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Train a model with a specific dropout rate.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        dropout_rate: float between 0 and 1</span>
<span class="sd">        epochs: number of training epochs</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        Final validation accuracy</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement this!</span>
    <span class="c1"># Hint: Create BestPracticesNetwork with the given dropout_rate</span>
    <span class="c1"># Hint: Train the model</span>
    <span class="c1"># Hint: Return the final validation accuracy</span>
    
    <span class="k">pass</span>  <span class="c1"># Replace with your implementation</span>

<span class="c1"># Test your implementation</span>
<span class="c1"># dropout_rates = [0.0, 0.1, 0.2, 0.3, 0.5, 0.7]</span>
<span class="c1"># accuracies = [test_dropout_rate(dr) for dr in dropout_rates]</span>
<span class="c1">#</span>
<span class="c1"># plt.plot(dropout_rates, accuracies, &#39;bo-&#39;)</span>
<span class="c1"># plt.xlabel(&#39;Dropout Rate&#39;)</span>
<span class="c1"># plt.ylabel(&#39;Validation Accuracy&#39;)</span>
<span class="c1"># plt.title(&#39;Effect of Dropout Rate&#39;)</span>
<span class="c1"># plt.grid(True, alpha=0.3)</span>
<span class="c1"># plt.show()</span>
<span class="c1">#</span>
<span class="c1"># best_idx = np.argmax(accuracies)</span>
<span class="c1"># print(f&quot;Best dropout rate: {dropout_rates[best_idx]} with accuracy {accuracies[best_idx]:.4f}&quot;)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<section id="key-concepts">
<h3>Key Concepts<a class="headerlink" href="#key-concepts" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Concept</p></th>
<th class="head"><p>Definition</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>SGD</strong></p></td>
<td><p>Simple gradient following</p></td>
<td><p>Conservative: one small setup change at a time</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Momentum</strong></p></td>
<td><p>Accumulate velocity from past gradients</p></td>
<td><p>Trend-following: if it kept working, keep going</p></td>
</tr>
<tr class="row-even"><td><p><strong>Adam</strong></p></td>
<td><p>Momentum + adaptive per-parameter rates</p></td>
<td><p>Veteran engineer: adapts strategy to each parameter</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Learning rate</strong></p></td>
<td><p>Step size for weight updates</p></td>
<td><p>How big each setup adjustment is</p></td>
</tr>
<tr class="row-even"><td><p><strong>LR schedulers</strong></p></td>
<td><p>Reduce LR over time</p></td>
<td><p>Bold changes early (FP1), tiny refinements late (qualifying)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>L2 regularization</strong></p></td>
<td><p>Penalize large weights</p></td>
<td><p>Penalizing extreme setup deviations from baseline</p></td>
</tr>
<tr class="row-even"><td><p><strong>Dropout</strong></p></td>
<td><p>Randomly disable neurons during training</p></td>
<td><p>Randomly disabling sensors to build robustness</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Early stopping</strong></p></td>
<td><p>Stop when validation degrades</p></td>
<td><p>Knowing when to lock in the setup</p></td>
</tr>
<tr class="row-even"><td><p><strong>BatchNorm</strong></p></td>
<td><p>Normalize activations per batch</p></td>
<td><p>Normalizing telemetry across different track conditions</p></td>
</tr>
<tr class="row-odd"><td><p><strong>LayerNorm</strong></p></td>
<td><p>Normalize activations per sample</p></td>
<td><p>Normalizing all sensors within a single lap</p></td>
</tr>
<tr class="row-even"><td><p><strong>Weight init</strong></p></td>
<td><p>Smart starting values for parameters</p></td>
<td><p>Starting from a known-good baseline setup</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="connection-to-deep-learning">
<h3>Connection to Deep Learning<a class="headerlink" href="#connection-to-deep-learning" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Technique</p></th>
<th class="head"><p>Application</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Adam/AdamW</p></td>
<td><p>Default for most tasks</p></td>
<td><p>Veteran engineer’s adaptive approach</p></td>
</tr>
<tr class="row-odd"><td><p>SGD + Momentum</p></td>
<td><p>Fine-tuning, achieving SOTA on vision</p></td>
<td><p>Patient, iterative refinement</p></td>
</tr>
<tr class="row-even"><td><p>Cosine LR Schedule</p></td>
<td><p>Modern training recipes</p></td>
<td><p>Smooth FP1 -&gt; qualifying transition</p></td>
</tr>
<tr class="row-odd"><td><p>Dropout</p></td>
<td><p>Fully connected layers</p></td>
<td><p>Sensor robustness training</p></td>
</tr>
<tr class="row-even"><td><p>BatchNorm</p></td>
<td><p>CNNs, faster training</p></td>
<td><p>Cross-condition telemetry normalization</p></td>
</tr>
<tr class="row-odd"><td><p>LayerNorm</p></td>
<td><p>Transformers, RNNs</p></td>
<td><p>Within-sample feature normalization</p></td>
</tr>
<tr class="row-even"><td><p>Kaiming Init</p></td>
<td><p>Any ReLU network</p></td>
<td><p>Engineering-informed baseline setup</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="checklist">
<h3>Checklist<a class="headerlink" href="#checklist" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>[ ] I can explain why Adam is often the default optimizer</p></li>
<li><p>[ ] I understand the effect of learning rate on training</p></li>
<li><p>[ ] I can choose appropriate regularization for different scenarios</p></li>
<li><p>[ ] I know when to use BatchNorm vs LayerNorm</p></li>
<li><p>[ ] I can select the right initialization for my activation function</p></li>
<li><p>[ ] I can build a complete training pipeline with best practices</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="next-steps">
<h2>Next Steps<a class="headerlink" href="#next-steps" title="Link to this heading">#</a></h2>
<p>Now that you understand how to train deep networks effectively, you’re ready to explore:</p>
<ol class="arabic simple">
<li><p><strong>Convolutional Neural Networks (CNNs)</strong>: Specialized architectures for image data</p></li>
<li><p><strong>Recurrent Neural Networks (RNNs)</strong>: Handling sequential data</p></li>
<li><p><strong>Transfer Learning</strong>: Using pretrained models as starting points</p></li>
<li><p><strong>Transformers</strong>: The architecture behind modern NLP and beyond</p></li>
</ol>
<p>The training techniques you learned here apply to all these architectures. Whether you’re training a simple classifier or a billion-parameter language model, you’ll use:</p>
<ul class="simple">
<li><p>Optimizers (usually Adam or AdamW) – your setup tuning strategy</p></li>
<li><p>Learning rate schedules (often cosine with warmup) – bold changes early, refinements late</p></li>
<li><p>Regularization (dropout, weight decay) – preventing the car from being overfit to one track</p></li>
<li><p>Normalization (BatchNorm for CNNs, LayerNorm for Transformers) – normalizing telemetry across conditions</p></li>
</ul>
<p><strong>Practical next steps:</strong></p>
<ul class="simple">
<li><p>Train a model on MNIST or CIFAR-10</p></li>
<li><p>Experiment with different optimizer/scheduler combinations</p></li>
<li><p>Use TensorBoard or Weights &amp; Biases to visualize training</p></li>
<li><p>Try implementing gradient clipping for very deep networks</p></li>
</ul>
<p>You now have the complete engineering toolkit to train deep neural networks. Like an F1 team heading into a race weekend, you know the car (architecture), the tuning process (optimizers), the development discipline (regularization), and the calibration systems (normalization). Time to go racing.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="11_pytorch_fundamentals.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Part 4.3: PyTorch Fundamentals</p>
      </div>
    </a>
    <a class="right-next"
       href="13_convolutional_neural_networks.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Part 5.1: Convolutional Neural Networks (CNNs)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizers">1. Optimizers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuitive-explanation">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sgd-stochastic-gradient-descent">1.1 SGD (Stochastic Gradient Descent)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#breaking-down-the-formula">Breaking down the formula:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#momentum">1.2 Momentum</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rmsprop">1.3 RMSprop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adam-adaptive-moment-estimation">1.4 Adam (Adaptive Moment Estimation)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-why-adam-is-usually-the-default">Deep Dive: Why Adam is Usually the Default</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#key-insight">Key Insight</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#common-misconceptions">Common Misconceptions</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-optimizer-paths-on-loss-surface">Visualization: Optimizer Paths on Loss Surface</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizer-comparison-table">Optimizer Comparison Table</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-this-matters-in-machine-learning">Why This Matters in Machine Learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate">2. Learning Rate</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-too-high-vs-too-low">Visualization: Too High vs Too Low</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-schedulers">2.1 Learning Rate Schedulers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warmup-explained">2.2 Warmup Explained</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-finder-brief-mention">2.3 Learning Rate Finder (Brief Mention)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization">3. Regularization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-the-overfitting-problem">Visualization: The Overfitting Problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#l2-regularization-weight-decay">3.1 L2 Regularization (Weight Decay)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#l1-regularization-sparsity">3.2 L1 Regularization (Sparsity)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout">3.3 Dropout</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#early-stopping">3.4 Early Stopping</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-comparison-table">Regularization Comparison Table</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Why This Matters in Machine Learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-normalization">4. Batch Normalization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuitive-explanation-the-problem">Intuitive Explanation: The Problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-batchnorm-does">What BatchNorm Does</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-vs-eval-mode">Training vs Eval Mode</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-why-batchnorm-helps">Deep Dive: Why BatchNorm Helps</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Key Insight</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Common Misconceptions</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layernorm-for-transformers">LayerNorm: For Transformers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-initialization">5. Weight Initialization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-why-initialization-matters">Visualization: Why Initialization Matters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#xavier-glorot-initialization">5.1 Xavier/Glorot Initialization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kaiming-he-initialization">5.2 Kaiming/He Initialization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#which-initialization-to-use">Which Initialization to Use?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together">6. Putting It All Together</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-optimizer-comparison">Exercise 1: Optimizer Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-implement-early-stopping">Exercise 2: Implement Early Stopping</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-dropout-rate-experiment">Exercise 3: Dropout Rate Experiment</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts">Key Concepts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-deep-learning">Connection to Deep Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checklist">Checklist</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">Next Steps</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dan Shah
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>