{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Part 1.4: Classical Machine Learning\n",
    "\n",
    "Before diving into deep learning, you need to understand the algorithms that dominated machine learning for decades — and still outperform neural networks on many real-world problems. Classical ML algorithms are the foundation:\n",
    "- They're **interpretable** — you can explain *why* a prediction was made\n",
    "- They work with **small datasets** where deep learning would overfit\n",
    "- They're **fast** to train and deploy\n",
    "- They're the **baseline** that every deep learning model must beat to justify its complexity\n",
    "\n",
    "This notebook covers decision trees, ensemble methods, SVMs, clustering, and model evaluation — the toolkit every ML practitioner needs before moving to neural networks.\n",
    "\n",
    "## Learning Objectives\n",
    "- [ ] Build decision trees from scratch and understand information gain / Gini impurity\n",
    "- [ ] Explain why ensembles (Random Forests, Gradient Boosting) beat single models\n",
    "- [ ] Visualize SVM decision boundaries and understand the kernel trick\n",
    "- [ ] Apply k-means and DBSCAN clustering to discover structure in data\n",
    "- [ ] Evaluate models with cross-validation, confusion matrices, and ROC curves\n",
    "- [ ] Know when classical ML is the right choice over deep learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.datasets import make_moons, make_blobs, make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Decision Trees\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "A decision tree works exactly like a game of 20 questions. To classify a data point, the tree asks a series of yes/no questions about its features, splitting the data at each step until it reaches a conclusion.\n",
    "\n",
    "**Example**: Should I play tennis today?\n",
    "1. Is it raining? **No** →\n",
    "2. Is it windy? **No** →\n",
    "3. Prediction: **Play tennis!**\n",
    "\n",
    "The key question is: **which question should the tree ask first?** The answer: whichever question best separates the classes. We measure this with *information gain* or *Gini impurity*.\n",
    "\n",
    "### Splitting Criteria\n",
    "\n",
    "| Criterion | Formula | Intuition | When to Use |\n",
    "|-----------|---------|-----------|-------------|\n",
    "| **Gini Impurity** | $G = 1 - \\sum_{k=1}^{K} p_k^2$ | Probability of misclassifying a randomly chosen sample | Default for classification (fast) |\n",
    "| **Entropy** | $H = -\\sum_{k=1}^{K} p_k \\log_2(p_k)$ | Amount of \"surprise\" or disorder | When you want more balanced trees |\n",
    "| **Information Gain** | $IG = H(parent) - \\sum \\frac{n_i}{n} H(child_i)$ | Reduction in uncertainty after splitting | Used to choose the best split |\n",
    "\n",
    "**What this means:** A pure node (all same class) has Gini = 0 and Entropy = 0. A maximally impure node (50/50 split) has Gini = 0.5 and Entropy = 1.0. The tree greedily picks the split that reduces impurity the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Gini Impurity and Entropy\n",
    "p = np.linspace(0.001, 0.999, 200)\n",
    "gini = 1 - p**2 - (1-p)**2\n",
    "entropy = -p * np.log2(p) - (1-p) * np.log2(1-p)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Gini vs Entropy\n",
    "axes[0].plot(p, gini, 'b-', linewidth=2, label='Gini Impurity')\n",
    "axes[0].plot(p, entropy, 'r-', linewidth=2, label='Entropy')\n",
    "axes[0].axvline(x=0.5, color='gray', linestyle='--', alpha=0.5, label='Maximum impurity (50/50)')\n",
    "axes[0].set_xlabel('Proportion of Class 1 (p)')\n",
    "axes[0].set_ylabel('Impurity')\n",
    "axes[0].set_title('Gini vs Entropy: Measuring Node Impurity')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Information Gain example\n",
    "# Parent: 50/50 split (max impurity)\n",
    "# After split: one child is 80/20, other is 20/80\n",
    "parent_gini = 0.5\n",
    "splits = np.linspace(0.5, 1.0, 100)\n",
    "child_gini = 1 - splits**2 - (1-splits)**2\n",
    "# Assume equal-sized children, mirror split\n",
    "ig = parent_gini - child_gini  # simplified for equal-sized, symmetric children\n",
    "\n",
    "axes[1].plot(splits, ig, 'g-', linewidth=2)\n",
    "axes[1].fill_between(splits, 0, ig, alpha=0.2, color='green')\n",
    "axes[1].set_xlabel('Purity of Children (proportion of majority class)')\n",
    "axes[1].set_ylabel('Information Gain')\n",
    "axes[1].set_title('Information Gain: Better Splits → More Gain')\n",
    "axes[1].annotate('Perfect split\\n(pure children)', xy=(1.0, 0.5), fontsize=10,\n",
    "                  ha='right', va='top', color='green')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key takeaway: Both Gini and Entropy measure the same thing — impurity.\")\n",
    "print(\"Gini is faster to compute; Entropy gives slightly more balanced trees.\")\n",
    "print(\"In practice, the difference is negligible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "### Deep Dive: Building a Decision Tree from Scratch\n",
    "\n",
    "Let's implement the core logic of a decision tree to understand how splitting works. The algorithm is:\n",
    "\n",
    "1. For each feature, try every possible split threshold\n",
    "2. Calculate the information gain (reduction in impurity) for each split\n",
    "3. Pick the split with the highest information gain\n",
    "4. Recurse on the left and right children\n",
    "5. Stop when a node is pure, reaches max depth, or has too few samples\n",
    "\n",
    "#### Key Insight\n",
    "\n",
    "Decision trees are **greedy** — they pick the locally best split at each step without considering future splits. This makes them fast but potentially suboptimal. This is why ensembles (coming next) work so much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(y):\n",
    "    \"\"\"\n",
    "    Calculate the Gini impurity of a set of labels.\n",
    "    \n",
    "    Args:\n",
    "        y: Array of class labels\n",
    "    \n",
    "    Returns:\n",
    "        Gini impurity score (0 = pure, 0.5 = maximally impure for binary)\n",
    "    \"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    proportions = counts / len(y)\n",
    "    return 1 - np.sum(proportions**2)\n",
    "\n",
    "\n",
    "def information_gain(y, left_mask):\n",
    "    \"\"\"\n",
    "    Calculate information gain from a binary split.\n",
    "    \n",
    "    Args:\n",
    "        y: Array of class labels\n",
    "        left_mask: Boolean mask for left child\n",
    "    \n",
    "    Returns:\n",
    "        Information gain (higher is better)\n",
    "    \"\"\"\n",
    "    parent_impurity = gini_impurity(y)\n",
    "    left_y = y[left_mask]\n",
    "    right_y = y[~left_mask]\n",
    "    \n",
    "    n = len(y)\n",
    "    n_left = len(left_y)\n",
    "    n_right = len(right_y)\n",
    "    \n",
    "    if n_left == 0 or n_right == 0:\n",
    "        return 0\n",
    "    \n",
    "    child_impurity = (n_left / n) * gini_impurity(left_y) + (n_right / n) * gini_impurity(right_y)\n",
    "    return parent_impurity - child_impurity\n",
    "\n",
    "\n",
    "def find_best_split(X, y):\n",
    "    \"\"\"\n",
    "    Find the best feature and threshold to split on.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (n_samples, n_features)\n",
    "        y: Class labels\n",
    "    \n",
    "    Returns:\n",
    "        best_feature, best_threshold, best_gain\n",
    "    \"\"\"\n",
    "    best_gain = 0\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "    \n",
    "    for feature in range(X.shape[1]):\n",
    "        thresholds = np.unique(X[:, feature])\n",
    "        for threshold in thresholds:\n",
    "            left_mask = X[:, feature] <= threshold\n",
    "            gain = information_gain(y, left_mask)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "    \n",
    "    return best_feature, best_threshold, best_gain\n",
    "\n",
    "\n",
    "# Demo: find the best split on simple data\n",
    "X_demo = np.array([[1, 2], [2, 3], [3, 1], [6, 5], [7, 8], [8, 6]])\n",
    "y_demo = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "best_feat, best_thresh, best_gain = find_best_split(X_demo, y_demo)\n",
    "print(f\"Best split: Feature {best_feat} <= {best_thresh}\")\n",
    "print(f\"Information gain: {best_gain:.4f}\")\n",
    "print(f\"\\nThis means: split on {'x' if best_feat == 0 else 'y'} <= {best_thresh}\")\n",
    "print(f\"Left group (class 0): {X_demo[X_demo[:, best_feat] <= best_thresh]}\")\n",
    "print(f\"Right group (class 1): {X_demo[X_demo[:, best_feat] > best_thresh]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use sklearn's DecisionTreeClassifier and visualize\n",
    "X, y = make_classification(n_samples=300, n_features=2, n_redundant=0,\n",
    "                           n_informative=2, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, max_depth in enumerate([1, 3, None]):\n",
    "    tree = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "    tree.fit(X, y)\n",
    "    \n",
    "    # Create mesh for decision boundary\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    Z = tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    \n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "    axes[idx].scatter(X[y==0, 0], X[y==0, 1], c='blue', edgecolors='k', alpha=0.6, label='Class 0')\n",
    "    axes[idx].scatter(X[y==1, 0], X[y==1, 1], c='red', edgecolors='k', alpha=0.6, label='Class 1')\n",
    "    \n",
    "    depth_label = max_depth if max_depth else 'Unlimited'\n",
    "    score = tree.score(X, y)\n",
    "    axes[idx].set_title(f'Depth = {depth_label} (Accuracy: {score:.2f})')\n",
    "    axes[idx].set_xlabel('Feature 1')\n",
    "    axes[idx].set_ylabel('Feature 2')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Decision Tree Decision Boundaries: Depth Matters', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Deeper trees create more complex (jagged) boundaries.\")\n",
    "print(\"Too deep → overfitting. Too shallow → underfitting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the actual tree structure\n",
    "tree_shallow = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_shallow.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plot_tree(tree_shallow, filled=True, feature_names=['Feature 1', 'Feature 2'],\n",
    "          class_names=['Class 0', 'Class 1'], rounded=True, fontsize=9)\n",
    "plt.title('Decision Tree Structure (depth=3)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Reading the tree:\")\n",
    "print(\"- Each node shows: the split condition, Gini impurity, sample count, class distribution\")\n",
    "print(\"- Blue nodes lean toward Class 0, orange nodes lean toward Class 1\")\n",
    "print(\"- Darker color = more confident (lower impurity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Common Misconceptions\n",
    "\n",
    "| Misconception | Reality |\n",
    "|---------------|--------|\n",
    "| Deeper trees are always better | Deeper trees overfit — they memorize the training data instead of learning patterns |\n",
    "| Decision trees find the globally optimal splits | They use **greedy** search — each split is locally optimal but the overall tree may not be |\n",
    "| Decision trees can't handle continuous features | They can! They find the best threshold to split a continuous feature into two groups |\n",
    "| Decision trees are too simple for real problems | Single trees are weak, but ensembles of trees (Random Forests, XGBoost) win Kaggle competitions |\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Ensemble Methods\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "A single decision tree is like asking one person for directions — they might be wrong. An ensemble is like asking 100 people and going with the majority vote. Even if each person is only slightly better than random, the crowd's collective answer is surprisingly accurate.\n",
    "\n",
    "This is the **wisdom of crowds** applied to machine learning.\n",
    "\n",
    "| Method | Strategy | Intuition | Strength |\n",
    "|--------|----------|-----------|----------|\n",
    "| **Bagging** (Random Forest) | Train many trees on random subsets, vote | Reduce variance by averaging | Robust, hard to overfit |\n",
    "| **Boosting** (Gradient Boosting) | Train trees sequentially, each fixing previous mistakes | Reduce bias by focusing on errors | Higher accuracy, can overfit |\n",
    "| **XGBoost** | Optimized gradient boosting with regularization | Best of both worlds | State-of-the-art on tabular data |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### Deep Dive: Why Ensembles Work\n",
    "\n",
    "The magic of ensembles comes from the **bias-variance tradeoff**:\n",
    "\n",
    "- **Bias**: How far off the model's average prediction is from the truth (systematic error)\n",
    "- **Variance**: How much predictions change when you train on different data (instability)\n",
    "- **Total Error = Bias² + Variance + Noise**\n",
    "\n",
    "A single deep decision tree has **low bias** (it can fit complex patterns) but **high variance** (small changes in data lead to very different trees). Ensembles fix this:\n",
    "\n",
    "- **Random Forests** (bagging): Average many high-variance trees → variance drops, bias stays low\n",
    "- **Gradient Boosting**: Sequentially add low-bias trees that correct residual errors → bias drops further\n",
    "\n",
    "#### Key Insight\n",
    "\n",
    "If each tree's errors are somewhat **independent** (which random feature selection encourages), then averaging N trees reduces variance by roughly 1/N. This is why Random Forests use random subsets of features at each split — it decorrelates the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: Visualize single tree vs forest\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.25, random_state=42)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Single tree\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_moons, y_moons)\n",
    "\n",
    "x_min, x_max = X_moons[:, 0].min() - 0.5, X_moons[:, 0].max() + 0.5\n",
    "y_min, y_max = X_moons[:, 1].min() - 0.5, X_moons[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "\n",
    "Z = tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "axes[0].contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "axes[0].scatter(X_moons[y_moons==0, 0], X_moons[y_moons==0, 1], c='blue', edgecolors='k', alpha=0.6)\n",
    "axes[0].scatter(X_moons[y_moons==1, 0], X_moons[y_moons==1, 1], c='red', edgecolors='k', alpha=0.6)\n",
    "axes[0].set_title(f'Single Decision Tree\\nAccuracy: {tree.score(X_moons, y_moons):.2f}')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Random Forest (10 trees)\n",
    "rf_small = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "rf_small.fit(X_moons, y_moons)\n",
    "Z = rf_small.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "axes[1].contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "axes[1].scatter(X_moons[y_moons==0, 0], X_moons[y_moons==0, 1], c='blue', edgecolors='k', alpha=0.6)\n",
    "axes[1].scatter(X_moons[y_moons==1, 0], X_moons[y_moons==1, 1], c='red', edgecolors='k', alpha=0.6)\n",
    "axes[1].set_title(f'Random Forest (10 trees)\\nAccuracy: {rf_small.score(X_moons, y_moons):.2f}')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Random Forest (100 trees)\n",
    "rf_large = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_large.fit(X_moons, y_moons)\n",
    "Z = rf_large.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "axes[2].contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "axes[2].scatter(X_moons[y_moons==0, 0], X_moons[y_moons==0, 1], c='blue', edgecolors='k', alpha=0.6)\n",
    "axes[2].scatter(X_moons[y_moons==1, 0], X_moons[y_moons==1, 1], c='red', edgecolors='k', alpha=0.6)\n",
    "axes[2].set_title(f'Random Forest (100 trees)\\nAccuracy: {rf_large.score(X_moons, y_moons):.2f}')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Single Tree vs Random Forest: The Power of Ensembles', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how the forest's boundary is smoother and more generalizable.\")\n",
    "print(\"The single tree overfits with jagged, irregular boundaries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting: Watch the model improve iteratively\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "for idx, n_estimators in enumerate([1, 5, 20, 100]):\n",
    "    gb = GradientBoostingClassifier(n_estimators=n_estimators, max_depth=2,\n",
    "                                    learning_rate=0.5, random_state=42)\n",
    "    gb.fit(X_moons, y_moons)\n",
    "    \n",
    "    Z = gb.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "    axes[idx].scatter(X_moons[y_moons==0, 0], X_moons[y_moons==0, 1], c='blue', edgecolors='k', alpha=0.5, s=15)\n",
    "    axes[idx].scatter(X_moons[y_moons==1, 0], X_moons[y_moons==1, 1], c='red', edgecolors='k', alpha=0.5, s=15)\n",
    "    axes[idx].set_title(f'{n_estimators} tree{\"s\" if n_estimators > 1 else \"\"}\\nAcc: {gb.score(X_moons, y_moons):.2f}')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Gradient Boosting: Each Tree Fixes Previous Mistakes', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key difference from Random Forest:\")\n",
    "print(\"- Random Forest: trees are built independently (parallel), then averaged\")\n",
    "print(\"- Gradient Boosting: trees are built sequentially, each one correcting residual errors\")\n",
    "print(\"- Gradient Boosting often achieves higher accuracy but is more prone to overfitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all tree-based methods\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_moons, y_moons, \n",
    "                                                     test_size=0.3, random_state=42)\n",
    "\n",
    "models = {\n",
    "    'Single Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest (100)': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting (100)': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "}\n",
    "\n",
    "print(f\"{'Model':<30} {'Train Accuracy':>15} {'Test Accuracy':>15} {'CV Score (5-fold)':>18}\")\n",
    "print('-' * 80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    cv_scores = cross_val_score(model, X_moons, y_moons, cv=5)\n",
    "    print(f\"{name:<30} {train_acc:>15.4f} {test_acc:>15.4f} {cv_scores.mean():>12.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "print(\"\\nNotice: The single tree likely overfits (high train, lower test).\")\n",
    "print(\"Ensembles generalize better, with Random Forest being more robust.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Support Vector Machines (SVMs)\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "Imagine you have two groups of points on a table and you want to separate them with a straight line. There are many possible lines, but the SVM finds the **best** one — the line that maximizes the **margin** (distance) between the line and the nearest points from each class.\n",
    "\n",
    "These nearest points are called **support vectors** because they \"support\" (define) the decision boundary. If you moved any other point, the boundary wouldn't change.\n",
    "\n",
    "**What this means:** SVMs are fundamentally about finding the widest possible \"highway\" between two classes. A wider margin means better generalization to new data.\n",
    "\n",
    "### The Kernel Trick\n",
    "\n",
    "What if the data isn't linearly separable? The **kernel trick** maps data into a higher-dimensional space where a linear boundary *does* exist — without actually computing the transformation.\n",
    "\n",
    "| Kernel | What It Does | When to Use |\n",
    "|--------|-------------|-------------|\n",
    "| **Linear** | No transformation (straight line) | Linearly separable data, high dimensions |\n",
    "| **RBF (Gaussian)** | Maps to infinite dimensions via similarity | Most common default, handles nonlinear data |\n",
    "| **Polynomial** | Maps to polynomial feature space | When you suspect polynomial decision boundaries |\n",
    "\n",
    "**Key Insight**: The kernel trick is mathematically elegant — it lets you compute dot products in a high-dimensional space without ever going there. The RBF kernel effectively measures how \"similar\" two points are; nearby points get high similarity, distant points get low similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM: Linear vs RBF kernel on moons data\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Scale data for SVM (SVMs are sensitive to feature scales)\n",
    "scaler = StandardScaler()\n",
    "X_moons_scaled = scaler.fit_transform(X_moons)\n",
    "\n",
    "x_min, x_max = X_moons_scaled[:, 0].min() - 0.5, X_moons_scaled[:, 0].max() + 0.5\n",
    "y_min, y_max = X_moons_scaled[:, 1].min() - 0.5, X_moons_scaled[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "kernel_names = ['Linear Kernel', 'RBF Kernel (Gaussian)', 'Polynomial Kernel (degree=3)']\n",
    "\n",
    "for idx, (kernel, name) in enumerate(zip(kernels, kernel_names)):\n",
    "    svm = SVC(kernel=kernel, C=1.0, random_state=42)\n",
    "    svm.fit(X_moons_scaled, y_moons)\n",
    "    \n",
    "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "    axes[idx].scatter(X_moons_scaled[y_moons==0, 0], X_moons_scaled[y_moons==0, 1], \n",
    "                      c='blue', edgecolors='k', alpha=0.6, label='Class 0')\n",
    "    axes[idx].scatter(X_moons_scaled[y_moons==1, 0], X_moons_scaled[y_moons==1, 1], \n",
    "                      c='red', edgecolors='k', alpha=0.6, label='Class 1')\n",
    "    \n",
    "    # Highlight support vectors\n",
    "    axes[idx].scatter(X_moons_scaled[svm.support_, 0], X_moons_scaled[svm.support_, 1],\n",
    "                      s=100, facecolors='none', edgecolors='green', linewidths=2, label='Support vectors')\n",
    "    \n",
    "    axes[idx].set_title(f'{name}\\nAccuracy: {svm.score(X_moons_scaled, y_moons):.2f}, SVs: {len(svm.support_)}')\n",
    "    axes[idx].set_xlabel('Feature 1 (scaled)')\n",
    "    axes[idx].set_ylabel('Feature 2 (scaled)')\n",
    "    axes[idx].legend(fontsize=8)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('SVM Decision Boundaries: The Kernel Trick', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Green circles = support vectors (the points that define the boundary)\")\n",
    "print(\"Linear kernel fails on nonlinear data; RBF handles it beautifully.\")\n",
    "print(\"The kernel trick: map to higher dimensions where data IS linearly separable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the margin concept with a simple 2D example\n",
    "np.random.seed(42)\n",
    "X_simple = np.vstack([\n",
    "    np.random.randn(30, 2) + np.array([2, 2]),\n",
    "    np.random.randn(30, 2) + np.array([-2, -2])\n",
    "])\n",
    "y_simple = np.array([0]*30 + [1]*30)\n",
    "\n",
    "svm_linear = SVC(kernel='linear', C=1.0)\n",
    "svm_linear.fit(X_simple, y_simple)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Decision boundary and margins\n",
    "x_min, x_max = X_simple[:, 0].min() - 1, X_simple[:, 0].max() + 1\n",
    "y_min, y_max = X_simple[:, 1].min() - 1, X_simple[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "\n",
    "# Get decision function values for margin visualization\n",
    "Z = svm_linear.decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.contour(xx, yy, Z, levels=[-1, 0, 1], colors=['blue', 'black', 'red'],\n",
    "            linestyles=['--', '-', '--'], linewidths=[1, 2, 1])\n",
    "plt.contourf(xx, yy, Z, levels=[-1, 1], alpha=0.1, colors=['yellow'])\n",
    "\n",
    "plt.scatter(X_simple[y_simple==0, 0], X_simple[y_simple==0, 1], c='blue', edgecolors='k', alpha=0.6, label='Class 0')\n",
    "plt.scatter(X_simple[y_simple==1, 0], X_simple[y_simple==1, 1], c='red', edgecolors='k', alpha=0.6, label='Class 1')\n",
    "plt.scatter(X_simple[svm_linear.support_, 0], X_simple[svm_linear.support_, 1],\n",
    "            s=150, facecolors='none', edgecolors='green', linewidths=2, label='Support vectors')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('SVM Maximum Margin: The Widest \"Highway\" Between Classes', fontsize=13)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.annotate('Margin\\n(maximize this)', xy=(0, 0), fontsize=11,\n",
    "             ha='center', va='center', color='orange', fontweight='bold',\n",
    "             bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.3))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Solid black line: decision boundary (where the SVM predicts the class changes)\")\n",
    "print(\"Dashed lines: margin boundaries (the 'edge of the highway')\")\n",
    "print(\"Yellow region: the margin — the SVM maximizes this distance\")\n",
    "print(f\"Number of support vectors: {len(svm_linear.support_)} out of {len(y_simple)} total points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### Deep Dive: Why SVMs Matter for Deep Learning\n",
    "\n",
    "SVMs introduce concepts that recur throughout deep learning:\n",
    "\n",
    "| SVM Concept | Deep Learning Connection |\n",
    "|-------------|-------------------------|\n",
    "| Maximum margin | Contrastive learning maximizes distance between embeddings |\n",
    "| Kernel trick | Neural networks learn nonlinear feature mappings automatically |\n",
    "| Support vectors | Hard examples in curriculum learning / hard negative mining |\n",
    "| Hinge loss | Still used in some architectures (e.g., face verification) |\n",
    "| Feature scaling | Batch normalization, layer normalization |\n",
    "\n",
    "**The kernel trick was revolutionary** because it showed you could work in infinite-dimensional spaces efficiently. Neural networks took a different approach: instead of a fixed kernel, they *learn* the feature mapping from data. But the core insight — transform the data until the problem becomes linear — is the same.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Clustering\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "Clustering is **unsupervised learning** — there are no labels, and the algorithm must discover structure on its own. The goal: group similar data points together.\n",
    "\n",
    "Think of it like sorting a pile of mixed laundry without labels. You'd naturally group items by color, fabric type, or size — clustering algorithms do the same with numerical features.\n",
    "\n",
    "| Algorithm | How It Works | Shape of Clusters | Needs K? |\n",
    "|-----------|-------------|-------------------|----------|\n",
    "| **k-Means** | Iteratively assign points to nearest center, update centers | Spherical/convex | Yes |\n",
    "| **DBSCAN** | Group points in dense regions, mark sparse points as noise | Arbitrary shapes | No (uses epsilon, min_samples) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Means: Visualize the iterative process\n",
    "X_blobs, y_blobs = make_blobs(n_samples=300, centers=4, cluster_std=0.8, random_state=42)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Manual k-means iterations to show the process\n",
    "from sklearn.cluster import KMeans as KM\n",
    "\n",
    "# Initialize with random centroids\n",
    "np.random.seed(42)\n",
    "initial_centers = X_blobs[np.random.choice(len(X_blobs), 4, replace=False)]\n",
    "\n",
    "# Show iterations\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "iterations = [1, 2, 3, 5, 10, 20]\n",
    "\n",
    "for idx, max_iter in enumerate(iterations):\n",
    "    row, col = idx // 3, idx % 3\n",
    "    \n",
    "    km = KMeans(n_clusters=4, init=initial_centers, n_init=1, max_iter=max_iter, random_state=42)\n",
    "    labels = km.fit_predict(X_blobs)\n",
    "    centers = km.cluster_centers_\n",
    "    \n",
    "    for k in range(4):\n",
    "        mask = labels == k\n",
    "        axes[row, col].scatter(X_blobs[mask, 0], X_blobs[mask, 1], \n",
    "                               c=colors[k], alpha=0.5, s=20)\n",
    "    \n",
    "    axes[row, col].scatter(centers[:, 0], centers[:, 1], c='black', marker='X', \n",
    "                           s=200, edgecolors='white', linewidths=2, zorder=5)\n",
    "    axes[row, col].set_title(f'Iteration {max_iter}')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('k-Means Clustering: Iterative Convergence\\n'\n",
    "             '(X marks = cluster centers, colors = assignments)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"k-Means algorithm:\")\n",
    "print(\"1. Initialize K random centers\")\n",
    "print(\"2. Assign each point to its nearest center\")\n",
    "print(\"3. Move each center to the mean of its assigned points\")\n",
    "print(\"4. Repeat steps 2-3 until convergence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method: How to choose K\n",
    "inertias = []\n",
    "K_range = range(1, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    km.fit(X_blobs)\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "plt.axvline(x=4, color='red', linestyle='--', alpha=0.7, label='Elbow at K=4 (true number of clusters)')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia (within-cluster sum of squares)')\n",
    "plt.title('The Elbow Method: Finding the Right Number of Clusters')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.annotate('\"Elbow\" — diminishing\\nreturns after this K',\n",
    "             xy=(4, inertias[3]), xytext=(6, inertias[1]),\n",
    "             arrowprops=dict(arrowstyle='->', color='red'),\n",
    "             fontsize=11, color='red')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"The elbow method: plot inertia vs K and look for the 'bend'.\")\n",
    "print(\"Before the elbow: adding clusters gives big improvement.\")\n",
    "print(\"After the elbow: adding clusters gives diminishing returns.\")\n",
    "print(f\"\\nInertia values: {[f'{x:.0f}' for x in inertias]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Means vs DBSCAN: Different strengths\n",
    "# Create data with non-convex clusters\n",
    "X_circles, y_circles = make_moons(n_samples=300, noise=0.08, random_state=42)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# True labels\n",
    "axes[0].scatter(X_circles[y_circles==0, 0], X_circles[y_circles==0, 1], c='blue', alpha=0.6, label='Class 0')\n",
    "axes[0].scatter(X_circles[y_circles==1, 0], X_circles[y_circles==1, 1], c='red', alpha=0.6, label='Class 1')\n",
    "axes[0].set_title('True Labels (moons data)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# k-Means (fails on non-convex)\n",
    "km = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "km_labels = km.fit_predict(X_circles)\n",
    "axes[1].scatter(X_circles[km_labels==0, 0], X_circles[km_labels==0, 1], c='blue', alpha=0.6)\n",
    "axes[1].scatter(X_circles[km_labels==1, 0], X_circles[km_labels==1, 1], c='red', alpha=0.6)\n",
    "axes[1].scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], c='black', marker='X', s=200, zorder=5)\n",
    "axes[1].set_title('k-Means (K=2) — FAILS on non-convex shapes')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# DBSCAN (handles non-convex)\n",
    "db = DBSCAN(eps=0.2, min_samples=5)\n",
    "db_labels = db.fit_predict(X_circles)\n",
    "for label in np.unique(db_labels):\n",
    "    if label == -1:\n",
    "        axes[2].scatter(X_circles[db_labels==label, 0], X_circles[db_labels==label, 1], \n",
    "                       c='gray', marker='x', alpha=0.5, label='Noise')\n",
    "    else:\n",
    "        color = ['blue', 'red', 'green'][label % 3]\n",
    "        axes[2].scatter(X_circles[db_labels==label, 0], X_circles[db_labels==label, 1], \n",
    "                       c=color, alpha=0.6, label=f'Cluster {label}')\n",
    "axes[2].set_title('DBSCAN — Finds arbitrary cluster shapes')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('k-Means vs DBSCAN: Choosing the Right Clustering Algorithm', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"k-Means assumes spherical clusters — it fails on crescent-shaped data.\")\n",
    "print(\"DBSCAN finds clusters of arbitrary shape by following dense regions.\")\n",
    "print(\"DBSCAN also detects noise points (gray X markers) that don't belong to any cluster.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "### Deep Dive: Clustering in the ML Pipeline\n",
    "\n",
    "Clustering isn't just an end in itself — it's a tool used throughout ML:\n",
    "\n",
    "| Application | How Clustering Is Used |\n",
    "|-------------|----------------------|\n",
    "| **Feature engineering** | Cluster IDs become features for downstream models |\n",
    "| **Data exploration** | Discover natural groups before building supervised models |\n",
    "| **Anomaly detection** | Points far from any cluster center are anomalies |\n",
    "| **Semi-supervised learning** | Propagate labels from labeled to unlabeled points in same cluster |\n",
    "| **Embeddings** | k-means on word embeddings discovers topic clusters |\n",
    "| **Vector quantization** | Compress continuous embeddings to discrete cluster IDs (used in VQ-VAE) |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. k-Nearest Neighbors and Naive Bayes\n",
    "\n",
    "### k-Nearest Neighbors (k-NN)\n",
    "\n",
    "The simplest classifier: to predict a new point, find the K closest training points and take a majority vote.\n",
    "\n",
    "**Strengths:** No training phase, works for any number of classes, intuitive.  \n",
    "**Weaknesses:** Slow at prediction time (must compare to all training points), sensitive to feature scales, struggles in high dimensions (\"curse of dimensionality\").\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "Uses Bayes' theorem with the \"naive\" assumption that features are independent:\n",
    "\n",
    "$$P(y \\mid x_1, x_2, \\ldots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)$$\n",
    "\n",
    "**Strengths:** Extremely fast, works well with high-dimensional sparse data (text), good with small training sets.  \n",
    "**Weaknesses:** The independence assumption is almost never true, so probability estimates are often poorly calibrated.\n",
    "\n",
    "| Algorithm | Type | Training | Prediction | Best For |\n",
    "|-----------|------|----------|------------|----------|\n",
    "| **k-NN** | Distance-based | None (stores data) | Slow (O(n) per query) | Small datasets, few features |\n",
    "| **Naive Bayes** | Probabilistic | Fast (O(n*d)) | Very fast (O(d)) | Text classification, spam detection |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN: Effect of K on decision boundary\n",
    "X_knn, y_knn = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "x_min, x_max = X_knn[:, 0].min() - 0.5, X_knn[:, 0].max() + 0.5\n",
    "y_min, y_max = X_knn[:, 1].min() - 0.5, X_knn[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "\n",
    "for idx, k in enumerate([1, 5, 15, 50]):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_knn, y_knn)\n",
    "    \n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "    axes[idx].scatter(X_knn[y_knn==0, 0], X_knn[y_knn==0, 1], c='blue', edgecolors='k', alpha=0.5, s=15)\n",
    "    axes[idx].scatter(X_knn[y_knn==1, 0], X_knn[y_knn==1, 1], c='red', edgecolors='k', alpha=0.5, s=15)\n",
    "    axes[idx].set_title(f'k = {k}\\nAcc: {knn.score(X_knn, y_knn):.2f}')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('k-NN: Effect of K on Decision Boundary\\n'\n",
    "             'Small K → overfits, Large K → underfits', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"k=1: Every training point is its own island (overfitting)\")\n",
    "print(\"k=50: Decision boundary is too smooth (underfitting)\")\n",
    "print(\"k=5 or k=15: Good balance between flexibility and smoothness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes vs k-NN comparison\n",
    "X_compare, y_compare = make_classification(n_samples=500, n_features=2, n_redundant=0,\n",
    "                                            n_informative=2, n_clusters_per_class=2, random_state=42)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "x_min, x_max = X_compare[:, 0].min() - 1, X_compare[:, 0].max() + 1\n",
    "y_min, y_max = X_compare[:, 1].min() - 1, X_compare[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "\n",
    "# Naive Bayes\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_compare, y_compare)\n",
    "Z = nb.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "axes[0].contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "axes[0].scatter(X_compare[y_compare==0, 0], X_compare[y_compare==0, 1], c='blue', edgecolors='k', alpha=0.5, s=15)\n",
    "axes[0].scatter(X_compare[y_compare==1, 0], X_compare[y_compare==1, 1], c='red', edgecolors='k', alpha=0.5, s=15)\n",
    "axes[0].set_title(f'Naive Bayes (Gaussian)\\nAccuracy: {nb.score(X_compare, y_compare):.2f}')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# k-NN\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_compare, y_compare)\n",
    "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "axes[1].contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "axes[1].scatter(X_compare[y_compare==0, 0], X_compare[y_compare==0, 1], c='blue', edgecolors='k', alpha=0.5, s=15)\n",
    "axes[1].scatter(X_compare[y_compare==1, 0], X_compare[y_compare==1, 1], c='red', edgecolors='k', alpha=0.5, s=15)\n",
    "axes[1].set_title(f'k-NN (k=7)\\nAccuracy: {knn.score(X_compare, y_compare):.2f}')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Naive Bayes vs k-NN: Probabilistic vs Distance-Based', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Naive Bayes: smooth, linear-ish boundary (assumes Gaussian features)\")\n",
    "print(\"k-NN: flexible, adapts to local data density\")\n",
    "print(\"\\nBoth are useful baselines — always try simple models first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Model Evaluation\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "Training accuracy is meaningless if the model memorizes the data. We need to evaluate on data the model has **never seen**. This section covers the essential toolkit for honest model evaluation.\n",
    "\n",
    "### Cross-Validation\n",
    "\n",
    "Instead of a single train/test split (which depends on which points ended up in which set), **k-fold cross-validation** rotates through K different splits and averages the results:\n",
    "\n",
    "1. Split data into K equal folds\n",
    "2. For each fold: train on K-1 folds, test on the held-out fold\n",
    "3. Average the K test scores\n",
    "\n",
    "**What this means:** Cross-validation gives a more reliable estimate of model performance by testing on every data point exactly once.\n",
    "\n",
    "### The Confusion Matrix\n",
    "\n",
    "For classification, accuracy alone is not enough. The confusion matrix breaks down predictions into four categories:\n",
    "\n",
    "|  | Predicted Positive | Predicted Negative |\n",
    "|--|-------------------|-------------------|\n",
    "| **Actually Positive** | True Positive (TP) | False Negative (FN) |\n",
    "| **Actually Negative** | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "From this matrix, we derive:\n",
    "\n",
    "| Metric | Formula | Intuition | Use When |\n",
    "|--------|---------|-----------|----------|\n",
    "| **Accuracy** | (TP+TN) / Total | Overall correctness | Classes are balanced |\n",
    "| **Precision** | TP / (TP+FP) | \"Of predicted positives, how many are correct?\" | Cost of false alarms is high |\n",
    "| **Recall** | TP / (TP+FN) | \"Of actual positives, how many did we catch?\" | Cost of missing positives is high |\n",
    "| **F1 Score** | 2 * (P*R) / (P+R) | Harmonic mean of precision and recall | Need to balance both |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation comparison of all models\n",
    "X_eval, y_eval = make_classification(n_samples=500, n_features=10, n_informative=5,\n",
    "                                      n_redundant=2, random_state=42)\n",
    "\n",
    "models_eval = {\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', random_state=42),\n",
    "    'k-NN (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "print(f\"{'Model':<25} {'CV Mean':>10} {'CV Std':>10} {'Min':>10} {'Max':>10}\")\n",
    "print('-' * 70)\n",
    "\n",
    "for name, model in models_eval.items():\n",
    "    scores = cross_val_score(model, X_eval, y_eval, cv=5, scoring='accuracy')\n",
    "    cv_results[name] = scores\n",
    "    print(f\"{name:<25} {scores.mean():>10.4f} {scores.std():>10.4f} {scores.min():>10.4f} {scores.max():>10.4f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "positions = range(len(cv_results))\n",
    "bp = plt.boxplot([scores for scores in cv_results.values()], labels=cv_results.keys(),\n",
    "                  patch_artist=True)\n",
    "\n",
    "colors_box = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6', '#f39c12', '#1abc9c']\n",
    "for patch, color in zip(bp['boxes'], colors_box):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "plt.ylabel('Accuracy (5-fold CV)')\n",
    "plt.title('Cross-Validation Comparison: All Classical ML Models')\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and Classification Report\n",
    "X_train_eval, X_test_eval, y_train_eval, y_test_eval = train_test_split(\n",
    "    X_eval, y_eval, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_eval, y_train_eval)\n",
    "y_pred = rf.predict(X_test_eval)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_eval, y_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Raw counts\n",
    "im = axes[0].imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=13)\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_yticks([0, 1])\n",
    "axes[0].set_xticklabels(['Negative (0)', 'Positive (1)'])\n",
    "axes[0].set_yticklabels(['Negative (0)', 'Positive (1)'])\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text_color = 'white' if cm[i, j] > cm.max() / 2 else 'black'\n",
    "        axes[0].text(j, i, f'{cm[i, j]}', ha='center', va='center', \n",
    "                    fontsize=20, color=text_color, fontweight='bold')\n",
    "\n",
    "# Normalized (percentages)\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "im2 = axes[1].imshow(cm_norm, interpolation='nearest', cmap='Blues', vmin=0, vmax=1)\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=13)\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xticks([0, 1])\n",
    "axes[1].set_yticks([0, 1])\n",
    "axes[1].set_xticklabels(['Negative (0)', 'Positive (1)'])\n",
    "axes[1].set_yticklabels(['Negative (0)', 'Positive (1)'])\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text_color = 'white' if cm_norm[i, j] > 0.5 else 'black'\n",
    "        axes[1].text(j, i, f'{cm_norm[i, j]:.2%}', ha='center', va='center', \n",
    "                    fontsize=16, color=text_color, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_eval, y_pred, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves: Compare multiple models\n",
    "# Need probability outputs, so we use models that support predict_proba\n",
    "roc_models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'k-NN (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "colors_roc = ['blue', 'red', 'green', 'orange']\n",
    "for (name, model), color in zip(roc_models.items(), colors_roc):\n",
    "    model.fit(X_train_eval, y_train_eval)\n",
    "    y_prob = model.predict_proba(X_test_eval)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test_eval, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color=color, linewidth=2, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "# Random baseline\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5, label='Random (AUC = 0.500)')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves: Comparing Model Performance')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Reading ROC curves:\")\n",
    "print(\"- Upper-left corner is perfect (all true positives, no false positives)\")\n",
    "print(\"- Diagonal line = random guessing\")\n",
    "print(\"- AUC (Area Under Curve): 1.0 = perfect, 0.5 = random\")\n",
    "print(\"- Higher AUC = better model at discriminating between classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "### Deep Dive: When to Use What Metric\n",
    "\n",
    "Choosing the right metric is one of the most important — and most overlooked — decisions in ML:\n",
    "\n",
    "| Scenario | Best Metric | Why |\n",
    "|----------|------------|-----|\n",
    "| Balanced classes, equal costs | **Accuracy** | Simple and interpretable |\n",
    "| Imbalanced classes | **F1 Score** or **AUC-ROC** | Accuracy is misleading (predicting majority class gets high accuracy) |\n",
    "| Spam filter (cost of false alarm) | **Precision** | You don't want real emails in spam |\n",
    "| Medical screening (cost of missing) | **Recall** | You don't want to miss a disease |\n",
    "| Ranking/recommendation | **AUC-ROC** | Cares about ordering, not absolute threshold |\n",
    "| Information retrieval | **Precision@K** | Only top K results matter |\n",
    "\n",
    "#### Common Misconceptions\n",
    "\n",
    "| Misconception | Reality |\n",
    "|---------------|--------|\n",
    "| High accuracy = good model | On imbalanced data, always predicting the majority class gives high accuracy |\n",
    "| AUC-ROC is always best | AUC-ROC can be misleading with severe class imbalance; use Precision-Recall AUC instead |\n",
    "| One metric tells the whole story | Always look at multiple metrics together |\n",
    "\n",
    "---\n",
    "\n",
    "## 7. When NOT to Use Deep Learning\n",
    "\n",
    "### Practical Decision Guide\n",
    "\n",
    "Deep learning is powerful but not always the right tool. Classical ML wins in many real-world scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Small data — where classical ML shines\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "sample_sizes = [30, 50, 100, 200, 500, 1000]\n",
    "results_rf = []\n",
    "results_gb = []\n",
    "results_nn = []\n",
    "\n",
    "for n_samples in sample_sizes:\n",
    "    # Generate data\n",
    "    X_size, y_size = make_classification(n_samples=n_samples, n_features=10, \n",
    "                                         n_informative=5, n_redundant=2, random_state=42)\n",
    "    \n",
    "    # Random Forest\n",
    "    rf_scores = cross_val_score(RandomForestClassifier(n_estimators=100, random_state=42), \n",
    "                                X_size, y_size, cv=5)\n",
    "    results_rf.append(rf_scores.mean())\n",
    "    \n",
    "    # Gradient Boosting\n",
    "    gb_scores = cross_val_score(GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "                                X_size, y_size, cv=5)\n",
    "    results_gb.append(gb_scores.mean())\n",
    "    \n",
    "    # Neural Network (small MLP)\n",
    "    nn_scores = cross_val_score(MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=500, \n",
    "                                              random_state=42),\n",
    "                                X_size, y_size, cv=5)\n",
    "    results_nn.append(nn_scores.mean())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sample_sizes, results_rf, 'g-o', linewidth=2, markersize=8, label='Random Forest')\n",
    "plt.plot(sample_sizes, results_gb, 'r-s', linewidth=2, markersize=8, label='Gradient Boosting')\n",
    "plt.plot(sample_sizes, results_nn, 'b-^', linewidth=2, markersize=8, label='Neural Network (MLP)')\n",
    "\n",
    "plt.xlabel('Number of Training Samples')\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.title('Sample Size vs Model Performance:\\nClassical ML Excels with Small Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.annotate('Classical ML wins here', xy=(50, results_rf[1]), xytext=(100, results_rf[1] - 0.05),\n",
    "             arrowprops=dict(arrowstyle='->', color='green'),\n",
    "             fontsize=11, color='green')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: With <200 samples, Random Forests and Gradient Boosting\")\n",
    "print(\"typically outperform neural networks. Neural nets need data to learn features;\")\n",
    "print(\"classical ML uses hand-crafted features more efficiently.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "### The Decision Framework\n",
    "\n",
    "| Situation | Use Classical ML | Use Deep Learning |\n",
    "|-----------|-----------------|------------------|\n",
    "| **Data size** | < 10K samples | > 100K samples |\n",
    "| **Data type** | Tabular (rows and columns) | Images, text, audio, video |\n",
    "| **Features** | Hand-crafted, meaningful | Raw pixels, tokens, waveforms |\n",
    "| **Interpretability** | Required (medical, legal, finance) | Not critical |\n",
    "| **Training budget** | Minutes on a laptop | Hours/days on GPUs |\n",
    "| **Deployment** | Edge devices, low latency | Server-side, batch processing |\n",
    "| **Baseline** | ALWAYS start here | After classical ML baseline |\n",
    "\n",
    "### The Real-World Truth About Tabular Data\n",
    "\n",
    "As of 2024, **gradient boosted trees (XGBoost, LightGBM, CatBoost) still outperform deep learning on most tabular datasets**. This is a well-studied phenomenon:\n",
    "\n",
    "1. Tabular data has **heterogeneous features** (mix of types, scales, meanings)\n",
    "2. Tree-based models handle this naturally; neural nets need careful preprocessing\n",
    "3. Trees are **rotation-invariant** to individual features; neural nets are not\n",
    "4. Tabular datasets are typically smaller, favoring classical ML\n",
    "\n",
    "**Rule of thumb:** If your data fits in a spreadsheet, try XGBoost before reaching for a neural network.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "### Exercise 1: Build a Complete Classification Pipeline\n",
    "\n",
    "Create a function that takes a dataset and compares multiple classifiers using cross-validation, returning the best model name and its score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 1: Complete classification pipeline\n",
    "def find_best_classifier(X, y, cv=5):\n",
    "    \"\"\"\n",
    "    Compare multiple classifiers using cross-validation and return the best one.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Labels\n",
    "        cv: Number of cross-validation folds\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (best_model_name, best_cv_score, results_dict)\n",
    "        where results_dict maps model names to their mean CV scores\n",
    "    \"\"\"\n",
    "    # TODO: Implement this!\n",
    "    # 1. Scale the features using StandardScaler\n",
    "    # 2. Define a dictionary of at least 4 models (Decision Tree, Random Forest,\n",
    "    #    Gradient Boosting, SVM, k-NN, Naive Bayes)\n",
    "    # 3. Run cross_val_score for each model\n",
    "    # 4. Return the name and score of the best model, plus all results\n",
    "    # Hint: Use StandardScaler().fit_transform(X) to scale features\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "# Test\n",
    "X_ex1, y_ex1 = make_classification(n_samples=500, n_features=10, n_informative=5,\n",
    "                                    n_redundant=2, random_state=42)\n",
    "\n",
    "result = find_best_classifier(X_ex1, y_ex1)\n",
    "if result is not None:\n",
    "    best_name, best_score, all_results = result\n",
    "    print(f\"Best model: {best_name} (CV score: {best_score:.4f})\")\n",
    "    print(f\"\\nAll results:\")\n",
    "    for name, score in sorted(all_results.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {name}: {score:.4f}\")\n",
    "    \n",
    "    # Verify\n",
    "    assert best_score > 0.80, f\"Best score should be > 0.80, got {best_score:.4f}\"\n",
    "    assert len(all_results) >= 4, f\"Should compare at least 4 models, got {len(all_results)}\"\n",
    "    print(f\"\\nAll checks passed!\")\n",
    "else:\n",
    "    print(\"TODO: Implement find_best_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "### Exercise 2: Clustering Evaluation\n",
    "\n",
    "Implement the **silhouette score** from scratch. The silhouette score measures how similar a point is to its own cluster vs the nearest neighboring cluster. It ranges from -1 (wrong cluster) to +1 (well-matched).\n",
    "\n",
    "For each point $i$:\n",
    "- $a(i)$ = average distance to all other points in the **same** cluster\n",
    "- $b(i)$ = average distance to all points in the **nearest other** cluster\n",
    "- $s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 2: Silhouette score from scratch\n",
    "def silhouette_score_manual(X, labels):\n",
    "    \"\"\"\n",
    "    Calculate the mean silhouette score for a clustering.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (n_samples, n_features)\n",
    "        labels: Cluster assignments for each point\n",
    "    \n",
    "    Returns:\n",
    "        Mean silhouette score (float between -1 and 1)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this!\n",
    "    # For each point i:\n",
    "    #   1. Compute a(i): mean distance to points in same cluster\n",
    "    #   2. Compute b(i): mean distance to points in nearest OTHER cluster\n",
    "    #   3. Compute s(i) = (b(i) - a(i)) / max(a(i), b(i))\n",
    "    # Return the mean of all s(i)\n",
    "    # Hint: Use np.linalg.norm(X[i] - X[j]) for distance\n",
    "    # Hint: Handle edge case where a cluster has only 1 point (s(i) = 0)\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "# Test\n",
    "from sklearn.metrics import silhouette_score as sklearn_silhouette\n",
    "\n",
    "X_ex2, y_ex2 = make_blobs(n_samples=150, centers=3, cluster_std=0.8, random_state=42)\n",
    "km_ex2 = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels_ex2 = km_ex2.fit_predict(X_ex2)\n",
    "\n",
    "manual_score = silhouette_score_manual(X_ex2, labels_ex2)\n",
    "if manual_score is not None:\n",
    "    sklearn_score = sklearn_silhouette(X_ex2, labels_ex2)\n",
    "    print(f\"Your silhouette score: {manual_score:.4f}\")\n",
    "    print(f\"sklearn silhouette score: {sklearn_score:.4f}\")\n",
    "    print(f\"Correct: {np.allclose(manual_score, sklearn_score, atol=1e-4)}\")\n",
    "else:\n",
    "    print(\"TODO: Implement silhouette_score_manual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "### Exercise 3: Decision Boundary Explorer\n",
    "\n",
    "Create a function that plots the decision boundary of any sklearn classifier on 2D data. Then use it to compare how different classifiers handle the `make_moons` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 3: Decision boundary explorer\n",
    "def plot_decision_boundary(clf, X, y, ax=None, title=''):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary of a fitted classifier.\n",
    "    \n",
    "    Args:\n",
    "        clf: A fitted sklearn classifier\n",
    "        X: Feature matrix (n_samples, 2) — must be 2D!\n",
    "        y: Labels\n",
    "        ax: Matplotlib axes (creates new figure if None)\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    # TODO: Implement this!\n",
    "    # 1. Create a mesh grid spanning the data range (with some padding)\n",
    "    # 2. Predict on every point in the mesh\n",
    "    # 3. Use contourf to color the regions\n",
    "    # 4. Scatter plot the actual data points on top\n",
    "    # 5. Add title with the model's test accuracy\n",
    "    # Hint: Use np.meshgrid and clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "# Test: Compare 6 classifiers on moons data\n",
    "X_ex3, y_ex3 = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "X_ex3_scaled = StandardScaler().fit_transform(X_ex3)\n",
    "\n",
    "classifiers_ex3 = [\n",
    "    ('Decision Tree', DecisionTreeClassifier(max_depth=5, random_state=42)),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "    ('SVM (RBF)', SVC(kernel='rbf', random_state=42)),\n",
    "    ('k-NN (k=5)', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('Naive Bayes', GaussianNB()),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "for idx, (name, clf) in enumerate(classifiers_ex3):\n",
    "    row, col = idx // 3, idx % 3\n",
    "    clf.fit(X_ex3_scaled, y_ex3)\n",
    "    plot_decision_boundary(clf, X_ex3_scaled, y_ex3, ax=axes[row, col], title=name)\n",
    "\n",
    "plt.suptitle('Decision Boundary Comparison: 6 Classifiers on Moons Data', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Compare the boundaries — which models handle the nonlinear shape best?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "### Exercise 4: Feature Importance Analysis\n",
    "\n",
    "Train a Random Forest on a classification dataset and analyze which features matter most. This is one of the biggest advantages of tree-based models — built-in feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": "# EXERCISE 4: Feature importance\ndef analyze_feature_importance(X, y, feature_names=None):\n    \"\"\"\n    Train a Random Forest and visualize feature importances.\n    \n    Args:\n        X: Feature matrix\n        y: Labels\n        feature_names: List of feature names (optional)\n    \n    Returns:\n        Sorted list of (feature_name, importance) tuples\n    \"\"\"\n    # TODO: Implement this!\n    # 1. Train a RandomForestClassifier on the data\n    # 2. Extract .feature_importances_\n    # 3. Sort features by importance (descending)\n    # 4. Create a horizontal bar chart\n    # 5. Return sorted (name, importance) tuples\n    # Hint: If feature_names is None, use [\"Feature 0\", \"Feature 1\", ...]\n    \n    pass\n\n\n# Test\nX_ex4, y_ex4 = make_classification(n_samples=500, n_features=10, n_informative=3,\n                                    n_redundant=2, random_state=42)\n\nfeature_names_ex4 = [f'Feature {i}' for i in range(10)]\nresult = analyze_feature_importance(X_ex4, y_ex4, feature_names_ex4)\n\nif result is not None:\n    print(\"Feature importance ranking:\")\n    for name, importance in result:\n        bar = '#' * int(importance * 100)\n        print(f\"  {name:>12}: {importance:.4f} {bar}\")\n    \n    # Verify: top 3 features should capture most importance\n    top3_importance = sum(imp for _, imp in result[:3])\n    print(f\"\\nTop 3 features capture {top3_importance:.1%} of total importance\")\n    assert top3_importance > 0.4, \"Top 3 informative features should capture >40% importance\"\n    print(\"Check passed!\")\nelse:\n    print(\"TODO: Implement analyze_feature_importance\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Decision Trees** split data using yes/no questions, choosing splits that maximize information gain. Simple but prone to overfitting.\n",
    "- **Random Forests** (bagging) combine many trees trained on random data subsets. Reduces variance while keeping low bias.\n",
    "- **Gradient Boosting** builds trees sequentially, each correcting previous errors. Often the highest accuracy on tabular data.\n",
    "- **SVMs** find the maximum-margin decision boundary. The kernel trick handles nonlinear data without explicit feature transformation.\n",
    "- **k-Means** clustering iteratively assigns points to nearest centers. Needs K specified upfront, assumes spherical clusters.\n",
    "- **DBSCAN** finds clusters by density, handles arbitrary shapes, and automatically detects noise points.\n",
    "- **k-NN** classifies by majority vote of nearest neighbors. Simple but slow at prediction time.\n",
    "- **Naive Bayes** uses Bayes' theorem with feature independence assumption. Extremely fast, great for text.\n",
    "- **Model evaluation** requires cross-validation, confusion matrices, and appropriate metrics for the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "### Connection to Deep Learning\n",
    "\n",
    "| Classical ML Concept | Deep Learning Connection |\n",
    "|---------------------|-------------------------|\n",
    "| Decision tree splits | Feature thresholding in neural network activations (ReLU) |\n",
    "| Ensemble averaging | Dropout as implicit ensemble, model ensembles in production |\n",
    "| Gradient boosting residuals | Residual connections in ResNets |\n",
    "| SVM kernel trick | Neural networks as learned feature mappings |\n",
    "| SVM maximum margin | Contrastive loss, triplet loss in embedding learning |\n",
    "| k-Means centroids | Learned prototypes in prototype networks, VQ-VAE codebook |\n",
    "| Cross-validation | Standard evaluation protocol for all ML models |\n",
    "| Feature importance | Attention weights, gradient-based attribution (Grad-CAM) |\n",
    "| Bias-variance tradeoff | Underfitting/overfitting in neural network training |\n",
    "| Confusion matrix & ROC | Same evaluation tools used for deep learning classifiers |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "### Algorithm Selection Cheat Sheet\n",
    "\n",
    "| Your Situation | Best Starting Point | Why |\n",
    "|---------------|--------------------|----- |\n",
    "| Tabular data, any size | Gradient Boosting (XGBoost) | State-of-the-art on tabular data |\n",
    "| Need interpretability | Decision Tree or Logistic Regression | Transparent decision process |\n",
    "| Small dataset (< 1K) | Random Forest or SVM | Robust with limited data |\n",
    "| High-dimensional sparse data | Naive Bayes or Linear SVM | Fast, handles many features |\n",
    "| No labels (unsupervised) | k-Means or DBSCAN | Discover natural groupings |\n",
    "| Quick baseline | k-NN or Naive Bayes | Minimal tuning required |\n",
    "| Images, text, audio | Deep Learning | Feature learning is key |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "### Checklist\n",
    "\n",
    "- [ ] I can explain how a decision tree chooses splits using Gini impurity or information gain\n",
    "- [ ] I understand why ensembles (bagging, boosting) outperform single models\n",
    "- [ ] I can explain the SVM maximum margin concept and the kernel trick\n",
    "- [ ] I can apply k-means and DBSCAN, and know when each is appropriate\n",
    "- [ ] I know the difference between precision, recall, F1, and when to use each\n",
    "- [ ] I can read a confusion matrix and ROC curve\n",
    "- [ ] I use cross-validation instead of single train/test splits\n",
    "- [ ] I know when to use classical ML vs deep learning\n",
    "- [ ] I always start with a simple baseline before trying complex models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In **Part 1.5: Optimization & Linear Programming**, we'll study the mathematical machinery that powers model training:\n",
    "- Gradient descent and its variants (SGD, Adam, RMSprop)\n",
    "- Convex vs non-convex optimization\n",
    "- Constrained optimization and Lagrange multipliers\n",
    "- Learning rate schedules and convergence\n",
    "\n",
    "Understanding optimization is critical because **every ML model** — from the simplest linear regression to the largest transformer — is trained by minimizing a loss function. The optimization algorithms we'll study next are the engine that makes learning possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}