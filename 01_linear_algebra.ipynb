{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.1: Linear Algebra for Deep Learning\n",
    "\n",
    "Linear algebra is the foundation of deep learning. Neural networks are essentially compositions of linear transformations (matrix multiplications) and nonlinear activation functions.\n",
    "\n",
    "## Learning Objectives\n",
    "- [ ] Understand vector spaces and linear transformations\n",
    "- [ ] Perform matrix operations fluently with NumPy\n",
    "- [ ] Explain the geometric intuition behind eigendecomposition\n",
    "- [ ] Apply SVD to dimensionality reduction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "# For nice inline plots\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vectors\n",
    "\n",
    "A **vector** is an ordered list of numbers. In machine learning:\n",
    "- A single data point (features) is a vector\n",
    "- Model parameters (weights) are vectors\n",
    "- Gradients are vectors\n",
    "\n",
    "### Geometric Interpretation\n",
    "A vector can be thought of as:\n",
    "1. A point in space\n",
    "2. An arrow from the origin to that point (direction + magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating vectors in NumPy\n",
    "v = np.array([3, 4])  # 2D vector\n",
    "w = np.array([1, 2, 3])  # 3D vector\n",
    "\n",
    "print(f\"Vector v: {v}\")\n",
    "print(f\"Shape of v: {v.shape}\")\n",
    "print(f\"Dimension (number of elements): {v.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a 2D vector\n",
    "def plot_vectors(vectors, colors, labels=None):\n",
    "    \"\"\"Plot 2D vectors from origin.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    for i, (vec, color) in enumerate(zip(vectors, colors)):\n",
    "        label = labels[i] if labels else None\n",
    "        ax.quiver(0, 0, vec[0], vec[1], angles='xy', scale_units='xy', scale=1, \n",
    "                  color=color, label=label, width=0.015)\n",
    "    \n",
    "    # Set axis limits\n",
    "    all_coords = np.array(vectors)\n",
    "    max_val = np.abs(all_coords).max() + 1\n",
    "    ax.set_xlim(-max_val, max_val)\n",
    "    ax.set_ylim(-max_val, max_val)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if labels:\n",
    "        ax.legend()\n",
    "    return ax\n",
    "\n",
    "# Plot vector v = [3, 4]\n",
    "plot_vectors([v], ['blue'], ['v = [3, 4]'])\n",
    "plt.title('A 2D Vector')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Operations\n",
    "\n",
    "#### 1. Vector Addition\n",
    "Vectors add element-wise. Geometrically, place the tail of the second vector at the head of the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([2, 1])\n",
    "b = np.array([1, 3])\n",
    "c = a + b  # Element-wise addition\n",
    "\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b}\")\n",
    "print(f\"a + b = {c}\")\n",
    "\n",
    "# Visualize vector addition\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.quiver(0, 0, a[0], a[1], angles='xy', scale_units='xy', scale=1, color='blue', label='a', width=0.015)\n",
    "ax.quiver(0, 0, b[0], b[1], angles='xy', scale_units='xy', scale=1, color='red', label='b', width=0.015)\n",
    "ax.quiver(0, 0, c[0], c[1], angles='xy', scale_units='xy', scale=1, color='green', label='a + b', width=0.015)\n",
    "# Show b starting from tip of a (parallelogram rule)\n",
    "ax.quiver(a[0], a[1], b[0], b[1], angles='xy', scale_units='xy', scale=1, color='red', alpha=0.3, width=0.015)\n",
    "ax.set_xlim(-1, 5)\n",
    "ax.set_ylim(-1, 5)\n",
    "ax.set_aspect('equal')\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.legend()\n",
    "ax.set_title('Vector Addition: a + b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Scalar Multiplication\n",
    "Multiplying a vector by a scalar scales its magnitude (and flips direction if negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([2, 1])\n",
    "scaled_v = 2 * v\n",
    "negative_v = -1 * v\n",
    "\n",
    "print(f\"v = {v}\")\n",
    "print(f\"2v = {scaled_v}\")\n",
    "print(f\"-v = {negative_v}\")\n",
    "\n",
    "plot_vectors([v, scaled_v, negative_v], ['blue', 'green', 'red'], ['v', '2v', '-v'])\n",
    "plt.title('Scalar Multiplication')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Dot Product\n",
    "\n",
    "The **dot product** (inner product) of two vectors is fundamental:\n",
    "\n",
    "$$\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{n} a_i b_i = |\\mathbf{a}| |\\mathbf{b}| \\cos\\theta$$\n",
    "\n",
    "Where $\\theta$ is the angle between the vectors.\n",
    "\n",
    "**Key insights:**\n",
    "- If dot product = 0, vectors are **orthogonal** (perpendicular)\n",
    "- If positive, vectors point in similar directions\n",
    "- If negative, vectors point in opposite directions\n",
    "- Used everywhere in neural networks: weighted sums!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 0])\n",
    "b = np.array([0, 1])\n",
    "c = np.array([1, 1])\n",
    "\n",
    "# Different ways to compute dot product\n",
    "print(f\"a · b = {np.dot(a, b)}\")  # Orthogonal vectors\n",
    "print(f\"a · c = {np.dot(a, c)}\")  # 45 degree angle\n",
    "print(f\"a · a = {np.dot(a, a)}\")  # Same vector (gives squared magnitude)\n",
    "\n",
    "# Using @ operator (preferred in modern NumPy)\n",
    "print(f\"a @ c = {a @ c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Computing angle between vectors using dot product\ndef angle_between(v1, v2):\n    \"\"\"Returns angle in degrees between vectors v1 and v2.\"\"\"\n    cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n    # Clip to handle numerical errors\n    cos_angle = np.clip(cos_angle, -1, 1)\n    return np.degrees(np.arccos(cos_angle))\n\na = np.array([1, 0])\nb = np.array([1, 1])\nc = np.array([0, 1])\nd = np.array([-1, 0])\n\nprint(f\"Angle between a=[1,0] and b=[1,1]: {angle_between(a, b):.1f}°\")\nprint(f\"Angle between a=[1,0] and c=[0,1]: {angle_between(a, c):.1f}°\")\nprint(f\"Angle between a=[1,0] and d=[-1,0]: {angle_between(a, d):.1f}°\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: Understanding the Dot Product Formula\n\nThere are **two equivalent ways** to define the dot product:\n\n**Definition 1 - Algebraic (how we compute it):**\n$$\\mathbf{a} \\cdot \\mathbf{b} = a_1 b_1 + a_2 b_2 + \\ldots + a_n b_n$$\n\n**Definition 2 - Geometric (what it means):**\n$$\\mathbf{a} \\cdot \\mathbf{b} = |\\mathbf{a}| \\cdot |\\mathbf{b}| \\cdot \\cos(\\theta)$$\n\nThese are mathematically proven to be equal (using the Law of Cosines).\n\n#### Breaking down the geometric formula:\n\n| Component | Meaning | Range |\n|-----------|---------|-------|\n| $\\|\\mathbf{a}\\|$ | Length of vector a | 0 to ∞ |\n| $\\|\\mathbf{b}\\|$ | Length of vector b | 0 to ∞ |\n| $\\cos(\\theta)$ | \"Alignment factor\" based on angle | -1 to 1 |\n\n#### What does cos(θ) do?\n\n| Angle θ | cos(θ) | Vectors are... | Dot product |\n|---------|--------|----------------|-------------|\n| 0° | 1 | Same direction | Maximum positive |\n| 45° | 0.71 | Somewhat aligned | Positive |\n| 90° | 0 | Perpendicular | Zero |\n| 135° | -0.71 | Somewhat opposite | Negative |\n| 180° | -1 | Opposite directions | Maximum negative |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Interactive visualization: How dot product changes with angle\n# Keep vector 'a' fixed, rotate vector 'b' around\n\na = np.array([2, 0])  # Fixed vector pointing right\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left plot: Show vectors at different angles\nangles_deg = [0, 45, 90, 135, 180]\ncolors = ['green', 'blue', 'orange', 'red', 'purple']\n\naxes[0].quiver(0, 0, a[0], a[1], angles='xy', scale_units='xy', scale=1, \n               color='black', width=0.03, label='a (fixed)')\n\nfor angle, color in zip(angles_deg, colors):\n    theta = np.radians(angle)\n    b = 1.5 * np.array([np.cos(theta), np.sin(theta)])  # |b| = 1.5\n    dot = a @ b\n    axes[0].quiver(0, 0, b[0], b[1], angles='xy', scale_units='xy', scale=1,\n                   color=color, width=0.02, alpha=0.7, label=f'θ={angle}°, a·b={dot:.2f}')\n\naxes[0].set_xlim(-3, 3)\naxes[0].set_ylim(-2, 2)\naxes[0].set_aspect('equal')\naxes[0].axhline(y=0, color='k', linewidth=0.5)\naxes[0].axvline(x=0, color='k', linewidth=0.5)\naxes[0].legend(loc='upper left', fontsize=9)\naxes[0].set_title('Vector b at different angles from a')\naxes[0].grid(True, alpha=0.3)\n\n# Right plot: Dot product as function of angle\nangles = np.linspace(0, 360, 100)\ndot_products = []\nfor angle in angles:\n    theta = np.radians(angle)\n    b = 1.5 * np.array([np.cos(theta), np.sin(theta)])\n    dot_products.append(a @ b)\n\naxes[1].plot(angles, dot_products, 'b-', linewidth=2)\naxes[1].axhline(y=0, color='k', linewidth=1)\naxes[1].set_xlabel('Angle θ (degrees)')\naxes[1].set_ylabel('Dot product (a · b)')\naxes[1].set_title('Dot product vs angle between vectors\\n|a|=2, |b|=1.5, so max = 2×1.5 = 3')\naxes[1].set_xticks([0, 45, 90, 135, 180, 225, 270, 315, 360])\naxes[1].grid(True, alpha=0.3)\n\n# Mark key points\nfor angle, color in zip(angles_deg, colors):\n    theta = np.radians(angle)\n    b = 1.5 * np.array([np.cos(theta), np.sin(theta)])\n    dot = a @ b\n    axes[1].scatter([angle], [dot], color=color, s=100, zorder=5)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key insight: The dot product follows a cosine curve!\")\nprint(\"This is because a·b = |a||b|cos(θ), and we're varying θ.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### The Projection Interpretation\n\nAnother powerful way to understand dot product: **projection**.\n\nThe dot product $\\mathbf{a} \\cdot \\mathbf{b}$ tells you: *\"How much of b points in the direction of a?\"*\n\nMore precisely:\n$$\\mathbf{a} \\cdot \\mathbf{b} = |\\mathbf{a}| \\times (\\text{length of b's shadow onto a})$$\n\nThis \"shadow\" is called the **scalar projection** of b onto a.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualizing projection\na = np.array([3, 0])  # Horizontal vector\nb = np.array([2, 2])  # Vector at 45 degrees\n\n# Scalar projection of b onto a: (a·b) / |a|\nscalar_proj = (a @ b) / np.linalg.norm(a)\n\n# Vector projection of b onto a: scalar_proj * (a / |a|)\na_unit = a / np.linalg.norm(a)\nvector_proj = scalar_proj * a_unit\n\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Draw vectors\nax.quiver(0, 0, a[0], a[1], angles='xy', scale_units='xy', scale=1,\n          color='blue', width=0.02, label=f'a = {a}')\nax.quiver(0, 0, b[0], b[1], angles='xy', scale_units='xy', scale=1,\n          color='red', width=0.02, label=f'b = {b}')\n\n# Draw projection\nax.quiver(0, 0, vector_proj[0], vector_proj[1], angles='xy', scale_units='xy', scale=1,\n          color='green', width=0.025, label=f'projection of b onto a')\n\n# Draw dashed line from b to its projection (perpendicular)\nax.plot([b[0], vector_proj[0]], [b[1], vector_proj[1]], 'k--', linewidth=1.5, alpha=0.5)\n\n# Annotations\nax.annotate('', xy=(vector_proj[0], -0.3), xytext=(0, -0.3),\n            arrowprops=dict(arrowstyle='<->', color='green'))\nax.text(vector_proj[0]/2, -0.6, f'scalar proj = {scalar_proj:.2f}', ha='center', fontsize=11, color='green')\n\nax.set_xlim(-1, 4)\nax.set_ylim(-1, 3)\nax.set_aspect('equal')\nax.axhline(y=0, color='k', linewidth=0.5)\nax.axvline(x=0, color='k', linewidth=0.5)\nax.legend(loc='upper left')\nax.set_title('Projection: \"How much of b points in the direction of a?\"')\nax.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"a · b = {a @ b}\")\nprint(f\"|a| = {np.linalg.norm(a)}\")\nprint(f\"Scalar projection of b onto a = (a·b)/|a| = {scalar_proj:.2f}\")\nprint(f\"\\nCheck: |a| × scalar_proj = {np.linalg.norm(a)} × {scalar_proj:.2f} = {np.linalg.norm(a) * scalar_proj:.2f} = a·b ✓\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Why Dot Product Matters in Machine Learning\n\nThe dot product appears everywhere in ML because it answers: **\"How similar are these two vectors?\"**\n\n| ML Application | What the dot product computes |\n|----------------|-------------------------------|\n| **Neural network layer** | `w · x + b` = \"How much does input x match what neuron w is looking for?\" |\n| **Word embeddings** | `word1 · word2` = \"How semantically similar are these words?\" |\n| **Attention (Transformers)** | `query · key` = \"How relevant is this key to this query?\" |\n| **Recommendation systems** | `user · item` = \"How much would this user like this item?\" |\n| **Cosine similarity** | `(a · b) / (|a| |b|)` = Pure directional similarity (-1 to 1) |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Vector Norm (Magnitude/Length)\n",
    "\n",
    "The **L2 norm** (Euclidean length) of a vector:\n",
    "\n",
    "$$||\\mathbf{v}||_2 = \\sqrt{\\sum_{i=1}^{n} v_i^2}$$\n",
    "\n",
    "Other norms used in ML:\n",
    "- **L1 norm**: $||\\mathbf{v}||_1 = \\sum |v_i|$ (Manhattan distance, used for sparsity)\n",
    "- **L∞ norm**: $||\\mathbf{v}||_\\infty = \\max |v_i|$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: What is a Vector Norm?\n\nA **norm** measures the \"size\" or \"length\" of a vector. Think of it as answering: *\"How far is this point from the origin?\"*\n\n#### The L2 (Euclidean) Norm - Most Common\n\n$$||\\mathbf{v}||_2 = \\sqrt{v_1^2 + v_2^2 + \\ldots + v_n^2}$$\n\nThis is just the **Pythagorean theorem** extended to n dimensions!\n\nFor `v = [3, 4]`: $||v|| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5$\n\n(This is the classic 3-4-5 right triangle)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualizing the L2 norm as distance from origin (Pythagorean theorem)\nv = np.array([3, 4])\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\n# Draw the vector\nax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1,\n          color='blue', width=0.02, label=f'v = {v}, ||v|| = {np.linalg.norm(v)}')\n\n# Draw the right triangle\nax.plot([0, v[0]], [0, 0], 'g-', linewidth=2, label=f'horizontal = {v[0]}')\nax.plot([v[0], v[0]], [0, v[1]], 'r-', linewidth=2, label=f'vertical = {v[1]}')\n\n# Right angle marker\nax.plot([v[0]-0.2, v[0]-0.2, v[0]], [0, 0.2, 0.2], 'k-', linewidth=1)\n\n# Labels\nax.text(v[0]/2, -0.4, '3', ha='center', fontsize=14, color='green')\nax.text(v[0]+0.3, v[1]/2, '4', ha='center', fontsize=14, color='red')\nax.text(v[0]/2 - 0.5, v[1]/2 + 0.3, '5', ha='center', fontsize=14, color='blue')\n\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 6)\nax.set_aspect('equal')\nax.axhline(y=0, color='k', linewidth=0.5)\nax.axvline(x=0, color='k', linewidth=0.5)\nax.legend(loc='upper left')\nax.set_title('L2 Norm = Pythagorean Theorem\\n||v|| = √(3² + 4²) = √25 = 5')\nax.grid(True, alpha=0.3)\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Comparing Different Norms\n\nDifferent norms measure \"size\" differently:\n\n| Norm | Formula | Intuition | Use in ML |\n|------|---------|-----------|-----------|\n| **L2** | $\\sqrt{\\sum v_i^2}$ | Straight-line distance | Default distance, weight decay |\n| **L1** | $\\sum \\|v_i\\|$ | \"Taxicab\" distance (walk on grid) | Sparsity (Lasso), makes weights exactly 0 |\n| **L∞** | $\\max \\|v_i\\|$ | Largest single component | Worst-case bounds |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualize \"unit balls\" - all points where ||v|| = 1 for different norms\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\ntheta = np.linspace(0, 2*np.pi, 100)\n\n# L2 norm: circle (x² + y² = 1)\nx_l2 = np.cos(theta)\ny_l2 = np.sin(theta)\naxes[0].plot(x_l2, y_l2, 'b-', linewidth=2)\naxes[0].fill(x_l2, y_l2, alpha=0.2)\naxes[0].set_title('L2 Norm (Euclidean)\\n||v||₂ = √(x² + y²) = 1\\nCircle')\naxes[0].set_xlabel('x')\naxes[0].set_ylabel('y')\n\n# L1 norm: diamond (|x| + |y| = 1)\nx_l1 = [1, 0, -1, 0, 1]\ny_l1 = [0, 1, 0, -1, 0]\naxes[1].plot(x_l1, y_l1, 'r-', linewidth=2)\naxes[1].fill(x_l1, y_l1, alpha=0.2, color='red')\naxes[1].set_title('L1 Norm (Manhattan)\\n||v||₁ = |x| + |y| = 1\\nDiamond')\naxes[1].set_xlabel('x')\naxes[1].set_ylabel('y')\n\n# L∞ norm: square (max(|x|, |y|) = 1)\nx_linf = [1, 1, -1, -1, 1]\ny_linf = [1, -1, -1, 1, 1]\naxes[2].plot(x_linf, y_linf, 'g-', linewidth=2)\naxes[2].fill(x_linf, y_linf, alpha=0.2, color='green')\naxes[2].set_title('L∞ Norm (Max)\\n||v||∞ = max(|x|, |y|) = 1\\nSquare')\naxes[2].set_xlabel('x')\naxes[2].set_ylabel('y')\n\nfor ax in axes:\n    ax.set_xlim(-1.5, 1.5)\n    ax.set_ylim(-1.5, 1.5)\n    ax.set_aspect('equal')\n    ax.axhline(y=0, color='k', linewidth=0.5)\n    ax.axvline(x=0, color='k', linewidth=0.5)\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Example with a specific vector\nv = np.array([3, 4])\nprint(f\"For v = {v}:\")\nprint(f\"  L2 norm: ||v||₂ = √(3² + 4²) = {np.linalg.norm(v, ord=2)}\")\nprint(f\"  L1 norm: ||v||₁ = |3| + |4| = {np.linalg.norm(v, ord=1)}\")\nprint(f\"  L∞ norm: ||v||∞ = max(|3|, |4|) = {np.linalg.norm(v, ord=np.inf)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Why Norms Matter in Machine Learning\n\n| Use Case | How Norms are Used |\n|----------|-------------------|\n| **Normalization** | Divide by norm to get unit vector: `v / ||v||`. Isolates direction from magnitude. |\n| **Regularization** | Add `λ||weights||²` to loss. Keeps weights small → prevents overfitting. |\n| **Distance** | Distance between points: `||a - b||`. Used in k-NN, clustering. |\n| **Gradient clipping** | If `||gradient|| > threshold`, scale it down. Prevents exploding gradients. |\n| **Embedding similarity** | Normalize embeddings so dot product = cosine similarity. |\n\n#### Connecting Dot Product and Norm\n\nThe dot product of a vector with itself gives the **squared norm**:\n\n$$\\mathbf{v} \\cdot \\mathbf{v} = v_1^2 + v_2^2 + \\ldots = ||\\mathbf{v}||^2$$\n\nSo: $||\\mathbf{v}|| = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}}$",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([3, 4])\n",
    "\n",
    "# L2 norm (default)\n",
    "l2_norm = np.linalg.norm(v)\n",
    "print(f\"L2 norm of {v}: {l2_norm}\")  # Should be 5 (3-4-5 triangle)\n",
    "\n",
    "# L1 norm\n",
    "l1_norm = np.linalg.norm(v, ord=1)\n",
    "print(f\"L1 norm of {v}: {l1_norm}\")  # 3 + 4 = 7\n",
    "\n",
    "# Unit vector (normalize)\n",
    "v_unit = v / np.linalg.norm(v)\n",
    "print(f\"Unit vector: {v_unit}\")\n",
    "print(f\"Magnitude of unit vector: {np.linalg.norm(v_unit)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Matrices\n",
    "\n",
    "A **matrix** is a 2D array of numbers. In deep learning:\n",
    "- Weight matrices connect layers\n",
    "- Batches of data are matrices (rows = samples, columns = features)\n",
    "- Attention scores form matrices\n",
    "\n",
    "### Matrix as Linear Transformation\n",
    "\n",
    "A matrix transforms vectors from one space to another. When you multiply a matrix by a vector, you're applying a linear transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating matrices\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "print(f\"Matrix A:\\n{A}\")\n",
    "print(f\"Shape: {A.shape}\")\n",
    "print(f\"Number of rows: {A.shape[0]}\")\n",
    "print(f\"Number of columns: {A.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-Vector Multiplication\n",
    "\n",
    "Matrix $\\mathbf{A}$ (m×n) times vector $\\mathbf{v}$ (n×1) produces vector (m×1):\n",
    "\n",
    "$$\\mathbf{Av} = \\begin{bmatrix} \\mathbf{a}_1 \\cdot \\mathbf{v} \\\\ \\mathbf{a}_2 \\cdot \\mathbf{v} \\\\ \\vdots \\\\ \\mathbf{a}_m \\cdot \\mathbf{v} \\end{bmatrix}$$\n",
    "\n",
    "Each element is a dot product of a row of A with vector v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[2, 0],\n",
    "              [0, 1]])\n",
    "v = np.array([1, 1])\n",
    "\n",
    "# Matrix-vector multiplication\n",
    "result = A @ v  # or np.dot(A, v)\n",
    "print(f\"A @ v = {result}\")\n",
    "\n",
    "# This stretches the x-component by 2, keeps y the same\n",
    "plot_vectors([v, result], ['blue', 'red'], ['original v', 'Av (transformed)'])\n",
    "plt.title('Matrix as Transformation (Scaling)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotation matrix (90 degrees counter-clockwise)\n",
    "theta = np.pi / 2  # 90 degrees\n",
    "R = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "              [np.sin(theta),  np.cos(theta)]])\n",
    "\n",
    "v = np.array([1, 0])\n",
    "rotated = R @ v\n",
    "\n",
    "print(f\"Rotation matrix R:\\n{R.round(3)}\")\n",
    "print(f\"Original: {v}\")\n",
    "print(f\"Rotated: {rotated.round(3)}\")\n",
    "\n",
    "plot_vectors([v, rotated], ['blue', 'red'], ['original', 'rotated 90°'])\n",
    "plt.title('Rotation Transformation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Linear Transformations\n",
    "\n",
    "Let's see how different matrices transform a grid of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transformation(A, title):\n",
    "    \"\"\"Visualize how matrix A transforms a unit square.\"\"\"\n",
    "    # Create a grid of points\n",
    "    n = 10\n",
    "    x = np.linspace(-1, 1, n)\n",
    "    y = np.linspace(-1, 1, n)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Original grid\n",
    "    for xi in x:\n",
    "        axes[0].plot([xi, xi], [-1, 1], 'b-', alpha=0.5)\n",
    "    for yi in y:\n",
    "        axes[0].plot([-1, 1], [yi, yi], 'b-', alpha=0.5)\n",
    "    # Highlight basis vectors\n",
    "    axes[0].quiver(0, 0, 1, 0, angles='xy', scale_units='xy', scale=1, color='red', width=0.02)\n",
    "    axes[0].quiver(0, 0, 0, 1, angles='xy', scale_units='xy', scale=1, color='green', width=0.02)\n",
    "    axes[0].set_xlim(-2, 2)\n",
    "    axes[0].set_ylim(-2, 2)\n",
    "    axes[0].set_aspect('equal')\n",
    "    axes[0].set_title('Original Space')\n",
    "    axes[0].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[0].axvline(x=0, color='k', linewidth=0.5)\n",
    "    \n",
    "    # Transformed grid\n",
    "    for xi in x:\n",
    "        points = np.array([[xi, yi] for yi in y])\n",
    "        transformed = (A @ points.T).T\n",
    "        axes[1].plot(transformed[:, 0], transformed[:, 1], 'b-', alpha=0.5)\n",
    "    for yi in y:\n",
    "        points = np.array([[xi, yi] for xi in x])\n",
    "        transformed = (A @ points.T).T\n",
    "        axes[1].plot(transformed[:, 0], transformed[:, 1], 'b-', alpha=0.5)\n",
    "    \n",
    "    # Transformed basis vectors\n",
    "    e1_transformed = A @ np.array([1, 0])\n",
    "    e2_transformed = A @ np.array([0, 1])\n",
    "    axes[1].quiver(0, 0, e1_transformed[0], e1_transformed[1], angles='xy', scale_units='xy', scale=1, color='red', width=0.02)\n",
    "    axes[1].quiver(0, 0, e2_transformed[0], e2_transformed[1], angles='xy', scale_units='xy', scale=1, color='green', width=0.02)\n",
    "    \n",
    "    axes[1].set_xlim(-2, 2)\n",
    "    axes[1].set_ylim(-2, 2)\n",
    "    axes[1].set_aspect('equal')\n",
    "    axes[1].set_title(f'After Transformation: {title}')\n",
    "    axes[1].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[1].axvline(x=0, color='k', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Matrix:\\n{A}\")\n",
    "    print(f\"Red basis vector [1,0] -> {e1_transformed}\")\n",
    "    print(f\"Green basis vector [0,1] -> {e2_transformed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "A_scale = np.array([[1.5, 0],\n",
    "                    [0, 0.5]])\n",
    "plot_transformation(A_scale, \"Scaling (1.5x, 0.5y)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotation\n",
    "theta = np.pi / 6  # 30 degrees\n",
    "A_rotate = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                     [np.sin(theta),  np.cos(theta)]])\n",
    "plot_transformation(A_rotate, \"Rotation (30°)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shear\n",
    "A_shear = np.array([[1, 0.5],\n",
    "                    [0, 1]])\n",
    "plot_transformation(A_shear, \"Shear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: Understanding Matrices as Transformations\n\n**Key Insight**: A matrix doesn't just \"do math\" - it describes a geometric transformation. Every matrix is a machine that takes vectors in and outputs transformed vectors.\n\n#### What Do the Columns of a Matrix Mean?\n\nHere's the most important insight about matrices:\n\n> **The columns of a matrix tell you where the basis vectors land after transformation.**\n\nFor a 2D matrix $\\mathbf{A} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$:\n- **Column 1** $\\begin{bmatrix} a \\\\ c \\end{bmatrix}$ = where the vector $\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ (pointing right) lands\n- **Column 2** $\\begin{bmatrix} b \\\\ d \\end{bmatrix}$ = where the vector $\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ (pointing up) lands\n\nThis means: **to design a transformation, just decide where you want the basis vectors to go!**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Demonstration: Columns of a matrix = where basis vectors land\n# Let's verify this with an example\n\nA = np.array([[2, -1],\n              [1,  1]])\n\n# Standard basis vectors\ne1 = np.array([1, 0])  # Points right\ne2 = np.array([0, 1])  # Points up\n\n# Transform them\nAe1 = A @ e1\nAe2 = A @ e2\n\nprint(\"Matrix A:\")\nprint(A)\nprint(f\"\\nColumn 1 of A: {A[:, 0]}\")\nprint(f\"A @ [1,0] = {Ae1}\")\nprint(f\"Same? {np.allclose(A[:, 0], Ae1)}\")\n\nprint(f\"\\nColumn 2 of A: {A[:, 1]}\")\nprint(f\"A @ [0,1] = {Ae2}\")\nprint(f\"Same? {np.allclose(A[:, 1], Ae2)}\")\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Before transformation\naxes[0].quiver(0, 0, 1, 0, angles='xy', scale_units='xy', scale=1, color='red', width=0.02, label='e1 = [1,0]')\naxes[0].quiver(0, 0, 0, 1, angles='xy', scale_units='xy', scale=1, color='green', width=0.02, label='e2 = [0,1]')\naxes[0].set_xlim(-2, 3)\naxes[0].set_ylim(-2, 3)\naxes[0].set_aspect('equal')\naxes[0].axhline(y=0, color='k', linewidth=0.5)\naxes[0].axvline(x=0, color='k', linewidth=0.5)\naxes[0].grid(True, alpha=0.3)\naxes[0].legend()\naxes[0].set_title('BEFORE: Standard Basis Vectors')\n\n# After transformation\naxes[1].quiver(0, 0, Ae1[0], Ae1[1], angles='xy', scale_units='xy', scale=1, color='red', width=0.02, \n               label=f'A @ e1 = {Ae1} (Column 1)')\naxes[1].quiver(0, 0, Ae2[0], Ae2[1], angles='xy', scale_units='xy', scale=1, color='green', width=0.02, \n               label=f'A @ e2 = {Ae2} (Column 2)')\naxes[1].set_xlim(-2, 3)\naxes[1].set_ylim(-2, 3)\naxes[1].set_aspect('equal')\naxes[1].axhline(y=0, color='k', linewidth=0.5)\naxes[1].axvline(x=0, color='k', linewidth=0.5)\naxes[1].grid(True, alpha=0.3)\naxes[1].legend()\naxes[1].set_title('AFTER: Basis Vectors = Columns of A')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey insight: Reading the columns of A directly tells you the transformation!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Common 2D Transformation Matrices\n\nOnce you understand \"columns = where basis vectors go,\" you can read or construct any transformation:\n\n| Transformation | Matrix | Column 1 (where [1,0] goes) | Column 2 (where [0,1] goes) |\n|----------------|--------|----------------------------|----------------------------|\n| **Identity** (do nothing) | $\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$ | [1, 0] stays at [1, 0] | [0, 1] stays at [0, 1] |\n| **Scale by k** | $\\begin{bmatrix} k & 0 \\\\ 0 & k \\end{bmatrix}$ | [1, 0] -> [k, 0] | [0, 1] -> [0, k] |\n| **Scale x by a, y by b** | $\\begin{bmatrix} a & 0 \\\\ 0 & b \\end{bmatrix}$ | [1, 0] -> [a, 0] | [0, 1] -> [0, b] |\n| **Rotate by θ** | $\\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix}$ | [1, 0] rotates to [cos θ, sin θ] | [0, 1] rotates to [-sin θ, cos θ] |\n| **Reflect across x-axis** | $\\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}$ | [1, 0] stays | [0, 1] -> [0, -1] |\n| **Reflect across y-axis** | $\\begin{bmatrix} -1 & 0 \\\\ 0 & 1 \\end{bmatrix}$ | [1, 0] -> [-1, 0] | [0, 1] stays |\n| **Reflect across y=x** | $\\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$ | [1, 0] -> [0, 1] | [0, 1] -> [1, 0] |\n| **Shear (horizontal)** | $\\begin{bmatrix} 1 & k \\\\ 0 & 1 \\end{bmatrix}$ | [1, 0] stays | [0, 1] -> [k, 1] |\n| **Shear (vertical)** | $\\begin{bmatrix} 1 & 0 \\\\ k & 1 \\end{bmatrix}$ | [1, 0] -> [1, k] | [0, 1] stays |\n| **Project onto x-axis** | $\\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix}$ | [1, 0] stays | [0, 1] -> [0, 0] (collapsed!) |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Why Matrix Multiplication is Composition of Transformations\n\nWhen you multiply matrices $\\mathbf{AB}$, you're creating a new transformation that does **B first, then A**.\n\n**Think of it this way:**\n- To apply $\\mathbf{AB}$ to vector $\\mathbf{v}$: $(\\mathbf{AB})\\mathbf{v} = \\mathbf{A}(\\mathbf{B}\\mathbf{v})$\n- First B transforms v, then A transforms the result\n\n**Why the \"backwards\" order?**\n\nBecause we read left-to-right but function application is right-to-left: $f(g(x))$ applies g first, then f.\n\nThis is exactly like composing functions: if `rotate()` and `scale()` are functions, then `rotate(scale(v))` scales first, rotates second.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Demonstration: Matrix multiplication = composition of transformations\n# Let's compose rotation (45 degrees) followed by scaling (2x in x, 0.5x in y)\n\n# Define individual transformations\ntheta = np.pi / 4  # 45 degrees\nRotate = np.array([[np.cos(theta), -np.sin(theta)],\n                   [np.sin(theta),  np.cos(theta)]])\n\nScale = np.array([[2.0, 0],\n                  [0, 0.5]])\n\n# Compose: Scale first, then Rotate (remember: right-to-left!)\n# So we write: Rotate @ Scale\nComposed = Rotate @ Scale\n\nprint(\"Rotation matrix (45 degrees):\")\nprint(Rotate.round(3))\nprint(\"\\nScaling matrix (2x, 0.5y):\")\nprint(Scale)\nprint(\"\\nComposed (Rotate @ Scale) - scales first, then rotates:\")\nprint(Composed.round(3))\n\n# Visualize the three transformations\nfig, axes = plt.subplots(1, 4, figsize=(20, 5))\n\n# Test vector\nv = np.array([1, 1])\n\n# Original\naxes[0].quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='blue', width=0.02)\naxes[0].set_title('Original vector [1, 1]')\n\n# After Scale only\nv_scaled = Scale @ v\naxes[1].quiver(0, 0, v_scaled[0], v_scaled[1], angles='xy', scale_units='xy', scale=1, color='green', width=0.02)\naxes[1].set_title(f'After Scale: {v_scaled}')\n\n# After Scale then Rotate (two steps)\nv_scaled_rotated = Rotate @ v_scaled\naxes[2].quiver(0, 0, v_scaled_rotated[0], v_scaled_rotated[1], angles='xy', scale_units='xy', scale=1, color='red', width=0.02)\naxes[2].set_title(f'After Scale then Rotate:\\n{v_scaled_rotated.round(3)}')\n\n# Using composed matrix (single step)\nv_composed = Composed @ v\naxes[3].quiver(0, 0, v_composed[0], v_composed[1], angles='xy', scale_units='xy', scale=1, color='purple', width=0.02)\naxes[3].set_title(f'Using Composed matrix:\\n{v_composed.round(3)}')\n\nfor ax in axes:\n    ax.set_xlim(-3, 3)\n    ax.set_ylim(-2, 2)\n    ax.set_aspect('equal')\n    ax.axhline(y=0, color='k', linewidth=0.5)\n    ax.axvline(x=0, color='k', linewidth=0.5)\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTwo-step result: {v_scaled_rotated.round(6)}\")\nprint(f\"Composed result: {v_composed.round(6)}\")\nprint(f\"Same? {np.allclose(v_scaled_rotated, v_composed)}\")\nprint(\"\\nKey insight: (Rotate @ Scale) @ v = Rotate @ (Scale @ v)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-Matrix Multiplication\n",
    "\n",
    "If $\\mathbf{A}$ is (m×n) and $\\mathbf{B}$ is (n×p), then $\\mathbf{AB}$ is (m×p).\n",
    "\n",
    "**Key insight**: Matrix multiplication = composition of transformations.\n",
    "\n",
    "If A rotates and B scales, then AB does both (B first, then A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "B = np.array([[5, 6],\n",
    "              [7, 8]])\n",
    "\n",
    "C = A @ B\n",
    "print(f\"A:\\n{A}\\n\")\n",
    "print(f\"B:\\n{B}\\n\")\n",
    "print(f\"A @ B:\\n{C}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Implement matrix multiplication from scratch\n",
    "def matmul(A, B):\n",
    "    \"\"\"\n",
    "    Multiply matrices A and B.\n",
    "    A: (m, n) matrix\n",
    "    B: (n, p) matrix\n",
    "    Returns: (m, p) matrix\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    n2, p = B.shape\n",
    "    assert n == n2, f\"Incompatible dimensions: {A.shape} and {B.shape}\"\n",
    "    \n",
    "    # TODO: Implement this!\n",
    "    # Hint: C[i,j] = sum over k of A[i,k] * B[k,j]\n",
    "    C = np.zeros((m, p))\n",
    "    \n",
    "    # Your code here\n",
    "    for i in range(m):\n",
    "        for j in range(p):\n",
    "            for k in range(n):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "    \n",
    "    return C\n",
    "\n",
    "# Test your implementation\n",
    "result = matmul(A, B)\n",
    "expected = A @ B\n",
    "print(f\"Your result:\\n{result}\")\n",
    "print(f\"Expected:\\n{expected}\")\n",
    "print(f\"Correct: {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Properties\n",
    "\n",
    "#### Transpose\n",
    "Swap rows and columns: $(\\mathbf{A}^T)_{ij} = \\mathbf{A}_{ji}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "print(f\"A (2x3):\\n{A}\\n\")\n",
    "print(f\"A^T (3x2):\\n{A.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identity Matrix\n",
    "The \"do nothing\" transformation. $\\mathbf{IA} = \\mathbf{AI} = \\mathbf{A}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.eye(3)  # 3x3 identity matrix\n",
    "print(f\"Identity matrix:\\n{I}\")\n",
    "\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "print(f\"\\nA @ I = A: {np.allclose(A @ I, A)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Inverse\n",
    "\n",
    "The inverse $\\mathbf{A}^{-1}$ \"undoes\" the transformation: $\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}$\n",
    "\n",
    "Not all matrices have inverses (singular matrices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[4, 7],\n",
    "              [2, 6]])\n",
    "\n",
    "A_inv = np.linalg.inv(A)\n",
    "print(f\"A:\\n{A}\\n\")\n",
    "print(f\"A^(-1):\\n{A_inv}\\n\")\n",
    "print(f\"A @ A^(-1):\\n{(A @ A_inv).round(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A singular matrix (no inverse)\n",
    "singular = np.array([[1, 2],\n",
    "                     [2, 4]])  # Row 2 = 2 * Row 1\n",
    "\n",
    "print(f\"Determinant: {np.linalg.det(singular)}\")\n",
    "# np.linalg.inv(singular)  # This would raise an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Tensors\n",
    "\n",
    "**Tensors** are generalizations to higher dimensions:\n",
    "- Scalar: 0D tensor\n",
    "- Vector: 1D tensor\n",
    "- Matrix: 2D tensor\n",
    "- 3D tensor: e.g., RGB image (height × width × channels)\n",
    "- 4D tensor: batch of images (batch × height × width × channels)\n",
    "\n",
    "In deep learning, we constantly work with tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensors in NumPy\n",
    "scalar = np.array(5)          # 0D\n",
    "vector = np.array([1, 2, 3])  # 1D\n",
    "matrix = np.array([[1, 2], [3, 4]])  # 2D\n",
    "tensor_3d = np.random.rand(3, 4, 5)  # 3D\n",
    "tensor_4d = np.random.rand(32, 28, 28, 3)  # Batch of 32 color images\n",
    "\n",
    "print(f\"Scalar shape: {scalar.shape}, ndim: {scalar.ndim}\")\n",
    "print(f\"Vector shape: {vector.shape}, ndim: {vector.ndim}\")\n",
    "print(f\"Matrix shape: {matrix.shape}, ndim: {matrix.ndim}\")\n",
    "print(f\"3D tensor shape: {tensor_3d.shape}, ndim: {tensor_3d.ndim}\")\n",
    "print(f\"4D tensor shape: {tensor_4d.shape}, ndim: {tensor_4d.ndim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "\n",
    "NumPy's broadcasting allows operations on arrays of different shapes. This is crucial for efficient ML code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting examples\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "# Scalar broadcast\n",
    "print(f\"A + 10:\\n{A + 10}\\n\")\n",
    "\n",
    "# Row vector broadcast (add to each row)\n",
    "row = np.array([10, 20, 30])\n",
    "print(f\"A + [10, 20, 30]:\\n{A + row}\\n\")\n",
    "\n",
    "# Column vector broadcast (add to each column)\n",
    "col = np.array([[100], [200]])\n",
    "print(f\"A + [[100], [200]]:\\n{A + col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Eigenvalues and Eigenvectors\n",
    "\n",
    "For a square matrix $\\mathbf{A}$, an **eigenvector** $\\mathbf{v}$ and **eigenvalue** $\\lambda$ satisfy:\n",
    "\n",
    "$$\\mathbf{Av} = \\lambda\\mathbf{v}$$\n",
    "\n",
    "**Meaning**: When you apply transformation A to eigenvector v, it only scales (by λ), doesn't change direction.\n",
    "\n",
    "**Applications in ML**:\n",
    "- PCA (Principal Component Analysis)\n",
    "- Understanding neural network dynamics\n",
    "- Spectral clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example\n",
    "A = np.array([[3, 1],\n",
    "              [0, 2]])\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "print(f\"Matrix A:\\n{A}\\n\")\n",
    "print(f\"Eigenvalues: {eigenvalues}\")\n",
    "print(f\"Eigenvectors (as columns):\\n{eigenvectors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: Av = λv\n",
    "for i in range(len(eigenvalues)):\n",
    "    λ = eigenvalues[i]\n",
    "    v = eigenvectors[:, i]  # Column i is eigenvector i\n",
    "    \n",
    "    Av = A @ v\n",
    "    λv = λ * v\n",
    "    \n",
    "    print(f\"\\nEigenvector {i+1}: {v}\")\n",
    "    print(f\"Eigenvalue: {λ}\")\n",
    "    print(f\"A @ v = {Av}\")\n",
    "    print(f\"λ * v = {λv}\")\n",
    "    print(f\"Equal: {np.allclose(Av, λv)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize eigenvectors: they don't change direction under transformation\n",
    "A = np.array([[2, 1],\n",
    "              [1, 2]])\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot many vectors and their transformations\n",
    "for theta in np.linspace(0, 2*np.pi, 16, endpoint=False):\n",
    "    v = np.array([np.cos(theta), np.sin(theta)])\n",
    "    Av = A @ v\n",
    "    ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, \n",
    "              color='blue', alpha=0.3, width=0.01)\n",
    "    ax.quiver(0, 0, Av[0], Av[1], angles='xy', scale_units='xy', scale=1, \n",
    "              color='red', alpha=0.3, width=0.01)\n",
    "\n",
    "# Highlight eigenvectors\n",
    "for i in range(2):\n",
    "    v = eigenvectors[:, i]\n",
    "    Av = A @ v\n",
    "    ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, \n",
    "              color='blue', width=0.02, label=f'eigenvector {i+1}' if i == 0 else '')\n",
    "    ax.quiver(0, 0, Av[0], Av[1], angles='xy', scale_units='xy', scale=1, \n",
    "              color='red', width=0.02, label=f'transformed' if i == 0 else '')\n",
    "\n",
    "ax.set_xlim(-4, 4)\n",
    "ax.set_ylim(-4, 4)\n",
    "ax.set_aspect('equal')\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_title('Blue: Original, Red: Transformed\\nEigenvectors only scale, not rotate')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Eigenvalues: {eigenvalues}\")\n",
    "print(\"Notice: eigenvectors (thick lines) stay on the same line after transformation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: The Intuition Behind Eigenvectors\n\n**The Big Picture**: Eigenvectors are the \"special directions\" of a transformation - directions that only get stretched or shrunk, never rotated.\n\n#### Why is this important?\n\nThink of a transformation as \"doing something\" to space. Most directions get both stretched AND rotated. But eigenvectors reveal the **natural axes** of that transformation - the directions where the action is simplest.\n\n> **Eigenvector intuition**: \"I'm a direction that this matrix only scales, never rotates. Apply the matrix to me, and I just get longer or shorter.\"\n\n#### Breaking Down the Equation\n\n$$\\mathbf{Av} = \\lambda\\mathbf{v}$$\n\n| Component | Meaning |\n|-----------|---------|\n| $\\mathbf{A}$ | The transformation matrix |\n| $\\mathbf{v}$ | An eigenvector (special direction) |\n| $\\lambda$ | The eigenvalue (how much v gets scaled) |\n| $\\mathbf{Av}$ | The result of transforming v |\n| $\\lambda\\mathbf{v}$ | Same direction as v, just scaled by lambda |\n\n#### What the Eigenvalue Tells You\n\n| Eigenvalue lambda | Geometric meaning |\n|-------------------|-------------------|\n| lambda > 1 | Eigenvector gets stretched |\n| 0 < lambda < 1 | Eigenvector gets shrunk |\n| lambda = 1 | Eigenvector unchanged (fixed direction) |\n| lambda = 0 | Eigenvector collapses to zero (null space) |\n| lambda < 0 | Eigenvector flips direction and scales |\n| Complex lambda | Rotation is involved (no purely scaled directions) |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Why Eigenvectors Matter in Machine Learning\n\n| Application | How Eigenvectors are Used |\n|------------|---------------------------|\n| **PCA** | Eigenvectors of covariance matrix = directions of maximum variance. The top eigenvectors are the \"principal components.\" |\n| **Spectral Clustering** | Eigenvectors of graph Laplacian reveal cluster structure. Points are embedded using eigenvectors, then clustered. |\n| **PageRank** | The dominant eigenvector of the link matrix gives page importance scores. |\n| **Neural Network Dynamics** | Eigenvalues of weight matrices affect gradient flow. Values > 1 cause exploding gradients, < 1 cause vanishing. |\n| **Covariance Analysis** | Eigenvectors show directions of correlation in data. Eigenvalues show how much variance in each direction. |\n| **Matrix Powers** | If you know eigenvectors, computing $A^n$ is easy: just raise eigenvalues to power n. Useful for Markov chains. |\n\n#### The PCA Connection\n\n**PCA finds eigenvectors of the covariance matrix.**\n\nWhy? The covariance matrix $\\mathbf{C}$ tells you how features vary together. Its eigenvectors point in directions where data varies most (or least), and eigenvalues tell you how much variance is in each direction.\n\nThe eigenvector with the **largest eigenvalue** = direction of **maximum variance** = first principal component.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Singular Value Decomposition (SVD)\n",
    "\n",
    "SVD decomposes ANY matrix (not just square) into:\n",
    "\n",
    "$$\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{U}$: Left singular vectors (orthonormal)\n",
    "- $\\mathbf{\\Sigma}$: Diagonal matrix of singular values (non-negative, sorted descending)\n",
    "- $\\mathbf{V}^T$: Right singular vectors (orthonormal)\n",
    "\n",
    "**Applications in ML**:\n",
    "- Dimensionality reduction (PCA uses SVD)\n",
    "- Image compression\n",
    "- Recommender systems\n",
    "- Latent semantic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD example\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9],\n",
    "              [10, 11, 12]])\n",
    "\n",
    "U, s, Vt = np.linalg.svd(A)\n",
    "\n",
    "print(f\"Original A shape: {A.shape}\")\n",
    "print(f\"U shape: {U.shape}\")\n",
    "print(f\"Singular values: {s}\")\n",
    "print(f\"V^T shape: {Vt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct A from SVD\n",
    "# Need to create the full Sigma matrix\n",
    "Sigma = np.zeros((U.shape[0], Vt.shape[0]))\n",
    "np.fill_diagonal(Sigma, s)\n",
    "\n",
    "A_reconstructed = U @ Sigma @ Vt\n",
    "print(f\"Original A:\\n{A}\\n\")\n",
    "print(f\"Reconstructed:\\n{A_reconstructed.round(10)}\\n\")\n",
    "print(f\"Reconstruction accurate: {np.allclose(A, A_reconstructed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-Rank Approximation\n",
    "\n",
    "By keeping only the top k singular values, we get the best rank-k approximation of A.\n",
    "\n",
    "This is the foundation of dimensionality reduction!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: Understanding SVD Geometrically\n\nSVD reveals the hidden structure of any matrix. Think of it as answering: *\"What are the fundamental building blocks of this transformation?\"*\n\n$$\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T$$\n\n#### What Each Component Represents\n\n| Component | Shape | What it represents | Geometric meaning |\n|-----------|-------|-------------------|-------------------|\n| $\\mathbf{V}^T$ | (n x n) | Input rotation | Rotate input to align with matrix's \"natural\" axes |\n| $\\mathbf{\\Sigma}$ | (m x n) | Scaling | Scale along each axis (singular values on diagonal) |\n| $\\mathbf{U}$ | (m x m) | Output rotation | Rotate to final output orientation |\n\n**The key insight**: ANY matrix transformation can be decomposed into: **rotate -> scale -> rotate**.\n\n#### Why Singular Values are Sorted by Importance\n\nThe singular values in $\\Sigma$ are always sorted: $\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r \\geq 0$\n\n**Why sorted?** Because they represent how much the matrix \"stretches\" space in each direction:\n- $\\sigma_1$ = maximum stretch factor (most important direction)\n- $\\sigma_2$ = second most stretch (second most important)\n- Small $\\sigma_i$ = nearly no stretch = \"noise\" or \"unimportant\"\n\nThis ordering is why keeping only the top-k singular values gives the **best** rank-k approximation!\n\n#### The Connection to PCA\n\nPCA and SVD are deeply connected:\n\n| If you have... | PCA finds... | Which equals... |\n|----------------|--------------|-----------------|\n| Data matrix $\\mathbf{X}$ (centered) | Eigenvectors of $\\mathbf{X}^T\\mathbf{X}$ | Right singular vectors $\\mathbf{V}$ from SVD of $\\mathbf{X}$ |\n| Principal components | $\\mathbf{X} \\cdot \\text{eigenvectors}$ | $\\mathbf{U} \\cdot \\Sigma$ from SVD |\n| Variance explained | Eigenvalues / total | $\\sigma_i^2 / \\sum \\sigma_j^2$ |\n\n**Bottom line**: PCA is just SVD on centered data! In practice, PCA is often computed using SVD because it's more numerically stable.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_rank_approx(A, k):\n",
    "    \"\"\"Return rank-k approximation of matrix A using SVD.\"\"\"\n",
    "    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "    return U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]\n",
    "\n",
    "# Example with random matrix\n",
    "np.random.seed(42)\n",
    "A = np.random.rand(10, 8)\n",
    "\n",
    "print(f\"Original matrix shape: {A.shape}\")\n",
    "print(f\"Full rank: {np.linalg.matrix_rank(A)}\")\n",
    "\n",
    "for k in [1, 2, 4, 8]:\n",
    "    A_k = low_rank_approx(A, k)\n",
    "    error = np.linalg.norm(A - A_k, 'fro')  # Frobenius norm\n",
    "    print(f\"Rank-{k} approximation error: {error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Exercise: Image Compression with SVD\n",
    "\n",
    "Let's compress an image using SVD!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample grayscale image (or load one)\n",
    "# We'll create a simple pattern\n",
    "x = np.linspace(-3, 3, 200)\n",
    "y = np.linspace(-3, 3, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "image = np.sin(X) * np.cos(Y) + 0.5 * np.sin(2*X) * np.cos(2*Y)\n",
    "image = (image - image.min()) / (image.max() - image.min())  # Normalize to [0, 1]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(f'Original Image ({image.shape[0]}×{image.shape[1]})')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress with different ranks\n",
    "U, s, Vt = np.linalg.svd(image, full_matrices=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "ranks = [1, 5, 10, 20, 50, 100]\n",
    "for ax, k in zip(axes.flat, ranks):\n",
    "    compressed = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]\n",
    "    \n",
    "    # Calculate compression ratio\n",
    "    original_size = image.shape[0] * image.shape[1]\n",
    "    compressed_size = k * (image.shape[0] + image.shape[1] + 1)\n",
    "    ratio = original_size / compressed_size\n",
    "    \n",
    "    ax.imshow(compressed, cmap='gray')\n",
    "    ax.set_title(f'Rank {k}\\nCompression: {ratio:.1f}x')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot singular values\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(s, 'b-')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Singular Value')\n",
    "plt.title('Singular Values')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.cumsum(s**2) / np.sum(s**2), 'b-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Variance Explained')\n",
    "plt.title('Cumulative Variance')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95%')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Implement Matrix Operations\n",
    "Implement the following functions without using NumPy's built-in functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose(A):\n",
    "    \"\"\"Return the transpose of matrix A.\"\"\"\n",
    "    m, n = A.shape\n",
    "    result = np.zeros((n, m))\n",
    "    # TODO: Implement\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            result[j, i] = A[i, j]\n",
    "    return result\n",
    "\n",
    "def dot_product(a, b):\n",
    "    \"\"\"Return the dot product of vectors a and b.\"\"\"\n",
    "    assert len(a) == len(b)\n",
    "    result = 0\n",
    "    # TODO: Implement\n",
    "    for i in range(len(a)):\n",
    "        result += a[i] * b[i]\n",
    "    return result\n",
    "\n",
    "def matrix_vector_mult(A, v):\n",
    "    \"\"\"Return A @ v.\"\"\"\n",
    "    m, n = A.shape\n",
    "    assert n == len(v)\n",
    "    result = np.zeros(m)\n",
    "    # TODO: Implement\n",
    "    for i in range(m):\n",
    "        result[i] = dot_product(A[i], v)\n",
    "    return result\n",
    "\n",
    "# Test\n",
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "v = np.array([1, 2, 3])\n",
    "\n",
    "print(f\"transpose(A) correct: {np.allclose(transpose(A), A.T)}\")\n",
    "print(f\"dot_product([1,2,3], [4,5,6]) = {dot_product(np.array([1,2,3]), np.array([4,5,6]))}\")\n",
    "print(f\"matrix_vector_mult correct: {np.allclose(matrix_vector_mult(A, v), A @ v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Linear Transformation Explorer\n",
    "Create different transformation matrices and visualize their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and visualize these transformations:\n",
    "# 1. Reflection across the x-axis\n",
    "# 2. Reflection across y=x line\n",
    "# 3. Projection onto the x-axis\n",
    "# 4. A combination: rotate 45° then scale by 2\n",
    "\n",
    "# Example: Reflection across x-axis\n",
    "reflect_x = np.array([[1, 0],\n",
    "                      [0, -1]])\n",
    "plot_transformation(reflect_x, \"Reflection across x-axis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Build a Simple Recommender System\n",
    "\n",
    "Use SVD for matrix factorization to build a basic recommender system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-Item rating matrix (users x items)\n",
    "# 0 means not rated\n",
    "ratings = np.array([\n",
    "    [5, 3, 0, 1, 4],\n",
    "    [4, 0, 0, 1, 3],\n",
    "    [1, 1, 0, 5, 4],\n",
    "    [0, 1, 5, 4, 0],\n",
    "    [0, 0, 4, 0, 4],\n",
    "    [2, 1, 3, 4, 5]\n",
    "])\n",
    "\n",
    "print(\"User-Item Ratings (0 = not rated):\")\n",
    "print(ratings)\n",
    "\n",
    "# TODO: \n",
    "# 1. Fill missing values with row means (simple imputation)\n",
    "# 2. Apply SVD to get low-rank approximation\n",
    "# 3. Use the approximation to predict missing ratings\n",
    "\n",
    "# Fill missing with row means\n",
    "ratings_filled = ratings.copy().astype(float)\n",
    "for i in range(ratings.shape[0]):\n",
    "    row = ratings[i]\n",
    "    mean = row[row > 0].mean()\n",
    "    ratings_filled[i, row == 0] = mean\n",
    "\n",
    "print(\"\\nFilled ratings:\")\n",
    "print(ratings_filled.round(2))\n",
    "\n",
    "# Low-rank approximation\n",
    "k = 2  # Use only 2 latent factors\n",
    "U, s, Vt = np.linalg.svd(ratings_filled, full_matrices=False)\n",
    "predicted = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]\n",
    "\n",
    "print(f\"\\nPredicted ratings (rank-{k}):\")\n",
    "print(predicted.round(2))\n",
    "\n",
    "# Show predictions for originally missing entries\n",
    "print(\"\\nPredictions for missing entries:\")\n",
    "for i in range(ratings.shape[0]):\n",
    "    for j in range(ratings.shape[1]):\n",
    "        if ratings[i, j] == 0:\n",
    "            print(f\"  User {i}, Item {j}: {predicted[i, j]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Vectors** represent data points, weights, and gradients\n",
    "2. **Dot product** is the core operation (weighted sums in neural networks)\n",
    "3. **Matrices** are linear transformations\n",
    "4. **Matrix multiplication** composes transformations\n",
    "5. **Eigenvectors** reveal the \"natural directions\" of a transformation\n",
    "6. **SVD** decomposes any matrix and enables dimensionality reduction\n",
    "\n",
    "### Connection to Deep Learning\n",
    "\n",
    "- **Forward pass**: Sequence of matrix multiplications + activations\n",
    "- **Weights**: Learned transformation matrices\n",
    "- **Backprop**: Uses chain rule on these matrix operations\n",
    "- **Embeddings**: Low-dimensional representations (like SVD)\n",
    "- **Attention**: Computed via dot products between vectors\n",
    "\n",
    "### Checklist\n",
    "- [ ] I can perform vector operations (addition, dot product, norm)\n",
    "- [ ] I understand matrices as linear transformations\n",
    "- [ ] I can multiply matrices and understand shape compatibility\n",
    "- [ ] I know what eigenvalues/eigenvectors represent\n",
    "- [ ] I can use SVD for dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to **Part 1.2: Calculus Refresher** where we'll cover:\n",
    "- Derivatives and the chain rule\n",
    "- Gradients and gradient descent\n",
    "- The mathematical foundation of backpropagation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}