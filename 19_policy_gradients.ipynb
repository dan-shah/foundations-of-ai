{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Part 6.3: Policy Gradient Methods — The Formula 1 Edition\n\nDQN learns a value function and derives a policy from it. But this approach has fundamental limitations: it can only handle discrete actions, and it can't learn stochastic policies. **Policy gradient methods** take a radically different approach — they parameterize the policy directly as a neural network $\\pi_\\theta(a|s)$ and optimize it using gradient ascent on expected reward.\n\nThis is the paradigm that powers **PPO**, the algorithm behind RLHF and ChatGPT's alignment. Understanding policy gradients is understanding the core mechanism that makes modern AI assistants helpful.\n\n**The F1 Connection:** Policy gradient methods are like training a race strategy *directly* by reinforcing good race decisions and suppressing bad ones. Instead of building a lookup table of Q-values (DQN), the network directly outputs: \"Given P3 with worn tires and rain approaching, the probability of each action is: pit for inters 65%, push one more lap 25%, conserve 10%.\" The strategy is learned by reinforcing decisions that led to good race outcomes and penalizing those that didn't. This is how a driver builds racecraft — not by memorizing a table, but by developing instincts through thousands of laps of experience.\n\n## Learning Objectives\n\n- [ ] Derive the policy gradient theorem and understand why it works\n- [ ] Implement REINFORCE, the simplest policy gradient algorithm\n- [ ] Understand the high-variance problem and why baselines help\n- [ ] Implement a variance-reducing baseline\n- [ ] Build an actor-critic architecture from scratch\n- [ ] Understand the advantage function and why it's central to modern RL\n- [ ] Implement Advantage Actor-Critic (A2C)\n- [ ] Compare policy gradient methods against DQN\n- [ ] Recognize why trust regions matter (motivation for PPO)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Part 6.3: Policy Gradient Methods\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 1. The Policy Gradient Theorem\n\n### The Key Idea\n\nInstead of learning Q-values and deriving a policy, we directly parameterize a policy:\n\n$$\\pi_\\theta(a|s) = P(a|s; \\theta)$$\n\nand optimize the **expected return**:\n\n$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T} \\gamma^t r_t\\right]$$\n\nWe want $\\nabla_\\theta J(\\theta)$ so we can do gradient ascent. But the expectation is over trajectories sampled from the policy — how do we differentiate through sampling?\n\n### The Policy Gradient Theorem\n\n$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t\\right]$$\n\nwhere $G_t = \\sum_{k=t}^{T} \\gamma^{k-t} r_k$ is the return from time $t$.\n\n**In words**: To improve the policy, increase the log-probability of actions that led to high returns, and decrease it for actions with low returns.\n\n### Intuitive Explanation\n\nThink of it like training a comedian:\n- Try different jokes (sample actions from policy)\n- Note which ones get big laughs (high returns)\n- Tell those jokes more often (increase their probability)\n- Stop telling jokes that bomb (decrease their probability)\n\nThe $\\log \\pi_\\theta$ term is the \"how to adjust\" and $G_t$ is the \"which direction.\"\n\n**F1 analogy:** Imagine a rookie driver learning racecraft over their first season. Each race, they make decisions: pit early at Barcelona (gained 3 positions — great return), pushed too hard at Spa (spun off — terrible return), defended conservatively at Monza (maintained position — decent return). The policy gradient says: *increase the probability of early pitting in similar situations, decrease the probability of pushing on worn tires in high-speed corners, keep the defensive approach at power circuits*. Over many races, the driver's instincts (policy) converge toward the strategy that maximizes championship points. The $\\log \\pi_\\theta$ is how to adjust the driving instincts, and $G_t$ is whether the race outcome was good or bad."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: How Policy Gradients Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Before update: uniform-ish policy\n",
    "actions = ['Left', 'Right', 'Up', 'Down']\n",
    "probs_before = [0.25, 0.25, 0.25, 0.25]\n",
    "returns = [-2.0, 5.0, 1.0, -1.0]  # Returns for each action in this state\n",
    "\n",
    "colors = ['#e74c3c' if r < 0 else '#2ecc71' for r in returns]\n",
    "\n",
    "axes[0].bar(actions, probs_before, color='#95a5a6', edgecolor='black')\n",
    "axes[0].set_ylabel('π(a|s)', fontsize=12)\n",
    "axes[0].set_title('Before Update', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylim(0, 0.7)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Gradient signal\n",
    "axes[1].bar(actions, returns, color=colors, edgecolor='black')\n",
    "axes[1].set_ylabel('Return G_t', fontsize=12)\n",
    "axes[1].set_title('Gradient Signal (Returns)', fontsize=13, fontweight='bold')\n",
    "axes[1].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "for i, r in enumerate(returns):\n",
    "    axes[1].text(i, r + (0.2 if r >= 0 else -0.4), f'{r:+.1f}',\n",
    "                ha='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "# After update: shifted toward high-return actions\n",
    "logits = np.array(probs_before) * np.exp(np.array(returns) * 0.3)\n",
    "probs_after = logits / logits.sum()\n",
    "\n",
    "bar_colors = ['#e74c3c' if r < 0 else '#2ecc71' for r in returns]\n",
    "axes[2].bar(actions, probs_after, color=bar_colors, edgecolor='black')\n",
    "axes[2].set_ylabel('π(a|s)', fontsize=12)\n",
    "axes[2].set_title('After Update', fontsize=13, fontweight='bold')\n",
    "axes[2].set_ylim(0, 0.7)\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Annotations\n",
    "for i, (before, after) in enumerate(zip(probs_before, probs_after)):\n",
    "    change = after - before\n",
    "    symbol = '↑' if change > 0 else '↓'\n",
    "    axes[2].text(i, after + 0.03, f'{symbol}{abs(change):.2f}',\n",
    "                ha='center', fontsize=9, color='#2c3e50')\n",
    "\n",
    "plt.suptitle('Policy Gradient: Increase Probability of High-Return Actions',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. CartPole Environment (Reused from Notebook 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleSimple:\n",
    "    \"\"\"Simplified CartPole environment.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masscart + self.masspole\n",
    "        self.length = 0.5\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02\n",
    "        self.x_threshold = 2.4\n",
    "        self.theta_threshold = 12 * np.pi / 180\n",
    "        self.state_dim = 4\n",
    "        self.n_actions = 2\n",
    "        self.state = None\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.random.uniform(-0.05, 0.05, size=4)\n",
    "        return self.state.copy()\n",
    "    \n",
    "    def step(self, action):\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        cos_theta = np.cos(theta)\n",
    "        sin_theta = np.sin(theta)\n",
    "        temp = (force + self.polemass_length * theta_dot**2 * sin_theta) / self.total_mass\n",
    "        theta_acc = (self.gravity * sin_theta - cos_theta * temp) / (\n",
    "            self.length * (4.0/3.0 - self.masspole * cos_theta**2 / self.total_mass))\n",
    "        x_acc = temp - self.polemass_length * theta_acc * cos_theta / self.total_mass\n",
    "        x += self.tau * x_dot\n",
    "        x_dot += self.tau * x_acc\n",
    "        theta += self.tau * theta_dot\n",
    "        theta_dot += self.tau * theta_acc\n",
    "        self.state = np.array([x, x_dot, theta, theta_dot])\n",
    "        done = (abs(x) > self.x_threshold or abs(theta) > self.theta_threshold)\n",
    "        reward = 1.0 if not done else 0.0\n",
    "        return self.state.copy(), reward, done\n",
    "\n",
    "\n",
    "env = CartPoleSimple()\n",
    "print(f\"State dim: {env.state_dim}, Actions: {env.n_actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. REINFORCE: The Simplest Policy Gradient\n",
    "\n",
    "REINFORCE (Williams, 1992) is the most direct implementation of the policy gradient theorem:\n",
    "\n",
    "1. Run the policy for a complete episode, collecting $(s_t, a_t, r_t)$\n",
    "2. Compute returns $G_t$ for each timestep\n",
    "3. Compute the policy gradient: $\\nabla_\\theta J \\approx \\sum_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t$\n",
    "4. Update: $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Simple policy network: outputs action probabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, n_actions, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_actions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.net(x)\n",
    "        return F.softmax(logits, dim=-1)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Sample an action from the policy.\"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        probs = self.forward(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "\n",
    "\n",
    "class REINFORCE:\n",
    "    \"\"\"REINFORCE policy gradient algorithm.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, n_actions, lr=1e-3, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        self.policy = PolicyNetwork(state_dim, n_actions)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        # Episode storage\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action and store log probability.\"\"\"\n",
    "        action, log_prob = self.policy.get_action(state)\n",
    "        self.log_probs.append(log_prob)\n",
    "        return action\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update policy after a complete episode.\"\"\"\n",
    "        # Compute discounted returns (backwards)\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(self.rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        returns = torch.FloatTensor(returns)\n",
    "        \n",
    "        # Normalize returns (variance reduction trick)\n",
    "        if len(returns) > 1:\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # Policy gradient loss: -log_prob * return (negative for gradient ascent)\n",
    "        policy_loss = []\n",
    "        for log_prob, G in zip(self.log_probs, returns):\n",
    "            policy_loss.append(-log_prob * G)\n",
    "        \n",
    "        loss = torch.stack(policy_loss).sum()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clear episode data\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "def train_reinforce(env, agent, n_episodes=1000, max_steps=500):\n",
    "    \"\"\"Train a REINFORCE agent.\"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    losses = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.store_reward(reward)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        loss = agent.update()\n",
    "        losses.append(loss)\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(step + 1)\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_len = np.mean(episode_lengths[-100:])\n",
    "            print(f\"Episode {episode+1:4d} | Avg Length: {avg_len:6.1f}\")\n",
    "    \n",
    "    return episode_rewards, episode_lengths, losses\n",
    "\n",
    "\n",
    "# Train REINFORCE\n",
    "env = CartPoleSimple()\n",
    "reinforce_agent = REINFORCE(state_dim=4, n_actions=2, lr=1e-3, gamma=0.99)\n",
    "r_reinforce, l_reinforce, loss_reinforce = train_reinforce(env, reinforce_agent, n_episodes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: REINFORCE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "window = 50\n",
    "\n",
    "# Episode lengths\n",
    "ax = axes[0]\n",
    "ax.plot(l_reinforce, alpha=0.2, color='#3498db')\n",
    "smoothed = np.convolve(l_reinforce, np.ones(window)/window, mode='valid')\n",
    "ax.plot(range(window-1, len(l_reinforce)), smoothed, color='#2c3e50', linewidth=2,\n",
    "        label=f'{window}-ep avg')\n",
    "ax.axhline(y=200, color='red', linestyle='--', label='Goal')\n",
    "ax.set_xlabel('Episode', fontsize=12)\n",
    "ax.set_ylabel('Episode Length', fontsize=12)\n",
    "ax.set_title('REINFORCE: Learning to Balance', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# High variance illustration\n",
    "ax = axes[1]\n",
    "# Show rolling std of episode lengths\n",
    "if len(l_reinforce) >= window:\n",
    "    rolling_mean = np.convolve(l_reinforce, np.ones(window)/window, mode='valid')\n",
    "    rolling_std = np.array([np.std(l_reinforce[max(0,i-window):i+1]) \n",
    "                            for i in range(window-1, len(l_reinforce))])\n",
    "    x = range(window-1, len(l_reinforce))\n",
    "    ax.fill_between(x, rolling_mean - rolling_std, rolling_mean + rolling_std,\n",
    "                    alpha=0.3, color='#e74c3c', label='±1 std')\n",
    "    ax.plot(x, rolling_mean, color='#2c3e50', linewidth=2, label='Mean')\n",
    "\n",
    "ax.set_xlabel('Episode', fontsize=12)\n",
    "ax.set_ylabel('Episode Length', fontsize=12)\n",
    "ax.set_title('REINFORCE: High Variance Problem', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice the high variance — REINFORCE learns, but noisily.\")\n",
    "print(\"This is because it uses complete episode returns as the gradient signal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 4. The Variance Problem and Baselines\n\nREINFORCE suffers from **high variance** because the return $G_t$ can vary wildly between episodes, even for the same state-action pair. This means the gradient estimates are noisy, leading to slow and unstable learning.\n\n### Baselines to the Rescue\n\nWe can subtract any function $b(s)$ that doesn't depend on the action without biasing the gradient:\n\n$$\\nabla_\\theta J(\\theta) = \\mathbb{E}\\left[\\sum_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot (G_t - b(s_t))\\right]$$\n\n**Why no bias?** Because $\\mathbb{E}_a[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot b(s)] = b(s) \\cdot \\nabla_\\theta \\sum_a \\pi_\\theta(a|s) = b(s) \\cdot \\nabla_\\theta 1 = 0$\n\nThe **optimal baseline** turns out to be close to $V(s)$ — the expected return from that state. This gives us:\n\n$$G_t - b(s_t) \\approx G_t - V(s_t)$$\n\nThis is the **advantage** — how much better this action's return was compared to what we expected. Actions with positive advantage get reinforced; negative advantage gets suppressed.\n\n**F1 analogy:** Without a baseline, the strategist reinforces *every* decision from a race that scored points — even the bad ones, because the overall return was positive. At a race where you finished P4 (great!), every pit stop call gets reinforced, including the one where you stayed out too long on worn tires. With a baseline (expected result for P4-quality car = P5), only the decisions that made you *better than expected* get reinforced. The baseline says \"P5 was expected, so only the specific decisions that got you from P5 to P4 deserve credit.\" This is the advantage — how much better was this decision compared to the average outcome from this situation?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the variance reduction from baselines\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate returns for a state where the true value is ~50\n",
    "n_samples = 1000\n",
    "true_value = 50\n",
    "returns = np.random.normal(true_value, 30, n_samples)  # High variance returns\n",
    "\n",
    "# Without baseline: gradient signal proportional to G_t\n",
    "signal_no_baseline = returns\n",
    "\n",
    "# With baseline: gradient signal proportional to G_t - V(s)\n",
    "signal_with_baseline = returns - true_value\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(signal_no_baseline, bins=50, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(x=0, color='black', linewidth=2)\n",
    "axes[0].set_xlabel('Gradient Signal (G_t)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title(f'Without Baseline\\nstd = {np.std(signal_no_baseline):.1f}',\n",
    "                  fontsize=13, fontweight='bold')\n",
    "axes[0].text(0.05, 0.95, 'Almost all positive!\\nEvery action gets reinforced',\n",
    "            transform=axes[0].transAxes, fontsize=10, va='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "axes[1].hist(signal_with_baseline, bins=50, color='#2ecc71', alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(x=0, color='black', linewidth=2)\n",
    "axes[1].set_xlabel('Gradient Signal (G_t - V(s))', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title(f'With Baseline\\nstd = {np.std(signal_with_baseline):.1f}',\n",
    "                  fontsize=13, fontweight='bold')\n",
    "axes[1].text(0.05, 0.95, 'Centered at zero!\\nOnly above-average\\nactions reinforced',\n",
    "            transform=axes[1].transAxes, fontsize=10, va='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.suptitle('Why Baselines Reduce Variance', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Variance without baseline: {np.var(signal_no_baseline):.1f}\")\n",
    "print(f\"Variance with baseline:    {np.var(signal_with_baseline):.1f}\")\n",
    "print(f\"Same mean gradient, but {np.var(signal_no_baseline)/np.var(signal_with_baseline):.0f}x lower variance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. REINFORCE with Baseline\n",
    "\n",
    "Let's add a learned baseline (value function) to REINFORCE. This is the first step toward actor-critic methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"Critic network: estimates V(s).\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "\n",
    "class REINFORCEWithBaseline:\n",
    "    \"\"\"REINFORCE with a learned value function baseline.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, n_actions, lr_policy=1e-3, lr_value=1e-3, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Actor (policy)\n",
    "        self.policy = PolicyNetwork(state_dim, n_actions)\n",
    "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr_policy)\n",
    "        \n",
    "        # Baseline (value function)\n",
    "        self.value_net = ValueNetwork(state_dim)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr_value)\n",
    "        \n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.states = []\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        action, log_prob = self.policy.get_action(state)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.states.append(state)\n",
    "        return action\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def update(self):\n",
    "        # Compute returns\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(self.rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.FloatTensor(returns)\n",
    "        \n",
    "        # Get baseline values\n",
    "        states = torch.FloatTensor(np.array(self.states))\n",
    "        values = self.value_net(states).detach()\n",
    "        \n",
    "        # Advantages = returns - baseline\n",
    "        advantages = returns - values\n",
    "        if len(advantages) > 1:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Policy loss (REINFORCE with baseline)\n",
    "        policy_loss = []\n",
    "        for log_prob, adv in zip(self.log_probs, advantages):\n",
    "            policy_loss.append(-log_prob * adv)\n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "        \n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        # Value loss (fit baseline to returns)\n",
    "        value_predictions = self.value_net(states)\n",
    "        value_loss = F.mse_loss(value_predictions, returns)\n",
    "        \n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "        \n",
    "        # Clear\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.states = []\n",
    "        \n",
    "        return policy_loss.item(), value_loss.item()\n",
    "\n",
    "\n",
    "# Train REINFORCE with baseline\n",
    "env = CartPoleSimple()\n",
    "baseline_agent = REINFORCEWithBaseline(state_dim=4, n_actions=2)\n",
    "\n",
    "r_baseline = []\n",
    "l_baseline = []\n",
    "\n",
    "for episode in range(1000):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(500):\n",
    "        action = baseline_agent.select_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        baseline_agent.store_reward(reward)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    baseline_agent.update()\n",
    "    r_baseline.append(total_reward)\n",
    "    l_baseline.append(step + 1)\n",
    "    \n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"Episode {episode+1:4d} | Avg Length: {np.mean(l_baseline[-100:]):6.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare REINFORCE vs REINFORCE with baseline\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "window = 50\n",
    "for data, label, color in [(l_reinforce, 'REINFORCE (no baseline)', '#e74c3c'),\n",
    "                            (l_baseline, 'REINFORCE + Baseline', '#2ecc71')]:\n",
    "    smoothed = np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(smoothed, label=label, color=color, linewidth=2.5)\n",
    "\n",
    "ax.axhline(y=200, color='gray', linestyle='--', alpha=0.5, label='Goal')\n",
    "ax.set_xlabel('Episode', fontsize=12)\n",
    "ax.set_ylabel('Episode Length (smoothed)', fontsize=12)\n",
    "ax.set_title('Baselines Reduce Variance and Speed Up Learning', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 6. The Advantage Function\n\nThe **advantage function** is the difference between the Q-value and the state value:\n\n$$A^\\pi(s, a) = Q^\\pi(s, a) - V^\\pi(s)$$\n\n**Interpretation**: \"How much better is action $a$ compared to the average action in state $s$?\"\n\n- $A > 0$: This action is better than average → increase its probability\n- $A < 0$: This action is worse than average → decrease its probability\n- $A = 0$: This action is exactly average\n\nThe advantage is **the** central concept in modern policy gradient methods. PPO, A2C, A3C, and TRPO all use the advantage to compute policy gradients.\n\n**F1 analogy:** The advantage answers: \"Was pitting on lap 25 better or worse than our average action from that position?\" If you're P3 with worn tires and the average outcome from this state is P4 (V(s) = 4 points), but pitting on lap 25 led to P2 (Q(s, pit_lap_25) = 18 points), the advantage is +14 — strongly reinforce that decision. If staying out led to P6 (Q(s, stay_out) = 8 points), the advantage is -4 — suppress that decision. The advantage strips away \"were we in a good position?\" and focuses purely on \"did THIS specific decision help or hurt?\"\n\n### Estimating the Advantage\n\nWe don't know $Q(s,a)$ directly, but we can estimate the advantage using the TD error:\n\n$$\\hat{A}_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n\nThis is a one-step estimate. In Notebook 20, we'll see **Generalized Advantage Estimation (GAE)**, which blends between one-step and full-return estimates."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the advantage concept\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Q, V, and A relationship\n",
    "ax = axes[0]\n",
    "actions = ['Left', 'Stay', 'Right']\n",
    "q_values = [3.0, 5.0, 8.0]\n",
    "v_value = 5.33  # Average Q\n",
    "advantages = [q - v_value for q in q_values]\n",
    "\n",
    "x = np.arange(len(actions))\n",
    "bars_q = ax.bar(x - 0.15, q_values, 0.3, label='Q(s,a)', color='#3498db', alpha=0.8)\n",
    "ax.axhline(y=v_value, color='#e74c3c', linewidth=2.5, linestyle='--', label=f'V(s) = {v_value:.2f}')\n",
    "\n",
    "# Show advantage annotations\n",
    "for i, (q, a) in enumerate(zip(q_values, advantages)):\n",
    "    color = '#2ecc71' if a > 0 else '#e74c3c'\n",
    "    ax.annotate(f'A = {a:+.2f}', xy=(i, q), xytext=(i + 0.4, q),\n",
    "               fontsize=10, fontweight='bold', color=color,\n",
    "               arrowprops=dict(arrowstyle='->', color=color))\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(actions)\n",
    "ax.set_ylabel('Value', fontsize=12)\n",
    "ax.set_title('Advantage = Q(s,a) - V(s)', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Right: How advantage shapes policy updates\n",
    "ax = axes[1]\n",
    "adv_range = np.linspace(-3, 3, 100)\n",
    "\n",
    "# Policy update magnitude: proportional to advantage\n",
    "ax.fill_between(adv_range[adv_range < 0], 0, adv_range[adv_range < 0],\n",
    "                alpha=0.3, color='#e74c3c', label='Decrease probability')\n",
    "ax.fill_between(adv_range[adv_range >= 0], 0, adv_range[adv_range >= 0],\n",
    "                alpha=0.3, color='#2ecc71', label='Increase probability')\n",
    "ax.plot(adv_range, adv_range, 'k-', linewidth=2)\n",
    "ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel('Advantage A(s,a)', fontsize=12)\n",
    "ax.set_ylabel('Policy gradient signal', fontsize=12)\n",
    "ax.set_title('Advantage Drives Policy Updates', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 7. Actor-Critic: The Best of Both Worlds\n\nREINFORCE (even with a baseline) waits until the end of an episode to update. **Actor-critic** methods update at every step, using the TD advantage estimate:\n\n- **Actor**: The policy network $\\pi_\\theta(a|s)$ — decides what to do\n- **Critic**: The value network $V_\\phi(s)$ — evaluates how good states are\n\nThe critic provides the advantage signal that the actor uses to improve.\n\n**F1 analogy:** The actor is the driver making real-time decisions on track — push, conserve, defend, pit. The critic is the race strategist on the pit wall evaluating the situation: \"We're in P4, that's worth X expected points given current tire state.\" The driver (actor) makes a decision, the strategist (critic) evaluates the outcome: \"That undercut moved us to P3 — better than the P4 we expected, advantage is positive.\" The driver adjusts their instincts (policy) based on the strategist's feedback. Neither works optimally alone — the driver has instincts but no big picture, the strategist has analysis but can't drive. Together, they form an actor-critic system.\n\n| Component | REINFORCE | Actor-Critic | F1 Parallel |\n|-----------|-----------|---------------|-------------|\n| **Updates** | After full episode | After each step | Post-race review vs. lap-by-lap radio calls |\n| **Gradient signal** | Full return $G_t$ | TD advantage $r + \\gamma V(s') - V(s)$ | Full race result vs. \"that lap was 0.3s better than expected\" |\n| **Variance** | High (even with baseline) | Lower (TD estimates) | Race outcomes vary wildly; lap-by-lap feedback is steadier |\n| **Bias** | Unbiased | Slightly biased (bootstrapping) | Full picture but noisy vs. immediate feedback with assumptions |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"Actor-Critic with shared feature layers.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, n_actions, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared feature extraction\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Actor head (policy)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_actions)\n",
    "        )\n",
    "        \n",
    "        # Critic head (value function)\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.shared(x)\n",
    "        action_probs = F.softmax(self.actor(features), dim=-1)\n",
    "        value = self.critic(features).squeeze(-1)\n",
    "        return action_probs, value\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        probs, value = self.forward(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), value\n",
    "\n",
    "\n",
    "class A2CAgent:\n",
    "    \"\"\"Advantage Actor-Critic (A2C) agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, n_actions, lr=3e-4, gamma=0.99,\n",
    "                 value_coef=0.5, entropy_coef=0.01):\n",
    "        self.gamma = gamma\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        self.network = ActorCritic(state_dim, n_actions)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        \n",
    "        # Episode storage\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.entropies = []\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        probs, value = self.network(state_tensor)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        self.log_probs.append(dist.log_prob(action))\n",
    "        self.values.append(value)\n",
    "        self.entropies.append(dist.entropy())\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def update(self, next_state, done):\n",
    "        \"\"\"Update after collecting a batch of experience.\"\"\"\n",
    "        # Bootstrap value of last state\n",
    "        with torch.no_grad():\n",
    "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "            _, next_value = self.network(next_state_tensor)\n",
    "            next_value = next_value * (1 - done)\n",
    "        \n",
    "        # Compute returns and advantages\n",
    "        returns = []\n",
    "        G = next_value.item()\n",
    "        for r in reversed(self.rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.FloatTensor(returns)\n",
    "        \n",
    "        values = torch.cat(self.values)\n",
    "        log_probs = torch.cat(self.log_probs)\n",
    "        entropies = torch.cat(self.entropies)\n",
    "        \n",
    "        advantages = returns - values.detach()\n",
    "        if len(advantages) > 1:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Actor loss: policy gradient with advantage\n",
    "        actor_loss = -(log_probs * advantages).mean()\n",
    "        \n",
    "        # Critic loss: fit value function to returns\n",
    "        critic_loss = F.mse_loss(values, returns)\n",
    "        \n",
    "        # Entropy bonus: encourage exploration\n",
    "        entropy_loss = -entropies.mean()\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (actor_loss + \n",
    "                     self.value_coef * critic_loss + \n",
    "                     self.entropy_coef * entropy_loss)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clear\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.entropies = []\n",
    "        \n",
    "        return actor_loss.item(), critic_loss.item()\n",
    "\n",
    "\n",
    "# Train A2C\n",
    "env = CartPoleSimple()\n",
    "a2c_agent = A2CAgent(state_dim=4, n_actions=2, lr=3e-4)\n",
    "\n",
    "l_a2c = []\n",
    "actor_losses = []\n",
    "critic_losses = []\n",
    "\n",
    "for episode in range(1000):\n",
    "    state = env.reset()\n",
    "    \n",
    "    for step in range(500):\n",
    "        action = a2c_agent.select_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        a2c_agent.store_reward(reward)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    a_loss, c_loss = a2c_agent.update(next_state, float(done))\n",
    "    l_a2c.append(step + 1)\n",
    "    actor_losses.append(a_loss)\n",
    "    critic_losses.append(c_loss)\n",
    "    \n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"Episode {episode+1:4d} | Avg Length: {np.mean(l_a2c[-100:]):6.1f} | \"\n",
    "              f\"Actor Loss: {np.mean(actor_losses[-100:]):7.3f} | \"\n",
    "              f\"Critic Loss: {np.mean(critic_losses[-100:]):7.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: A2C Training and Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Performance comparison\n",
    "ax = axes[0]\n",
    "window = 50\n",
    "for data, label, color in [(l_reinforce, 'REINFORCE', '#e74c3c'),\n",
    "                            (l_baseline, 'REINFORCE + Baseline', '#f39c12'),\n",
    "                            (l_a2c, 'A2C', '#2ecc71')]:\n",
    "    smoothed = np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(smoothed, label=label, color=color, linewidth=2.5)\n",
    "\n",
    "ax.axhline(y=200, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Episode', fontsize=12)\n",
    "ax.set_ylabel('Episode Length', fontsize=12)\n",
    "ax.set_title('Policy Gradient Methods Comparison', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Actor and critic losses\n",
    "ax = axes[1]\n",
    "window = 20\n",
    "a_smooth = np.convolve(actor_losses, np.ones(window)/window, mode='valid')\n",
    "c_smooth = np.convolve(critic_losses, np.ones(window)/window, mode='valid')\n",
    "ax.plot(a_smooth, label='Actor Loss', color='#3498db', linewidth=2)\n",
    "ax.plot(c_smooth, label='Critic Loss', color='#e74c3c', linewidth=2)\n",
    "ax.set_xlabel('Episode', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('A2C: Actor and Critic Losses', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture diagram\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "ax.set_title('Actor-Critic Architecture', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Input\n",
    "box = mpatches.FancyBboxPatch((3.5, 6.5), 3, 0.8, boxstyle=\"round,pad=0.2\",\n",
    "                               facecolor='#95a5a6', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(box)\n",
    "ax.text(5, 6.9, 'State s', ha='center', va='center', fontsize=12, fontweight='bold', color='white')\n",
    "\n",
    "# Shared layers\n",
    "box = mpatches.FancyBboxPatch((3, 5), 4, 0.8, boxstyle=\"round,pad=0.2\",\n",
    "                               facecolor='#9b59b6', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(box)\n",
    "ax.text(5, 5.4, 'Shared Features', ha='center', va='center', fontsize=12, fontweight='bold', color='white')\n",
    "ax.annotate('', xy=(5, 5.8), xytext=(5, 6.5), arrowprops=dict(arrowstyle='->', lw=2, color='gray'))\n",
    "\n",
    "# Actor head\n",
    "box = mpatches.FancyBboxPatch((1, 3), 3, 0.8, boxstyle=\"round,pad=0.2\",\n",
    "                               facecolor='#3498db', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(box)\n",
    "ax.text(2.5, 3.4, 'Actor π(a|s)', ha='center', va='center', fontsize=12, fontweight='bold', color='white')\n",
    "ax.annotate('', xy=(3.5, 3.8), xytext=(4.5, 5), arrowprops=dict(arrowstyle='->', lw=2, color='gray'))\n",
    "\n",
    "# Critic head\n",
    "box = mpatches.FancyBboxPatch((6, 3), 3, 0.8, boxstyle=\"round,pad=0.2\",\n",
    "                               facecolor='#e74c3c', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(box)\n",
    "ax.text(7.5, 3.4, 'Critic V(s)', ha='center', va='center', fontsize=12, fontweight='bold', color='white')\n",
    "ax.annotate('', xy=(6.5, 3.8), xytext=(5.5, 5), arrowprops=dict(arrowstyle='->', lw=2, color='gray'))\n",
    "\n",
    "# Outputs\n",
    "ax.text(2.5, 2.3, 'Action\\nprobabilities', ha='center', va='center', fontsize=10,\n",
    "        color='#3498db', style='italic')\n",
    "ax.text(7.5, 2.3, 'State\\nvalue', ha='center', va='center', fontsize=10,\n",
    "        color='#e74c3c', style='italic')\n",
    "\n",
    "# Advantage\n",
    "box = mpatches.FancyBboxPatch((3.5, 1), 3, 0.8, boxstyle=\"round,pad=0.2\",\n",
    "                               facecolor='#2ecc71', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(box)\n",
    "ax.text(5, 1.4, 'Advantage A(s,a)', ha='center', va='center', fontsize=12, fontweight='bold', color='white')\n",
    "\n",
    "ax.annotate('', xy=(4, 1.8), xytext=(2.5, 2.3), arrowprops=dict(arrowstyle='->', lw=1.5, color='gray'))\n",
    "ax.annotate('', xy=(6, 1.8), xytext=(7.5, 2.3), arrowprops=dict(arrowstyle='->', lw=1.5, color='gray'))\n",
    "\n",
    "ax.text(5, 0.3, 'A = r + γV(s\\') - V(s)', ha='center', fontsize=11, style='italic', color='#2c3e50')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 8. The Entropy Bonus: Encouraging Exploration\n\nNotice the `entropy_coef` term in A2C. The **entropy** of the policy measures how \"spread out\" the action distribution is:\n\n$$H(\\pi) = -\\sum_a \\pi(a|s) \\log \\pi(a|s)$$\n\n- **High entropy**: Policy is uniform (maximum exploration)\n- **Low entropy**: Policy is deterministic (maximum exploitation)\n\nBy adding an entropy bonus to the objective, we prevent the policy from collapsing to a deterministic distribution too early.\n\n**F1 analogy:** Without the entropy bonus, a team's strategy would quickly converge to always making the same call — \"always one-stop, always medium-hard\" — even if the optimal strategy varies by circuit. The entropy bonus keeps the strategy distribution \"open-minded\" during training, like a team principal saying \"I know the one-stop usually works, but keep the two-stop and three-stop options on the table — we haven't raced enough circuits to be certain.\" It prevents premature commitment to a single strategy before enough data has been collected."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize entropy's effect on policy distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4.5))\n",
    "\n",
    "# Three example policies with different entropies\n",
    "policies = [\n",
    "    ([0.97, 0.01, 0.01, 0.01], 'Nearly Deterministic'),\n",
    "    ([0.5, 0.3, 0.15, 0.05], 'Moderate Entropy'),\n",
    "    ([0.25, 0.25, 0.25, 0.25], 'Maximum Entropy (Uniform)'),\n",
    "]\n",
    "\n",
    "actions = ['A₁', 'A₂', 'A₃', 'A₄']\n",
    "colors_map = ['#e74c3c', '#f39c12', '#2ecc71']\n",
    "\n",
    "for ax, (probs, title), color in zip(axes, policies, colors_map):\n",
    "    entropy = -sum(p * np.log(p + 1e-8) for p in probs)\n",
    "    ax.bar(actions, probs, color=color, edgecolor='black', alpha=0.8)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.set_ylabel('π(a|s)')\n",
    "    ax.set_title(f'{title}\\nH = {entropy:.3f}', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Policy Entropy: From Deterministic to Uniform', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The entropy bonus prevents premature convergence to a deterministic policy.\")\n",
    "print(\"Without it, the agent might stop exploring before finding the best strategy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 9. The Trust Region Problem\n\nPolicy gradient methods have a subtle but critical problem: **large policy updates can be catastrophic**.\n\nIf we take too large a step in parameter space, the policy can change drastically — an action that was chosen 80% of the time might suddenly be chosen 5% of the time. This can destroy good behavior that took many episodes to learn.\n\n### Why This Matters\n\nIn supervised learning, a bad gradient step loses a bit of accuracy, and the next batch corrects it. In RL, a bad policy update changes the *data distribution itself* — the agent starts visiting different states, getting different rewards, leading to further bad updates. This feedback loop can cause complete collapse.\n\n**F1 analogy:** Imagine a team that's been running a successful medium-hard strategy all season. After one bad race (Abu Dhabi, unusual conditions), the strategy model over-corrects: \"medium-hard is terrible, switch everything to soft-soft-medium.\" At the next race, the three-stop fails spectacularly, generating even worse data, causing another wild swing. This is the policy collapse feedback loop. In F1, experienced teams avoid this by making *conservative, incremental* strategy adjustments — they don't throw away a proven approach based on one race. PPO (Notebook 20) formalizes this wisdom mathematically.\n\n### The Solution: Constrain Policy Updates\n\n**TRPO** (Trust Region Policy Optimization) constrains the KL divergence between old and new policies:\n\n$$\\text{maximize } L(\\theta) \\quad \\text{subject to } D_{KL}(\\pi_{\\theta_{old}} \\| \\pi_\\theta) \\leq \\delta$$\n\n**PPO** (Proximal Policy Optimization) achieves a similar effect much more simply using a clipped objective. This is what we'll build in Notebook 20 — stable strategy updates that don't swing too wildly between races."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the trust region problem\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Policy before and after a large update\n",
    "ax = axes[0]\n",
    "x = np.linspace(-3, 3, 200)\n",
    "old_policy = np.exp(-0.5 * (x - 0.5)**2) / np.sqrt(2 * np.pi)\n",
    "# Small update\n",
    "small_update = np.exp(-0.5 * (x - 0.8)**2) / np.sqrt(2 * np.pi)\n",
    "# Large update\n",
    "large_update = np.exp(-0.5 * (x + 1.5)**2 / 0.3) / np.sqrt(2 * np.pi * 0.3)\n",
    "\n",
    "ax.plot(x, old_policy, 'b-', linewidth=2.5, label='Old policy')\n",
    "ax.plot(x, small_update, 'g--', linewidth=2.5, label='Small update (safe)')\n",
    "ax.plot(x, large_update, 'r--', linewidth=2.5, label='Large update (dangerous!)')\n",
    "ax.fill_between(x, old_policy, large_update, alpha=0.1, color='red')\n",
    "ax.set_xlabel('Action', fontsize=12)\n",
    "ax.set_ylabel('π(a|s)', fontsize=12)\n",
    "ax.set_title('Large Policy Updates Are Dangerous', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: The collapse feedback loop\n",
    "ax = axes[1]\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "ax.set_title('Policy Collapse Feedback Loop', fontsize=13, fontweight='bold')\n",
    "\n",
    "steps = [\n",
    "    (5, 7, 'Large policy update'),\n",
    "    (8.5, 5.5, 'Visit different states'),\n",
    "    (7, 3, 'Get different rewards'),\n",
    "    (3, 3, 'Worse gradient estimates'),\n",
    "    (1.5, 5.5, 'Even larger bad update'),\n",
    "]\n",
    "\n",
    "for i, (x, y, text) in enumerate(steps):\n",
    "    color = '#e74c3c' if i > 0 else '#f39c12'\n",
    "    circle = plt.Circle((x, y), 0.8, color=color, alpha=0.3)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(x, y, f'{i+1}. {text}', ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "# Arrows forming a cycle\n",
    "for i in range(len(steps) - 1):\n",
    "    x1, y1, _ = steps[i]\n",
    "    x2, y2, _ = steps[i + 1]\n",
    "    ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n",
    "               arrowprops=dict(arrowstyle='->', lw=2, color='gray',\n",
    "                              connectionstyle='arc3,rad=0.2'))\n",
    "# Close the loop\n",
    "ax.annotate('', xy=(steps[0][0], steps[0][1]), xytext=(steps[-1][0], steps[-1][1]),\n",
    "           arrowprops=dict(arrowstyle='->', lw=2, color='gray',\n",
    "                          connectionstyle='arc3,rad=0.2'))\n",
    "\n",
    "ax.text(5, 0.5, 'PPO solves this with clipped updates (Notebook 20)',\n",
    "       ha='center', fontsize=11, style='italic', color='#2ecc71',\n",
    "       bbox=dict(boxstyle='round', facecolor='#ecf0f1', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 10. Continuous Action Spaces (Preview)\n\nOne of policy gradient's biggest advantages: they naturally handle **continuous actions**. Instead of outputting a probability over discrete actions, the policy outputs the **parameters of a distribution** (e.g., mean and standard deviation of a Gaussian).\n\n$$\\pi_\\theta(a|s) = \\mathcal{N}(\\mu_\\theta(s), \\sigma_\\theta(s))$$\n\nThis is essential for robotics (joint torques), autonomous driving (steering angle), and any task where actions are continuous.\n\n**F1 analogy:** This is the difference between discrete strategy calls (\"pit\" or \"don't pit\") and continuous driver inputs (steering angle: 12.7 degrees, throttle: 83%, brake pressure: 42%). A discrete DQN can decide *whether* to pit, but a continuous policy gradient can learn the optimal steering trajectory through a corner — output a Gaussian distribution over steering angles, centered on the ideal line with some variance for uncertainty. Modern F1 simulators use exactly this kind of continuous control policy for driver-in-the-loop testing."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousPolicyNetwork(nn.Module):\n",
    "    \"\"\"Policy network for continuous action spaces.\n",
    "    Outputs mean and log_std of a Gaussian distribution.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mean_head = nn.Linear(hidden_dim, action_dim)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))  # Learnable\n",
    "    \n",
    "    def forward(self, state):\n",
    "        features = self.shared(state)\n",
    "        mean = self.mean_head(features)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return mean, std\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        mean, std = self.forward(state)\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action).sum(-1)\n",
    "        return action.squeeze(0).numpy(), log_prob\n",
    "\n",
    "\n",
    "# Demonstrate\n",
    "policy = ContinuousPolicyNetwork(state_dim=4, action_dim=2)\n",
    "state = np.random.randn(4)\n",
    "action, log_prob = policy.get_action(state)\n",
    "\n",
    "print(\"Continuous Policy Network:\")\n",
    "print(f\"  State: {state}\")\n",
    "print(f\"  Sampled action: {action}\")\n",
    "print(f\"  Log probability: {log_prob.item():.4f}\")\n",
    "\n",
    "# Visualize the action distribution\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "mean, std = policy(state_tensor)\n",
    "mean, std = mean.detach().numpy()[0], std.detach().numpy()\n",
    "\n",
    "x = np.linspace(-4, 4, 200)\n",
    "for i, (m, s, label) in enumerate(zip(mean, std, ['Action dim 1', 'Action dim 2'])):\n",
    "    pdf = np.exp(-0.5 * ((x - m) / s)**2) / (s * np.sqrt(2 * np.pi))\n",
    "    ax.plot(x, pdf, linewidth=2.5, label=f'{label}: μ={m:.2f}, σ={s:.2f}')\n",
    "    ax.fill_between(x, pdf, alpha=0.2)\n",
    "\n",
    "ax.set_xlabel('Action value', fontsize=12)\n",
    "ax.set_ylabel('Probability density', fontsize=12)\n",
    "ax.set_title('Continuous Policy: Gaussian Action Distribution', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercises\n\n### Exercise 1: REINFORCE on a Harder Problem — Tighter Margins at Monaco\n\nModify the CartPole environment to have a tighter angle threshold (6 degrees instead of 12). Train REINFORCE with and without a baseline. This is like making the \"track\" narrower — Monaco's street circuit gives almost no margin for error compared to Silverstone's wide runoffs. How much does the baseline help on this harder, less-forgiving problem?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Hint: Create a HarderCartPole class with theta_threshold = 6 * np.pi / 180\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2: n-Step Returns for A2C — How Many Laps of Hindsight?\n\nOur A2C uses full episode returns. Implement **n-step returns** where the agent updates every n steps:\n\n$$G_t^{(n)} = \\sum_{k=0}^{n-1} \\gamma^k r_{t+k} + \\gamma^n V(s_{t+n})$$\n\nCompare n=1, n=5, n=20, and full episode. In F1 terms, n=1 is updating strategy after every single lap (fast feedback, but can't see multi-lap patterns). n=20 is waiting 20 laps to assess (sees longer-term tire degradation trends). Full episode is waiting until the checkered flag. Which n works best — how many laps of hindsight does the strategist need?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Hint: Modify A2CAgent.update() to compute n-step returns\n",
    "# instead of full-episode returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 3: A2C vs DQN Head-to-Head — Instinct vs. Lookup Table\n\nImport the DQNAgent from Notebook 18 (or recreate it) and run a fair comparison against A2C on CartPole. Use the same number of environment interactions for both. This is the ultimate question: does a driver who learns strategy by instinct (A2C/policy gradients) outperform one who memorizes a Q-value lookup table (DQN)? Which method:\n- Converges faster? (Who learns the track quicker?)\n- Achieves higher final performance? (Who's faster at the end of the season?)\n- Has more stable training? (Who's more consistent race-to-race?)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "# Hint: Make sure both agents get the same total number of env.step() calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Summary\n\n### Key Concepts\n\n| Concept | What It Means | F1 Parallel |\n|---------|--------------|-------------|\n| **Policy gradient theorem** | Optimize policy directly: increase prob of high-return actions | Reinforce winning race decisions, suppress losing ones |\n| **REINFORCE** | Simplest policy gradient, but high variance | Learn from full race outcomes — noisy but unbiased |\n| **Baselines** | Subtract expected return to reduce variance | \"How much better than expected was this decision?\" |\n| **Advantage function** | A(s,a) = Q(s,a) - V(s): how much better than average | \"Was pitting NOW specifically better than our average action here?\" |\n| **Actor-Critic** | Actor (policy) + Critic (value function) | Driver (decisions) + Strategist (evaluation) |\n| **Entropy bonus** | Prevent premature convergence to one strategy | Keep multiple strategy options open during learning |\n| **Trust region problem** | Large updates cause policy collapse | Don't overhaul proven strategy based on one bad race |\n| **Continuous actions** | Output distribution parameters instead of discrete probs | Steering angle, throttle percentage, brake pressure |\n\n### Fundamental Insight\n\nThe policy gradient theorem elegantly solves the problem of differentiating through stochastic sampling. By using $\\nabla \\log \\pi$ as a \"score function,\" we can optimize any reward signal — not just differentiable losses. This generality is why policy gradients power RLHF: the reward model's output doesn't need to be differentiable with respect to the generated text. In F1 terms, you can optimize for \"championship points\" — a non-differentiable, delayed, sparse reward — by reinforcing the strategic decisions that led to them."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Next Steps\n\nWe now have all the building blocks for the algorithm that aligns modern language models. In **Notebook 20: PPO and Modern RL**, we'll:\n\n- Implement PPO's clipped surrogate objective — stable strategy updates that don't swing too wildly between races\n- Build Generalized Advantage Estimation (GAE) for better advantage estimates\n- Train a full PPO agent from scratch\n- Connect everything to **RLHF**: reward models, KL penalties, and the complete pipeline\n- See how RL makes language models helpful, harmless, and honest\n\nPPO is the finish line of our RL journey — and the starting grid for understanding how modern AI assistants are aligned with human preferences."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}