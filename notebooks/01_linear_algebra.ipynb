{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Part 1.1: Linear Algebra for Deep Learning\n\nLinear algebra is the foundation of deep learning. Neural networks are essentially compositions of linear transformations (matrix multiplications) and nonlinear activation functions.\n\nBut let's make this concrete: **F1 is a data sport**. Every car generates gigabytes of telemetry per lap — tire temperatures, aerodynamic forces, suspension loads, engine mappings. All of it lives in vectors and matrices. Understanding linear algebra isn't just academic — it's how teams like Red Bull, Mercedes, and Ferrari extract tenths of a second from their cars.\n\nThroughout this notebook, we'll learn the math of deep learning through the lens of Formula 1.\n\n## Learning Objectives\n- [ ] Understand vector spaces and linear transformations\n- [ ] Perform matrix operations fluently with NumPy\n- [ ] Explain the geometric intuition behind eigendecomposition\n- [ ] Apply SVD to dimensionality reduction\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "# For nice inline plots\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Vectors\n\nA **vector** is an ordered list of numbers. In machine learning:\n- A single data point (features) is a vector\n- Model parameters (weights) are vectors\n- Gradients are vectors\n\n### The F1 Connection\n\nThink of a vector as a **car's telemetry snapshot**. At any given moment on track, a car's state can be described as a vector of measurements:\n\n$$\\text{car\\_state} = [\\text{speed}, \\text{throttle}, \\text{brake\\_pressure}, \\text{steering\\_angle}, \\text{tire\\_temp}, \\ldots]$$\n\nEach dimension captures a different aspect of performance — just like in ML, where each dimension of a feature vector captures a different attribute of a data point.\n\n### Geometric Interpretation\nA vector can be thought of as:\n1. A point in space (a car's position on the circuit)\n2. An arrow from the origin to that point (direction + magnitude — like a velocity vector showing which way the car is headed and how fast)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Creating vectors in NumPy\n# A car's velocity vector on a straight: 300 km/h in x-direction, slight drift in y\nvelocity = np.array([300, 5])  # 2D velocity vector (km/h)\n\n# A car's telemetry snapshot: [speed, downforce_kN, tire_temp_C]\ntelemetry = np.array([310, 4.5, 105])  # 3D telemetry vector\n\nprint(f\"Velocity vector: {velocity}\")\nprint(f\"Shape of velocity: {velocity.shape}\")\nprint(f\"Dimension (number of measurements): {velocity.shape[0]}\")\nprint(f\"\\nTelemetry vector: {telemetry}\")\nprint(f\"Telemetry dimensions: {telemetry.shape[0]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize a 2D vector — car velocity on track\ndef plot_vectors(vectors, colors, labels=None):\n    \"\"\"Plot 2D vectors from origin.\"\"\"\n    fig, ax = plt.subplots(figsize=(8, 8))\n    \n    for i, (vec, color) in enumerate(zip(vectors, colors)):\n        label = labels[i] if labels else None\n        ax.quiver(0, 0, vec[0], vec[1], angles='xy', scale_units='xy', scale=1, \n                  color=color, label=label, width=0.015)\n    \n    # Set axis limits\n    all_coords = np.array(vectors)\n    max_val = np.abs(all_coords).max() + 1\n    ax.set_xlim(-max_val, max_val)\n    ax.set_ylim(-max_val, max_val)\n    ax.set_aspect('equal')\n    ax.axhline(y=0, color='k', linewidth=0.5)\n    ax.axvline(x=0, color='k', linewidth=0.5)\n    ax.grid(True, alpha=0.3)\n    if labels:\n        ax.legend()\n    return ax\n\n# Plot a car's velocity vector approaching a corner\nv = np.array([3, 4])  # heading northeast on the circuit\nplot_vectors([v], ['blue'], ['car velocity = [3, 4]'])\nplt.title(\"A Car's Velocity Vector on Circuit\")\nplt.xlabel('Track x-direction (m/s)')\nplt.ylabel('Track y-direction (m/s)')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Vector Operations\n\n#### 1. Vector Addition\nVectors add element-wise. Geometrically, place the tail of the second vector at the head of the first.\n\n**F1 analogy**: Forces on a car combine as vectors. The engine pushes forward, aerodynamic drag pulls back, cornering force pulls sideways. The **net force** on the car is the vector sum of all individual forces — and that resultant determines how the car actually moves."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Vector addition: forces on an F1 car mid-corner\nengine_force = np.array([2, 0])    # engine pushes forward (x-direction)\ncornering_force = np.array([0, 3]) # tires generate lateral grip (y-direction)\nnet_force = engine_force + cornering_force  # resultant force\n\nprint(f\"Engine force   = {engine_force}\")\nprint(f\"Cornering force = {cornering_force}\")\nprint(f\"Net force       = {net_force}\")\n\n# Visualize force addition on an F1 car\nfig, ax = plt.subplots(figsize=(8, 8))\nax.quiver(0, 0, engine_force[0], engine_force[1], angles='xy', scale_units='xy', scale=1,\n          color='red', label='Engine (forward)', width=0.015)\nax.quiver(0, 0, cornering_force[0], cornering_force[1], angles='xy', scale_units='xy', scale=1,\n          color='blue', label='Cornering grip (lateral)', width=0.015)\nax.quiver(0, 0, net_force[0], net_force[1], angles='xy', scale_units='xy', scale=1,\n          color='green', label='Net force on car', width=0.015)\n# Show cornering force starting from tip of engine force (parallelogram rule)\nax.quiver(engine_force[0], engine_force[1], cornering_force[0], cornering_force[1],\n          angles='xy', scale_units='xy', scale=1, color='blue', alpha=0.3, width=0.015)\nax.set_xlim(-1, 5)\nax.set_ylim(-1, 5)\nax.set_aspect('equal')\nax.axhline(y=0, color='k', linewidth=0.5)\nax.axvline(x=0, color='k', linewidth=0.5)\nax.legend()\nax.set_title('Force Vectors on an F1 Car Mid-Corner')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### 2. Scalar Multiplication\nMultiplying a vector by a scalar scales its magnitude (and flips direction if negative).\n\n**F1 analogy**: Imagine a car's velocity vector. Hitting DRS on the straight effectively scales that velocity vector — same direction, more magnitude. Braking is like multiplying by a fraction (shrinking). And spinning out? That's multiplying by -1 — same line, opposite direction."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Scalar multiplication: DRS boost, braking, and spinning\ncar_velocity = np.array([2, 1])     # car heading mostly forward, slight lateral\ndrs_boost = 2 * car_velocity        # DRS open — double the speed!\nspin_out = -1 * car_velocity        # car spins — velocity reverses\n\nprint(f\"Car velocity     = {car_velocity}\")\nprint(f\"DRS boost (2x)   = {drs_boost}\")\nprint(f\"Spin out (-1x)   = {spin_out}\")\n\nplot_vectors([car_velocity, drs_boost, spin_out], ['blue', 'green', 'red'],\n             ['Normal pace', 'DRS boost (2x)', 'Spin out (-1x)'])\nplt.title('Scalar Multiplication: Speed Changes on Track')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### 3. Dot Product\n\nThe **dot product** (inner product) of two vectors is fundamental:\n\n$$\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{n} a_i b_i = |\\mathbf{a}| |\\mathbf{b}| \\cos\\theta$$\n\nWhere $\\theta$ is the angle between the vectors.\n\n**F1 analogy**: Think of the dot product as measuring **alignment between two things**:\n- **Setup vs. track**: A car's setup is a vector of parameters [downforce, ride_height, tire_pressure, ...]. A track's demands are another vector. The dot product tells you *\"how well does this setup match what the track needs?\"*\n- **Slipstream**: When car B is directly behind car A (vectors aligned), the slipstream effect is maximized. At 90 degrees (side by side), there's no drafting benefit.\n\n**Key insights:**\n- If dot product = 0, vectors are **orthogonal** (perpendicular) — like a car's speed and crosswind being independent\n- If positive, vectors point in similar directions — setup matches the track\n- If negative, vectors point in opposite directions — a setup optimized for Monza (low downforce) at Monaco (high downforce needed)\n- Used everywhere in neural networks: weighted sums!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Dot product: How well does a car setup match the track?\n# Imagine simplified setup vectors: [downforce, mechanical_grip]\n\nmonza_demands = np.array([1, 0])     # Monza = all about low drag / straight-line speed\nmonaco_demands = np.array([0, 1])    # Monaco = all about mechanical grip\nsilverstone_demands = np.array([1, 1])  # Silverstone = needs both\n\n# How well does a high-downforce setup match each track?\nhigh_df_setup = np.array([1, 1])\n\nprint(f\"High-downforce setup · Monza demands = {np.dot(high_df_setup, monza_demands)}\")\nprint(f\"High-downforce setup · Monaco demands = {np.dot(high_df_setup, monaco_demands)}\")\nprint(f\"High-downforce setup · Silverstone demands = {np.dot(high_df_setup, silverstone_demands)}\")\n\n# Using @ operator (preferred in modern NumPy)\nprint(f\"\\nUsing @ operator: setup @ silverstone = {high_df_setup @ silverstone_demands}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Computing angle between vectors using dot product\n# How different are the driving lines of two cars through a corner?\ndef angle_between(v1, v2):\n    \"\"\"Returns angle in degrees between vectors v1 and v2.\"\"\"\n    cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n    # Clip to handle numerical errors\n    cos_angle = np.clip(cos_angle, -1, 1)\n    return np.degrees(np.arccos(cos_angle))\n\n# Compare driving lines through Turn 1\nverstappen_line = np.array([1, 0])   # late apex, straight exit\nhamilton_line = np.array([1, 1])     # wider entry, carries more speed\nnorris_line = np.array([0, 1])      # very different line\nopposite_line = np.array([-1, 0])   # going the wrong way!\n\nprint(f\"VER vs HAM lines: {angle_between(verstappen_line, hamilton_line):.1f}° apart\")\nprint(f\"VER vs NOR lines: {angle_between(verstappen_line, norris_line):.1f}° apart\")\nprint(f\"VER vs wrong way: {angle_between(verstappen_line, opposite_line):.1f}° apart\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: Understanding the Dot Product Formula\n\nThere are **two equivalent ways** to define the dot product:\n\n**Definition 1 - Algebraic (how we compute it):**\n$$\\mathbf{a} \\cdot \\mathbf{b} = a_1 b_1 + a_2 b_2 + \\ldots + a_n b_n$$\n\n**Definition 2 - Geometric (what it means):**\n$$\\mathbf{a} \\cdot \\mathbf{b} = |\\mathbf{a}| \\cdot |\\mathbf{b}| \\cdot \\cos(\\theta)$$\n\nThese are mathematically proven to be equal (using the Law of Cosines).\n\n#### Breaking down the geometric formula:\n\n| Component | Meaning | F1 Analogy | Range |\n|-----------|---------|------------|-------|\n| $\\|\\mathbf{a}\\|$ | Length of vector a | How strong the car's setup preference is | 0 to ∞ |\n| $\\|\\mathbf{b}\\|$ | Length of vector b | How demanding the track's characteristics are | 0 to ∞ |\n| $\\cos(\\theta)$ | \"Alignment factor\" based on angle | How well setup matches track demands | -1 to 1 |\n\n#### What does cos(θ) do? Think of it as a \"match score\":\n\n| Angle θ | cos(θ) | F1 Meaning | Dot product |\n|---------|--------|------------|-------------|\n| 0° | 1 | Setup perfectly matches track (Red Bull at any track in 2023) | Maximum positive |\n| 45° | 0.71 | Decent match (a balanced setup at a mixed circuit) | Positive |\n| 90° | 0 | Completely independent (tire compound vs. wind direction) | Zero |\n| 135° | -0.71 | Poor match (high downforce setup at Monza) | Negative |\n| 180° | -1 | Exact opposite of what's needed | Maximum negative |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Interactive visualization: How dot product changes with angle\n# Keep vector 'a' fixed, rotate vector 'b' around\n\na = np.array([2, 0])  # Fixed vector pointing right\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left plot: Show vectors at different angles\nangles_deg = [0, 45, 90, 135, 180]\ncolors = ['green', 'blue', 'orange', 'red', 'purple']\n\naxes[0].quiver(0, 0, a[0], a[1], angles='xy', scale_units='xy', scale=1, \n               color='black', width=0.03, label='a (fixed)')\n\nfor angle, color in zip(angles_deg, colors):\n    theta = np.radians(angle)\n    b = 1.5 * np.array([np.cos(theta), np.sin(theta)])  # |b| = 1.5\n    dot = a @ b\n    axes[0].quiver(0, 0, b[0], b[1], angles='xy', scale_units='xy', scale=1,\n                   color=color, width=0.02, alpha=0.7, label=f'θ={angle}°, a·b={dot:.2f}')\n\naxes[0].set_xlim(-3, 3)\naxes[0].set_ylim(-2, 2)\naxes[0].set_aspect('equal')\naxes[0].axhline(y=0, color='k', linewidth=0.5)\naxes[0].axvline(x=0, color='k', linewidth=0.5)\naxes[0].legend(loc='upper left', fontsize=9)\naxes[0].set_title('Vector b at different angles from a')\naxes[0].grid(True, alpha=0.3)\n\n# Right plot: Dot product as function of angle\nangles = np.linspace(0, 360, 100)\ndot_products = []\nfor angle in angles:\n    theta = np.radians(angle)\n    b = 1.5 * np.array([np.cos(theta), np.sin(theta)])\n    dot_products.append(a @ b)\n\naxes[1].plot(angles, dot_products, 'b-', linewidth=2)\naxes[1].axhline(y=0, color='k', linewidth=1)\naxes[1].set_xlabel('Angle θ (degrees)')\naxes[1].set_ylabel('Dot product (a · b)')\naxes[1].set_title('Dot product vs angle between vectors\\n|a|=2, |b|=1.5, so max = 2×1.5 = 3')\naxes[1].set_xticks([0, 45, 90, 135, 180, 225, 270, 315, 360])\naxes[1].grid(True, alpha=0.3)\n\n# Mark key points\nfor angle, color in zip(angles_deg, colors):\n    theta = np.radians(angle)\n    b = 1.5 * np.array([np.cos(theta), np.sin(theta)])\n    dot = a @ b\n    axes[1].scatter([angle], [dot], color=color, s=100, zorder=5)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key insight: The dot product follows a cosine curve!\")\nprint(\"This is because a·b = |a||b|cos(θ), and we're varying θ.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### The Projection Interpretation\n\nAnother powerful way to understand dot product: **projection**.\n\nThe dot product $\\mathbf{a} \\cdot \\mathbf{b}$ tells you: *\"How much of b points in the direction of a?\"*\n\nMore precisely:\n$$\\mathbf{a} \\cdot \\mathbf{b} = |\\mathbf{a}| \\times (\\text{length of b's shadow onto a})$$\n\nThis \"shadow\" is called the **scalar projection** of b onto a.\n\n**F1 analogy**: Imagine a car exits a corner not perfectly straight — its velocity has both a forward component and a sideways component. The **projection** onto the straight tells you: *\"How much of my speed is actually useful for going down the straight?\"* The sideways part is wasted — it doesn't help your lap time.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualizing projection — car exit speed projected onto the straight\nstraight_direction = np.array([3, 0])  # the straight runs along the x-axis\ncar_exit_velocity = np.array([2, 2])   # car exits corner at 45 degrees\n\n# Scalar projection of car velocity onto the straight: (a·b) / |a|\nscalar_proj = (straight_direction @ car_exit_velocity) / np.linalg.norm(straight_direction)\n\n# Vector projection: scalar_proj * unit vector of straight\nstraight_unit = straight_direction / np.linalg.norm(straight_direction)\nvector_proj = scalar_proj * straight_unit\n\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Draw vectors\nax.quiver(0, 0, straight_direction[0], straight_direction[1], angles='xy', scale_units='xy', scale=1,\n          color='blue', width=0.02, label=f'Straight direction')\nax.quiver(0, 0, car_exit_velocity[0], car_exit_velocity[1], angles='xy', scale_units='xy', scale=1,\n          color='red', width=0.02, label=f'Car exit velocity')\n\n# Draw projection (useful speed)\nax.quiver(0, 0, vector_proj[0], vector_proj[1], angles='xy', scale_units='xy', scale=1,\n          color='green', width=0.025, label=f'Useful speed (projection)')\n\n# Draw dashed line from car velocity to its projection (wasted lateral speed)\nax.plot([car_exit_velocity[0], vector_proj[0]], [car_exit_velocity[1], vector_proj[1]],\n        'k--', linewidth=1.5, alpha=0.5, label='Wasted lateral speed')\n\n# Annotations\nax.annotate('', xy=(vector_proj[0], -0.3), xytext=(0, -0.3),\n            arrowprops=dict(arrowstyle='<->', color='green'))\nax.text(vector_proj[0]/2, -0.6, f'useful speed = {scalar_proj:.2f}', ha='center', fontsize=11, color='green')\n\nax.set_xlim(-1, 4)\nax.set_ylim(-1, 3)\nax.set_aspect('equal')\nax.axhline(y=0, color='k', linewidth=0.5)\nax.axvline(x=0, color='k', linewidth=0.5)\nax.legend(loc='upper left')\nax.set_title('Corner Exit: How Much Speed is Actually Useful?')\nax.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"Car exit speed: {np.linalg.norm(car_exit_velocity):.2f} m/s total\")\nprint(f\"Useful speed along the straight: {scalar_proj:.2f} m/s\")\nprint(f\"Wasted lateral speed: {np.sqrt(np.linalg.norm(car_exit_velocity)**2 - scalar_proj**2):.2f} m/s\")\nprint(f\"\\nThis is why corner exit matters so much — you want max projection onto the straight!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Why Dot Product Matters in Machine Learning (and F1)\n\nThe dot product appears everywhere because it answers: **\"How similar are these two vectors?\"**\n\n| Application | What the dot product computes | F1 Parallel |\n|-------------|-------------------------------|-------------|\n| **Neural network layer** | `w · x + b` = \"How much does input match what this neuron looks for?\" | How well telemetry matches a known fast-lap pattern |\n| **Word embeddings** | `word1 · word2` = \"How semantically similar?\" | How similar are two drivers' styles? |\n| **Attention (Transformers)** | `query · key` = \"How relevant is this key to this query?\" | \"Which past laps are most relevant to predicting this one?\" |\n| **Recommendation systems** | `user · item` = \"How much would this user like this item?\" | \"How well would Verstappen perform at a new circuit?\" |\n| **Cosine similarity** | `(a · b) / (\\|a\\| \\|b\\|)` = Pure directional similarity | Comparing driving styles regardless of overall pace |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### 4. Vector Norm (Magnitude/Length)\n\nThe **L2 norm** (Euclidean length) of a vector:\n\n$$||\\mathbf{v}||_2 = \\sqrt{\\sum_{i=1}^{n} v_i^2}$$\n\n**F1 analogy**: The norm is the **total magnitude** of a vector. For a velocity vector, it's the car's actual speed. For a telemetry vector, it captures the overall \"intensity\" of the car's state.\n\nOther norms used in ML:\n- **L1 norm**: $||\\mathbf{v}||_1 = \\sum |v_i|$ (Manhattan distance, used for sparsity)\n- **L∞ norm**: $||\\mathbf{v}||_\\infty = \\max |v_i|$"
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: What is a Vector Norm?\n\nA **norm** measures the \"size\" or \"length\" of a vector. Think of it as answering: *\"How far is this point from the origin?\"*\n\n#### The L2 (Euclidean) Norm - Most Common\n\n$$||\\mathbf{v}||_2 = \\sqrt{v_1^2 + v_2^2 + \\ldots + v_n^2}$$\n\nThis is just the **Pythagorean theorem** extended to n dimensions!\n\nFor a car's velocity `v = [3, 4]` m/s (3 m/s forward, 4 m/s lateral through a corner):\n\n$||v|| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5$ m/s actual speed\n\nThe speed gun reads 5 m/s — regardless of the split between forward and lateral motion.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualizing the L2 norm: car speed through a corner\nv = np.array([3, 4])  # 3 m/s forward, 4 m/s lateral\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\n# Draw the velocity vector\nax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1,\n          color='blue', width=0.02, label=f'Actual speed = {np.linalg.norm(v)} m/s')\n\n# Draw the right triangle (decomposed into forward and lateral)\nax.plot([0, v[0]], [0, 0], 'g-', linewidth=2, label=f'Forward speed = {v[0]} m/s')\nax.plot([v[0], v[0]], [0, v[1]], 'r-', linewidth=2, label=f'Lateral speed = {v[1]} m/s')\n\n# Right angle marker\nax.plot([v[0]-0.2, v[0]-0.2, v[0]], [0, 0.2, 0.2], 'k-', linewidth=1)\n\n# Labels\nax.text(v[0]/2, -0.4, '3 m/s', ha='center', fontsize=14, color='green')\nax.text(v[0]+0.4, v[1]/2, '4 m/s', ha='center', fontsize=14, color='red')\nax.text(v[0]/2 - 0.5, v[1]/2 + 0.3, '5 m/s', ha='center', fontsize=14, color='blue')\n\nax.set_xlim(-1, 6)\nax.set_ylim(-1, 6)\nax.set_aspect('equal')\nax.axhline(y=0, color='k', linewidth=0.5)\nax.axvline(x=0, color='k', linewidth=0.5)\nax.legend(loc='upper left')\nax.set_title('Car Speed Through a Corner: L2 Norm = Pythagorean Theorem\\n||v|| = √(3² + 4²) = 5 m/s')\nax.grid(True, alpha=0.3)\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Comparing Different Norms\n\nDifferent norms measure \"size\" differently — and this matters when evaluating F1 performance:\n\n| Norm | Formula | Intuition | F1 Analogy | Use in ML |\n|------|---------|-----------|------------|-----------|\n| **L2** | $\\sqrt{\\sum v_i^2}$ | Straight-line distance | Actual car speed (Pythagorean) | Default distance, weight decay |\n| **L1** | $\\sum \\|v_i\\|$ | \"Taxicab\" distance | Sum of all individual forces acting on the car | Sparsity (Lasso), makes weights exactly 0 |\n| **L∞** | $\\max \\|v_i\\|$ | Largest single component | The single highest g-force in any direction (peak stress) | Worst-case bounds |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualize \"unit balls\" - all points where ||v|| = 1 for different norms\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\ntheta = np.linspace(0, 2*np.pi, 100)\n\n# L2 norm: circle (x² + y² = 1)\nx_l2 = np.cos(theta)\ny_l2 = np.sin(theta)\naxes[0].plot(x_l2, y_l2, 'b-', linewidth=2)\naxes[0].fill(x_l2, y_l2, alpha=0.2)\naxes[0].set_title('L2 Norm (Euclidean)\\n||v||₂ = √(x² + y²) = 1\\nCircle')\naxes[0].set_xlabel('x')\naxes[0].set_ylabel('y')\n\n# L1 norm: diamond (|x| + |y| = 1)\nx_l1 = [1, 0, -1, 0, 1]\ny_l1 = [0, 1, 0, -1, 0]\naxes[1].plot(x_l1, y_l1, 'r-', linewidth=2)\naxes[1].fill(x_l1, y_l1, alpha=0.2, color='red')\naxes[1].set_title('L1 Norm (Manhattan)\\n||v||₁ = |x| + |y| = 1\\nDiamond')\naxes[1].set_xlabel('x')\naxes[1].set_ylabel('y')\n\n# L∞ norm: square (max(|x|, |y|) = 1)\nx_linf = [1, 1, -1, -1, 1]\ny_linf = [1, -1, -1, 1, 1]\naxes[2].plot(x_linf, y_linf, 'g-', linewidth=2)\naxes[2].fill(x_linf, y_linf, alpha=0.2, color='green')\naxes[2].set_title('L∞ Norm (Max)\\n||v||∞ = max(|x|, |y|) = 1\\nSquare')\naxes[2].set_xlabel('x')\naxes[2].set_ylabel('y')\n\nfor ax in axes:\n    ax.set_xlim(-1.5, 1.5)\n    ax.set_ylim(-1.5, 1.5)\n    ax.set_aspect('equal')\n    ax.axhline(y=0, color='k', linewidth=0.5)\n    ax.axvline(x=0, color='k', linewidth=0.5)\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Example with a specific vector\nv = np.array([3, 4])\nprint(f\"For v = {v}:\")\nprint(f\"  L2 norm: ||v||₂ = √(3² + 4²) = {np.linalg.norm(v, ord=2)}\")\nprint(f\"  L1 norm: ||v||₁ = |3| + |4| = {np.linalg.norm(v, ord=1)}\")\nprint(f\"  L∞ norm: ||v||∞ = max(|3|, |4|) = {np.linalg.norm(v, ord=np.inf)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Why Norms Matter in Machine Learning (and F1 Engineering)\n\n| Use Case | How Norms are Used | F1 Parallel |\n|----------|-------------------|-------------|\n| **Normalization** | Divide by norm to get unit vector: `v / \\|\\|v\\|\\|`. Isolates direction from magnitude. | Comparing driving *lines* regardless of speed |\n| **Regularization** | Add `λ\\|\\|weights\\|\\|²` to loss. Keeps weights small → prevents overfitting. | Budget cap keeping team spending in check |\n| **Distance** | Distance between points: `\\|\\|a - b\\|\\|`. Used in k-NN, clustering. | Gap between cars in qualifying lap times |\n| **Gradient clipping** | If `\\|\\|gradient\\|\\| > threshold`, scale it down. Prevents exploding gradients. | Rev limiter — cap the engine RPM to prevent blowup |\n| **Embedding similarity** | Normalize embeddings so dot product = cosine similarity. | Comparing driver styles regardless of car performance |\n\n#### Connecting Dot Product and Norm\n\nThe dot product of a vector with itself gives the **squared norm**:\n\n$$\\mathbf{v} \\cdot \\mathbf{v} = v_1^2 + v_2^2 + \\ldots = ||\\mathbf{v}||^2$$\n\nSo: $||\\mathbf{v}|| = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}}$\n\n*Speed² = forward² + lateral² — a car's kinetic energy is proportional to the squared norm of its velocity!*",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Norms in F1 context: g-forces on a car\ng_forces = np.array([3, 4])  # 3g lateral, 4g longitudinal (hard braking into corner)\n\n# L2 norm (default) — total g-force magnitude\nl2_norm = np.linalg.norm(g_forces)\nprint(f\"G-forces [lateral, longitudinal]: {g_forces}\")\nprint(f\"Total g-force (L2 norm): {l2_norm}g\")  # Pythagorean: 5g total\n\n# L1 norm — sum of all forces\nl1_norm = np.linalg.norm(g_forces, ord=1)\nprint(f\"Sum of forces (L1 norm): {l1_norm}g\")  # 3 + 4 = 7g\n\n# L∞ norm — peak force in any single direction\nlinf_norm = np.linalg.norm(g_forces, ord=np.inf)\nprint(f\"Peak single-axis g-force (L∞ norm): {linf_norm}g\")  # max(3, 4) = 4g\n\n# Unit vector (normalize) — isolate the direction of the g-force\ng_unit = g_forces / np.linalg.norm(g_forces)\nprint(f\"\\nG-force direction (unit vector): {g_unit}\")\nprint(f\"Magnitude of unit vector: {np.linalg.norm(g_unit)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 2. Matrices\n\nA **matrix** is a 2D array of numbers. In deep learning:\n- Weight matrices connect layers\n- Batches of data are matrices (rows = samples, columns = features)\n- Attention scores form matrices\n\n### The F1 Connection\n\nMatrices are everywhere in F1 engineering:\n- **Telemetry data**: Each row is a time sample, each column is a sensor → a matrix of the entire lap\n- **Race results**: Rows = drivers, columns = races → a season performance matrix\n- **Setup parameters**: A matrix can represent how changing one setting affects multiple outputs\n\n### Matrix as Linear Transformation\n\nA matrix transforms vectors from one space to another. In F1 terms: a setup change matrix takes the car's base performance vector and transforms it into a new performance profile."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Creating matrices — a lap telemetry snapshot\n# Rows = time samples, Columns = [speed_kph, throttle_pct]\ntelemetry_matrix = np.array([[310, 100],   # Full throttle on straight\n                             [280, 80],    # Lifting slightly\n                             [120, 0],     # Hard braking zone\n                             [95, 30]])    # Apex of corner\n\nprint(f\"Telemetry matrix (4 time samples x 2 channels):\\n{telemetry_matrix}\")\nprint(f\"Shape: {telemetry_matrix.shape}\")\nprint(f\"Number of time samples: {telemetry_matrix.shape[0]}\")\nprint(f\"Number of channels: {telemetry_matrix.shape[1]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Matrix-Vector Multiplication\n\nMatrix $\\mathbf{A}$ (m×n) times vector $\\mathbf{v}$ (n×1) produces vector (m×1):\n\n$$\\mathbf{Av} = \\begin{bmatrix} \\mathbf{a}_1 \\cdot \\mathbf{v} \\\\ \\mathbf{a}_2 \\cdot \\mathbf{v} \\\\ \\vdots \\\\ \\mathbf{a}_m \\cdot \\mathbf{v} \\end{bmatrix}$$\n\nEach element is a dot product of a row of A with vector v.\n\n**F1 analogy**: Think of the matrix as a \"setup change\" and the vector as the car's current performance. The matrix-vector multiplication produces the car's *new* performance after the setup change. Each row of the matrix defines how one output metric (e.g., straight-line speed, cornering grip) depends on the input parameters."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Matrix-vector multiplication: a \"Monza trim\" setup change\n# This setup boosts straight-line speed (x) by 2x, keeps cornering (y) the same\nmonza_setup = np.array([[2, 0],    # straight-line speed doubled\n                        [0, 1]])   # cornering unchanged\nbase_performance = np.array([1, 1])  # balanced car\n\nnew_performance = monza_setup @ base_performance  # or np.dot(monza_setup, base_performance)\nprint(f\"Monza setup applied: {new_performance}\")\nprint(\"Straight-line speed doubled, cornering unchanged — classic low-downforce trim!\")\n\nplot_vectors([base_performance, new_performance], ['blue', 'red'],\n             ['Base car (balanced)', 'Monza trim (low downforce)'])\nplt.title('Matrix as Setup Change: Going to Monza Spec')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Rotation matrix — steering input rotates the car's velocity vector\n# Turning 90 degrees into a hairpin\ntheta = np.pi / 2  # 90 degrees\nsteering_rotation = np.array([[np.cos(theta), -np.sin(theta)],\n                              [np.sin(theta),  np.cos(theta)]])\n\napproach_velocity = np.array([1, 0])  # car heading straight along the straight\nexit_velocity = steering_rotation @ approach_velocity\n\nprint(f\"Steering rotation matrix:\\n{steering_rotation.round(3)}\")\nprint(f\"Approach velocity (on straight): {approach_velocity}\")\nprint(f\"Exit velocity (after 90° turn):  {exit_velocity.round(3)}\")\n\nplot_vectors([approach_velocity, exit_velocity], ['blue', 'red'],\n             ['Approach (on straight)', 'Exit (after 90° hairpin)'])\nplt.title('Rotation Matrix: Turning into a Hairpin')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Visualizing Linear Transformations\n\nLet's see how different matrices transform a grid of points — like watching how a setup change warps the entire performance envelope of a car."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transformation(A, title):\n",
    "    \"\"\"Visualize how matrix A transforms a unit square.\"\"\"\n",
    "    # Create a grid of points\n",
    "    n = 10\n",
    "    x = np.linspace(-1, 1, n)\n",
    "    y = np.linspace(-1, 1, n)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Original grid\n",
    "    for xi in x:\n",
    "        axes[0].plot([xi, xi], [-1, 1], 'b-', alpha=0.5)\n",
    "    for yi in y:\n",
    "        axes[0].plot([-1, 1], [yi, yi], 'b-', alpha=0.5)\n",
    "    # Highlight basis vectors\n",
    "    axes[0].quiver(0, 0, 1, 0, angles='xy', scale_units='xy', scale=1, color='red', width=0.02)\n",
    "    axes[0].quiver(0, 0, 0, 1, angles='xy', scale_units='xy', scale=1, color='green', width=0.02)\n",
    "    axes[0].set_xlim(-2, 2)\n",
    "    axes[0].set_ylim(-2, 2)\n",
    "    axes[0].set_aspect('equal')\n",
    "    axes[0].set_title('Original Space')\n",
    "    axes[0].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[0].axvline(x=0, color='k', linewidth=0.5)\n",
    "    \n",
    "    # Transformed grid\n",
    "    for xi in x:\n",
    "        points = np.array([[xi, yi] for yi in y])\n",
    "        transformed = (A @ points.T).T\n",
    "        axes[1].plot(transformed[:, 0], transformed[:, 1], 'b-', alpha=0.5)\n",
    "    for yi in y:\n",
    "        points = np.array([[xi, yi] for xi in x])\n",
    "        transformed = (A @ points.T).T\n",
    "        axes[1].plot(transformed[:, 0], transformed[:, 1], 'b-', alpha=0.5)\n",
    "    \n",
    "    # Transformed basis vectors\n",
    "    e1_transformed = A @ np.array([1, 0])\n",
    "    e2_transformed = A @ np.array([0, 1])\n",
    "    axes[1].quiver(0, 0, e1_transformed[0], e1_transformed[1], angles='xy', scale_units='xy', scale=1, color='red', width=0.02)\n",
    "    axes[1].quiver(0, 0, e2_transformed[0], e2_transformed[1], angles='xy', scale_units='xy', scale=1, color='green', width=0.02)\n",
    "    \n",
    "    axes[1].set_xlim(-2, 2)\n",
    "    axes[1].set_ylim(-2, 2)\n",
    "    axes[1].set_aspect('equal')\n",
    "    axes[1].set_title(f'After Transformation: {title}')\n",
    "    axes[1].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[1].axvline(x=0, color='k', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Matrix:\\n{A}\")\n",
    "    print(f\"Red basis vector [1,0] -> {e1_transformed}\")\n",
    "    print(f\"Green basis vector [0,1] -> {e2_transformed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Monza trim: boost straight-line speed, sacrifice cornering\nmonza_trim = np.array([[1.5, 0],\n                       [0, 0.5]])\nplot_transformation(monza_trim, \"Monza Trim (1.5x speed, 0.5x cornering)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Car turning through a 30-degree sweeping corner\ntheta = np.pi / 6  # 30 degrees\nturn_30 = np.array([[np.cos(theta), -np.sin(theta)],\n                    [np.sin(theta),  np.cos(theta)]])\nplot_transformation(turn_30, \"30° Corner (Rotation)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Crosswind effect: wind shears the car's trajectory sideways\ncrosswind = np.array([[1, 0.5],   # x-velocity gets a lateral push\n                      [0, 1]])     # y-velocity unaffected\nplot_transformation(crosswind, \"Crosswind Shear Effect\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: Understanding Matrices as Transformations\n\n**Key Insight**: A matrix doesn't just \"do math\" - it describes a geometric transformation. Every matrix is a machine that takes vectors in and outputs transformed vectors.\n\nIn F1 terms: a matrix is like an **engineering change** to the car. Feed in the car's current performance profile, and the matrix spits out the new one.\n\n#### What Do the Columns of a Matrix Mean?\n\nHere's the most important insight about matrices:\n\n> **The columns of a matrix tell you where the basis vectors land after transformation.**\n\nFor a 2D matrix $\\mathbf{A} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$:\n- **Column 1** $\\begin{bmatrix} a \\\\ c \\end{bmatrix}$ = where the vector $\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ (pure straight-line speed) lands\n- **Column 2** $\\begin{bmatrix} b \\\\ d \\end{bmatrix}$ = where the vector $\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ (pure cornering grip) lands\n\nThis means: **to design a transformation, just decide where you want the basis vectors to go!**\n\n*Imagine you're the chief engineer: \"I want pure straight-line speed to also give us some cornering (column 1), and pure cornering to stay as cornering (column 2).\" You just designed a matrix!*",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Demonstration: Columns of a matrix = where basis vectors land\n# Let's verify this with an example\n\nA = np.array([[2, -1],\n              [1,  1]])\n\n# Standard basis vectors\ne1 = np.array([1, 0])  # Points right\ne2 = np.array([0, 1])  # Points up\n\n# Transform them\nAe1 = A @ e1\nAe2 = A @ e2\n\nprint(\"Matrix A:\")\nprint(A)\nprint(f\"\\nColumn 1 of A: {A[:, 0]}\")\nprint(f\"A @ [1,0] = {Ae1}\")\nprint(f\"Same? {np.allclose(A[:, 0], Ae1)}\")\n\nprint(f\"\\nColumn 2 of A: {A[:, 1]}\")\nprint(f\"A @ [0,1] = {Ae2}\")\nprint(f\"Same? {np.allclose(A[:, 1], Ae2)}\")\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Before transformation\naxes[0].quiver(0, 0, 1, 0, angles='xy', scale_units='xy', scale=1, color='red', width=0.02, label='e1 = [1,0]')\naxes[0].quiver(0, 0, 0, 1, angles='xy', scale_units='xy', scale=1, color='green', width=0.02, label='e2 = [0,1]')\naxes[0].set_xlim(-2, 3)\naxes[0].set_ylim(-2, 3)\naxes[0].set_aspect('equal')\naxes[0].axhline(y=0, color='k', linewidth=0.5)\naxes[0].axvline(x=0, color='k', linewidth=0.5)\naxes[0].grid(True, alpha=0.3)\naxes[0].legend()\naxes[0].set_title('BEFORE: Standard Basis Vectors')\n\n# After transformation\naxes[1].quiver(0, 0, Ae1[0], Ae1[1], angles='xy', scale_units='xy', scale=1, color='red', width=0.02, \n               label=f'A @ e1 = {Ae1} (Column 1)')\naxes[1].quiver(0, 0, Ae2[0], Ae2[1], angles='xy', scale_units='xy', scale=1, color='green', width=0.02, \n               label=f'A @ e2 = {Ae2} (Column 2)')\naxes[1].set_xlim(-2, 3)\naxes[1].set_ylim(-2, 3)\naxes[1].set_aspect('equal')\naxes[1].axhline(y=0, color='k', linewidth=0.5)\naxes[1].axvline(x=0, color='k', linewidth=0.5)\naxes[1].grid(True, alpha=0.3)\naxes[1].legend()\naxes[1].set_title('AFTER: Basis Vectors = Columns of A')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey insight: Reading the columns of A directly tells you the transformation!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Common 2D Transformation Matrices (with F1 Intuition)\n\nOnce you understand \"columns = where basis vectors go,\" you can read or construct any transformation:\n\n| Transformation | Matrix | F1 Intuition |\n|----------------|--------|--------------|\n| **Identity** (do nothing) | $\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$ | Keep the car as-is |\n| **Scale by k** | $\\begin{bmatrix} k & 0 \\\\ 0 & k \\end{bmatrix}$ | Uniform upgrade (better engine + better aero) |\n| **Scale x by a, y by b** | $\\begin{bmatrix} a & 0 \\\\ 0 & b \\end{bmatrix}$ | Monza trim: boost speed (a), sacrifice cornering (b) |\n| **Rotate by θ** | $\\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix}$ | Steering: rotate the car's velocity vector |\n| **Reflect across x-axis** | $\\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}$ | Mirror the car's lateral behavior (left→right) |\n| **Shear (horizontal)** | $\\begin{bmatrix} 1 & k \\\\ 0 & 1 \\end{bmatrix}$ | Crosswind: lateral force adds to forward speed |\n| **Project onto x-axis** | $\\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix}$ | Ignore cornering entirely — only straight-line speed matters |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Why Matrix Multiplication is Composition of Transformations\n\nWhen you multiply matrices $\\mathbf{AB}$, you're creating a new transformation that does **B first, then A**.\n\n**Think of it this way:**\n- To apply $\\mathbf{AB}$ to vector $\\mathbf{v}$: $(\\mathbf{AB})\\mathbf{v} = \\mathbf{A}(\\mathbf{B}\\mathbf{v})$\n- First B transforms v, then A transforms the result\n\n**F1 analogy**: It's like applying multiple setup changes in sequence. First the team bolts on a new front wing (matrix B), then they adjust ride height (matrix A). The combined effect (AB) is a single matrix that captures both changes.\n\n**Why the \"backwards\" order?** Because we read left-to-right but function application is right-to-left: $f(g(x))$ applies g first, then f. Just like the engineer who installs the wing first, then adjusts ride height — the final setup AB reads \"ride height change applied to wing change.\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Demonstration: Composing setup changes\n# Step 1: Scale (Monza aero trim: 2x speed, 0.5x cornering)\n# Step 2: Rotate (car turns 45 degrees through a sweeping corner)\n\ntheta = np.pi / 4  # 45 degree corner\nCorner = np.array([[np.cos(theta), -np.sin(theta)],\n                   [np.sin(theta),  np.cos(theta)]])\n\nAeroTrim = np.array([[2.0, 0],\n                     [0, 0.5]])\n\n# Compose: AeroTrim first, then Corner (remember: right-to-left!)\n# So we write: Corner @ AeroTrim\nFullSetup = Corner @ AeroTrim\n\nprint(\"Corner (45° turn):\")\nprint(Corner.round(3))\nprint(\"\\nAero Trim (2x speed, 0.5x cornering):\")\nprint(AeroTrim)\nprint(\"\\nComposed (Corner @ AeroTrim) — trim first, then turn:\")\nprint(FullSetup.round(3))\n\n# Visualize the composition\nfig, axes = plt.subplots(1, 4, figsize=(20, 5))\nv = np.array([1, 1])  # balanced car performance\n\naxes[0].quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='blue', width=0.02)\naxes[0].set_title('Base car [1, 1]')\n\nv_trimmed = AeroTrim @ v\naxes[1].quiver(0, 0, v_trimmed[0], v_trimmed[1], angles='xy', scale_units='xy', scale=1, color='green', width=0.02)\naxes[1].set_title(f'After Monza trim: {v_trimmed}')\n\nv_trimmed_cornered = Corner @ v_trimmed\naxes[2].quiver(0, 0, v_trimmed_cornered[0], v_trimmed_cornered[1], angles='xy', scale_units='xy', scale=1, color='red', width=0.02)\naxes[2].set_title(f'Then through corner:\\n{v_trimmed_cornered.round(3)}')\n\nv_composed = FullSetup @ v\naxes[3].quiver(0, 0, v_composed[0], v_composed[1], angles='xy', scale_units='xy', scale=1, color='purple', width=0.02)\naxes[3].set_title(f'Composed matrix:\\n{v_composed.round(3)}')\n\nfor ax in axes:\n    ax.set_xlim(-3, 3)\n    ax.set_ylim(-2, 2)\n    ax.set_aspect('equal')\n    ax.axhline(y=0, color='k', linewidth=0.5)\n    ax.axvline(x=0, color='k', linewidth=0.5)\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTwo-step result: {v_trimmed_cornered.round(6)}\")\nprint(f\"Composed result: {v_composed.round(6)}\")\nprint(f\"Same? {np.allclose(v_trimmed_cornered, v_composed)}\")\nprint(\"\\nKey insight: (Corner @ AeroTrim) @ v = Corner @ (AeroTrim @ v)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Matrix-Matrix Multiplication\n\nIf $\\mathbf{A}$ is (m×n) and $\\mathbf{B}$ is (n×p), then $\\mathbf{AB}$ is (m×p).\n\n**Key insight**: Matrix multiplication = composition of transformations.\n\nIf A is the corner transformation and B is the aero trim, then AB does both — aero trim first, then the corner. One single matrix captures the combined effect of multiple engineering changes."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "B = np.array([[5, 6],\n",
    "              [7, 8]])\n",
    "\n",
    "C = A @ B\n",
    "print(f\"A:\\n{A}\\n\")\n",
    "print(f\"B:\\n{B}\\n\")\n",
    "print(f\"A @ B:\\n{C}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Implement matrix multiplication from scratch\n",
    "def matmul(A, B):\n",
    "    \"\"\"\n",
    "    Multiply matrices A and B.\n",
    "    A: (m, n) matrix\n",
    "    B: (n, p) matrix\n",
    "    Returns: (m, p) matrix\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    n2, p = B.shape\n",
    "    assert n == n2, f\"Incompatible dimensions: {A.shape} and {B.shape}\"\n",
    "    \n",
    "    # TODO: Implement this!\n",
    "    # Hint: C[i,j] = sum over k of A[i,k] * B[k,j]\n",
    "    C = np.zeros((m, p))\n",
    "    \n",
    "    # Your code here\n",
    "    for i in range(m):\n",
    "        for j in range(p):\n",
    "            for k in range(n):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "    \n",
    "    return C\n",
    "\n",
    "# Test your implementation\n",
    "result = matmul(A, B)\n",
    "expected = A @ B\n",
    "print(f\"Your result:\\n{result}\")\n",
    "print(f\"Expected:\\n{expected}\")\n",
    "print(f\"Correct: {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Properties\n",
    "\n",
    "#### Transpose\n",
    "Swap rows and columns: $(\\mathbf{A}^T)_{ij} = \\mathbf{A}_{ji}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "print(f\"A (2x3):\\n{A}\\n\")\n",
    "print(f\"A^T (3x2):\\n{A.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Identity Matrix\nThe \"do nothing\" transformation. $\\mathbf{IA} = \\mathbf{AI} = \\mathbf{A}$\n\n*Like running the car in its default configuration — no setup changes applied.*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.eye(3)  # 3x3 identity matrix\n",
    "print(f\"Identity matrix:\\n{I}\")\n",
    "\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "print(f\"\\nA @ I = A: {np.allclose(A @ I, A)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Matrix Inverse\n\nThe inverse $\\mathbf{A}^{-1}$ \"undoes\" the transformation: $\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}$\n\nNot all matrices have inverses (singular matrices).\n\n**F1 analogy**: If a matrix represents a setup change, the inverse is the change that **reverts** the car back to baseline. Bolted on a new front wing? The inverse matrix is unbolting it. But some changes are irreversible — if you crashed the car (collapsed all performance to zero), there's no inverse that brings it back. That's a singular matrix."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[4, 7],\n",
    "              [2, 6]])\n",
    "\n",
    "A_inv = np.linalg.inv(A)\n",
    "print(f\"A:\\n{A}\\n\")\n",
    "print(f\"A^(-1):\\n{A_inv}\\n\")\n",
    "print(f\"A @ A^(-1):\\n{(A @ A_inv).round(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# A singular matrix (no inverse) — like a setup that destroys information\n# This projects everything onto one line: cornering = 2 * straight-line speed\nsingular = np.array([[1, 2],\n                     [2, 4]])  # Row 2 = 2 * Row 1\n\nprint(f\"Determinant: {np.linalg.det(singular)}\")\nprint(\"Determinant = 0 → this matrix is singular (no inverse)\")\nprint(\"It collapses 2D space into a line — like a crash that destroys the car!\")\n# np.linalg.inv(singular)  # This would raise an error"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 3. Tensors\n\n**Tensors** are generalizations to higher dimensions:\n- Scalar: 0D tensor (a single lap time: 1:32.456)\n- Vector: 1D tensor (one car's telemetry at one moment)\n- Matrix: 2D tensor (one car's full-lap telemetry: time × sensors)\n- 3D tensor: all cars' full-lap telemetry (cars × time × sensors)\n- 4D tensor: all cars across all sessions (sessions × cars × time × sensors)\n\nIn deep learning, we constantly work with tensors. In F1, the data is naturally high-dimensional — and tensors are how we organize it."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tensors in NumPy — F1 data at every scale\nlap_time = np.array(92.456)                      # 0D: a single lap time (seconds)\ntelemetry_snapshot = np.array([310, 4.5, 105])    # 1D: one moment [speed, downforce, tire_temp]\none_lap = np.random.rand(500, 5)                  # 2D: 500 time samples × 5 sensors\nall_cars = np.random.rand(20, 500, 5)             # 3D: 20 cars × 500 samples × 5 sensors\nfull_weekend = np.random.rand(5, 20, 500, 5)      # 4D: 5 sessions × 20 cars × 500 × 5\n\nprint(f\"Lap time shape:        {lap_time.shape}, ndim: {lap_time.ndim}  (scalar)\")\nprint(f\"Telemetry snap shape:  {telemetry_snapshot.shape}, ndim: {telemetry_snapshot.ndim}  (vector)\")\nprint(f\"One lap shape:         {one_lap.shape}, ndim: {one_lap.ndim}  (matrix)\")\nprint(f\"All cars shape:        {all_cars.shape}, ndim: {all_cars.ndim}  (3D tensor)\")\nprint(f\"Full weekend shape:    {full_weekend.shape}, ndim: {full_weekend.ndim}  (4D tensor)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Broadcasting\n\nNumPy's broadcasting allows operations on arrays of different shapes. This is crucial for efficient ML code.\n\n**F1 analogy**: Suppose you have a matrix of lap times for 6 drivers across 3 races. You want to subtract each driver's *average* to see who improved. Broadcasting lets you subtract a vector from a matrix naturally."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Broadcasting examples with F1 data\n# Lap times for 2 drivers across 3 races (seconds)\nlap_times = np.array([[91.5, 82.3, 78.1],    # Driver A: Monza, Spa, Silverstone\n                      [92.1, 83.0, 77.8]])    # Driver B: Monza, Spa, Silverstone\n\n# Scalar broadcast: convert to milliseconds\nprint(f\"Lap times in ms:\\n{lap_times * 1000}\\n\")\n\n# Row vector broadcast: add track-specific time penalties (rain delay per track)\nrain_penalty = np.array([5.0, 8.0, 3.0])  # Monza, Spa, Silverstone rain penalties\nprint(f\"After rain penalties:\\n{lap_times + rain_penalty}\\n\")\n\n# Column vector broadcast: driver-specific fuel load penalty\nfuel_penalty = np.array([[0.5], [0.8]])  # Driver A lighter, Driver B heavier\nprint(f\"After fuel load penalty:\\n{lap_times + fuel_penalty}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 4. Eigenvalues and Eigenvectors\n\nFor a square matrix $\\mathbf{A}$, an **eigenvector** $\\mathbf{v}$ and **eigenvalue** $\\lambda$ satisfy:\n\n$$\\mathbf{Av} = \\lambda\\mathbf{v}$$\n\n**Meaning**: When you apply transformation A to eigenvector v, it only scales (by λ), doesn't change direction.\n\n### The F1 Connection\n\nEvery car has certain **natural performance axes** — directions where changing something only amplifies or diminishes performance without redirecting it. For example:\n- A car might have a \"straight-line speed axis\" — more engine power scales speed without affecting cornering\n- And a \"cornering axis\" — more downforce scales cornering grip without much speed change\n\nThese natural axes are the **eigenvectors**. The **eigenvalues** tell you how sensitive the car is along each axis. A large eigenvalue means a small change has a big effect in that direction.\n\n**Applications in ML**:\n- PCA (Principal Component Analysis)\n- Understanding neural network dynamics\n- Spectral clustering"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple example: a car's performance transformation\n# This matrix boosts straight-line speed more than cornering\ncar_perf = np.array([[3, 1],\n                     [0, 2]])\n\neigenvalues, eigenvectors = np.linalg.eig(car_perf)\n\nprint(f\"Car performance matrix:\\n{car_perf}\\n\")\nprint(f\"Eigenvalues (sensitivity along natural axes): {eigenvalues}\")\nprint(f\"Eigenvectors (natural performance axes, as columns):\\n{eigenvectors}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify: Av = λv (the eigenvector equation)\nA = car_perf\nfor i in range(len(eigenvalues)):\n    λ = eigenvalues[i]\n    v = eigenvectors[:, i]  # Column i is eigenvector i\n    \n    Av = A @ v\n    λv = λ * v\n    \n    print(f\"\\nNatural axis {i+1}: {v}\")\n    print(f\"Sensitivity (eigenvalue): {λ}\")\n    print(f\"A @ v = {Av}\")\n    print(f\"λ * v = {λv}\")\n    print(f\"Only scaled, not rotated: {np.allclose(Av, λv)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize eigenvectors: they don't change direction under transformation\n# Like finding the \"natural axes\" of a car's handling matrix\nA = np.array([[2, 1],\n              [1, 2]])\n\neigenvalues, eigenvectors = np.linalg.eig(A)\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\n# Plot many performance vectors and their transformations\nfor theta in np.linspace(0, 2*np.pi, 16, endpoint=False):\n    v = np.array([np.cos(theta), np.sin(theta)])\n    Av = A @ v\n    ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, \n              color='blue', alpha=0.3, width=0.01)\n    ax.quiver(0, 0, Av[0], Av[1], angles='xy', scale_units='xy', scale=1, \n              color='red', alpha=0.3, width=0.01)\n\n# Highlight eigenvectors — the natural axes\nfor i in range(2):\n    v = eigenvectors[:, i]\n    Av = A @ v\n    ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, \n              color='blue', width=0.02, label=f'natural axis {i+1}' if i == 0 else '')\n    ax.quiver(0, 0, Av[0], Av[1], angles='xy', scale_units='xy', scale=1, \n              color='red', width=0.02, label=f'after transformation' if i == 0 else '')\n\nax.set_xlim(-4, 4)\nax.set_ylim(-4, 4)\nax.set_aspect('equal')\nax.axhline(y=0, color='k', linewidth=0.5)\nax.axvline(x=0, color='k', linewidth=0.5)\nax.set_title('Blue: Original, Red: Transformed\\nEigenvectors (thick) only scale — they are the natural axes')\nax.legend()\nplt.show()\n\nprint(f\"Eigenvalues: {eigenvalues}\")\nprint(\"The eigenvectors (thick lines) stay on the same line after transformation!\")\nprint(\"Most directions get rotated AND scaled — eigenvectors are the special ones that only scale.\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: The Intuition Behind Eigenvectors\n\n**The Big Picture**: Eigenvectors are the \"special directions\" of a transformation - directions that only get stretched or shrunk, never rotated.\n\nIn F1: imagine you tweak the car's setup (that's the matrix). Most aspects of performance change in complicated ways — more downforce helps cornering but hurts top speed. But there are **natural axes** where the effect is pure: push along this axis and you just get \"more\" (or \"less\") of the same thing.\n\n> **Eigenvector intuition**: \"I'm a direction that this matrix only scales, never rotates. Apply the matrix to me, and I just get longer or shorter.\"\n\n#### Breaking Down the Equation\n\n$$\\mathbf{Av} = \\lambda\\mathbf{v}$$\n\n| Component | Meaning | F1 Analogy |\n|-----------|---------|------------|\n| $\\mathbf{A}$ | The transformation matrix | The car's setup/handling characteristics |\n| $\\mathbf{v}$ | An eigenvector (special direction) | A natural performance axis |\n| $\\lambda$ | The eigenvalue (how much v gets scaled) | Sensitivity — how much the car responds along that axis |\n| $\\mathbf{Av}$ | The result of transforming v | Performance after the setup is applied |\n| $\\lambda\\mathbf{v}$ | Same direction as v, just scaled | Same axis, just amplified or diminished |\n\n#### What the Eigenvalue Tells You\n\n| Eigenvalue λ | Geometric meaning | F1 Meaning |\n|--------------|-------------------|------------|\n| λ > 1 | Eigenvector gets stretched | High sensitivity — small input, big performance gain |\n| 0 < λ < 1 | Eigenvector gets shrunk | Diminishing returns along this axis |\n| λ = 1 | Eigenvector unchanged | This axis is immune to the setup change |\n| λ = 0 | Eigenvector collapses to zero | Setup completely kills this performance dimension |\n| λ < 0 | Eigenvector flips and scales | Perverse effect — more input makes things worse |\n| Complex λ | Rotation is involved | Oscillatory behavior (e.g., porpoising!) |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Why Eigenvectors Matter in Machine Learning (and F1)\n\n| Application | How Eigenvectors are Used | F1 Parallel |\n|------------|---------------------------|-------------|\n| **PCA** | Eigenvectors of covariance matrix = directions of maximum variance | Finding the main axes of driver performance variation |\n| **Spectral Clustering** | Eigenvectors of graph Laplacian reveal cluster structure | Grouping similar circuits (street circuits vs. power circuits) |\n| **PageRank** | Dominant eigenvector gives importance scores | Ranking drivers by race-result dominance |\n| **Neural Network Dynamics** | Eigenvalues of weight matrices affect gradient flow. >1 = exploding, <1 = vanishing. | Engine RPM: too high = blowup, too low = stalls |\n| **Covariance Analysis** | Eigenvectors show directions of correlation in data | Which performance metrics are most correlated? |\n| **Matrix Powers** | $A^n$ easy via eigendecomposition | Predicting long-term championship trends |\n\n#### The PCA Connection\n\n**PCA finds eigenvectors of the covariance matrix.**\n\nImagine you have telemetry data across hundreds of laps: speed, throttle, brake, tire temp, fuel load... The covariance matrix captures how these all vary together. Its eigenvectors point in the directions of **maximum variation** — maybe the first principal component is \"overall pace\" and the second is \"tire management style.\"\n\nThe eigenvector with the **largest eigenvalue** = direction of **maximum variance** = the factor that explains the most performance difference between drivers.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 5. Singular Value Decomposition (SVD)\n\nSVD decomposes ANY matrix (not just square) into:\n\n$$\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T$$\n\nWhere:\n- $\\mathbf{U}$: Left singular vectors (orthonormal)\n- $\\mathbf{\\Sigma}$: Diagonal matrix of singular values (non-negative, sorted descending)\n- $\\mathbf{V}^T$: Right singular vectors (orthonormal)\n\n### The F1 Connection\n\nThink of a **driver × circuit performance matrix** — rows are drivers, columns are circuits, entries are average finishing positions. SVD decomposes this into:\n- $\\mathbf{U}$: **Driver profiles** — each driver described by hidden factors (e.g., \"rain skill,\" \"street circuit skill\")\n- $\\mathbf{\\Sigma}$: **Importance** of each factor\n- $\\mathbf{V}^T$: **Circuit profiles** — how much each circuit demands each factor\n\nThis is exactly how Netflix recommends movies, but we're recommending *which circuits each driver would dominate*.\n\n**Applications in ML**:\n- Dimensionality reduction (PCA uses SVD)\n- Image compression\n- Recommender systems\n- Latent semantic analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD example\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9],\n",
    "              [10, 11, 12]])\n",
    "\n",
    "U, s, Vt = np.linalg.svd(A)\n",
    "\n",
    "print(f\"Original A shape: {A.shape}\")\n",
    "print(f\"U shape: {U.shape}\")\n",
    "print(f\"Singular values: {s}\")\n",
    "print(f\"V^T shape: {Vt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct A from SVD\n",
    "# Need to create the full Sigma matrix\n",
    "Sigma = np.zeros((U.shape[0], Vt.shape[0]))\n",
    "np.fill_diagonal(Sigma, s)\n",
    "\n",
    "A_reconstructed = U @ Sigma @ Vt\n",
    "print(f\"Original A:\\n{A}\\n\")\n",
    "print(f\"Reconstructed:\\n{A_reconstructed.round(10)}\\n\")\n",
    "print(f\"Reconstruction accurate: {np.allclose(A, A_reconstructed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-Rank Approximation\n",
    "\n",
    "By keeping only the top k singular values, we get the best rank-k approximation of A.\n",
    "\n",
    "This is the foundation of dimensionality reduction!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Deep Dive: Understanding SVD Geometrically\n\nSVD reveals the hidden structure of any matrix. Think of it as answering: *\"What are the fundamental building blocks of this transformation?\"*\n\n$$\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T$$\n\n#### What Each Component Represents\n\n| Component | Shape | What it represents | F1 Analogy |\n|-----------|-------|-------------------|------------|\n| $\\mathbf{V}^T$ | (n x n) | Input rotation | Rotate from \"circuit features\" to hidden factors |\n| $\\mathbf{\\Sigma}$ | (m x n) | Scaling | How important each hidden factor is |\n| $\\mathbf{U}$ | (m x m) | Output rotation | Rotate from hidden factors to \"driver profiles\" |\n\n**The key insight**: ANY matrix transformation can be decomposed into: **rotate → scale → rotate**.\n\nIn F1 terms: you can understand any driver-circuit performance matrix as: (1) find the hidden skill factors that matter, (2) weight them by importance, (3) map them to specific drivers.\n\n#### Why Singular Values are Sorted by Importance\n\nThe singular values in $\\Sigma$ are always sorted: $\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r \\geq 0$\n\n**Why sorted?** Because they represent how much the matrix \"stretches\" space in each direction:\n- $\\sigma_1$ = the most important factor (maybe \"overall car pace\")\n- $\\sigma_2$ = second most important (maybe \"wet weather skill\")\n- Small $\\sigma_i$ = \"noise\" (random variation that doesn't represent real skill)\n\nThis ordering is why keeping only the top-k singular values gives the **best** rank-k approximation! Keep the signal, drop the noise.\n\n#### The Connection to PCA\n\nPCA and SVD are deeply connected:\n\n| If you have... | PCA finds... | Which equals... |\n|----------------|--------------|-----------------|\n| Data matrix $\\mathbf{X}$ (centered) | Eigenvectors of $\\mathbf{X}^T\\mathbf{X}$ | Right singular vectors $\\mathbf{V}$ from SVD of $\\mathbf{X}$ |\n| Principal components | $\\mathbf{X} \\cdot \\text{eigenvectors}$ | $\\mathbf{U} \\cdot \\Sigma$ from SVD |\n| Variance explained | Eigenvalues / total | $\\sigma_i^2 / \\sum \\sigma_j^2$ |\n\n**Bottom line**: PCA is just SVD on centered data! In practice, PCA is often computed using SVD because it's more numerically stable.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_rank_approx(A, k):\n",
    "    \"\"\"Return rank-k approximation of matrix A using SVD.\"\"\"\n",
    "    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "    return U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]\n",
    "\n",
    "# Example with random matrix\n",
    "np.random.seed(42)\n",
    "A = np.random.rand(10, 8)\n",
    "\n",
    "print(f\"Original matrix shape: {A.shape}\")\n",
    "print(f\"Full rank: {np.linalg.matrix_rank(A)}\")\n",
    "\n",
    "for k in [1, 2, 4, 8]:\n",
    "    A_k = low_rank_approx(A, k)\n",
    "    error = np.linalg.norm(A - A_k, 'fro')  # Frobenius norm\n",
    "    print(f\"Rank-{k} approximation error: {error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Practical Exercise: Image Compression with SVD\n\nLet's compress an image using SVD — just like how F1 teams compress massive telemetry datasets to find the essential patterns, throwing away noise while keeping the signal."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample grayscale image (or load one)\n",
    "# We'll create a simple pattern\n",
    "x = np.linspace(-3, 3, 200)\n",
    "y = np.linspace(-3, 3, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "image = np.sin(X) * np.cos(Y) + 0.5 * np.sin(2*X) * np.cos(2*Y)\n",
    "image = (image - image.min()) / (image.max() - image.min())  # Normalize to [0, 1]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(f'Original Image ({image.shape[0]}×{image.shape[1]})')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress with different ranks\n",
    "U, s, Vt = np.linalg.svd(image, full_matrices=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "ranks = [1, 5, 10, 20, 50, 100]\n",
    "for ax, k in zip(axes.flat, ranks):\n",
    "    compressed = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]\n",
    "    \n",
    "    # Calculate compression ratio\n",
    "    original_size = image.shape[0] * image.shape[1]\n",
    "    compressed_size = k * (image.shape[0] + image.shape[1] + 1)\n",
    "    ratio = original_size / compressed_size\n",
    "    \n",
    "    ax.imshow(compressed, cmap='gray')\n",
    "    ax.set_title(f'Rank {k}\\nCompression: {ratio:.1f}x')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot singular values\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(s, 'b-')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Singular Value')\n",
    "plt.title('Singular Values')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.cumsum(s**2) / np.sum(s**2), 'b-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Variance Explained')\n",
    "plt.title('Cumulative Variance')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95%')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercises\n\n### Exercise 1: Implement Matrix Operations\nImplement the following functions without using NumPy's built-in functions.\n\n*Think of it as building your own telemetry processing toolkit from scratch — the engineers at Williams started from humble beginnings too!*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose(A):\n",
    "    \"\"\"Return the transpose of matrix A.\"\"\"\n",
    "    m, n = A.shape\n",
    "    result = np.zeros((n, m))\n",
    "    # TODO: Implement\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            result[j, i] = A[i, j]\n",
    "    return result\n",
    "\n",
    "def dot_product(a, b):\n",
    "    \"\"\"Return the dot product of vectors a and b.\"\"\"\n",
    "    assert len(a) == len(b)\n",
    "    result = 0\n",
    "    # TODO: Implement\n",
    "    for i in range(len(a)):\n",
    "        result += a[i] * b[i]\n",
    "    return result\n",
    "\n",
    "def matrix_vector_mult(A, v):\n",
    "    \"\"\"Return A @ v.\"\"\"\n",
    "    m, n = A.shape\n",
    "    assert n == len(v)\n",
    "    result = np.zeros(m)\n",
    "    # TODO: Implement\n",
    "    for i in range(m):\n",
    "        result[i] = dot_product(A[i], v)\n",
    "    return result\n",
    "\n",
    "# Test\n",
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "v = np.array([1, 2, 3])\n",
    "\n",
    "print(f\"transpose(A) correct: {np.allclose(transpose(A), A.T)}\")\n",
    "print(f\"dot_product([1,2,3], [4,5,6]) = {dot_product(np.array([1,2,3]), np.array([4,5,6]))}\")\n",
    "print(f\"matrix_vector_mult correct: {np.allclose(matrix_vector_mult(A, v), A @ v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2: F1 Transformation Explorer\nCreate different transformation matrices and visualize their effects on a car's performance envelope.\n\nTry to create:\n1. **Monaco trim**: Scale cornering way up, speed slightly down\n2. **Mirror setup**: Reflect the car's handling (swap left-right bias)\n3. **Spa special**: Rotate 45° then scale (sweeping corners + power)\n4. **Project onto top speed**: Ignore everything except straight-line performance"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Create and visualize these F1 transformations:\n# 1. Monaco trim (high cornering, lower speed)\n# 2. Mirror setup (reflect across x-axis — swap left/right handling)\n# 3. Spa special (rotate 45° then scale by 2)\n# 4. Project onto top speed (x-axis projection — ignore cornering)\n\n# Example: Monaco trim — boost cornering, sacrifice straight-line speed\nmonaco_trim = np.array([[0.7, 0],    # reduce top speed to 70%\n                        [0, 1.5]])   # boost cornering by 50%\nplot_transformation(monaco_trim, \"Monaco Trim (0.7x speed, 1.5x cornering)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 3: Build an F1 Driver-Circuit Predictor\n\nUse SVD for matrix factorization to predict how drivers would perform at circuits they haven't raced at — the same math Netflix uses to recommend movies!\n\n**Scenario**: You have a driver × circuit rating matrix where each entry is a performance score (1-5). Some entries are missing (driver hasn't raced there yet). Can SVD help predict them?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Driver-Circuit performance matrix (drivers × circuits)\n# Scores 1-5 (5 = dominant, 1 = struggled). 0 = hasn't raced there.\n#                    Monza  Monaco  Spa  Silverstone  Suzuka\nperformance = np.array([\n    [5, 3, 0, 4, 5],   # Verstappen: fast everywhere, unknown at Spa\n    [4, 0, 0, 4, 3],   # Norris: quick, unknown at Monaco & Spa\n    [2, 5, 0, 3, 2],   # Leclerc: Monaco king, struggles on power tracks\n    [0, 2, 5, 4, 0],   # Hamilton: rain master, unknown at Monza & Suzuka\n    [0, 0, 4, 0, 3],   # Piastri: limited data\n    [3, 4, 3, 5, 4]    # Alonso: consistent everywhere\n])\n\ndrivers = ['VER', 'NOR', 'LEC', 'HAM', 'PIA', 'ALO']\ncircuits = ['Monza', 'Monaco', 'Spa', 'Silverstone', 'Suzuka']\n\nprint(\"Driver-Circuit Performance (0 = unknown):\")\nprint(f\"{'':>5}\", end='')\nfor c in circuits:\n    print(f\"{c:>12}\", end='')\nprint()\nfor i, d in enumerate(drivers):\n    print(f\"{d:>5}\", end='')\n    for j in range(len(circuits)):\n        val = performance[i, j]\n        print(f\"{'?' if val == 0 else val:>12}\", end='')\n    print()\n\n# Step 1: Fill missing values with driver's average (simple imputation)\nperf_filled = performance.copy().astype(float)\nfor i in range(performance.shape[0]):\n    row = performance[i]\n    mean = row[row > 0].mean()\n    perf_filled[i, row == 0] = mean\n\nprint(\"\\nFilled with driver averages:\")\nprint(perf_filled.round(2))\n\n# Step 2: SVD low-rank approximation — find hidden \"skill factors\"\nk = 2  # 2 latent factors (maybe \"power circuit skill\" and \"technical circuit skill\")\nU, s, Vt = np.linalg.svd(perf_filled, full_matrices=False)\npredicted = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]\n\nprint(f\"\\nPredicted performance (rank-{k} — 2 hidden skill factors):\")\nprint(predicted.round(2))\n\n# Step 3: Show predictions for originally missing entries\nprint(\"\\n--- PREDICTIONS FOR UNKNOWN CIRCUITS ---\")\nfor i in range(performance.shape[0]):\n    for j in range(performance.shape[1]):\n        if performance[i, j] == 0:\n            print(f\"  {drivers[i]} at {circuits[j]}: {predicted[i, j]:.1f}/5\")\n\nprint(\"\\nThe SVD found hidden factors and used them to predict!\")\nprint(f\"Singular values: {s[:k].round(2)} — these are the importance of each hidden factor\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Summary\n\n### Key Concepts (with the F1 Lens)\n\n1. **Vectors** represent data points, weights, and gradients — or a car's telemetry, velocity, and force vectors\n2. **Dot product** measures alignment — how well a setup matches a track, or how similar two drivers' styles are\n3. **Norms** measure magnitude — actual speed, total g-force, or overall performance intensity\n4. **Matrices** are linear transformations — setup changes, coordinate rotations, or data organization\n5. **Matrix multiplication** composes transformations — applying multiple engineering changes in sequence\n6. **Eigenvectors** reveal the \"natural axes\" of a transformation — the car's fundamental performance directions\n7. **SVD** decomposes any matrix into rotate→scale→rotate — revealing hidden structure like driver skill factors\n\n### Connection to Deep Learning (and F1 Strategy)\n\n| Deep Learning | F1 Parallel |\n|--------------|-------------|\n| **Forward pass**: Matrix multiplications + activations | Setup changes + nonlinear aero effects |\n| **Weights**: Learned transformation matrices | Optimized car setup parameters |\n| **Backprop**: Chain rule on matrix operations | Sensitivity analysis: which setup change had the most effect? |\n| **Embeddings**: Low-dimensional representations (SVD) | Driver/circuit profiles from sparse results data |\n| **Attention**: Dot products between vectors | \"Which past laps are most relevant to this situation?\" |\n\n### Checklist\n- [ ] I can perform vector operations (addition, dot product, norm) — and explain them with forces on an F1 car\n- [ ] I understand matrices as linear transformations — like setup changes\n- [ ] I can multiply matrices and understand shape compatibility\n- [ ] I know what eigenvalues/eigenvectors represent — the natural performance axes\n- [ ] I can use SVD for dimensionality reduction — and to predict driver performance at new circuits"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to **Part 1.2: Calculus Refresher** where we'll cover:\n",
    "- Derivatives and the chain rule\n",
    "- Gradients and gradient descent\n",
    "- The mathematical foundation of backpropagation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
