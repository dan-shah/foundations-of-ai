{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Part 8.1: Tokenization & Language Model Training — The Formula 1 Edition\n\nBefore an LLM can process text, it must convert characters into numbers. This seemingly mundane step — **tokenization** — turns out to be one of the most consequential design decisions in modern AI. The choice of tokenizer affects model performance, multilingual capabilities, inference cost, and even what the model can learn.\n\n**F1 analogy:** Think of tokenization as how an F1 team encodes telemetry data. Raw sensor readings arrive as continuous streams — throttle position, brake pressure, tire temperatures, GPS coordinates. Before the pit wall can analyze them, these signals must be discretized into a vocabulary of meaningful patterns. Just as BPE discovers common character sequences and merges them into tokens, an F1 data system discovers common telemetry patterns (e.g., \"hard braking into a slow corner\" or \"DRS activation on a straight\") and encodes them as reusable units. The vocabulary size is a tradeoff: too small and you miss rare but critical events (a one-off sensor anomaly), too large and the system becomes unwieldy — just like choosing between 32K and 100K tokens.\n\nIn this notebook, we'll build tokenizers from scratch, understand how language models are trained, and explore the **scaling laws** that govern how model performance improves with size, data, and compute.\n\n## Learning Objectives\n\n- [ ] Understand why tokenization matters and its impact on model behavior\n- [ ] Implement Byte Pair Encoding (BPE) from scratch\n- [ ] Build a WordPiece tokenizer and compare with BPE\n- [ ] Understand pretraining objectives: causal LM vs masked LM\n- [ ] Implement a small language model training loop from scratch\n- [ ] Explore scaling laws: how performance relates to model size, data, and compute\n- [ ] Understand training dynamics: loss curves, learning rate schedules, warm-up"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom collections import defaultdict, Counter\nimport re\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nnp.random.seed(42)\ntorch.manual_seed(42)\n\nprint(\"Part 8.1: Tokenization & Language Model Training — The Formula 1 Edition\")\nprint(\"=\" * 72)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "---\n\n## 1. Why Tokenization Matters\n\nModels don't see text — they see sequences of integer IDs. The tokenizer defines this mapping.\n\n| Approach | Example: \"unhappiness\" | Vocab Size | Trade-off | F1 Parallel |\n|----------|----------------------|------------|----------|-------------|\n| **Character** | u, n, h, a, p, p, i, n, e, s, s | ~256 | Short vocab, long sequences | Recording every individual sensor tick — precise but overwhelming |\n| **Word** | unhappiness | ~100K+ | Short sequences, huge vocab, OOV problem | One code per entire maneuver — compact but can't represent novel situations |\n| **Subword** | un, happi, ness | ~32K-100K | Best of both worlds | Encoding reusable telemetry patterns — \"brake-turn-in\", \"apex-throttle\" |\n\n### Why Subword Tokenization Won\n\n1. **No OOV**: Can represent any word by composing subwords\n2. **Shared morphology**: \"unhappy\" and \"happiness\" share subwords\n3. **Efficient**: Common words stay whole, rare words decompose\n4. **Multilingual**: Works across languages with shared scripts\n\n**F1 analogy:** Subword tokenization is like the telemetry compression an F1 team uses over race weekend. Common sequences — a clean lap through Maggotts-Becketts, a standard pit stop — get encoded as single compact tokens. But a rare event like a puncture or a safety car restart can still be represented by composing smaller known patterns. The corpus of race transcripts, telemetry logs, and FIA regulations all feed into building this vocabulary."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tokenization approaches\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "text = \"unhappiness\"\n",
    "\n",
    "approaches = [\n",
    "    ('Character-level', list(text), '#e74c3c'),\n",
    "    ('Word-level', [text], '#3498db'),\n",
    "    ('Subword (BPE)', ['un', 'happi', 'ness'], '#2ecc71'),\n",
    "]\n",
    "\n",
    "for ax, (name, tokens, color) in zip(axes, approaches):\n",
    "    ax.set_xlim(-0.5, max(len(tokens), 4) - 0.5)\n",
    "    ax.set_ylim(-0.5, 1.5)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(name, fontsize=13, fontweight='bold')\n",
    "    \n",
    "    total_width = len(tokens)\n",
    "    start_x = (max(len(tokens), 4) - total_width) / 2\n",
    "    \n",
    "    for i, tok in enumerate(tokens):\n",
    "        box = mpatches.FancyBboxPatch((start_x + i - 0.4, 0.2), 0.8, 0.8,\n",
    "                                       boxstyle=\"round,pad=0.05\", facecolor=color,\n",
    "                                       edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "        ax.add_patch(box)\n",
    "        ax.text(start_x + i, 0.6, tok, ha='center', va='center',\n",
    "               fontsize=10 if len(tok) < 5 else 8, fontweight='bold', color='white')\n",
    "    \n",
    "    ax.text(max(len(tokens), 4) / 2, -0.2, f'{len(tokens)} token{\"s\" if len(tokens) > 1 else \"\"}',\n",
    "           ha='center', fontsize=10, color='gray')\n",
    "\n",
    "plt.suptitle(f'Tokenizing: \"{text}\"', fontsize=14, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": "---\n\n## 2. Byte Pair Encoding (BPE)\n\nBPE is the most widely used tokenization algorithm (GPT, LLaMA, etc.). It works by iteratively merging the most frequent pair of adjacent tokens.\n\n### Algorithm\n\n1. Start with a vocabulary of individual characters\n2. Count all adjacent pairs in the corpus\n3. Merge the most frequent pair into a new token\n4. Repeat until desired vocabulary size is reached\n\n### Example\n```\nCorpus: \"low lower newest\"\nStart: l o w _ l o w e r _ n e w e s t\nMerge 'e','w' -> 'ew':  l o w _ l o w ew r _ n ew e s t  \nMerge 'l','o' -> 'lo':  lo w _ lo w ew r _ n ew e s t\n...\n```\n\n**F1 analogy:** BPE is like the way an F1 engineer learns to read telemetry traces over a season. At first, every data point is individual — throttle at 73%, brake pressure 42 bar, steering angle 12 degrees. Over thousands of laps, the engineer starts recognizing frequent *pairs* of events: \"lift-and-coast\" (throttle drop + coasting), \"trail-braking\" (brake + turn-in together). These get merged into single recognized patterns. The most common sequences get their own shorthand first, while rare corner-specific patterns stay decomposed."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    \"\"\"Byte Pair Encoding tokenizer from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.merges = {}  # (a, b) -> merged token\n",
    "        self.vocab = {}   # token -> id\n",
    "        self.inverse_vocab = {}  # id -> token\n",
    "        self.merge_history = []  # For visualization\n",
    "    \n",
    "    def _get_word_freqs(self, text):\n",
    "        \"\"\"Split text into words and count frequencies.\"\"\"\n",
    "        words = re.findall(r'\\S+', text.lower())\n",
    "        word_freqs = Counter()\n",
    "        for word in words:\n",
    "            # Add end-of-word marker\n",
    "            chars = tuple(list(word) + ['</w>'])\n",
    "            word_freqs[chars] += 1\n",
    "        return word_freqs\n",
    "    \n",
    "    def _get_pair_counts(self, word_freqs):\n",
    "        \"\"\"Count adjacent pairs across all words.\"\"\"\n",
    "        pairs = Counter()\n",
    "        for word, freq in word_freqs.items():\n",
    "            for i in range(len(word) - 1):\n",
    "                pairs[(word[i], word[i+1])] += freq\n",
    "        return pairs\n",
    "    \n",
    "    def _merge_pair(self, word_freqs, pair):\n",
    "        \"\"\"Merge a pair in all words.\"\"\"\n",
    "        new_word_freqs = {}\n",
    "        merged = pair[0] + pair[1]\n",
    "        \n",
    "        for word, freq in word_freqs.items():\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                if i < len(word) - 1 and word[i] == pair[0] and word[i+1] == pair[1]:\n",
    "                    new_word.append(merged)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word_freqs[tuple(new_word)] = freq\n",
    "        \n",
    "        return new_word_freqs\n",
    "    \n",
    "    def train(self, text, num_merges=20, verbose=True):\n",
    "        \"\"\"Train BPE tokenizer on text.\"\"\"\n",
    "        word_freqs = self._get_word_freqs(text)\n",
    "        \n",
    "        # Initial vocab: all characters + end-of-word\n",
    "        all_chars = set()\n",
    "        for word in word_freqs:\n",
    "            all_chars.update(word)\n",
    "        \n",
    "        self.vocab = {ch: i for i, ch in enumerate(sorted(all_chars))}\n",
    "        self.merge_history = []\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Initial vocab ({len(self.vocab)} tokens): {sorted(self.vocab.keys())}\")\n",
    "            print(f\"\\nTraining {num_merges} merges...\\n\")\n",
    "        \n",
    "        for step in range(num_merges):\n",
    "            pairs = self._get_pair_counts(word_freqs)\n",
    "            if not pairs:\n",
    "                break\n",
    "            \n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            best_count = pairs[best_pair]\n",
    "            \n",
    "            # Merge\n",
    "            word_freqs = self._merge_pair(word_freqs, best_pair)\n",
    "            merged_token = best_pair[0] + best_pair[1]\n",
    "            self.merges[best_pair] = merged_token\n",
    "            self.vocab[merged_token] = len(self.vocab)\n",
    "            \n",
    "            self.merge_history.append({\n",
    "                'step': step + 1,\n",
    "                'pair': best_pair,\n",
    "                'merged': merged_token,\n",
    "                'count': best_count,\n",
    "                'vocab_size': len(self.vocab)\n",
    "            })\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Step {step+1}: merge '{best_pair[0]}' + '{best_pair[1]}' \"\n",
    "                      f\"-> '{merged_token}' (freq={best_count}, vocab={len(self.vocab)})\")\n",
    "        \n",
    "        self.inverse_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        return self\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to token IDs.\"\"\"\n",
    "        words = re.findall(r'\\S+', text.lower())\n",
    "        all_tokens = []\n",
    "        \n",
    "        for word in words:\n",
    "            tokens = list(word) + ['</w>']\n",
    "            \n",
    "            # Apply merges in order\n",
    "            for pair, merged in self.merges.items():\n",
    "                i = 0\n",
    "                new_tokens = []\n",
    "                while i < len(tokens):\n",
    "                    if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
    "                        new_tokens.append(merged)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_tokens.append(tokens[i])\n",
    "                        i += 1\n",
    "                tokens = new_tokens\n",
    "            \n",
    "            all_tokens.extend(tokens)\n",
    "        \n",
    "        return [self.vocab.get(t, 0) for t in all_tokens], all_tokens\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"Decode token IDs back to text.\"\"\"\n",
    "        tokens = [self.inverse_vocab.get(i, '?') for i in ids]\n",
    "        text = ''.join(tokens).replace('</w>', ' ').strip()\n",
    "        return text\n",
    "\n",
    "\n",
    "# Training corpus\n",
    "corpus = \"\"\"the cat sat on the mat. the cat ate the rat. \n",
    "the dog sat on the log. the dog chased the cat.\n",
    "a cat is a small animal. a dog is a loyal animal.\n",
    "the cat and the dog are friends. the mat is on the floor.\"\"\"\n",
    "\n",
    "tokenizer = BPETokenizer()\n",
    "tokenizer.train(corpus, num_merges=25)\n",
    "\n",
    "# Test encoding\n",
    "test_texts = [\"the cat\", \"the dog sat\", \"animal friends\"]\n",
    "print(\"\\nEncoding examples:\")\n",
    "for text in test_texts:\n",
    "    ids, tokens = tokenizer.encode(text)\n",
    "    print(f\"  '{text}' -> {tokens} -> {ids}\")\n",
    "    decoded = tokenizer.decode(ids)\n",
    "    print(f\"  Decoded: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize BPE merge history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Vocab size growth\n",
    "ax = axes[0]\n",
    "steps = [h['step'] for h in tokenizer.merge_history]\n",
    "vocab_sizes = [h['vocab_size'] for h in tokenizer.merge_history]\n",
    "ax.plot(steps, vocab_sizes, 'b-o', linewidth=2, markersize=5)\n",
    "ax.set_xlabel('Merge Step', fontsize=11)\n",
    "ax.set_ylabel('Vocabulary Size', fontsize=11)\n",
    "ax.set_title('BPE Vocabulary Growth', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Merge frequency (how frequent each merged pair was)\n",
    "ax = axes[1]\n",
    "merge_freqs = [h['count'] for h in tokenizer.merge_history]\n",
    "merge_labels = [h['merged'][:8] for h in tokenizer.merge_history]\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(merge_freqs)))\n",
    "ax.bar(range(len(merge_freqs)), merge_freqs, color=colors, edgecolor='black', alpha=0.8)\n",
    "ax.set_xticks(range(len(merge_labels)))\n",
    "ax.set_xticklabels(merge_labels, rotation=60, ha='right', fontsize=7)\n",
    "ax.set_xlabel('Merged Token', fontsize=11)\n",
    "ax.set_ylabel('Pair Frequency', fontsize=11)\n",
    "ax.set_title('BPE Merge Frequencies', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "---\n\n## 3. WordPiece Tokenization\n\nWordPiece (used by BERT) is similar to BPE but uses a different criterion for choosing merges: instead of frequency, it maximizes the **likelihood of the training data**.\n\n$$\\text{score}(a, b) = \\frac{\\text{freq}(ab)}{\\text{freq}(a) \\times \\text{freq}(b)}$$\n\nThis favors merging pairs where the combination is more frequent *relative* to the individual pieces — capturing meaningful subwords rather than just frequent character sequences.\n\n**F1 analogy:** If BPE merges whatever telemetry patterns appear most often (like \"throttle-on + gear-up\" which happens on every straight), WordPiece asks a smarter question: \"Is this pair appearing together *more than you'd expect by chance*?\" A rare but always-cooccurring pair like \"anti-stall activation + clutch override\" would score high in WordPiece even though each event alone is rare — because whenever one happens, the other always follows. That's a more meaningful pattern than just \"the two most common signals.\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPieceTokenizer:\n",
    "    \"\"\"WordPiece tokenizer (BERT-style) from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.merges = {}\n",
    "        self.merge_history = []\n",
    "    \n",
    "    def _get_word_freqs(self, text):\n",
    "        words = re.findall(r'\\S+', text.lower())\n",
    "        word_freqs = Counter()\n",
    "        for word in words:\n",
    "            # WordPiece uses ## prefix for continuation tokens\n",
    "            chars = tuple([word[0]] + ['##' + c for c in word[1:]])\n",
    "            word_freqs[chars] += 1\n",
    "        return word_freqs\n",
    "    \n",
    "    def _get_pair_scores(self, word_freqs):\n",
    "        \"\"\"Score pairs by likelihood ratio (WordPiece criterion).\"\"\"\n",
    "        pair_freqs = Counter()\n",
    "        token_freqs = Counter()\n",
    "        \n",
    "        for word, freq in word_freqs.items():\n",
    "            for i in range(len(word) - 1):\n",
    "                pair_freqs[(word[i], word[i+1])] += freq\n",
    "            for token in word:\n",
    "                token_freqs[token] += freq\n",
    "        \n",
    "        scores = {}\n",
    "        for pair, freq in pair_freqs.items():\n",
    "            denom = token_freqs[pair[0]] * token_freqs[pair[1]]\n",
    "            scores[pair] = freq / denom if denom > 0 else 0\n",
    "        \n",
    "        return scores, pair_freqs\n",
    "    \n",
    "    def _merge_pair(self, word_freqs, pair):\n",
    "        merged = pair[0] + pair[1].replace('##', '')\n",
    "        new_word_freqs = {}\n",
    "        \n",
    "        for word, freq in word_freqs.items():\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                if i < len(word) - 1 and word[i] == pair[0] and word[i+1] == pair[1]:\n",
    "                    new_word.append(merged)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word_freqs[tuple(new_word)] = freq\n",
    "        \n",
    "        return new_word_freqs, merged\n",
    "    \n",
    "    def train(self, text, num_merges=20, verbose=True):\n",
    "        word_freqs = self._get_word_freqs(text)\n",
    "        \n",
    "        all_tokens = set()\n",
    "        for word in word_freqs:\n",
    "            all_tokens.update(word)\n",
    "        self.vocab = {t: i for i, t in enumerate(sorted(all_tokens))}\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Initial vocab ({len(self.vocab)} tokens)\")\n",
    "            print(f\"Training {num_merges} merges...\\n\")\n",
    "        \n",
    "        for step in range(num_merges):\n",
    "            scores, pair_freqs = self._get_pair_scores(word_freqs)\n",
    "            if not scores:\n",
    "                break\n",
    "            \n",
    "            best_pair = max(scores, key=scores.get)\n",
    "            word_freqs, merged_token = self._merge_pair(word_freqs, best_pair)\n",
    "            \n",
    "            self.merges[best_pair] = merged_token\n",
    "            self.vocab[merged_token] = len(self.vocab)\n",
    "            \n",
    "            self.merge_history.append({\n",
    "                'step': step + 1,\n",
    "                'pair': best_pair,\n",
    "                'merged': merged_token,\n",
    "                'score': scores[best_pair],\n",
    "                'freq': pair_freqs[best_pair]\n",
    "            })\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Step {step+1}: merge '{best_pair[0]}' + '{best_pair[1]}' \"\n",
    "                      f\"-> '{merged_token}' (score={scores[best_pair]:.4f}, freq={pair_freqs[best_pair]})\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "# Train WordPiece on the same corpus\n",
    "wp_tokenizer = WordPieceTokenizer()\n",
    "wp_tokenizer.train(corpus, num_merges=15)\n",
    "\n",
    "# Compare merge choices\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"BPE vs WordPiece merge comparison (first 10):\")\n",
    "print(f\"{'Step':>5} {'BPE Merge':>20} {'WordPiece Merge':>20}\")\n",
    "for i in range(min(10, len(tokenizer.merge_history), len(wp_tokenizer.merge_history))):\n",
    "    bpe = tokenizer.merge_history[i]['merged']\n",
    "    wp = wp_tokenizer.merge_history[i]['merged']\n",
    "    print(f\"{i+1:>5} {bpe:>20} {wp:>20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "---\n\n## 4. Tokenization Impact on Models\n\nThe tokenizer directly affects what the model \"sees\". Let's measure how different tokenization granularities change sequence length and vocabulary coverage.\n\n**F1 analogy:** This is the fundamental tradeoff every F1 data team faces: how granular should your telemetry encoding be? Character-level is like logging every sensor at 1000Hz — you capture everything but drown in data. Word-level is like logging only \"completed a lap\" — efficient but you've lost all the nuance. BPE-style subword encoding hits the sweet spot: recognizable patterns like \"chicane-sequence\" or \"tire-deg-phase\" that are compact but still informative."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tokenization approaches on different texts\n",
    "test_corpus = [\n",
    "    \"The transformer architecture revolutionized natural language processing.\",\n",
    "    \"Backpropagation computes gradients efficiently using the chain rule.\",\n",
    "    \"Reinforcement learning from human feedback aligns language models.\",\n",
    "    \"Self-attention enables parallel processing of sequential data.\",\n",
    "    \"Neural networks approximate complex nonlinear functions.\",\n",
    "]\n",
    "\n",
    "def char_tokenize(text):\n",
    "    return list(text.lower())\n",
    "\n",
    "def word_tokenize(text):\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def bpe_tokenize(text, tokenizer):\n",
    "    _, tokens = tokenizer.encode(text)\n",
    "    return tokens\n",
    "\n",
    "# Train BPE on test corpus for fair comparison\n",
    "all_text = ' '.join(test_corpus)\n",
    "test_bpe = BPETokenizer()\n",
    "test_bpe.train(all_text, num_merges=40, verbose=False)\n",
    "\n",
    "print(\"Tokenization Comparison\\n\")\n",
    "print(f\"{'Text (first 50 chars)':>55} {'Char':>6} {'Word':>6} {'BPE':>6}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "char_lens, word_lens, bpe_lens = [], [], []\n",
    "\n",
    "for text in test_corpus:\n",
    "    cl = len(char_tokenize(text))\n",
    "    wl = len(word_tokenize(text))\n",
    "    bl = len(bpe_tokenize(text, test_bpe))\n",
    "    \n",
    "    char_lens.append(cl)\n",
    "    word_lens.append(wl)\n",
    "    bpe_lens.append(bl)\n",
    "    \n",
    "    print(f\"{text[:55]:>55} {cl:>6} {wl:>6} {bl:>6}\")\n",
    "\n",
    "print(f\"\\n{'Average':>55} {np.mean(char_lens):>6.1f} {np.mean(word_lens):>6.1f} {np.mean(bpe_lens):>6.1f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "x = np.arange(len(test_corpus))\n",
    "w = 0.25\n",
    "ax.bar(x - w, char_lens, w, label='Character', color='#e74c3c', edgecolor='black', alpha=0.8)\n",
    "ax.bar(x, bpe_lens, w, label='BPE', color='#2ecc71', edgecolor='black', alpha=0.8)\n",
    "ax.bar(x + w, word_lens, w, label='Word', color='#3498db', edgecolor='black', alpha=0.8)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'Text {i+1}' for i in range(len(test_corpus))])\n",
    "ax.set_ylabel('Sequence Length', fontsize=11)\n",
    "ax.set_title('Sequence Length by Tokenization Method', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "---\n\n## 5. Pretraining Objectives\n\nHow do we train a language model? The choice of **pretraining objective** determines what the model learns.\n\n### Causal Language Modeling (CLM) — GPT-style\nPredict the next token given all previous tokens:\n$$P(x_t | x_1, x_2, \\ldots, x_{t-1})$$\n\n### Masked Language Modeling (MLM) — BERT-style\nRandomly mask tokens and predict them from context:\n$$P(x_\\text{mask} | x_1, \\ldots, x_{\\text{mask}-1}, x_{\\text{mask}+1}, \\ldots, x_n)$$\n\n| Feature | CLM (GPT) | MLM (BERT) | F1 Parallel |\n|---------|-----------|------------|-------------|\n| Direction | Left-to-right only | Bidirectional | Predicting the next sector time vs. inferring a missing mid-sector from surrounding data |\n| Generation | Natural text generation | Not designed for generation | Live strategy calls (what happens next?) vs. post-race gap analysis |\n| Understanding | Good but unidirectional | Excellent bidirectional | Forecasting from history vs. understanding a full race in hindsight |\n| Use case | Chatbots, code gen, writing | Classification, NER, QA | Real-time pit wall prediction vs. post-race data classification |\n\n**F1 analogy:** Causal LM is like the pit wall predicting what will happen *next* on track — given everything up to now, what's the next event? Masked LM is like a post-race analyst filling in missing telemetry gaps: \"Given the braking zone entry and the corner exit, what must have happened at the apex?\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate both pretraining objectives\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    \"\"\"Minimal word-level tokenizer for demonstrations.\"\"\"\n",
    "    def __init__(self, texts):\n",
    "        words = set()\n",
    "        for text in texts:\n",
    "            words.update(re.findall(r'\\w+', text.lower()))\n",
    "        \n",
    "        self.word2id = {'<pad>': 0, '<mask>': 1, '<unk>': 2}\n",
    "        for i, w in enumerate(sorted(words)):\n",
    "            self.word2id[w] = i + 3\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.vocab_size = len(self.word2id)\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return [self.word2id.get(w, 2) for w in re.findall(r'\\w+', text.lower())]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        return ' '.join(self.id2word.get(i, '?') for i in ids)\n",
    "\n",
    "\n",
    "# Training data\n",
    "train_texts = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog chased the cat\",\n",
    "    \"a bird flew over the tree\",\n",
    "    \"the fish swam in the pond\",\n",
    "    \"a cat is a small animal\",\n",
    "    \"the dog is a loyal friend\",\n",
    "    \"birds can fly very high\",\n",
    "    \"fish live in the water\",\n",
    "]\n",
    "\n",
    "tok = SimpleTokenizer(train_texts)\n",
    "print(f\"Vocabulary size: {tok.vocab_size}\")\n",
    "\n",
    "# CLM: show next-token prediction setup\n",
    "print(\"\\n--- Causal Language Modeling (Next Token Prediction) ---\")\n",
    "example = \"the cat sat on the mat\"\n",
    "tokens = example.split()\n",
    "for i in range(1, len(tokens)):\n",
    "    context = ' '.join(tokens[:i])\n",
    "    target = tokens[i]\n",
    "    print(f\"  Input: '{context}' -> Predict: '{target}'\")\n",
    "\n",
    "# MLM: show masked prediction setup\n",
    "print(\"\\n--- Masked Language Modeling ---\")\n",
    "for text in train_texts[:3]:\n",
    "    tokens = text.split()\n",
    "    mask_idx = np.random.randint(0, len(tokens))\n",
    "    original = tokens[mask_idx]\n",
    "    masked = tokens.copy()\n",
    "    masked[mask_idx] = '[MASK]'\n",
    "    print(f\"  Input: '{' '.join(masked)}' -> Predict: '{original}' at position {mask_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": "---\n\n## 6. Training a Small Language Model\n\nLet's train a tiny transformer language model from scratch to see the complete training loop: data preparation, causal masking, loss computation, and generation.\n\n**F1 analogy:** This is like building a miniature version of the simulation system F1 teams use. Real teams train their models on hundreds of thousands of virtual laps. Our tiny LM is like a simplified simulator that learns patterns from a small dataset of race transcripts — not powerful enough for race day, but perfect for understanding how the learning process works."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyLM(nn.Module):\n",
    "    \"\"\"Tiny transformer language model for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=64, n_heads=4, n_layers=2, max_seq_len=32):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        # Transformer decoder layers\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=d_model * 4,\n",
    "            dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        positions = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T)\n",
    "        \n",
    "        h = self.token_emb(x) + self.pos_emb(positions)\n",
    "        \n",
    "        # Causal mask (upper triangular = masked)\n",
    "        causal_mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
    "        \n",
    "        # Use transformer decoder with self-attention only (no encoder)\n",
    "        h = self.transformer(h, h, tgt_mask=causal_mask, memory_mask=causal_mask)\n",
    "        h = self.ln(h)\n",
    "        logits = self.head(h)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def generate(self, start_tokens, max_new_tokens=10, temperature=1.0):\n",
    "        \"\"\"Autoregressive generation.\"\"\"\n",
    "        self.eval()\n",
    "        tokens = start_tokens.clone()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                logits = self(tokens)\n",
    "                next_logits = logits[:, -1, :] / temperature\n",
    "                probs = F.softmax(next_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1)\n",
    "                tokens = torch.cat([tokens, next_token], dim=1)\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "\n",
    "# Prepare training data for CLM\n",
    "def prepare_clm_data(texts, tokenizer, max_len=16):\n",
    "    \"\"\"Create input-target pairs for causal LM training.\"\"\"\n",
    "    all_ids = []\n",
    "    for text in texts:\n",
    "        ids = tokenizer.encode(text)\n",
    "        if len(ids) > max_len:\n",
    "            ids = ids[:max_len]\n",
    "        else:\n",
    "            ids = ids + [0] * (max_len - len(ids))\n",
    "        all_ids.append(ids)\n",
    "    \n",
    "    data = torch.tensor(all_ids)\n",
    "    # Input: all tokens except last; Target: all tokens except first\n",
    "    inputs = data[:, :-1]\n",
    "    targets = data[:, 1:]\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "inputs, targets = prepare_clm_data(train_texts, tok)\n",
    "print(f\"Training data: {inputs.shape[0]} sequences, length {inputs.shape[1]}\")\n",
    "print(f\"Vocab size: {tok.vocab_size}\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Input:  {tok.decode(inputs[0].tolist())}\")\n",
    "print(f\"  Target: {tok.decode(targets[0].tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = TinyLM(tok.vocab_size, d_model=64, n_heads=4, n_layers=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params:,}\")\n",
    "\n",
    "losses = []\n",
    "n_epochs = 200\n",
    "\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    logits = model(inputs)\n",
    "    \n",
    "    # Cross-entropy loss (ignoring padding)\n",
    "    loss = F.cross_entropy(\n",
    "        logits.reshape(-1, tok.vocab_size),\n",
    "        targets.reshape(-1),\n",
    "        ignore_index=0  # Ignore padding\n",
    "    )\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 40 == 0:\n",
    "        print(f\"  Epoch {epoch+1}/{n_epochs}: loss = {loss.item():.4f}\")\n",
    "\n",
    "# Generate some text\n",
    "print(\"\\nGeneration examples:\")\n",
    "prompts = [\"the cat\", \"a bird\", \"the dog\"]\n",
    "for prompt in prompts:\n",
    "    start = torch.tensor([tok.encode(prompt)])\n",
    "    generated = model.generate(start, max_new_tokens=6, temperature=0.8)\n",
    "    print(f\"  '{prompt}' -> '{tok.decode(generated[0].tolist())}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curve\n",
    "ax = axes[0]\n",
    "ax.plot(losses, color='#3498db', linewidth=1, alpha=0.3)\n",
    "# Smoothed\n",
    "window = 10\n",
    "smoothed = [np.mean(losses[max(0,i-window):i+1]) for i in range(len(losses))]\n",
    "ax.plot(smoothed, color='#3498db', linewidth=2, label='Smoothed')\n",
    "ax.set_xlabel('Epoch', fontsize=11)\n",
    "ax.set_ylabel('Cross-Entropy Loss', fontsize=11)\n",
    "ax.set_title('Language Model Training Loss', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Token probability heatmap for a test sequence\n",
    "ax = axes[1]\n",
    "test_input = torch.tensor([tok.encode(\"the cat sat on the\")])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(test_input)\n",
    "    probs = F.softmax(logits[0], dim=-1)\n",
    "\n",
    "# Show top-5 predictions for each position\n",
    "input_tokens = \"the cat sat on the\".split()\n",
    "n_pos = len(input_tokens)\n",
    "top_k = 5\n",
    "\n",
    "heatmap = np.zeros((top_k, n_pos))\n",
    "labels_y = []\n",
    "\n",
    "for pos in range(n_pos):\n",
    "    top_probs, top_ids = probs[pos].topk(top_k)\n",
    "    for k in range(top_k):\n",
    "        heatmap[k, pos] = top_probs[k].item()\n",
    "        if pos == 0:\n",
    "            labels_y.append(f\"Top-{k+1}\")\n",
    "\n",
    "im = ax.imshow(heatmap, cmap='YlOrRd', aspect='auto')\n",
    "ax.set_xticks(range(n_pos))\n",
    "ax.set_xticklabels(input_tokens, fontsize=10)\n",
    "ax.set_yticks(range(top_k))\n",
    "ax.set_yticklabels(labels_y, fontsize=10)\n",
    "ax.set_xlabel('Input Position', fontsize=11)\n",
    "ax.set_title('Next Token Probabilities', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Add text annotations\n",
    "for pos in range(n_pos):\n",
    "    top_probs, top_ids = probs[pos].topk(top_k)\n",
    "    for k in range(top_k):\n",
    "        word = tok.id2word.get(top_ids[k].item(), '?')\n",
    "        prob = top_probs[k].item()\n",
    "        color = 'white' if prob > 0.3 else 'black'\n",
    "        ax.text(pos, k, f'{word}\\n{prob:.2f}', ha='center', va='center',\n",
    "               fontsize=7, color=color, fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='Probability')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": "---\n\n## 7. Scaling Laws\n\nOne of the most important discoveries in modern AI: model performance follows **predictable power laws** as you scale model size, dataset size, and compute.\n\n### Chinchilla Scaling Laws (Hoffmann et al., 2022)\n\nFor a given compute budget $C$:\n$$L(N, D) = \\frac{A}{N^\\alpha} + \\frac{B}{D^\\beta} + L_\\infty$$\n\nwhere $N$ = parameters, $D$ = tokens, and $\\alpha \\approx 0.34$, $\\beta \\approx 0.28$.\n\n**Key insight**: Models should be trained on ~20x their parameter count in tokens. A 1B parameter model needs ~20B tokens.\n\n### What This Means in Practice\n\n| Model Size | Optimal Training Tokens | Approximate Compute | F1 Parallel |\n|-----------|------------------------|--------------------|-------------|\n| 1B | ~20B tokens | ~$10K | A backmarker team's simulation budget — basic wind tunnel + limited CFD |\n| 7B | ~140B tokens | ~$100K | A midfield team's off-season development program |\n| 70B | ~1.4T tokens | ~$2M | A top team's full-scale simulation farm for one season |\n| 400B | ~8T tokens | ~$50M+ | The entire F1 grid's combined simulation capacity |\n\n**F1 analogy:** Scaling laws in AI are remarkably similar to the diminishing returns in F1 development. Spending your first $10M on aero development gives huge lap time gains. The next $10M gives less. At some point, you need more *data* (track time, wind tunnel hours) not just a bigger model (more engineers). The Chinchilla insight — balance model size with training data — is exactly like the FIA cost cap philosophy: don't just throw money at the car; invest proportionally in testing, simulation, and track time."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate and visualize scaling laws\n",
    "\n",
    "def scaling_law_loss(N, D, A=406.4, B=410.7, alpha=0.34, beta=0.28, L_inf=1.69):\n",
    "    \"\"\"Chinchilla-style scaling law for loss prediction.\n",
    "    \n",
    "    N: number of parameters\n",
    "    D: number of training tokens\n",
    "    \"\"\"\n",
    "    return A / (N ** alpha) + B / (D ** beta) + L_inf\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 1. Loss vs model size (fixed data)\n",
    "ax = axes[0]\n",
    "param_counts = np.logspace(6, 11, 50)  # 1M to 100B\n",
    "for D_label, D in [('1B tokens', 1e9), ('10B tokens', 1e10), ('100B tokens', 1e11), ('1T tokens', 1e12)]:\n",
    "    losses_pred = [scaling_law_loss(N, D) for N in param_counts]\n",
    "    ax.plot(param_counts, losses_pred, linewidth=2, label=D_label)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Parameters (N)', fontsize=11)\n",
    "ax.set_ylabel('Loss', fontsize=11)\n",
    "ax.set_title('Loss vs Model Size', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Loss vs data size (fixed model)\n",
    "ax = axes[1]\n",
    "token_counts = np.logspace(8, 13, 50)  # 100M to 10T\n",
    "for N_label, N in [('100M params', 1e8), ('1B params', 1e9), ('10B params', 1e10), ('70B params', 7e10)]:\n",
    "    losses_pred = [scaling_law_loss(N, D) for D in token_counts]\n",
    "    ax.plot(token_counts, losses_pred, linewidth=2, label=N_label)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Training Tokens (D)', fontsize=11)\n",
    "ax.set_ylabel('Loss', fontsize=11)\n",
    "ax.set_title('Loss vs Data Size', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Compute-optimal frontier (Chinchilla)\n",
    "ax = axes[2]\n",
    "compute_budgets = np.logspace(18, 24, 30)  # FLOPs\n",
    "\n",
    "# For each compute budget, find optimal N and D\n",
    "# Chinchilla: D ≈ 20 * N, and C ≈ 6 * N * D\n",
    "optimal_N = []\n",
    "optimal_D = []\n",
    "optimal_loss = []\n",
    "\n",
    "for C in compute_budgets:\n",
    "    # C = 6ND, D = 20N -> C = 120N^2 -> N = sqrt(C/120)\n",
    "    N = math.sqrt(C / 120)\n",
    "    D = 20 * N\n",
    "    optimal_N.append(N)\n",
    "    optimal_D.append(D)\n",
    "    optimal_loss.append(scaling_law_loss(N, D))\n",
    "\n",
    "ax.plot(compute_budgets, optimal_loss, 'r-', linewidth=2, label='Compute-optimal')\n",
    "\n",
    "# Also show suboptimal: too-big model, too-small model\n",
    "for factor, label, color in [(0.2, 'Too small model', '#3498db'), (5, 'Too big model', '#f39c12')]:\n",
    "    sub_losses = []\n",
    "    for C in compute_budgets:\n",
    "        N = factor * math.sqrt(C / 120)\n",
    "        D = C / (6 * N)\n",
    "        sub_losses.append(scaling_law_loss(N, D))\n",
    "    ax.plot(compute_budgets, sub_losses, '--', linewidth=2, label=label, color=color)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Compute (FLOPs)', fontsize=11)\n",
    "ax.set_ylabel('Loss', fontsize=11)\n",
    "ax.set_title('Compute-Optimal Scaling', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Neural Scaling Laws', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some specific predictions\n",
    "print(\"Scaling Law Predictions:\\n\")\n",
    "for N_label, N in [('1B', 1e9), ('7B', 7e9), ('70B', 7e10)]:\n",
    "    D_optimal = 20 * N\n",
    "    loss = scaling_law_loss(N, D_optimal)\n",
    "    print(f\"  {N_label} params + {D_optimal/1e9:.0f}B tokens: predicted loss = {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": "---\n\n## 8. Training Dynamics\n\nSuccessfully training a language model requires careful management of the training process.\n\n### Key Concepts\n\n| Technique | Purpose | Details | F1 Parallel |\n|-----------|---------|--------|-------------|\n| **Learning rate warmup** | Stabilize early training | Linearly increase LR for first N steps | Warming up tires on an out-lap before pushing — go too hard too early and you spin |\n| **Cosine schedule** | Gradual LR decay | Smoothly decrease LR following cosine curve | Fuel-load management — push hard early in a stint, then manage as tires degrade |\n| **Gradient clipping** | Prevent exploding gradients | Cap gradient norm at max value | Rev limiter — prevents the engine from destroying itself under full load |\n| **Weight decay** | Regularization | L2 penalty on parameters | Minimum weight regulations — prevents the car from being optimized into fragility |\n| **Mixed precision** | Speed + memory savings | FP16 compute, FP32 accumulation | Using lower-precision sensors where acceptable, high-precision only where critical |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRScheduler:\n",
    "    \"\"\"Learning rate schedules commonly used in LLM training.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosine_with_warmup(step, total_steps, warmup_steps, max_lr, min_lr=0):\n",
    "        \"\"\"Cosine schedule with linear warmup.\"\"\"\n",
    "        if step < warmup_steps:\n",
    "            # Linear warmup\n",
    "            return max_lr * step / warmup_steps\n",
    "        \n",
    "        # Cosine decay\n",
    "        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "        return min_lr + 0.5 * (max_lr - min_lr) * (1 + math.cos(math.pi * progress))\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear_with_warmup(step, total_steps, warmup_steps, max_lr):\n",
    "        \"\"\"Linear decay with warmup.\"\"\"\n",
    "        if step < warmup_steps:\n",
    "            return max_lr * step / warmup_steps\n",
    "        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "        return max_lr * (1 - progress)\n",
    "    \n",
    "    @staticmethod\n",
    "    def constant_with_warmup(step, total_steps, warmup_steps, max_lr):\n",
    "        \"\"\"Constant LR with warmup.\"\"\"\n",
    "        if step < warmup_steps:\n",
    "            return max_lr * step / warmup_steps\n",
    "        return max_lr\n",
    "\n",
    "\n",
    "# Visualize schedules\n",
    "total_steps = 1000\n",
    "warmup_steps = 100\n",
    "max_lr = 3e-4\n",
    "\n",
    "steps = range(total_steps)\n",
    "\n",
    "schedules = {\n",
    "    'Cosine + Warmup': [LRScheduler.cosine_with_warmup(s, total_steps, warmup_steps, max_lr, max_lr * 0.1) for s in steps],\n",
    "    'Linear + Warmup': [LRScheduler.linear_with_warmup(s, total_steps, warmup_steps, max_lr) for s in steps],\n",
    "    'Constant + Warmup': [LRScheduler.constant_with_warmup(s, total_steps, warmup_steps, max_lr) for s in steps],\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# LR schedules\n",
    "ax = axes[0]\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "for (name, lrs), color in zip(schedules.items(), colors):\n",
    "    ax.plot(steps, lrs, linewidth=2, label=name, color=color)\n",
    "\n",
    "ax.axvline(x=warmup_steps, color='gray', linestyle=':', alpha=0.5, label='Warmup end')\n",
    "ax.set_xlabel('Training Step', fontsize=11)\n",
    "ax.set_ylabel('Learning Rate', fontsize=11)\n",
    "ax.set_title('Learning Rate Schedules', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Simulated training loss for different schedules\n",
    "ax = axes[1]\n",
    "np.random.seed(42)\n",
    "base_loss = 4.0\n",
    "\n",
    "for (name, lrs), color in zip(schedules.items(), colors):\n",
    "    # Simulate loss curve (lower LR = smoother convergence)\n",
    "    sim_loss = [base_loss]\n",
    "    for i in range(1, total_steps):\n",
    "        lr = lrs[i]\n",
    "        # Loss decreases proportional to LR, with noise\n",
    "        decrease = lr / max_lr * 0.005\n",
    "        noise = np.random.normal(0, 0.02)\n",
    "        new_loss = max(1.5, sim_loss[-1] - decrease + noise)\n",
    "        sim_loss.append(new_loss)\n",
    "    \n",
    "    # Smooth for display\n",
    "    w = 20\n",
    "    smoothed = [np.mean(sim_loss[max(0,i-w):i+1]) for i in range(len(sim_loss))]\n",
    "    ax.plot(steps, smoothed, linewidth=2, label=name, color=color)\n",
    "\n",
    "ax.set_xlabel('Training Step', fontsize=11)\n",
    "ax.set_ylabel('Loss', fontsize=11)\n",
    "ax.set_title('Simulated Training Loss', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": "---\n\n## Exercises\n\n### Exercise 1: Unigram Tokenizer\n\nImplement a **Unigram** tokenizer (used by SentencePiece/T5). Unlike BPE which builds up by merging, Unigram starts with a large vocabulary and prunes tokens that contribute least to the likelihood of the training data. Compare its vocabulary with BPE on the same corpus.\n\n**F1 scenario:** Imagine you start with an enormous telemetry codebook containing every possible sensor pattern (all substrings up to length k). Your job is to slim it down by removing the patterns that matter least to reconstructing real race data — keeping the codebook compact while still covering the critical patterns."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Hint: Start with all substrings up to length k, then iteratively remove\n",
    "# the token whose removal increases training loss the least.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": "### Exercise 2: Masked Language Model\n\nModify the TinyLM to support masked language modeling (BERT-style). Randomly mask 15% of tokens, and train the model to predict the masked tokens. Compare the learned representations with the CLM model.\n\n**F1 scenario:** Instead of predicting \"what happens next on track,\" train a model that fills in missing telemetry gaps. Mask out 15% of a lap's data points and train the model to reconstruct them from context — like a race engineer reconstructing corrupted sectors from the data around them."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Hint: Remove the causal mask, add random masking to inputs,\n",
    "# and only compute loss on masked positions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": "### Exercise 3: Scaling Experiment\n\nTrain 3 versions of TinyLM with different sizes (e.g., d_model=32, 64, 128) on the same data. Plot their loss curves together and verify that larger models converge faster. Does the scaling law prediction hold even at this tiny scale?\n\n**F1 scenario:** Think of this as comparing three different simulation rigs: a basic desktop sim (d_model=32), a professional driver-in-the-loop simulator (d_model=64), and a full-scale hydraulic platform (d_model=128). All trained on the same track data. The bigger rigs should converge to accurate predictions faster — but do the scaling laws hold even at \"model car\" scale?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "# Hint: Loop over model sizes, train each, collect loss curves, plot together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": "---\n\n## Summary\n\n### Key Concepts\n\n| Concept | What It Does | F1 Parallel |\n|---------|-------------|-------------|\n| **Subword tokenization** (BPE, WordPiece) | Balances vocabulary size with sequence length | Building a codebook of telemetry patterns — common sequences get single tokens, rare events decompose |\n| **BPE** | Iteratively merges the most frequent adjacent pairs | Learning the most common sensor co-occurrences across thousands of laps |\n| **WordPiece** | Uses a likelihood ratio instead of raw frequency | Finding patterns that are *surprisingly* co-occurring, not just frequent |\n| **Causal LM** (GPT-style) | Predicts the next token | Pit wall predicting the next event on track from everything so far |\n| **Masked LM** (BERT-style) | Fills in masked tokens from bidirectional context | Reconstructing corrupted telemetry from surrounding data |\n| **Scaling laws** | Loss decreases as a power law with model size, data, and compute | Diminishing returns on development spend — but predictable ones |\n| **Chinchilla optimal** | ~20 tokens per parameter | Balance car development budget with testing/simulation time |\n| **Training dynamics** | Warmup, cosine decay, gradient clipping for stability | Tire warmup laps, fuel management, rev limiters for the training process |\n\n### Why This Matters\n\nTokenization and pretraining are the foundation that everything else builds on. The tokenizer determines what the model can represent — like how the telemetry encoding determines what patterns the pit wall can detect. The pretraining objective determines what it learns — predicting the future (CLM) or understanding context (MLM). The scaling laws tell us how to allocate our compute budget — just as an F1 team must allocate its cost cap between car development, testing, and race operations. Every downstream capability — from following instructions to writing code — depends on getting these foundations right."
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": "---\n\n## Next Steps\n\nNow that we understand how language models are trained from scratch — building the telemetry codebook (tokenization), teaching the model to predict the next event (pretraining), and understanding the budget tradeoffs (scaling laws) — the next question is: how do we make these models *fast* in production? In **Notebook 30: Inference Optimization**, we'll explore the techniques that take a model from the simulation farm to the live pit wall — quantization, KV caching, speculative decoding, and more."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}