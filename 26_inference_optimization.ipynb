{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Part 8.2: LLM Inference Optimization\n",
    "\n",
    "Training a model is expensive, but **inference** is where the real cost lives. Every user query, every API call, every token generated costs compute. At scale, a model that's 2x faster or uses half the memory saves millions of dollars.\n",
    "\n",
    "LLM inference has unique challenges: models are enormous (7B-400B+ parameters), generation is sequential (each token depends on the last), and users expect low latency. This notebook covers the key techniques that make production LLM serving feasible.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- [ ] Understand the inference bottleneck: why LLM generation is slow\n",
    "- [ ] Implement model quantization (INT8, INT4) from scratch\n",
    "- [ ] Build a KV cache to eliminate redundant computation\n",
    "- [ ] Understand speculative decoding for faster generation\n",
    "- [ ] Implement continuous batching for throughput optimization\n",
    "- [ ] Build a model distillation pipeline (teacher → student)\n",
    "- [ ] Compare optimization techniques on speed, memory, and quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Part 8.2: LLM Inference Optimization\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The Inference Bottleneck\n",
    "\n",
    "LLM inference has two phases:\n",
    "\n",
    "1. **Prefill**: Process the entire prompt in parallel (compute-bound)\n",
    "2. **Decode**: Generate tokens one at a time, each depending on all previous (memory-bound)\n",
    "\n",
    "The decode phase is the bottleneck because:\n",
    "- Each token requires loading the **entire model** from memory\n",
    "- Generation is inherently **sequential** (can't parallelize across tokens)\n",
    "- Most time is spent moving weights from memory to compute, not doing math\n",
    "\n",
    "| Phase | Compute Pattern | Bottleneck | Tokens/sec |\n",
    "|-------|----------------|------------|------------|\n",
    "| Prefill | Matrix multiply (batched) | Compute | High (thousands) |\n",
    "| Decode | Matrix-vector product | Memory bandwidth | Low (tens) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the inference pipeline\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Prefill vs Decode phases\n",
    "ax = axes[0]\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.axis('off')\n",
    "ax.set_title('LLM Inference Phases', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Prefill\n",
    "box = mpatches.FancyBboxPatch((0.5, 3), 5, 2, boxstyle=\"round,pad=0.2\",\n",
    "                               facecolor='#3498db', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(box)\n",
    "ax.text(3, 4.3, 'PREFILL', ha='center', fontsize=12, fontweight='bold', color='white')\n",
    "ax.text(3, 3.6, 'Process entire prompt\\nin parallel (fast)', ha='center', fontsize=9, color='white')\n",
    "\n",
    "# Decode tokens\n",
    "for i in range(5):\n",
    "    x = 7 + i * 1.3\n",
    "    box = mpatches.FancyBboxPatch((x, 3), 1, 2, boxstyle=\"round,pad=0.1\",\n",
    "                                   facecolor='#e74c3c', edgecolor='black', linewidth=1.5)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x + 0.5, 4, f'T{i+1}', ha='center', fontsize=10, fontweight='bold', color='white')\n",
    "\n",
    "ax.text(10.2, 3.3, 'DECODE: one token at a time (slow)', ha='center', fontsize=9, color='gray')\n",
    "\n",
    "ax.annotate('', xy=(6.8, 4), xytext=(5.5, 4),\n",
    "           arrowprops=dict(arrowstyle='->', lw=2, color='gray'))\n",
    "\n",
    "# Prompt tokens\n",
    "for i in range(6):\n",
    "    ax.text(0.8 + i * 0.8, 1.5, f'p{i}', ha='center', fontsize=9,\n",
    "           bbox=dict(boxstyle='round', facecolor='#ecf0f1', edgecolor='gray'))\n",
    "ax.text(3, 0.8, 'Input prompt tokens', ha='center', fontsize=9, color='gray')\n",
    "\n",
    "# Time breakdown\n",
    "ax = axes[1]\n",
    "categories = ['7B Model', '13B Model', '70B Model']\n",
    "prefill_times = [0.05, 0.1, 0.5]  # seconds for 512 token prompt\n",
    "decode_times = [2.0, 3.8, 20.0]   # seconds for 256 output tokens\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "w = 0.35\n",
    "ax.bar(x - w/2, prefill_times, w, label='Prefill (512 tokens)', color='#3498db', edgecolor='black')\n",
    "ax.bar(x + w/2, decode_times, w, label='Decode (256 tokens)', color='#e74c3c', edgecolor='black')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.set_ylabel('Time (seconds)', fontsize=11)\n",
    "ax.set_title('Prefill vs Decode Time', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Quantization\n",
    "\n",
    "**Quantization** reduces the precision of model weights from 32-bit floats to 8-bit or 4-bit integers. This cuts memory usage by 4-8x and speeds up inference (less data to move from memory).\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Map floating-point values to a smaller integer range:\n",
    "$$x_\\text{quant} = \\text{round}\\left(\\frac{x - \\text{zero\\_point}}{\\text{scale}}\\right)$$\n",
    "$$x_\\text{dequant} = x_\\text{quant} \\times \\text{scale} + \\text{zero\\_point}$$\n",
    "\n",
    "| Precision | Bits | Memory per 1B params | Typical Quality Loss |\n",
    "|-----------|------|---------------------|--------------------|\n",
    "| FP32 | 32 | 4 GB | Baseline |\n",
    "| FP16/BF16 | 16 | 2 GB | Negligible |\n",
    "| INT8 | 8 | 1 GB | <1% degradation |\n",
    "| INT4 (GPTQ/AWQ) | 4 | 0.5 GB | 1-3% degradation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantizer:\n",
    "    \"\"\"Weight quantization from scratch.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def quantize_absmax(weights, n_bits=8):\n",
    "        \"\"\"Absmax (symmetric) quantization.\n",
    "        \n",
    "        Maps [-max, max] to [-2^(n-1), 2^(n-1)-1]\n",
    "        \"\"\"\n",
    "        qmax = 2**(n_bits - 1) - 1\n",
    "        scale = weights.abs().max() / qmax\n",
    "        quantized = torch.round(weights / scale).clamp(-qmax, qmax).to(torch.int8)\n",
    "        return quantized, scale\n",
    "    \n",
    "    @staticmethod\n",
    "    def dequantize_absmax(quantized, scale):\n",
    "        \"\"\"Dequantize absmax back to float.\"\"\"\n",
    "        return quantized.float() * scale\n",
    "    \n",
    "    @staticmethod\n",
    "    def quantize_zeropoint(weights, n_bits=8):\n",
    "        \"\"\"Zero-point (asymmetric) quantization.\n",
    "        \n",
    "        Maps [min, max] to [0, 2^n - 1]\n",
    "        \"\"\"\n",
    "        qmin, qmax = 0, 2**n_bits - 1\n",
    "        w_min, w_max = weights.min(), weights.max()\n",
    "        \n",
    "        scale = (w_max - w_min) / (qmax - qmin)\n",
    "        zero_point = torch.round(-w_min / scale).clamp(qmin, qmax)\n",
    "        \n",
    "        quantized = torch.round(weights / scale + zero_point).clamp(qmin, qmax).to(torch.uint8)\n",
    "        return quantized, scale, zero_point\n",
    "    \n",
    "    @staticmethod\n",
    "    def dequantize_zeropoint(quantized, scale, zero_point):\n",
    "        \"\"\"Dequantize zero-point back to float.\"\"\"\n",
    "        return (quantized.float() - zero_point) * scale\n",
    "    \n",
    "    @staticmethod\n",
    "    def quantize_per_channel(weights, n_bits=8):\n",
    "        \"\"\"Per-channel quantization (one scale per output channel).\n",
    "        \n",
    "        Better quality than per-tensor because each channel has its own range.\n",
    "        \"\"\"\n",
    "        qmax = 2**(n_bits - 1) - 1\n",
    "        # Scale per output channel (row)\n",
    "        scales = weights.abs().max(dim=1, keepdim=True).values / qmax\n",
    "        scales = scales.clamp(min=1e-8)  # Avoid division by zero\n",
    "        quantized = torch.round(weights / scales).clamp(-qmax, qmax).to(torch.int8)\n",
    "        return quantized, scales.squeeze()\n",
    "    \n",
    "    @staticmethod\n",
    "    def dequantize_per_channel(quantized, scales):\n",
    "        return quantized.float() * scales.unsqueeze(1)\n",
    "\n",
    "\n",
    "# Create a sample weight matrix (simulating a linear layer)\n",
    "torch.manual_seed(42)\n",
    "weights = torch.randn(256, 512) * 0.02  # Typical weight scale\n",
    "\n",
    "quantizer = Quantizer()\n",
    "\n",
    "# Quantize with different methods\n",
    "q8_absmax, scale_8 = quantizer.quantize_absmax(weights, n_bits=8)\n",
    "dq8_absmax = quantizer.dequantize_absmax(q8_absmax, scale_8)\n",
    "\n",
    "q4_absmax, scale_4 = quantizer.quantize_absmax(weights, n_bits=4)\n",
    "dq4_absmax = quantizer.dequantize_absmax(q4_absmax, scale_4)\n",
    "\n",
    "q8_pc, scales_pc = quantizer.quantize_per_channel(weights, n_bits=8)\n",
    "dq8_pc = quantizer.dequantize_per_channel(q8_pc, scales_pc)\n",
    "\n",
    "# Measure quantization error\n",
    "def quantization_error(original, dequantized):\n",
    "    mse = ((original - dequantized) ** 2).mean().item()\n",
    "    max_err = (original - dequantized).abs().max().item()\n",
    "    snr = 10 * math.log10(original.var().item() / mse) if mse > 0 else float('inf')\n",
    "    return {'mse': mse, 'max_error': max_err, 'snr_db': snr}\n",
    "\n",
    "print(\"Quantization Results\\n\")\n",
    "print(f\"Original weights: {weights.shape}, dtype={weights.dtype}\")\n",
    "print(f\"Memory: {weights.numel() * 4 / 1024:.1f} KB (FP32)\\n\")\n",
    "\n",
    "methods = [\n",
    "    ('INT8 absmax', dq8_absmax, weights.numel() * 1),\n",
    "    ('INT4 absmax', dq4_absmax, weights.numel() * 0.5),\n",
    "    ('INT8 per-channel', dq8_pc, weights.numel() * 1),\n",
    "]\n",
    "\n",
    "print(f\"{'Method':>20} {'MSE':>12} {'Max Error':>12} {'SNR (dB)':>10} {'Memory (KB)':>12} {'Compression':>12}\")\n",
    "print(\"-\" * 80)\n",
    "for name, dq, mem_bytes in methods:\n",
    "    err = quantization_error(weights, dq)\n",
    "    mem_kb = mem_bytes / 1024\n",
    "    orig_kb = weights.numel() * 4 / 1024\n",
    "    print(f\"{name:>20} {err['mse']:>12.2e} {err['max_error']:>12.4f} {err['snr_db']:>10.1f} \"\n",
    "          f\"{mem_kb:>12.1f} {orig_kb/mem_kb:>11.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize quantization effects\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Weight distribution: original vs quantized\n",
    "ax = axes[0, 0]\n",
    "ax.hist(weights.flatten().numpy(), bins=100, alpha=0.6, label='FP32 original',\n",
    "       color='#3498db', edgecolor='black', density=True)\n",
    "ax.hist(dq8_absmax.flatten().numpy(), bins=100, alpha=0.6, label='INT8 dequantized',\n",
    "       color='#e74c3c', edgecolor='black', density=True)\n",
    "ax.set_xlabel('Weight Value', fontsize=10)\n",
    "ax.set_ylabel('Density', fontsize=10)\n",
    "ax.set_title('Weight Distribution: FP32 vs INT8', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# Quantization error distribution\n",
    "ax = axes[0, 1]\n",
    "error_8 = (weights - dq8_absmax).flatten().numpy()\n",
    "error_4 = (weights - dq4_absmax).flatten().numpy()\n",
    "ax.hist(error_8, bins=100, alpha=0.6, label='INT8 error', color='#2ecc71', density=True)\n",
    "ax.hist(error_4, bins=100, alpha=0.6, label='INT4 error', color='#e74c3c', density=True)\n",
    "ax.set_xlabel('Quantization Error', fontsize=10)\n",
    "ax.set_ylabel('Density', fontsize=10)\n",
    "ax.set_title('Quantization Error Distribution', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# Scatter: original vs quantized values\n",
    "ax = axes[1, 0]\n",
    "sample = np.random.choice(weights.numel(), 2000, replace=False)\n",
    "ax.scatter(weights.flatten()[sample].numpy(), dq8_absmax.flatten()[sample].numpy(),\n",
    "          alpha=0.2, s=5, color='#3498db', label='INT8')\n",
    "ax.scatter(weights.flatten()[sample].numpy(), dq4_absmax.flatten()[sample].numpy(),\n",
    "          alpha=0.2, s=5, color='#e74c3c', label='INT4')\n",
    "lim = 0.06\n",
    "ax.plot([-lim, lim], [-lim, lim], 'k--', alpha=0.3)\n",
    "ax.set_xlabel('Original (FP32)', fontsize=10)\n",
    "ax.set_ylabel('Dequantized', fontsize=10)\n",
    "ax.set_title('Original vs Dequantized Values', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# Memory comparison\n",
    "ax = axes[1, 1]\n",
    "model_sizes_gb = [7, 13, 70]  # Billion params\n",
    "precisions = ['FP32', 'FP16', 'INT8', 'INT4']\n",
    "multipliers = [4, 2, 1, 0.5]  # Bytes per param\n",
    "\n",
    "x = np.arange(len(model_sizes_gb))\n",
    "w = 0.2\n",
    "colors = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c']\n",
    "\n",
    "for i, (prec, mult, color) in enumerate(zip(precisions, multipliers, colors)):\n",
    "    mem = [s * mult for s in model_sizes_gb]\n",
    "    ax.bar(x + i * w, mem, w, label=prec, color=color, edgecolor='black', alpha=0.8)\n",
    "\n",
    "ax.set_xticks(x + 1.5 * w)\n",
    "ax.set_xticklabels([f'{s}B params' for s in model_sizes_gb])\n",
    "ax.set_ylabel('Memory (GB)', fontsize=10)\n",
    "ax.set_title('Model Memory by Precision', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. KV Cache\n",
    "\n",
    "The **KV cache** is the single most important optimization for autoregressive generation. Without it, generating token $t$ requires recomputing attention for all $t-1$ previous tokens.\n",
    "\n",
    "### The Problem\n",
    "In self-attention, we compute:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "When generating token $t$, the keys and values for tokens $1$ through $t-1$ haven't changed — but without caching, we recompute them every time.\n",
    "\n",
    "### The Solution\n",
    "Cache the K and V matrices from previous tokens. When generating token $t$:\n",
    "1. Compute Q, K, V for **only the new token**\n",
    "2. Append new K, V to the cache\n",
    "3. Attend to the full cached K, V\n",
    "\n",
    "This reduces per-token compute from O(t²) to O(t)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedAttention(nn.Module):\n",
    "    \"\"\"Self-attention with KV cache for efficient generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, cache=None):\n",
    "        \"\"\"Forward with optional KV cache.\n",
    "        \n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "            cache: tuple of (cached_K, cached_V) or None\n",
    "        \n",
    "        Returns:\n",
    "            output, (new_K, new_V)\n",
    "        \"\"\"\n",
    "        B, T, _ = x.shape\n",
    "        \n",
    "        Q = self.q_proj(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        K = self.k_proj(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        V = self.v_proj(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # Append to cache\n",
    "        if cache is not None:\n",
    "            cached_K, cached_V = cache\n",
    "            K = torch.cat([cached_K, K], dim=2)\n",
    "            V = torch.cat([cached_V, V], dim=2)\n",
    "        \n",
    "        # Standard attention\n",
    "        scale = math.sqrt(self.d_head)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / scale\n",
    "        \n",
    "        # Causal mask\n",
    "        seq_len_k = K.shape[2]\n",
    "        seq_len_q = Q.shape[2]\n",
    "        mask = torch.triu(torch.ones(seq_len_q, seq_len_k, device=x.device),\n",
    "                         diagonal=seq_len_k - seq_len_q + 1).bool()\n",
    "        scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, self.d_model)\n",
    "        \n",
    "        return self.out_proj(out), (K, V)\n",
    "\n",
    "\n",
    "# Benchmark: with vs without KV cache\n",
    "d_model = 128\n",
    "n_heads = 4\n",
    "attn = CachedAttention(d_model, n_heads)\n",
    "attn.eval()\n",
    "\n",
    "# Simulate generating 32 tokens\n",
    "gen_len = 32\n",
    "prompt = torch.randn(1, 8, d_model)  # 8-token prompt\n",
    "\n",
    "# WITHOUT cache: recompute everything each step\n",
    "start = time.perf_counter()\n",
    "all_tokens_no_cache = prompt.clone()\n",
    "with torch.no_grad():\n",
    "    for i in range(gen_len):\n",
    "        out, _ = attn(all_tokens_no_cache, cache=None)\n",
    "        new_token = out[:, -1:, :]  # Take last token output\n",
    "        all_tokens_no_cache = torch.cat([all_tokens_no_cache, new_token], dim=1)\n",
    "time_no_cache = time.perf_counter() - start\n",
    "\n",
    "# WITH cache: only process new token each step\n",
    "start = time.perf_counter()\n",
    "with torch.no_grad():\n",
    "    # Prefill: process prompt\n",
    "    out, cache = attn(prompt, cache=None)\n",
    "    generated = [out[:, -1:, :]]\n",
    "    \n",
    "    # Decode: one token at a time\n",
    "    for i in range(gen_len - 1):\n",
    "        out, cache = attn(generated[-1], cache=cache)\n",
    "        generated.append(out)\n",
    "time_with_cache = time.perf_counter() - start\n",
    "\n",
    "print(\"KV Cache Benchmark\\n\")\n",
    "print(f\"  Without cache: {time_no_cache*1000:.1f}ms\")\n",
    "print(f\"  With cache:    {time_with_cache*1000:.1f}ms\")\n",
    "print(f\"  Speedup:       {time_no_cache/time_with_cache:.1f}x\")\n",
    "print(f\"  Cache size:    K={cache[0].shape}, V={cache[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize KV cache mechanics and scaling\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Computation per step: with vs without cache\n",
    "ax = axes[0]\n",
    "seq_lens = np.arange(1, 65)\n",
    "# Without cache: O(t^2) total compute at step t\n",
    "no_cache_ops = seq_lens ** 2\n",
    "# With cache: O(t) compute at step t\n",
    "cache_ops = seq_lens\n",
    "\n",
    "ax.plot(seq_lens, no_cache_ops, 'r-', linewidth=2, label='Without KV cache (O(t²))')\n",
    "ax.plot(seq_lens, cache_ops, 'g-', linewidth=2, label='With KV cache (O(t))')\n",
    "ax.fill_between(seq_lens, cache_ops, no_cache_ops, alpha=0.15, color='red', label='Saved compute')\n",
    "ax.set_xlabel('Sequence Position', fontsize=11)\n",
    "ax.set_ylabel('Compute Operations', fontsize=11)\n",
    "ax.set_title('Per-Step Compute: Cache vs No Cache', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# KV cache memory usage\n",
    "ax = axes[1]\n",
    "seq_lengths = [512, 1024, 2048, 4096, 8192, 16384]\n",
    "# Cache size per layer = 2 * seq_len * d_model * batch_size (K and V)\n",
    "# For 7B model: ~32 layers, d_model=4096\n",
    "d = 4096\n",
    "n_layers = 32\n",
    "batch_sizes = [1, 4, 16]\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    cache_gb = [2 * sl * d * n_layers * bs * 2 / 1e9 for sl in seq_lengths]  # FP16\n",
    "    ax.plot(seq_lengths, cache_gb, 'o-', linewidth=2, markersize=6, label=f'Batch size {bs}')\n",
    "\n",
    "ax.set_xlabel('Sequence Length', fontsize=11)\n",
    "ax.set_ylabel('KV Cache Memory (GB)', fontsize=11)\n",
    "ax.set_title('KV Cache Memory (7B model, FP16)', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Speculative Decoding\n",
    "\n",
    "**Speculative decoding** uses a small, fast \"draft\" model to propose multiple tokens at once, then verifies them with the large model in a single forward pass.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Draft**: Small model generates K candidate tokens quickly\n",
    "2. **Verify**: Large model scores all K tokens in parallel (one forward pass)\n",
    "3. **Accept/reject**: Keep tokens where draft agrees with large model\n",
    "4. **Resample**: If rejected at position i, sample from large model's distribution there\n",
    "\n",
    "The key insight: the large model does the **same amount of work** regardless of how many draft tokens are accepted, but each accepted token saves a full decode step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeculativeDecoder:\n",
    "    \"\"\"Speculative decoding simulation.\"\"\"\n",
    "    \n",
    "    def __init__(self, target_model_time_ms, draft_model_time_ms, acceptance_rate=0.7):\n",
    "        \"\"\"\n",
    "        target_model_time_ms: Time for one forward pass of large model\n",
    "        draft_model_time_ms: Time for one forward pass of small model\n",
    "        acceptance_rate: Probability draft token matches target\n",
    "        \"\"\"\n",
    "        self.target_time = target_model_time_ms\n",
    "        self.draft_time = draft_model_time_ms\n",
    "        self.acceptance_rate = acceptance_rate\n",
    "    \n",
    "    def simulate_normal_decoding(self, n_tokens):\n",
    "        \"\"\"Standard autoregressive decoding.\"\"\"\n",
    "        return n_tokens * self.target_time\n",
    "    \n",
    "    def simulate_speculative(self, n_tokens, k=5):\n",
    "        \"\"\"Speculative decoding with k draft tokens.\"\"\"\n",
    "        total_time = 0\n",
    "        generated = 0\n",
    "        n_verify_calls = 0\n",
    "        n_draft_calls = 0\n",
    "        \n",
    "        while generated < n_tokens:\n",
    "            # Draft: generate k tokens with small model\n",
    "            total_time += k * self.draft_time\n",
    "            n_draft_calls += k\n",
    "            \n",
    "            # Verify: one forward pass of large model for all k tokens\n",
    "            total_time += self.target_time\n",
    "            n_verify_calls += 1\n",
    "            \n",
    "            # Accept tokens until first rejection\n",
    "            accepted = 0\n",
    "            for _ in range(k):\n",
    "                if np.random.random() < self.acceptance_rate:\n",
    "                    accepted += 1\n",
    "                else:\n",
    "                    accepted += 1  # We still get 1 token from large model at rejection point\n",
    "                    break\n",
    "            \n",
    "            generated += accepted\n",
    "        \n",
    "        return total_time, n_verify_calls, n_draft_calls\n",
    "\n",
    "\n",
    "# Compare decoding strategies\n",
    "decoder = SpeculativeDecoder(\n",
    "    target_model_time_ms=50,  # Large model: 50ms per token\n",
    "    draft_model_time_ms=5,    # Small model: 5ms per token\n",
    "    acceptance_rate=0.7\n",
    ")\n",
    "\n",
    "n_tokens = 256\n",
    "\n",
    "normal_time = decoder.simulate_normal_decoding(n_tokens)\n",
    "\n",
    "print(f\"Generating {n_tokens} tokens\\n\")\n",
    "print(f\"Normal decoding: {normal_time:.0f}ms ({n_tokens} target calls)\")\n",
    "\n",
    "# Try different k values\n",
    "results = []\n",
    "for k in [2, 3, 5, 8, 12]:\n",
    "    times = []\n",
    "    for _ in range(50):  # Average over runs\n",
    "        t, n_verify, n_draft = decoder.simulate_speculative(n_tokens, k=k)\n",
    "        times.append(t)\n",
    "    avg_time = np.mean(times)\n",
    "    speedup = normal_time / avg_time\n",
    "    results.append({'k': k, 'time': avg_time, 'speedup': speedup})\n",
    "    print(f\"Speculative (k={k:>2}): {avg_time:.0f}ms -> {speedup:.2f}x speedup\")\n",
    "\n",
    "# Also vary acceptance rate\n",
    "print(\"\\nEffect of acceptance rate (k=5):\")\n",
    "for rate in [0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    decoder.acceptance_rate = rate\n",
    "    times = [decoder.simulate_speculative(n_tokens, k=5)[0] for _ in range(50)]\n",
    "    avg = np.mean(times)\n",
    "    print(f\"  acceptance={rate:.0%}: {avg:.0f}ms -> {normal_time/avg:.2f}x speedup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize speculative decoding\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Speedup vs k\n",
    "ax = axes[0]\n",
    "ks = [r['k'] for r in results]\n",
    "speedups = [r['speedup'] for r in results]\n",
    "ax.plot(ks, speedups, 'bo-', linewidth=2, markersize=8)\n",
    "ax.axhline(y=1, color='gray', linestyle='--', alpha=0.5, label='No speedup')\n",
    "ax.set_xlabel('Draft Length (k)', fontsize=11)\n",
    "ax.set_ylabel('Speedup vs Normal Decoding', fontsize=11)\n",
    "ax.set_title('Speculative Decoding Speedup', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Speedup vs acceptance rate\n",
    "ax = axes[1]\n",
    "rates = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "for k in [3, 5, 8]:\n",
    "    speedups_rate = []\n",
    "    for rate in rates:\n",
    "        decoder.acceptance_rate = rate\n",
    "        times = [decoder.simulate_speculative(n_tokens, k=k)[0] for _ in range(30)]\n",
    "        speedups_rate.append(normal_time / np.mean(times))\n",
    "    ax.plot(rates, speedups_rate, 'o-', linewidth=2, markersize=6, label=f'k={k}')\n",
    "\n",
    "ax.set_xlabel('Acceptance Rate', fontsize=11)\n",
    "ax.set_ylabel('Speedup', fontsize=11)\n",
    "ax.set_title('Speedup vs Draft Quality', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Continuous Batching\n",
    "\n",
    "In naive batching, all requests in a batch must wait for the longest request to finish. **Continuous batching** (also called \"inflight batching\") allows new requests to join and completed requests to leave the batch dynamically.\n",
    "\n",
    "| Approach | Throughput | Latency | GPU Utilization |\n",
    "|----------|-----------|---------|----------------|\n",
    "| No batching | Low | Optimal per-request | Very low |\n",
    "| Static batching | Medium | Worst-case per-batch | Medium |\n",
    "| Continuous batching | High | Near-optimal | High |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchingSimulator:\n",
    "    \"\"\"Simulate different batching strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self, time_per_token_ms=10, max_batch_size=16):\n",
    "        self.time_per_token = time_per_token_ms\n",
    "        self.max_batch = max_batch_size\n",
    "    \n",
    "    def no_batching(self, requests):\n",
    "        \"\"\"Process requests one at a time.\"\"\"\n",
    "        results = []\n",
    "        current_time = 0\n",
    "        \n",
    "        for req in requests:\n",
    "            start = max(current_time, req['arrival'])\n",
    "            duration = req['output_tokens'] * self.time_per_token\n",
    "            end = start + duration\n",
    "            results.append({\n",
    "                'latency': end - req['arrival'],\n",
    "                'start': start, 'end': end\n",
    "            })\n",
    "            current_time = end\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def static_batching(self, requests, batch_size=4):\n",
    "        \"\"\"Process requests in fixed-size batches.\"\"\"\n",
    "        results = [None] * len(requests)\n",
    "        current_time = 0\n",
    "        \n",
    "        for i in range(0, len(requests), batch_size):\n",
    "            batch = requests[i:i+batch_size]\n",
    "            batch_start = max(current_time, max(r['arrival'] for r in batch))\n",
    "            \n",
    "            # All requests must wait for the longest one\n",
    "            max_tokens = max(r['output_tokens'] for r in batch)\n",
    "            # Batched: ~same time as single request (parallel on GPU)\n",
    "            duration = max_tokens * self.time_per_token\n",
    "            batch_end = batch_start + duration\n",
    "            \n",
    "            for j, req in enumerate(batch):\n",
    "                results[i + j] = {\n",
    "                    'latency': batch_end - req['arrival'],\n",
    "                    'start': batch_start, 'end': batch_end\n",
    "                }\n",
    "            current_time = batch_end\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def continuous_batching(self, requests, max_batch=8):\n",
    "        \"\"\"Process requests with continuous batching.\"\"\"\n",
    "        results = [None] * len(requests)\n",
    "        active = []  # (request_idx, tokens_remaining)\n",
    "        queue = list(range(len(requests)))\n",
    "        current_time = 0\n",
    "        \n",
    "        while queue or active:\n",
    "            # Add new requests to batch\n",
    "            while queue and len(active) < max_batch:\n",
    "                idx = queue[0]\n",
    "                if requests[idx]['arrival'] <= current_time:\n",
    "                    queue.pop(0)\n",
    "                    active.append((idx, requests[idx]['output_tokens']))\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            if not active:\n",
    "                if queue:\n",
    "                    current_time = requests[queue[0]]['arrival']\n",
    "                continue\n",
    "            \n",
    "            # Process one step for all active requests\n",
    "            current_time += self.time_per_token\n",
    "            \n",
    "            new_active = []\n",
    "            for idx, remaining in active:\n",
    "                if remaining <= 1:\n",
    "                    # Request complete\n",
    "                    results[idx] = {\n",
    "                        'latency': current_time - requests[idx]['arrival'],\n",
    "                        'end': current_time\n",
    "                    }\n",
    "                else:\n",
    "                    new_active.append((idx, remaining - 1))\n",
    "            active = new_active\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Simulate requests\n",
    "np.random.seed(42)\n",
    "n_requests = 20\n",
    "requests = []\n",
    "for i in range(n_requests):\n",
    "    requests.append({\n",
    "        'arrival': i * 50,  # 50ms between arrivals\n",
    "        'output_tokens': np.random.randint(10, 100),\n",
    "    })\n",
    "\n",
    "sim = BatchingSimulator(time_per_token_ms=10)\n",
    "\n",
    "no_batch_results = sim.no_batching(requests)\n",
    "static_results = sim.static_batching(requests, batch_size=4)\n",
    "continuous_results = sim.continuous_batching(requests, max_batch=8)\n",
    "\n",
    "print(\"Batching Strategy Comparison\\n\")\n",
    "for name, results in [('No batching', no_batch_results),\n",
    "                       ('Static (bs=4)', static_results),\n",
    "                       ('Continuous (max=8)', continuous_results)]:\n",
    "    latencies = [r['latency'] for r in results if r is not None]\n",
    "    total_time = max(r['end'] for r in results if r is not None)\n",
    "    throughput = n_requests / (total_time / 1000)  # req/sec\n",
    "    print(f\"  {name:>25}: avg latency={np.mean(latencies):.0f}ms, \"\n",
    "          f\"p99={np.percentile(latencies, 99):.0f}ms, \"\n",
    "          f\"throughput={throughput:.1f} req/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize batching comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Latency distribution\n",
    "ax = axes[0]\n",
    "strategies = [\n",
    "    ('No batching', no_batch_results, '#e74c3c'),\n",
    "    ('Static', static_results, '#f39c12'),\n",
    "    ('Continuous', continuous_results, '#2ecc71'),\n",
    "]\n",
    "\n",
    "for name, results, color in strategies:\n",
    "    latencies = sorted([r['latency'] for r in results if r is not None])\n",
    "    ax.plot(range(len(latencies)), latencies, 'o-', color=color, linewidth=2,\n",
    "           markersize=4, label=name)\n",
    "\n",
    "ax.set_xlabel('Request (sorted by latency)', fontsize=11)\n",
    "ax.set_ylabel('Latency (ms)', fontsize=11)\n",
    "ax.set_title('Per-Request Latency', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Summary bars\n",
    "ax = axes[1]\n",
    "names = ['No\\nbatching', 'Static\\n(bs=4)', 'Continuous\\n(max=8)']\n",
    "avg_latencies = []\n",
    "throughputs = []\n",
    "\n",
    "for _, results, _ in strategies:\n",
    "    latencies = [r['latency'] for r in results if r is not None]\n",
    "    total_time = max(r['end'] for r in results if r is not None)\n",
    "    avg_latencies.append(np.mean(latencies))\n",
    "    throughputs.append(n_requests / (total_time / 1000))\n",
    "\n",
    "x = np.arange(len(names))\n",
    "w = 0.35\n",
    "colors_bar = ['#3498db', '#e74c3c']\n",
    "ax.bar(x - w/2, avg_latencies, w, label='Avg Latency (ms)', color='#3498db', edgecolor='black')\n",
    "ax2 = ax.twinx()\n",
    "ax2.bar(x + w/2, throughputs, w, label='Throughput (req/s)', color='#2ecc71', edgecolor='black')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(names)\n",
    "ax.set_ylabel('Avg Latency (ms)', fontsize=10, color='#3498db')\n",
    "ax2.set_ylabel('Throughput (req/s)', fontsize=10, color='#2ecc71')\n",
    "ax.set_title('Batching Strategy Comparison', fontsize=13, fontweight='bold')\n",
    "ax.legend(loc='upper left', fontsize=9)\n",
    "ax2.legend(loc='upper right', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Knowledge Distillation\n",
    "\n",
    "**Distillation** trains a small \"student\" model to mimic a large \"teacher\" model. The student learns from the teacher's soft probability distributions, which carry more information than hard labels.\n",
    "\n",
    "$$\\mathcal{L}_{\\text{distill}} = \\alpha \\cdot T^2 \\cdot \\text{KL}\\left(\\sigma\\left(\\frac{z_s}{T}\\right) \\| \\sigma\\left(\\frac{z_t}{T}\\right)\\right) + (1-\\alpha) \\cdot \\text{CE}(y, z_s)$$\n",
    "\n",
    "where $T$ is the temperature and $\\alpha$ balances distillation vs. hard-label loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationTrainer:\n",
    "    \"\"\"Knowledge distillation from teacher to student model.\"\"\"\n",
    "    \n",
    "    def __init__(self, teacher, student, temperature=4.0, alpha=0.7):\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def distillation_loss(self, student_logits, teacher_logits, targets):\n",
    "        \"\"\"Compute combined distillation + hard label loss.\"\"\"\n",
    "        T = self.temperature\n",
    "        \n",
    "        # Soft target loss (KL divergence on softened distributions)\n",
    "        student_soft = F.log_softmax(student_logits / T, dim=-1)\n",
    "        teacher_soft = F.softmax(teacher_logits / T, dim=-1)\n",
    "        soft_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (T * T)\n",
    "        \n",
    "        # Hard target loss\n",
    "        hard_loss = F.cross_entropy(student_logits, targets)\n",
    "        \n",
    "        return self.alpha * soft_loss + (1 - self.alpha) * hard_loss\n",
    "    \n",
    "    def train_step(self, x, targets, optimizer):\n",
    "        \"\"\"One training step.\"\"\"\n",
    "        self.teacher.eval()\n",
    "        self.student.train()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            teacher_logits = self.teacher(x)\n",
    "        \n",
    "        student_logits = self.student(x)\n",
    "        loss = self.distillation_loss(student_logits, teacher_logits, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "# Create teacher (large) and student (small) models\n",
    "n_classes = 10\n",
    "input_dim = 64\n",
    "\n",
    "teacher = nn.Sequential(\n",
    "    nn.Linear(input_dim, 256), nn.ReLU(),\n",
    "    nn.Linear(256, 128), nn.ReLU(),\n",
    "    nn.Linear(128, n_classes)\n",
    ")\n",
    "\n",
    "student = nn.Sequential(\n",
    "    nn.Linear(input_dim, 32), nn.ReLU(),\n",
    "    nn.Linear(32, n_classes)\n",
    ")\n",
    "\n",
    "student_no_distill = nn.Sequential(\n",
    "    nn.Linear(input_dim, 32), nn.ReLU(),\n",
    "    nn.Linear(32, n_classes)\n",
    ")\n",
    "# Copy initial weights so comparison is fair\n",
    "student_no_distill.load_state_dict(student.state_dict())\n",
    "\n",
    "teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "student_params = sum(p.numel() for p in student.parameters())\n",
    "print(f\"Teacher: {teacher_params:,} params\")\n",
    "print(f\"Student: {student_params:,} params ({student_params/teacher_params:.1%} of teacher)\")\n",
    "\n",
    "# Generate synthetic dataset\n",
    "n_train = 500\n",
    "X = torch.randn(n_train, input_dim)\n",
    "# Teacher generates \"ground truth\"\n",
    "teacher.eval()\n",
    "with torch.no_grad():\n",
    "    y = teacher(X).argmax(dim=-1)\n",
    "\n",
    "# Train teacher to convergence first\n",
    "teacher_opt = torch.optim.Adam(teacher.parameters(), lr=1e-3)\n",
    "teacher.train()\n",
    "for _ in range(200):\n",
    "    loss = F.cross_entropy(teacher(X), y)\n",
    "    teacher_opt.zero_grad()\n",
    "    loss.backward()\n",
    "    teacher_opt.step()\n",
    "\n",
    "teacher.eval()\n",
    "teacher_acc = (teacher(X).argmax(dim=-1) == y).float().mean().item()\n",
    "print(f\"\\nTeacher accuracy: {teacher_acc:.1%}\")\n",
    "\n",
    "# Train student WITH distillation\n",
    "distiller = DistillationTrainer(teacher, student, temperature=4.0, alpha=0.7)\n",
    "student_opt = torch.optim.Adam(student.parameters(), lr=1e-3)\n",
    "\n",
    "distill_losses = []\n",
    "for epoch in range(200):\n",
    "    loss = distiller.train_step(X, y, student_opt)\n",
    "    distill_losses.append(loss)\n",
    "\n",
    "# Train student WITHOUT distillation (hard labels only)\n",
    "no_distill_opt = torch.optim.Adam(student_no_distill.parameters(), lr=1e-3)\n",
    "no_distill_losses = []\n",
    "student_no_distill.train()\n",
    "for epoch in range(200):\n",
    "    logits = student_no_distill(X)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    no_distill_opt.zero_grad()\n",
    "    loss.backward()\n",
    "    no_distill_opt.step()\n",
    "    no_distill_losses.append(loss.item())\n",
    "\n",
    "# Evaluate\n",
    "student.eval()\n",
    "student_no_distill.eval()\n",
    "distill_acc = (student(X).argmax(dim=-1) == y).float().mean().item()\n",
    "no_distill_acc = (student_no_distill(X).argmax(dim=-1) == y).float().mean().item()\n",
    "\n",
    "print(f\"Student (with distillation): {distill_acc:.1%}\")\n",
    "print(f\"Student (without distillation): {no_distill_acc:.1%}\")\n",
    "print(f\"\\nDistillation advantage: {distill_acc - no_distill_acc:+.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distillation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training loss comparison\n",
    "ax = axes[0]\n",
    "w = 10\n",
    "smooth_distill = [np.mean(distill_losses[max(0,i-w):i+1]) for i in range(len(distill_losses))]\n",
    "smooth_no = [np.mean(no_distill_losses[max(0,i-w):i+1]) for i in range(len(no_distill_losses))]\n",
    "ax.plot(smooth_distill, linewidth=2, label='With distillation', color='#2ecc71')\n",
    "ax.plot(smooth_no, linewidth=2, label='Without distillation', color='#e74c3c')\n",
    "ax.set_xlabel('Epoch', fontsize=11)\n",
    "ax.set_ylabel('Loss', fontsize=11)\n",
    "ax.set_title('Student Training Loss', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy comparison bar chart\n",
    "ax = axes[1]\n",
    "models = ['Teacher\\n(large)', 'Student +\\nDistillation', 'Student\\n(hard labels)']\n",
    "accs = [teacher_acc, distill_acc, no_distill_acc]\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "params = [teacher_params, student_params, student_params]\n",
    "\n",
    "bars = ax.bar(models, accs, color=colors, edgecolor='black', alpha=0.8)\n",
    "for bar, acc, p in zip(bars, accs, params):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{acc:.1%}\\n({p:,} params)', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Accuracy', fontsize=11)\n",
    "ax.set_title('Model Comparison', fontsize=13, fontweight='bold')\n",
    "ax.set_ylim(0, 1.15)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Optimization Techniques Compared\n",
    "\n",
    "Let's compare all optimization techniques on the dimensions that matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison\n",
    "techniques = {\n",
    "    'Baseline (FP32)':     {'memory': 1.0, 'speed': 1.0, 'quality': 1.0, 'complexity': 1},\n",
    "    'FP16':                {'memory': 0.5, 'speed': 1.5, 'quality': 0.99, 'complexity': 1},\n",
    "    'INT8 Quant':          {'memory': 0.25, 'speed': 2.0, 'quality': 0.97, 'complexity': 2},\n",
    "    'INT4 Quant':          {'memory': 0.125, 'speed': 2.5, 'quality': 0.93, 'complexity': 3},\n",
    "    'KV Cache':            {'memory': 1.1, 'speed': 3.0, 'quality': 1.0, 'complexity': 2},\n",
    "    'Speculative':         {'memory': 1.3, 'speed': 2.5, 'quality': 1.0, 'complexity': 4},\n",
    "    'Continuous Batch':    {'memory': 1.0, 'speed': 2.0, 'quality': 1.0, 'complexity': 3},\n",
    "    'Distillation':        {'memory': 0.3, 'speed': 3.0, 'quality': 0.90, 'complexity': 4},\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Speed vs Memory tradeoff\n",
    "ax = axes[0]\n",
    "for name, vals in techniques.items():\n",
    "    color = plt.cm.viridis(vals['quality'])\n",
    "    size = (1.1 - vals['memory']) * 200 + 50  # Bigger = less memory\n",
    "    ax.scatter(vals['memory'], vals['speed'], s=size, c=[vals['quality']],\n",
    "              cmap='RdYlGn', vmin=0.85, vmax=1.0, edgecolors='black', linewidth=1, zorder=5)\n",
    "    ax.annotate(name, (vals['memory'], vals['speed']),\n",
    "               textcoords='offset points', xytext=(8, 5), fontsize=8)\n",
    "\n",
    "ax.set_xlabel('Memory (relative to baseline)', fontsize=11)\n",
    "ax.set_ylabel('Speed (relative to baseline)', fontsize=11)\n",
    "ax.set_title('Speed vs Memory (color=quality)', fontsize=13, fontweight='bold')\n",
    "ax.axhline(y=1, color='gray', linestyle='--', alpha=0.3)\n",
    "ax.axvline(x=1, color='gray', linestyle='--', alpha=0.3)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Bar comparison\n",
    "ax = axes[1]\n",
    "names = list(techniques.keys())\n",
    "x = np.arange(len(names))\n",
    "w = 0.2\n",
    "\n",
    "metrics = ['speed', 'quality']\n",
    "colors_bar = ['#3498db', '#2ecc71']\n",
    "labels = ['Speed', 'Quality']\n",
    "\n",
    "for i, (metric, color, label) in enumerate(zip(metrics, colors_bar, labels)):\n",
    "    vals = [techniques[n][metric] for n in names]\n",
    "    ax.bar(x + i * w, vals, w, label=label, color=color, edgecolor='black', alpha=0.8)\n",
    "\n",
    "ax.set_xticks(x + w/2)\n",
    "ax.set_xticklabels(names, rotation=40, ha='right', fontsize=8)\n",
    "ax.set_ylabel('Relative to Baseline', fontsize=10)\n",
    "ax.set_title('Speed and Quality by Technique', fontsize=13, fontweight='bold')\n",
    "ax.axhline(y=1, color='gray', linestyle='--', alpha=0.3)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Group Quantization\n",
    "\n",
    "Implement **group quantization** where weights are quantized in groups of 128 (each group has its own scale). Compare error against per-tensor and per-channel quantization at INT4 precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Hint: Reshape weights into groups of 128, quantize each group independently,\n",
    "# then reshape back. Compare MSE against per-tensor and per-channel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### Exercise 2: PagedAttention Simulator\n",
    "\n",
    "Implement a simplified version of **PagedAttention** (used in vLLM). Instead of pre-allocating KV cache for max sequence length, allocate fixed-size pages on demand. Show memory savings compared to naive pre-allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Hint: Create a page table that maps sequence positions to memory pages.\n",
    "# Track allocated vs wasted memory compared to contiguous allocation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### Exercise 3: Distillation with Temperature Sweep\n",
    "\n",
    "Run distillation experiments at temperatures T = 1, 2, 4, 8, 16. Plot student accuracy vs temperature. What temperature works best and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- LLM inference is **memory-bound** during decoding — most time is spent moving weights, not computing\n",
    "- **Quantization** (INT8/INT4) reduces memory 4-8x with minimal quality loss; per-channel > per-tensor\n",
    "- **KV cache** eliminates redundant computation, turning O(t²) per token into O(t)\n",
    "- **Speculative decoding** uses a cheap draft model to propose tokens verified by the large model in parallel\n",
    "- **Continuous batching** keeps the GPU busy by dynamically adding/removing requests from batches\n",
    "- **Distillation** trains a small student to mimic a large teacher, transferring knowledge through soft targets\n",
    "- These techniques **stack**: a production system uses quantization + KV cache + continuous batching together\n",
    "\n",
    "### The Optimization Stack\n",
    "\n",
    "In production, you don't pick one technique — you layer them:\n",
    "1. **Distillation** → Smaller model\n",
    "2. **Quantization** → Less memory per parameter\n",
    "3. **KV cache** → Less redundant compute\n",
    "4. **Continuous batching** → Higher throughput\n",
    "5. **Speculative decoding** → Lower latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Optimizing inference is about making models fast. But building reliable ML systems requires more than fast models — it requires **experiment tracking, reproducibility, and systematic model management**. In **Notebook 27: ML Systems & Experiment Tracking**, we'll build the infrastructure that makes ML development systematic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
