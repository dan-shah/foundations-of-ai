{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3.2: Backpropagation\n",
    "\n",
    "Backpropagation is the algorithm that makes deep learning possible. It efficiently computes how to adjust every weight in a neural network to reduce error. Without it, training networks with millions of parameters would be computationally infeasible.\n",
    "\n",
    "## Learning Objectives\n",
    "- [ ] Understand the credit assignment problem and why backpropagation solves it\n",
    "- [ ] Draw and interpret computational graphs for neural networks\n",
    "- [ ] Apply the chain rule to compute gradients through composed functions\n",
    "- [ ] Manually compute gradients for a 2-layer neural network\n",
    "- [ ] Implement backpropagation from scratch using NumPy\n",
    "- [ ] Train a network to solve XOR (what perceptrons couldn't!)\n",
    "- [ ] Recognize vanishing and exploding gradient problems\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Credit Assignment Problem\n",
    "\n",
    "### The Core Challenge\n",
    "\n",
    "Imagine a neural network makes a prediction and gets it wrong. The error is clear at the output, but **how do we know which weights deep inside the network are responsible?**\n",
    "\n",
    "This is the **credit assignment problem**: distributing \"blame\" (or credit) for the final error back to all the weights that contributed to it.\n",
    "\n",
    "### Why This Is Hard\n",
    "\n",
    "Consider a simple 2-layer network:\n",
    "\n",
    "```\n",
    "Input -> [Hidden Layer] -> [Output Layer] -> Prediction -> Error\n",
    "           w1, w2              w3, w4\n",
    "```\n",
    "\n",
    "The error depends on w3 and w4 directly, but it depends on w1 and w2 **indirectly** through the hidden layer. How much should we adjust w1 versus w3?\n",
    "\n",
    "### The Key Insight: Chain Rule\n",
    "\n",
    "The breakthrough insight is that we can use the **chain rule from calculus** to decompose the problem:\n",
    "\n",
    "$$\\frac{\\partial \\text{Error}}{\\partial w_1} = \\frac{\\partial \\text{Error}}{\\partial \\text{hidden}} \\times \\frac{\\partial \\text{hidden}}{\\partial w_1}$$\n",
    "\n",
    "**What this means:** To find how w1 affects the error, we trace the path from w1 to the error, multiplying the local gradients along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the credit assignment problem\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Simple network diagram\n",
    "ax = axes[0]\n",
    "ax.set_xlim(-0.5, 4.5)\n",
    "ax.set_ylim(-0.5, 3.5)\n",
    "\n",
    "# Draw nodes\n",
    "node_positions = {\n",
    "    'x': (0, 1.5),\n",
    "    'h1': (1.5, 2.5),\n",
    "    'h2': (1.5, 0.5),\n",
    "    'y': (3, 1.5),\n",
    "    'L': (4, 1.5)\n",
    "}\n",
    "\n",
    "for name, pos in node_positions.items():\n",
    "    circle = plt.Circle(pos, 0.3, fill=True, \n",
    "                       color='lightblue' if name not in ['L'] else 'lightcoral',\n",
    "                       edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(pos[0], pos[1], name, ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Draw edges with weight labels\n",
    "edges = [\n",
    "    ('x', 'h1', 'w1'),\n",
    "    ('x', 'h2', 'w2'),\n",
    "    ('h1', 'y', 'w3'),\n",
    "    ('h2', 'y', 'w4'),\n",
    "    ('y', 'L', '')\n",
    "]\n",
    "\n",
    "for start, end, label in edges:\n",
    "    start_pos = node_positions[start]\n",
    "    end_pos = node_positions[end]\n",
    "    ax.annotate('', xy=(end_pos[0]-0.3, end_pos[1]), xytext=(start_pos[0]+0.3, start_pos[1]),\n",
    "                arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n",
    "    if label:\n",
    "        mid = ((start_pos[0] + end_pos[0])/2, (start_pos[1] + end_pos[1])/2 + 0.2)\n",
    "        ax.text(mid[0], mid[1], label, fontsize=10, color='darkblue')\n",
    "\n",
    "ax.set_title('The Credit Assignment Problem\\nHow much does each weight contribute to the loss?', fontsize=12)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "\n",
    "# Right: Gradient flow (backward)\n",
    "ax = axes[1]\n",
    "ax.set_xlim(-0.5, 4.5)\n",
    "ax.set_ylim(-0.5, 3.5)\n",
    "\n",
    "for name, pos in node_positions.items():\n",
    "    circle = plt.Circle(pos, 0.3, fill=True,\n",
    "                       color='lightyellow' if name not in ['L'] else 'lightcoral',\n",
    "                       edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(pos[0], pos[1], name, ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Draw backward edges (gradients flow backward)\n",
    "backward_edges = [\n",
    "    ('L', 'y', r'$\\frac{\\partial L}{\\partial y}$'),\n",
    "    ('y', 'h1', r'$\\frac{\\partial L}{\\partial h_1}$'),\n",
    "    ('y', 'h2', r'$\\frac{\\partial L}{\\partial h_2}$'),\n",
    "]\n",
    "\n",
    "for start, end, label in backward_edges:\n",
    "    start_pos = node_positions[start]\n",
    "    end_pos = node_positions[end]\n",
    "    ax.annotate('', xy=(end_pos[0]+0.3, end_pos[1]), xytext=(start_pos[0]-0.3, start_pos[1]),\n",
    "                arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "    mid = ((start_pos[0] + end_pos[0])/2 - 0.3, (start_pos[1] + end_pos[1])/2 + 0.3)\n",
    "    ax.text(mid[0], mid[1], label, fontsize=10, color='darkred')\n",
    "\n",
    "ax.set_title('Backpropagation Solution\\nGradients flow backward through the network', fontsize=12)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: Backpropagation computes gradients by flowing information BACKWARD\")\n",
    "print(\"from the loss to each weight, using the chain rule at each step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: Why Not Just Compute Gradients Directly?\n",
    "\n",
    "You might wonder: why not just compute $\\frac{\\partial L}{\\partial w_i}$ directly for each weight?\n",
    "\n",
    "#### The Computational Cost Problem\n",
    "\n",
    "| Approach | Cost for n weights | For 1 million weights |\n",
    "|----------|-------------------|----------------------|\n",
    "| Numerical gradient (perturb each weight) | O(n) forward passes | 1,000,000 forward passes |\n",
    "| Backpropagation | O(1) forward + O(1) backward | 2 passes total |\n",
    "\n",
    "**Backpropagation is ~500,000x more efficient for large networks!**\n",
    "\n",
    "#### Key Insight\n",
    "\n",
    "Backpropagation reuses intermediate computations. When computing $\\frac{\\partial L}{\\partial w_1}$ and $\\frac{\\partial L}{\\partial w_2}$, both share the same $\\frac{\\partial L}{\\partial h}$ term. Backprop computes this once and reuses it.\n",
    "\n",
    "#### Historical Note\n",
    "\n",
    "Backpropagation was independently discovered multiple times:\n",
    "- 1960s: Henry J. Kelley (control theory)\n",
    "- 1970: Seppo Linnainmaa (automatic differentiation)\n",
    "- 1986: Rumelhart, Hinton, Williams (popularized for neural networks)\n",
    "\n",
    "The 1986 paper is considered the birth of modern deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Computational Graphs\n",
    "\n",
    "A **computational graph** is a visual way to represent mathematical expressions as a series of operations. It's the foundation for understanding backpropagation.\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "Any complex function can be broken down into simple operations:\n",
    "- **Nodes** = operations (add, multiply, sigmoid, etc.)\n",
    "- **Edges** = values flowing between operations\n",
    "- **Forward pass** = compute the output, saving intermediate values\n",
    "- **Backward pass** = compute gradients using saved values\n",
    "\n",
    "### Example: $f(x, y, z) = (x + y) \\cdot z$\n",
    "\n",
    "```\n",
    "     x --+\n",
    "         +--[+]--q--[*]--f\n",
    "     y --+         |\n",
    "                   |\n",
    "     z ------------+\n",
    "```\n",
    "\n",
    "Where $q = x + y$ and $f = q \\cdot z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive computational graph example\n",
    "def visualize_computational_graph():\n",
    "    \"\"\"Visualize forward and backward pass through a simple computational graph.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Values for our example\n",
    "    x, y, z = 2.0, 3.0, 4.0\n",
    "    \n",
    "    # Forward pass\n",
    "    q = x + y  # = 5\n",
    "    f = q * z  # = 20\n",
    "    \n",
    "    # Backward pass (assuming df/df = 1)\n",
    "    df_df = 1.0\n",
    "    df_dq = z  # = 4 (partial of q*z w.r.t. q)\n",
    "    df_dz = q  # = 5 (partial of q*z w.r.t. z)\n",
    "    df_dx = df_dq * 1  # = 4 (chain rule: df/dq * dq/dx)\n",
    "    df_dy = df_dq * 1  # = 4 (chain rule: df/dq * dq/dy)\n",
    "    \n",
    "    # Left plot: Forward pass\n",
    "    ax = axes[0]\n",
    "    ax.set_xlim(-0.5, 4.5)\n",
    "    ax.set_ylim(-0.5, 3.5)\n",
    "    \n",
    "    # Node positions\n",
    "    positions = {\n",
    "        'x': (0, 2.5), 'y': (0, 1.5), 'z': (0, 0.5),\n",
    "        '+': (1.5, 2), 'q': (2.5, 2),\n",
    "        '*': (3, 1.5), 'f': (4, 1.5)\n",
    "    }\n",
    "    \n",
    "    # Draw nodes\n",
    "    for name, pos in positions.items():\n",
    "        if name in ['+', '*']:\n",
    "            circle = plt.Circle(pos, 0.25, fill=True, color='lightgreen', edgecolor='black', linewidth=2)\n",
    "        else:\n",
    "            circle = plt.Circle(pos, 0.25, fill=True, color='lightblue', edgecolor='black', linewidth=2)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(pos[0], pos[1], name, ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Draw forward edges with values\n",
    "    forward_edges = [\n",
    "        ('x', '+', f'x={x}'),\n",
    "        ('y', '+', f'y={y}'),\n",
    "        ('+', 'q', ''),\n",
    "        ('q', '*', f'q={q}'),\n",
    "        ('z', '*', f'z={z}'),\n",
    "        ('*', 'f', f'f={f}')\n",
    "    ]\n",
    "    \n",
    "    for start, end, label in forward_edges:\n",
    "        sp, ep = positions[start], positions[end]\n",
    "        ax.annotate('', xy=(ep[0]-0.25, ep[1]), xytext=(sp[0]+0.25, sp[1]),\n",
    "                    arrowprops=dict(arrowstyle='->', color='blue', lw=1.5))\n",
    "        if label:\n",
    "            mid = ((sp[0] + ep[0])/2, (sp[1] + ep[1])/2 + 0.25)\n",
    "            ax.text(mid[0], mid[1], label, fontsize=9, color='blue')\n",
    "    \n",
    "    ax.set_title('Forward Pass: Compute Values\\nf(x,y,z) = (x+y)*z', fontsize=12)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Right plot: Backward pass\n",
    "    ax = axes[1]\n",
    "    ax.set_xlim(-0.5, 4.5)\n",
    "    ax.set_ylim(-0.5, 3.5)\n",
    "    \n",
    "    # Draw nodes with gradients\n",
    "    gradients = {'x': df_dx, 'y': df_dy, 'z': df_dz, 'q': df_dq, 'f': df_df}\n",
    "    \n",
    "    for name, pos in positions.items():\n",
    "        if name in ['+', '*']:\n",
    "            circle = plt.Circle(pos, 0.25, fill=True, color='lightyellow', edgecolor='black', linewidth=2)\n",
    "            ax.add_patch(circle)\n",
    "            ax.text(pos[0], pos[1], name, ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "        else:\n",
    "            circle = plt.Circle(pos, 0.25, fill=True, color='lightcoral', edgecolor='black', linewidth=2)\n",
    "            ax.add_patch(circle)\n",
    "            grad = gradients.get(name, '')\n",
    "            ax.text(pos[0], pos[1], name, ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "            ax.text(pos[0], pos[1]-0.45, f'grad={grad}', ha='center', va='center', fontsize=8, color='darkred')\n",
    "    \n",
    "    # Draw backward edges\n",
    "    backward_edges = [\n",
    "        ('f', '*'),\n",
    "        ('*', 'q'),\n",
    "        ('*', 'z'),\n",
    "        ('q', '+'),\n",
    "        ('+', 'x'),\n",
    "        ('+', 'y'),\n",
    "    ]\n",
    "    \n",
    "    for start, end in backward_edges:\n",
    "        sp, ep = positions[start], positions[end]\n",
    "        ax.annotate('', xy=(ep[0]+0.25, ep[1]), xytext=(sp[0]-0.25, sp[1]),\n",
    "                    arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "    \n",
    "    ax.set_title('Backward Pass: Compute Gradients\\nGradients flow backward', fontsize=12)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed explanation\n",
    "    print(\"Forward Pass:\")\n",
    "    print(f\"  q = x + y = {x} + {y} = {q}\")\n",
    "    print(f\"  f = q * z = {q} * {z} = {f}\")\n",
    "    print(\"\\nBackward Pass:\")\n",
    "    print(f\"  df/df = {df_df} (by definition)\")\n",
    "    print(f\"  df/dq = z = {df_dq} (local gradient of multiplication)\")\n",
    "    print(f\"  df/dz = q = {df_dz} (local gradient of multiplication)\")\n",
    "    print(f\"  df/dx = df/dq * dq/dx = {df_dq} * 1 = {df_dx} (chain rule!)\")\n",
    "    print(f\"  df/dy = df/dq * dq/dy = {df_dq} * 1 = {df_dy} (chain rule!)\")\n",
    "\n",
    "visualize_computational_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: Local Gradients\n",
    "\n",
    "Each operation has **local gradients** - how its output changes with respect to its inputs.\n",
    "\n",
    "| Operation | Forward | Local Gradients |\n",
    "|-----------|---------|----------------|\n",
    "| Add: $c = a + b$ | $c = a + b$ | $\\frac{\\partial c}{\\partial a} = 1$, $\\frac{\\partial c}{\\partial b} = 1$ |\n",
    "| Multiply: $c = a \\cdot b$ | $c = a \\cdot b$ | $\\frac{\\partial c}{\\partial a} = b$, $\\frac{\\partial c}{\\partial b} = a$ |\n",
    "| Power: $c = a^n$ | $c = a^n$ | $\\frac{\\partial c}{\\partial a} = n \\cdot a^{n-1}$ |\n",
    "| Exp: $c = e^a$ | $c = e^a$ | $\\frac{\\partial c}{\\partial a} = e^a$ |\n",
    "| Sigmoid: $c = \\sigma(a)$ | $c = \\frac{1}{1+e^{-a}}$ | $\\frac{\\partial c}{\\partial a} = c(1-c)$ |\n",
    "| ReLU: $c = \\max(0, a)$ | $c = \\max(0, a)$ | $\\frac{\\partial c}{\\partial a} = \\mathbb{1}_{a > 0}$ |\n",
    "\n",
    "**Key Insight:** The backward pass multiplies the incoming gradient by the local gradient and passes it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement basic operations as \"gates\" with forward and backward\n",
    "\n",
    "class AddGate:\n",
    "    \"\"\"Addition gate: c = a + b\"\"\"\n",
    "    def forward(self, a, b):\n",
    "        self.a, self.b = a, b\n",
    "        return a + b\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # Local gradients are both 1\n",
    "        return grad_output * 1, grad_output * 1\n",
    "\n",
    "class MultiplyGate:\n",
    "    \"\"\"Multiplication gate: c = a * b\"\"\"\n",
    "    def forward(self, a, b):\n",
    "        self.a, self.b = a, b\n",
    "        return a * b\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # Local gradients are the OTHER input\n",
    "        return grad_output * self.b, grad_output * self.a\n",
    "\n",
    "class SigmoidGate:\n",
    "    \"\"\"Sigmoid gate: c = 1 / (1 + exp(-a))\"\"\"\n",
    "    def forward(self, a):\n",
    "        self.output = 1 / (1 + np.exp(-a))\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # Sigmoid derivative: sigma(x) * (1 - sigma(x))\n",
    "        return grad_output * self.output * (1 - self.output)\n",
    "\n",
    "# Example: compute f = sigmoid((x * y) + z)\n",
    "x, y, z = 2.0, -3.0, 1.0\n",
    "\n",
    "# Create gates\n",
    "mult = MultiplyGate()\n",
    "add = AddGate()\n",
    "sig = SigmoidGate()\n",
    "\n",
    "# Forward pass\n",
    "mult_out = mult.forward(x, y)  # x * y = -6\n",
    "add_out = add.forward(mult_out, z)  # -6 + 1 = -5\n",
    "f = sig.forward(add_out)  # sigmoid(-5) = 0.0067\n",
    "\n",
    "print(\"Forward pass:\")\n",
    "print(f\"  x * y = {x} * {y} = {mult_out}\")\n",
    "print(f\"  (x*y) + z = {mult_out} + {z} = {add_out}\")\n",
    "print(f\"  sigmoid({add_out}) = {f:.6f}\")\n",
    "\n",
    "# Backward pass (starting with df/df = 1)\n",
    "grad_f = 1.0\n",
    "grad_add = sig.backward(grad_f)\n",
    "grad_mult, grad_z = add.backward(grad_add)\n",
    "grad_x, grad_y = mult.backward(grad_mult)\n",
    "\n",
    "print(\"\\nBackward pass:\")\n",
    "print(f\"  df/df = {grad_f}\")\n",
    "print(f\"  df/d(add_out) = {grad_add:.6f}\")\n",
    "print(f\"  df/d(mult_out) = {grad_mult:.6f}\")\n",
    "print(f\"  df/dz = {grad_z:.6f}\")\n",
    "print(f\"  df/dx = {grad_x:.6f}\")\n",
    "print(f\"  df/dy = {grad_y:.6f}\")\n",
    "\n",
    "# Verify with numerical gradient\n",
    "def f_func(x, y, z):\n",
    "    return 1 / (1 + np.exp(-(x*y + z)))\n",
    "\n",
    "h = 1e-5\n",
    "numerical_grad_x = (f_func(x+h, y, z) - f_func(x-h, y, z)) / (2*h)\n",
    "\n",
    "print(f\"\\nVerification (numerical gradient for x): {numerical_grad_x:.6f}\")\n",
    "print(f\"Match: {np.isclose(grad_x, numerical_grad_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Chain Rule Deep Dive\n",
    "\n",
    "The chain rule is the mathematical foundation of backpropagation. Let's build deep intuition.\n",
    "\n",
    "### Single Variable Chain Rule\n",
    "\n",
    "If $y = f(g(x))$, then:\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{dy}{dg} \\cdot \\frac{dg}{dx}$$\n",
    "\n",
    "**Intuition:** If $x$ changes by a tiny amount $\\epsilon$:\n",
    "1. $g$ changes by $\\frac{dg}{dx} \\cdot \\epsilon$\n",
    "2. This change in $g$ causes $y$ to change by $\\frac{dy}{dg} \\cdot (\\frac{dg}{dx} \\cdot \\epsilon)$\n",
    "3. So the total change in $y$ per unit change in $x$ is $\\frac{dy}{dg} \\cdot \\frac{dg}{dx}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the chain rule with a concrete example\n",
    "# y = (3x + 1)^2\n",
    "# Let g(x) = 3x + 1, f(g) = g^2\n",
    "\n",
    "def visualize_chain_rule():\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    x = np.linspace(-2, 2, 200)\n",
    "    \n",
    "    # g(x) = 3x + 1\n",
    "    g = 3*x + 1\n",
    "    dg_dx = 3  # constant\n",
    "    \n",
    "    # f(g) = g^2  (but we plot f as function of x for comparison)\n",
    "    f_of_g = g**2\n",
    "    \n",
    "    # dy/dg = 2g (derivative of g^2)\n",
    "    df_dg = 2*g\n",
    "    \n",
    "    # Chain rule: dy/dx = dy/dg * dg/dx = 2g * 3 = 6g = 6(3x+1) = 18x + 6\n",
    "    df_dx = 6*(3*x + 1)\n",
    "    \n",
    "    # Plot 1: The functions\n",
    "    axes[0].plot(x, g, 'b-', linewidth=2, label='g(x) = 3x + 1')\n",
    "    axes[0].plot(x, f_of_g, 'r-', linewidth=2, label='f(g(x)) = (3x+1)^2')\n",
    "    axes[0].set_xlabel('x')\n",
    "    axes[0].set_ylabel('y')\n",
    "    axes[0].set_title('The Composed Function')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_ylim(-5, 20)\n",
    "    \n",
    "    # Plot 2: The individual derivatives\n",
    "    axes[1].axhline(y=dg_dx, color='blue', linewidth=2, label='dg/dx = 3 (constant)')\n",
    "    axes[1].plot(x, df_dg, 'g-', linewidth=2, label='df/dg = 2g = 2(3x+1)')\n",
    "    axes[1].set_xlabel('x')\n",
    "    axes[1].set_ylabel('Derivative value')\n",
    "    axes[1].set_title('Individual Derivatives')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: The chain rule result\n",
    "    axes[2].plot(x, df_dx, 'purple', linewidth=2, label='df/dx = (df/dg)(dg/dx)')\n",
    "    # Also show product of individual derivatives at specific points\n",
    "    x_points = np.array([-1, 0, 0.5, 1])\n",
    "    for xp in x_points:\n",
    "        g_val = 3*xp + 1\n",
    "        product = (2*g_val) * 3\n",
    "        axes[2].scatter([xp], [product], s=100, c='red', zorder=5)\n",
    "        axes[2].annotate(f'{product:.0f}', xy=(xp, product), xytext=(xp+0.1, product+2))\n",
    "    \n",
    "    axes[2].set_xlabel('x')\n",
    "    axes[2].set_ylabel('df/dx')\n",
    "    axes[2].set_title('Chain Rule Result\\ndf/dx = 2(3x+1) * 3 = 18x + 6')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Chain Rule in Action:\")\n",
    "    print(\"  y = f(g(x)) = (3x + 1)^2\")\n",
    "    print(\"  g(x) = 3x + 1  ->  dg/dx = 3\")\n",
    "    print(\"  f(g) = g^2     ->  df/dg = 2g\")\n",
    "    print(\"  ----------------------------------------\")\n",
    "    print(\"  dy/dx = df/dg * dg/dx = 2g * 3 = 6(3x+1)\")\n",
    "\n",
    "visualize_chain_rule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariable Chain Rule\n",
    "\n",
    "When a variable affects the output through **multiple paths**, we **sum** the contributions:\n",
    "\n",
    "If $L = f(a, b)$ where both $a$ and $b$ depend on $x$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial x} + \\frac{\\partial L}{\\partial b} \\cdot \\frac{\\partial b}{\\partial x}$$\n",
    "\n",
    "**This is crucial for neural networks:** A hidden unit's output often feeds into multiple neurons in the next layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multivariable chain rule\n",
    "# L = a*b where a = x+1 and b = x*2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.set_xlim(-0.5, 4.5)\n",
    "ax.set_ylim(-0.5, 3.5)\n",
    "\n",
    "# Positions\n",
    "positions = {\n",
    "    'x': (0, 1.5),\n",
    "    'a': (2, 2.5),\n",
    "    'b': (2, 0.5),\n",
    "    'L': (4, 1.5)\n",
    "}\n",
    "\n",
    "# Draw nodes\n",
    "for name, pos in positions.items():\n",
    "    color = 'lightblue' if name != 'L' else 'lightcoral'\n",
    "    circle = plt.Circle(pos, 0.3, fill=True, color=color, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(pos[0], pos[1], name, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Forward edges\n",
    "ax.annotate('', xy=(1.7, 2.3), xytext=(0.3, 1.7),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "ax.text(0.7, 2.2, 'a = x+1', fontsize=10, color='blue')\n",
    "\n",
    "ax.annotate('', xy=(1.7, 0.7), xytext=(0.3, 1.3),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "ax.text(0.7, 0.7, 'b = 2x', fontsize=10, color='blue')\n",
    "\n",
    "ax.annotate('', xy=(3.7, 1.7), xytext=(2.3, 2.3),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "\n",
    "ax.annotate('', xy=(3.7, 1.3), xytext=(2.3, 0.7),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "\n",
    "ax.text(3, 2.3, 'L = a*b', fontsize=10, color='blue')\n",
    "\n",
    "# Backward edges (gradients) - shown as curved red arrows\n",
    "ax.annotate('', xy=(0.3, 1.7), xytext=(1.7, 2.3),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2, connectionstyle='arc3,rad=0.3'))\n",
    "ax.text(0.5, 2.6, r'$\\frac{\\partial L}{\\partial a}\\cdot\\frac{\\partial a}{\\partial x}$', \n",
    "        fontsize=10, color='red')\n",
    "\n",
    "ax.annotate('', xy=(0.3, 1.3), xytext=(1.7, 0.7),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2, connectionstyle='arc3,rad=-0.3'))\n",
    "ax.text(0.5, 0.2, r'$\\frac{\\partial L}{\\partial b}\\cdot\\frac{\\partial b}{\\partial x}$', \n",
    "        fontsize=10, color='red')\n",
    "\n",
    "ax.set_title('Multivariable Chain Rule: Sum over all paths\\n' + \n",
    "             r'$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial a}\\frac{\\partial a}{\\partial x} + \\frac{\\partial L}{\\partial b}\\frac{\\partial b}{\\partial x}$',\n",
    "             fontsize=12)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Numerical example\n",
    "x = 3.0\n",
    "a = x + 1  # = 4\n",
    "b = 2 * x  # = 6\n",
    "L = a * b  # = 24\n",
    "\n",
    "# Gradients\n",
    "dL_da = b  # = 6\n",
    "dL_db = a  # = 4\n",
    "da_dx = 1\n",
    "db_dx = 2\n",
    "\n",
    "# Chain rule (sum over paths)\n",
    "dL_dx = dL_da * da_dx + dL_db * db_dx\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"a = x + 1 = {a}\")\n",
    "print(f\"b = 2x = {b}\")\n",
    "print(f\"L = a * b = {L}\")\n",
    "print(f\"\\nPath 1: dL/da * da/dx = {dL_da} * {da_dx} = {dL_da * da_dx}\")\n",
    "print(f\"Path 2: dL/db * db/dx = {dL_db} * {db_dx} = {dL_db * db_dx}\")\n",
    "print(f\"Total:  dL/dx = {dL_dx}\")\n",
    "\n",
    "# Verify\n",
    "def L_func(x):\n",
    "    return (x + 1) * (2 * x)\n",
    "\n",
    "h = 1e-5\n",
    "numerical = (L_func(x + h) - L_func(x - h)) / (2 * h)\n",
    "print(f\"\\nNumerical verification: {numerical:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: Why the Chain Rule Works\n",
    "\n",
    "The chain rule captures a fundamental truth about composition: **local sensitivities multiply along paths**.\n",
    "\n",
    "Think of it like currency exchange:\n",
    "- 1 USD = 0.85 EUR (sensitivity of EUR to USD)\n",
    "- 1 EUR = 0.88 GBP (sensitivity of GBP to EUR)\n",
    "- Therefore: 1 USD = 0.85 * 0.88 = 0.75 GBP\n",
    "\n",
    "The sensitivities multiply along the chain!\n",
    "\n",
    "| Concept | In Currency | In Neural Networks |\n",
    "|---------|-------------|-------------------|\n",
    "| Single path | USD -> EUR -> GBP | input -> hidden -> output |\n",
    "| Multiply sensitivities | 0.85 * 0.88 | dL/dh * dh/dx |\n",
    "| Multiple paths | USD -> EUR -> GBP and USD -> JPY -> GBP | hidden unit feeds into multiple outputs |\n",
    "| Sum contributions | Total GBP from both paths | Sum gradients from all paths |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Backprop Through a Neural Network\n",
    "\n",
    "Now let's apply these concepts to an actual neural network. We'll work through a 2-layer network step by step.\n",
    "\n",
    "### Network Architecture\n",
    "\n",
    "```\n",
    "Input (x) -> [W1, b1] -> ReLU -> h -> [W2, b2] -> sigmoid -> y_pred -> Loss\n",
    "```\n",
    "\n",
    "### Forward Pass Equations\n",
    "\n",
    "1. **Linear 1:** $z_1 = W_1 \\cdot x + b_1$\n",
    "2. **Activation 1:** $h = \\text{ReLU}(z_1) = \\max(0, z_1)$\n",
    "3. **Linear 2:** $z_2 = W_2 \\cdot h + b_2$\n",
    "4. **Activation 2:** $\\hat{y} = \\sigma(z_2) = \\frac{1}{1 + e^{-z_2}}$\n",
    "5. **Loss:** $L = -[y \\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})]$ (binary cross-entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize network architecture with forward pass values\n",
    "\n",
    "def visualize_network_forward():\n",
    "    \"\"\"Draw network architecture showing forward pass.\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    ax.set_xlim(-1, 11)\n",
    "    ax.set_ylim(-1, 7)\n",
    "    \n",
    "    # Layer positions\n",
    "    layers = {\n",
    "        'input': [(0, 4), (0, 2)],\n",
    "        'hidden': [(3, 5), (3, 3), (3, 1)],\n",
    "        'output': [(6, 3)],\n",
    "        'loss': [(9, 3)]\n",
    "    }\n",
    "    \n",
    "    labels = {\n",
    "        'input': ['x1', 'x2'],\n",
    "        'hidden': ['h1', 'h2', 'h3'],\n",
    "        'output': ['y_pred'],\n",
    "        'loss': ['L']\n",
    "    }\n",
    "    \n",
    "    colors = {\n",
    "        'input': 'lightblue',\n",
    "        'hidden': 'lightgreen',\n",
    "        'output': 'lightyellow',\n",
    "        'loss': 'lightcoral'\n",
    "    }\n",
    "    \n",
    "    # Draw nodes\n",
    "    for layer, positions in layers.items():\n",
    "        for i, pos in enumerate(positions):\n",
    "            circle = plt.Circle(pos, 0.4, fill=True, color=colors[layer], \n",
    "                              edgecolor='black', linewidth=2)\n",
    "            ax.add_patch(circle)\n",
    "            ax.text(pos[0], pos[1], labels[layer][i], ha='center', va='center', \n",
    "                   fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Draw connections\n",
    "    for inp in layers['input']:\n",
    "        for hid in layers['hidden']:\n",
    "            ax.plot([inp[0]+0.4, hid[0]-0.4], [inp[1], hid[1]], 'b-', alpha=0.3, lw=1)\n",
    "    \n",
    "    for hid in layers['hidden']:\n",
    "        for out in layers['output']:\n",
    "            ax.plot([hid[0]+0.4, out[0]-0.4], [hid[1], out[1]], 'g-', alpha=0.3, lw=1)\n",
    "    \n",
    "    ax.plot([6.4, 8.6], [3, 3], 'r-', lw=2)\n",
    "    \n",
    "    # Add labels for operations\n",
    "    ax.text(1.5, 6, 'z1 = W1*x + b1', fontsize=11, ha='center')\n",
    "    ax.text(1.5, 5.5, 'h = ReLU(z1)', fontsize=11, ha='center')\n",
    "    ax.text(4.5, 6, 'z2 = W2*h + b2', fontsize=11, ha='center')\n",
    "    ax.text(4.5, 5.5, 'y = sigmoid(z2)', fontsize=11, ha='center')\n",
    "    ax.text(7.5, 4, 'L = BCE(y, target)', fontsize=11, ha='center')\n",
    "    \n",
    "    # Add layer labels\n",
    "    ax.text(0, 0, 'Input\\nLayer', ha='center', fontsize=10)\n",
    "    ax.text(3, 0, 'Hidden\\nLayer', ha='center', fontsize=10)\n",
    "    ax.text(6, 0, 'Output\\nLayer', ha='center', fontsize=10)\n",
    "    ax.text(9, 0, 'Loss', ha='center', fontsize=10)\n",
    "    \n",
    "    ax.set_title('2-Layer Neural Network: Forward Pass', fontsize=14)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "visualize_network_forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass: Computing Gradients\n",
    "\n",
    "We compute gradients in reverse order, applying the chain rule at each step.\n",
    "\n",
    "#### Step 1: Gradient of Loss w.r.t. Output\n",
    "\n",
    "For binary cross-entropy with sigmoid output:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\hat{y}} = -\\frac{y}{\\hat{y}} + \\frac{1-y}{1-\\hat{y}}$$\n",
    "\n",
    "Combined with sigmoid derivative, we get a beautiful simplification:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_2} = \\hat{y} - y$$\n",
    "\n",
    "#### Step 2: Gradients for Output Layer Weights\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial z_2} \\cdot h^T$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b_2} = \\frac{\\partial L}{\\partial z_2}$$\n",
    "\n",
    "#### Step 3: Gradient flowing to Hidden Layer\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial h} = W_2^T \\cdot \\frac{\\partial L}{\\partial z_2}$$\n",
    "\n",
    "#### Step 4: Through ReLU\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_1} = \\frac{\\partial L}{\\partial h} \\odot \\mathbb{1}_{z_1 > 0}$$\n",
    "\n",
    "(Element-wise multiplication with indicator function)\n",
    "\n",
    "#### Step 5: Gradients for Hidden Layer Weights\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial z_1} \\cdot x^T$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b_1} = \\frac{\\partial L}{\\partial z_1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-step backpropagation with concrete numbers\n",
    "\n",
    "def backprop_walkthrough():\n",
    "    \"\"\"Walk through backprop with actual numbers.\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"BACKPROPAGATION WALKTHROUGH\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Network: 2 inputs -> 3 hidden -> 1 output\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Initialize weights (small random values)\n",
    "    W1 = np.array([[0.1, 0.2], \n",
    "                   [0.3, 0.4], \n",
    "                   [0.5, 0.6]])  # (3, 2)\n",
    "    b1 = np.array([[0.1], [0.1], [0.1]])  # (3, 1)\n",
    "    \n",
    "    W2 = np.array([[0.7, 0.8, 0.9]])  # (1, 3)\n",
    "    b2 = np.array([[0.1]])  # (1, 1)\n",
    "    \n",
    "    # Input and target\n",
    "    x = np.array([[1.0], [2.0]])  # (2, 1)\n",
    "    y = np.array([[1.0]])  # (1, 1) - target\n",
    "    \n",
    "    print(\"\\n--- FORWARD PASS ---\")\n",
    "    \n",
    "    # Layer 1\n",
    "    z1 = W1 @ x + b1\n",
    "    print(f\"\\nz1 = W1*x + b1:\")\n",
    "    print(f\"  W1 @ x = \\n{W1 @ x}\")\n",
    "    print(f\"  z1 = \\n{z1}\")\n",
    "    \n",
    "    h = np.maximum(0, z1)  # ReLU\n",
    "    print(f\"\\nh = ReLU(z1) = \\n{h}\")\n",
    "    \n",
    "    # Layer 2\n",
    "    z2 = W2 @ h + b2\n",
    "    print(f\"\\nz2 = W2*h + b2 = {z2[0,0]:.4f}\")\n",
    "    \n",
    "    y_pred = 1 / (1 + np.exp(-z2))  # Sigmoid\n",
    "    print(f\"y_pred = sigmoid(z2) = {y_pred[0,0]:.4f}\")\n",
    "    \n",
    "    # Loss (Binary Cross-Entropy)\n",
    "    epsilon = 1e-7  # For numerical stability\n",
    "    L = -(y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred + epsilon))\n",
    "    print(f\"\\nLoss = {L[0,0]:.4f}\")\n",
    "    \n",
    "    print(\"\\n--- BACKWARD PASS ---\")\n",
    "    \n",
    "    # Output layer gradient\n",
    "    dL_dz2 = y_pred - y  # Beautiful simplification for BCE + sigmoid\n",
    "    print(f\"\\ndL/dz2 = y_pred - y = {y_pred[0,0]:.4f} - {y[0,0]:.4f} = {dL_dz2[0,0]:.4f}\")\n",
    "    \n",
    "    # Gradients for W2 and b2\n",
    "    dL_dW2 = dL_dz2 @ h.T\n",
    "    dL_db2 = dL_dz2\n",
    "    print(f\"\\ndL/dW2 = dL/dz2 * h^T = \\n{dL_dW2}\")\n",
    "    print(f\"dL/db2 = {dL_db2[0,0]:.4f}\")\n",
    "    \n",
    "    # Gradient flowing back to hidden layer\n",
    "    dL_dh = W2.T @ dL_dz2\n",
    "    print(f\"\\ndL/dh = W2^T * dL/dz2 = \\n{dL_dh}\")\n",
    "    \n",
    "    # Through ReLU\n",
    "    dL_dz1 = dL_dh * (z1 > 0).astype(float)\n",
    "    print(f\"\\ndL/dz1 = dL/dh * (z1 > 0) = \\n{dL_dz1}\")\n",
    "    print(f\"  (ReLU derivative is 1 where z1 > 0, else 0)\")\n",
    "    \n",
    "    # Gradients for W1 and b1\n",
    "    dL_dW1 = dL_dz1 @ x.T\n",
    "    dL_db1 = dL_dz1\n",
    "    print(f\"\\ndL/dW1 = dL/dz1 * x^T = \\n{dL_dW1}\")\n",
    "    print(f\"dL/db1 = \\n{dL_db1}\")\n",
    "    \n",
    "    print(\"\\n--- VERIFICATION (Numerical Gradients) ---\")\n",
    "    \n",
    "    # Verify one gradient numerically\n",
    "    def compute_loss(W1, b1, W2, b2, x, y):\n",
    "        z1 = W1 @ x + b1\n",
    "        h = np.maximum(0, z1)\n",
    "        z2 = W2 @ h + b2\n",
    "        y_pred = 1 / (1 + np.exp(-z2))\n",
    "        return -(y * np.log(y_pred + 1e-7) + (1-y) * np.log(1 - y_pred + 1e-7))\n",
    "    \n",
    "    h_epsilon = 1e-5\n",
    "    W2_plus = W2.copy()\n",
    "    W2_plus[0, 0] += h_epsilon\n",
    "    W2_minus = W2.copy()\n",
    "    W2_minus[0, 0] -= h_epsilon\n",
    "    \n",
    "    numerical_grad = (compute_loss(W1, b1, W2_plus, b2, x, y) - \n",
    "                     compute_loss(W1, b1, W2_minus, b2, x, y)) / (2 * h_epsilon)\n",
    "    \n",
    "    print(f\"\\nFor W2[0,0]:\")\n",
    "    print(f\"  Backprop gradient: {dL_dW2[0,0]:.6f}\")\n",
    "    print(f\"  Numerical gradient: {numerical_grad[0,0]:.6f}\")\n",
    "    print(f\"  Match: {np.isclose(dL_dW2[0,0], numerical_grad[0,0])}\")\n",
    "    \n",
    "    return W1, b1, W2, b2, dL_dW1, dL_db1, dL_dW2, dL_db2\n",
    "\n",
    "_ = backprop_walkthrough()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient flow through the network\n",
    "\n",
    "def visualize_gradient_flow():\n",
    "    \"\"\"Visualize how gradients flow backward through layers.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Simulate gradient magnitudes at each layer (typical behavior)\n",
    "    layers = ['Loss', 'Output\\n(sigmoid)', 'Hidden\\n(ReLU)', 'Input']\n",
    "    \n",
    "    # Scenario 1: Healthy gradients\n",
    "    healthy_grads = [1.0, 0.8, 0.6, 0.5]\n",
    "    axes[0].barh(layers, healthy_grads, color=['coral', 'lightgreen', 'lightgreen', 'lightblue'])\n",
    "    axes[0].set_xlabel('Gradient Magnitude')\n",
    "    axes[0].set_title('Healthy Gradient Flow\\n(ReLU + proper initialization)')\n",
    "    axes[0].set_xlim(0, 1.5)\n",
    "    for i, v in enumerate(healthy_grads):\n",
    "        axes[0].text(v + 0.05, i, f'{v:.2f}', va='center')\n",
    "    \n",
    "    # Scenario 2: Vanishing gradients (sigmoid everywhere)\n",
    "    vanishing_grads = [1.0, 0.25, 0.06, 0.015]  # sigmoid max derivative is 0.25\n",
    "    axes[1].barh(layers, vanishing_grads, color=['coral', 'lightyellow', 'lightyellow', 'lightblue'])\n",
    "    axes[1].set_xlabel('Gradient Magnitude')\n",
    "    axes[1].set_title('Vanishing Gradients\\n(All sigmoid activations)')\n",
    "    axes[1].set_xlim(0, 1.5)\n",
    "    for i, v in enumerate(vanishing_grads):\n",
    "        axes[1].text(v + 0.05, i, f'{v:.3f}', va='center')\n",
    "    \n",
    "    # Scenario 3: Exploding gradients\n",
    "    exploding_grads = [1.0, 2.0, 4.0, 8.0]\n",
    "    axes[2].barh(layers, exploding_grads, color=['coral', 'lightcoral', 'red', 'darkred'])\n",
    "    axes[2].set_xlabel('Gradient Magnitude')\n",
    "    axes[2].set_title('Exploding Gradients\\n(Large weights, no regularization)')\n",
    "    axes[2].set_xlim(0, 10)\n",
    "    for i, v in enumerate(exploding_grads):\n",
    "        axes[2].text(v + 0.1, i, f'{v:.1f}', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Key observations:\")\n",
    "    print(\"  - Healthy: Gradients stay roughly the same magnitude across layers\")\n",
    "    print(\"  - Vanishing: Gradients shrink exponentially (early layers barely learn)\")\n",
    "    print(\"  - Exploding: Gradients grow exponentially (training becomes unstable)\")\n",
    "\n",
    "visualize_gradient_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why This Matters in Machine Learning\n",
    "\n",
    "| Concept | What It Means | Practical Impact |\n",
    "|---------|---------------|------------------|\n",
    "| Chain rule | Gradients multiply through layers | Deep networks can have vanishing/exploding gradients |\n",
    "| Local gradients | Each operation contributes | Activation functions determine gradient flow |\n",
    "| Weight gradients | How to update weights | Larger gradient = larger weight update |\n",
    "| Gradient accumulation | Sum over all paths | Hidden units feeding multiple outputs get combined gradients |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Implementing Backprop from Scratch\n",
    "\n",
    "Now let's implement a complete 2-layer neural network with backpropagation and train it to solve the XOR problem - the classic problem that single-layer perceptrons couldn't solve!\n",
    "\n",
    "### The XOR Problem\n",
    "\n",
    "| Input 1 | Input 2 | XOR Output |\n",
    "|---------|---------|------------|\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 |\n",
    "\n",
    "XOR is **not linearly separable** - you cannot draw a single line to separate the 0s from the 1s. This is why single-layer perceptrons fail, and why we need hidden layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize why XOR needs a hidden layer\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# AND gate - linearly separable\n",
    "ax = axes[0]\n",
    "ax.scatter([0, 0, 1], [0, 1, 0], c='red', s=200, marker='o', label='Output: 0')\n",
    "ax.scatter([1], [1], c='blue', s=200, marker='s', label='Output: 1')\n",
    "x_line = np.linspace(-0.5, 1.5, 100)\n",
    "ax.plot(x_line, -x_line + 1.5, 'g--', linewidth=2, label='Decision boundary')\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "ax.set_xlabel('Input 1')\n",
    "ax.set_ylabel('Input 2')\n",
    "ax.set_title('AND Gate\\n(Linearly Separable)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# OR gate - linearly separable  \n",
    "ax = axes[1]\n",
    "ax.scatter([0], [0], c='red', s=200, marker='o', label='Output: 0')\n",
    "ax.scatter([0, 1, 1], [1, 0, 1], c='blue', s=200, marker='s', label='Output: 1')\n",
    "ax.plot(x_line, -x_line + 0.5, 'g--', linewidth=2, label='Decision boundary')\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "ax.set_xlabel('Input 1')\n",
    "ax.set_ylabel('Input 2')\n",
    "ax.set_title('OR Gate\\n(Linearly Separable)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# XOR gate - NOT linearly separable\n",
    "ax = axes[2]\n",
    "ax.scatter([0, 1], [0, 1], c='red', s=200, marker='o', label='Output: 0')\n",
    "ax.scatter([0, 1], [1, 0], c='blue', s=200, marker='s', label='Output: 1')\n",
    "ax.annotate('No single line\\ncan separate!', xy=(0.5, 0.5), fontsize=10, ha='center',\n",
    "           bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "ax.set_xlabel('Input 1')\n",
    "ax.set_ylabel('Input 2')\n",
    "ax.set_title('XOR Gate\\n(NOT Linearly Separable!)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The XOR problem exposed the limitation of single-layer perceptrons.\")\n",
    "print(\"Minsky & Papert's 1969 book on this led to the first 'AI Winter'.\")\n",
    "print(\"The solution: hidden layers + backpropagation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    A 2-layer neural network implemented from scratch.\n",
    "    Architecture: Input -> Hidden (ReLU) -> Output (Sigmoid)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize the network with random weights.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_size: Number of hidden neurons\n",
    "            output_size: Number of output neurons\n",
    "        \"\"\"\n",
    "        # Xavier initialization for better gradient flow\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.W2 = np.random.randn(output_size, hidden_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "        \n",
    "        # Store intermediate values for backward pass\n",
    "        self.cache = {}\n",
    "        \n",
    "    def relu(self, z):\n",
    "        \"\"\"ReLU activation function.\"\"\"\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        \"\"\"Derivative of ReLU.\"\"\"\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            X: Input data of shape (input_size, num_samples)\n",
    "            \n",
    "        Returns:\n",
    "            Output predictions of shape (output_size, num_samples)\n",
    "        \"\"\"\n",
    "        # Layer 1: Linear -> ReLU\n",
    "        self.cache['z1'] = self.W1 @ X + self.b1\n",
    "        self.cache['h'] = self.relu(self.cache['z1'])\n",
    "        \n",
    "        # Layer 2: Linear -> Sigmoid\n",
    "        self.cache['z2'] = self.W2 @ self.cache['h'] + self.b2\n",
    "        self.cache['y_pred'] = self.sigmoid(self.cache['z2'])\n",
    "        \n",
    "        # Store input for backward pass\n",
    "        self.cache['X'] = X\n",
    "        \n",
    "        return self.cache['y_pred']\n",
    "    \n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute binary cross-entropy loss.\n",
    "        \n",
    "        Args:\n",
    "            y_pred: Predicted probabilities\n",
    "            y_true: True labels\n",
    "            \n",
    "        Returns:\n",
    "            Scalar loss value\n",
    "        \"\"\"\n",
    "        epsilon = 1e-7\n",
    "        m = y_true.shape[1]  # Number of samples\n",
    "        loss = -np.mean(y_true * np.log(y_pred + epsilon) + \n",
    "                       (1 - y_true) * np.log(1 - y_pred + epsilon))\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients using backpropagation.\n",
    "        \n",
    "        Args:\n",
    "            y_true: True labels of shape (output_size, num_samples)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of gradients for all parameters\n",
    "        \"\"\"\n",
    "        m = y_true.shape[1]  # Number of samples\n",
    "        \n",
    "        # Retrieve cached values\n",
    "        y_pred = self.cache['y_pred']\n",
    "        h = self.cache['h']\n",
    "        z1 = self.cache['z1']\n",
    "        X = self.cache['X']\n",
    "        \n",
    "        # Output layer gradients\n",
    "        # For BCE + sigmoid: dL/dz2 = y_pred - y_true\n",
    "        dz2 = y_pred - y_true  # (output_size, m)\n",
    "        \n",
    "        dW2 = (1/m) * dz2 @ h.T  # (output_size, hidden_size)\n",
    "        db2 = (1/m) * np.sum(dz2, axis=1, keepdims=True)  # (output_size, 1)\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        dh = self.W2.T @ dz2  # (hidden_size, m)\n",
    "        dz1 = dh * self.relu_derivative(z1)  # (hidden_size, m)\n",
    "        \n",
    "        dW1 = (1/m) * dz1 @ X.T  # (hidden_size, input_size)\n",
    "        db1 = (1/m) * np.sum(dz1, axis=1, keepdims=True)  # (hidden_size, 1)\n",
    "        \n",
    "        gradients = {\n",
    "            'dW1': dW1, 'db1': db1,\n",
    "            'dW2': dW2, 'db2': db2\n",
    "        }\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def update_parameters(self, gradients, learning_rate):\n",
    "        \"\"\"\n",
    "        Update parameters using gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            gradients: Dictionary of gradients\n",
    "            learning_rate: Learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        self.W1 -= learning_rate * gradients['dW1']\n",
    "        self.b1 -= learning_rate * gradients['db1']\n",
    "        self.W2 -= learning_rate * gradients['dW2']\n",
    "        self.b2 -= learning_rate * gradients['db2']\n",
    "    \n",
    "    def train(self, X, y, epochs, learning_rate, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the network.\n",
    "        \n",
    "        Args:\n",
    "            X: Training inputs\n",
    "            y: Training labels\n",
    "            epochs: Number of training epochs\n",
    "            learning_rate: Learning rate\n",
    "            verbose: Whether to print progress\n",
    "            \n",
    "        Returns:\n",
    "            List of losses during training\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(y_pred, y)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            gradients = self.backward(y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.update_parameters(gradients, learning_rate)\n",
    "            \n",
    "            if verbose and (epoch + 1) % 1000 == 0:\n",
    "                predictions = (y_pred > 0.5).astype(int)\n",
    "                accuracy = np.mean(predictions == y)\n",
    "                print(f\"Epoch {epoch+1:5d} | Loss: {loss:.4f} | Accuracy: {accuracy:.2%}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions (returns probabilities).\"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "print(\"Neural Network class defined successfully!\")\n",
    "print(\"\\nKey methods:\")\n",
    "print(\"  - forward(X): Computes predictions, caches intermediate values\")\n",
    "print(\"  - backward(y): Computes gradients using backpropagation\")\n",
    "print(\"  - update_parameters(): Applies gradient descent\")\n",
    "print(\"  - train(): Full training loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on XOR!\n",
    "\n",
    "# XOR dataset\n",
    "X_xor = np.array([[0, 0, 1, 1],\n",
    "                  [0, 1, 0, 1]])  # (2, 4)\n",
    "\n",
    "y_xor = np.array([[0, 1, 1, 0]])  # (1, 4)\n",
    "\n",
    "print(\"XOR Dataset:\")\n",
    "print(f\"X = \\n{X_xor}\")\n",
    "print(f\"y = {y_xor}\")\n",
    "\n",
    "# Create and train network\n",
    "np.random.seed(42)\n",
    "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
    "\n",
    "print(\"\\nTraining...\")\n",
    "losses = nn.train(X_xor, y_xor, epochs=10000, learning_rate=1.0)\n",
    "\n",
    "# Final predictions\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*40)\n",
    "y_pred = nn.predict(X_xor)\n",
    "print(\"\\nPredictions (probabilities):\")\n",
    "for i in range(4):\n",
    "    x1, x2 = X_xor[0, i], X_xor[1, i]\n",
    "    pred = y_pred[0, i]\n",
    "    true = y_xor[0, i]\n",
    "    print(f\"  XOR({x1}, {x2}) = {pred:.4f} (target: {true}, predicted: {int(pred > 0.5)})\")\n",
    "\n",
    "final_accuracy = np.mean((y_pred > 0.5).astype(int) == y_xor)\n",
    "print(f\"\\nFinal Accuracy: {final_accuracy:.0%}\")\n",
    "print(\"\\nWe solved XOR! The hidden layer learned to transform the space!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress and learned decision boundary\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Loss curve\n",
    "ax = axes[0]\n",
    "ax.plot(losses, 'b-', linewidth=1)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss (Binary Cross-Entropy)')\n",
    "ax.set_title('Training Loss Over Time')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Plot 2: Decision boundary\n",
    "ax = axes[1]\n",
    "\n",
    "# Create mesh grid\n",
    "xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 100),\n",
    "                     np.linspace(-0.5, 1.5, 100))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()].T  # (2, 10000)\n",
    "\n",
    "# Get predictions for grid\n",
    "Z = nn.predict(grid)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "contour = ax.contourf(xx, yy, Z, levels=50, cmap='RdYlBu', alpha=0.8)\n",
    "plt.colorbar(contour, ax=ax, label='Predicted probability')\n",
    "\n",
    "# Plot data points\n",
    "for i in range(4):\n",
    "    color = 'blue' if y_xor[0, i] == 1 else 'red'\n",
    "    marker = 's' if y_xor[0, i] == 1 else 'o'\n",
    "    ax.scatter(X_xor[0, i], X_xor[1, i], c=color, s=200, marker=marker, \n",
    "              edgecolor='black', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Input 1')\n",
    "ax.set_ylabel('Input 2')\n",
    "ax.set_title('Learned Decision Boundary\\n(Blue = 1, Red = 0)')\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "\n",
    "# Plot 3: Hidden layer representations\n",
    "ax = axes[2]\n",
    "\n",
    "# Get hidden layer activations for each input\n",
    "_ = nn.forward(X_xor)\n",
    "hidden_activations = nn.cache['h']  # (hidden_size, 4)\n",
    "\n",
    "# Plot first two hidden units (if we have at least 2)\n",
    "colors = ['red', 'blue', 'blue', 'red']  # Based on XOR output\n",
    "markers = ['o', 's', 's', 'o']\n",
    "\n",
    "for i in range(4):\n",
    "    x1, x2 = X_xor[0, i], X_xor[1, i]\n",
    "    h1, h2 = hidden_activations[0, i], hidden_activations[1, i]\n",
    "    ax.scatter(h1, h2, c=colors[i], s=200, marker=markers[i], \n",
    "              edgecolor='black', linewidth=2)\n",
    "    ax.annotate(f'({x1},{x2})', (h1, h2), xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "ax.set_xlabel('Hidden Unit 1')\n",
    "ax.set_ylabel('Hidden Unit 2')\n",
    "ax.set_title('Hidden Layer Representation\\n(Data becomes linearly separable!)')\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The magic of hidden layers:\")\n",
    "print(\"- The hidden layer TRANSFORMS the input space\")\n",
    "print(\"- In the new space, XOR becomes linearly separable!\")\n",
    "print(\"- This is what deep learning is all about: learning useful representations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: What Did the Network Learn?\n",
    "\n",
    "The hidden layer learned to transform the 2D input into a representation where XOR is solvable.\n",
    "\n",
    "| Original Input | Hidden Representation | Why It Works |\n",
    "|----------------|----------------------|---------------|\n",
    "| (0,0) -> 0 | Maps to region A | Both (0,0) and (1,1) map to same side |\n",
    "| (0,1) -> 1 | Maps to region B | (0,1) and (1,0) map to same side |\n",
    "| (1,0) -> 1 | Maps to region B | Opposite side from A |\n",
    "| (1,1) -> 0 | Maps to region A | Now linearly separable! |\n",
    "\n",
    "**This is the fundamental insight of deep learning:** Hidden layers learn transformations that make the problem easier to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Common Issues\n",
    "\n",
    "Backpropagation can fail in several ways. Understanding these issues is crucial for training deep networks.\n",
    "\n",
    "### Vanishing Gradients\n",
    "\n",
    "When gradients become extremely small as they flow backward, early layers barely learn.\n",
    "\n",
    "**Cause:** Activation functions like sigmoid squash gradients. Sigmoid's maximum derivative is 0.25, so with each layer, gradients shrink by at least 75%!\n",
    "\n",
    "**Symptoms:**\n",
    "- Early layers' weights barely change\n",
    "- Loss decreases very slowly\n",
    "- Network seems \"stuck\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate vanishing gradients with sigmoid\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Simulate gradient flow through layers\n",
    "num_layers = 10\n",
    "gradient = 1.0  # Starting gradient\n",
    "gradients_sigmoid = [gradient]\n",
    "\n",
    "# Assume average input to sigmoid is 0 (where derivative is max = 0.25)\n",
    "for _ in range(num_layers):\n",
    "    gradient *= 0.25  # Maximum sigmoid derivative\n",
    "    gradients_sigmoid.append(gradient)\n",
    "\n",
    "# Compare with ReLU\n",
    "gradient = 1.0\n",
    "gradients_relu = [gradient]\n",
    "for _ in range(num_layers):\n",
    "    # ReLU derivative is 1 for positive inputs\n",
    "    gradient *= 1.0  # Assuming positive activations\n",
    "    gradients_relu.append(gradient)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "layers = range(num_layers + 1)\n",
    "ax.plot(layers, gradients_sigmoid, 'ro-', linewidth=2, markersize=8, label='Sigmoid')\n",
    "ax.plot(layers, gradients_relu, 'bs-', linewidth=2, markersize=8, label='ReLU')\n",
    "ax.set_xlabel('Layer (from output)')\n",
    "ax.set_ylabel('Gradient Magnitude')\n",
    "ax.set_title('Gradient Magnitude vs Layer Depth')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Show activation function derivatives\n",
    "ax = axes[1]\n",
    "x = np.linspace(-5, 5, 200)\n",
    "ax.plot(x, sigmoid_derivative(x), 'r-', linewidth=2, label='Sigmoid derivative (max=0.25)')\n",
    "ax.plot(x, (x > 0).astype(float), 'b-', linewidth=2, label='ReLU derivative (0 or 1)')\n",
    "ax.axhline(y=0.25, color='r', linestyle='--', alpha=0.5)\n",
    "ax.axhline(y=1.0, color='b', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Input')\n",
    "ax.set_ylabel('Derivative')\n",
    "ax.set_title('Activation Function Derivatives')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"After {num_layers} layers:\")\n",
    "print(f\"  Sigmoid gradient: {gradients_sigmoid[-1]:.2e} (basically zero!)\")\n",
    "print(f\"  ReLU gradient: {gradients_relu[-1]:.2e} (preserved!)\")\n",
    "print(f\"\\nThis is why ReLU revolutionized deep learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploding Gradients\n",
    "\n",
    "When gradients become extremely large, weights update too drastically and training becomes unstable.\n",
    "\n",
    "**Cause:** Large weight values cause gradients to multiply and grow exponentially.\n",
    "\n",
    "**Symptoms:**\n",
    "- Loss suddenly becomes NaN or Inf\n",
    "- Weights become extremely large\n",
    "- Training diverges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate exploding gradients\n",
    "\n",
    "def simulate_gradient_flow(weight_scale, num_layers=10):\n",
    "    \"\"\"Simulate gradient magnitude through layers.\"\"\"\n",
    "    gradient = 1.0\n",
    "    gradients = [gradient]\n",
    "    \n",
    "    for _ in range(num_layers):\n",
    "        # Gradient gets multiplied by weight magnitude at each layer\n",
    "        gradient *= weight_scale\n",
    "        gradients.append(gradient)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# Different weight scales\n",
    "weight_scales = [0.5, 1.0, 1.5, 2.0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "for scale, color in zip(weight_scales, colors):\n",
    "    gradients = simulate_gradient_flow(scale)\n",
    "    ax.plot(range(len(gradients)), gradients, 'o-', color=color, \n",
    "           linewidth=2, markersize=6, label=f'Weight scale = {scale}')\n",
    "\n",
    "ax.set_xlabel('Layer (from output)')\n",
    "ax.set_ylabel('Gradient Magnitude')\n",
    "ax.set_title('Gradient Flow with Different Weight Scales')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('symlog', linthresh=1)\n",
    "ax.axhline(y=1, color='k', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"  - Weight scale < 1: Vanishing gradients (shrinks exponentially)\")\n",
    "print(\"  - Weight scale = 1: Perfect gradient flow (stays constant)\")\n",
    "print(\"  - Weight scale > 1: Exploding gradients (grows exponentially)\")\n",
    "print(\"\\nThis is why weight initialization is so important!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions to Gradient Problems\n",
    "\n",
    "| Problem | Solution | How It Helps |\n",
    "|---------|----------|-------------|\n",
    "| Vanishing gradients | ReLU activation | Derivative is 1 for positive inputs |\n",
    "| Vanishing gradients | Skip connections (ResNet) | Gradients can bypass layers |\n",
    "| Exploding gradients | Gradient clipping | Caps gradient magnitude |\n",
    "| Both | Proper initialization | Xavier/He initialization keeps gradients stable |\n",
    "| Both | Batch normalization | Normalizes activations, stabilizes gradients |\n",
    "| Both | Learning rate scheduling | Smaller steps when needed |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate gradient clipping\n",
    "\n",
    "def gradient_clip(gradients, max_norm):\n",
    "    \"\"\"Clip gradients to maximum norm.\"\"\"\n",
    "    total_norm = np.sqrt(sum(np.sum(g**2) for g in gradients))\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    if clip_coef < 1:\n",
    "        return [g * clip_coef for g in gradients]\n",
    "    return gradients\n",
    "\n",
    "# Simulate training with exploding gradients\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create large random gradients (simulating explosion)\n",
    "grad_W1 = np.random.randn(10, 5) * 100  # Huge gradients!\n",
    "grad_W2 = np.random.randn(3, 10) * 100\n",
    "\n",
    "original_grads = [grad_W1, grad_W2]\n",
    "clipped_grads = gradient_clip(original_grads, max_norm=5.0)\n",
    "\n",
    "orig_norm = np.sqrt(sum(np.sum(g**2) for g in original_grads))\n",
    "clip_norm = np.sqrt(sum(np.sum(g**2) for g in clipped_grads))\n",
    "\n",
    "print(\"Gradient Clipping Example:\")\n",
    "print(f\"  Original gradient norm: {orig_norm:.2f}\")\n",
    "print(f\"  Clipped gradient norm: {clip_norm:.2f}\")\n",
    "print(f\"  Max allowed norm: 5.0\")\n",
    "print(f\"\\nGradient clipping preserved direction but limited magnitude!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Manual Backprop\n",
    "\n",
    "Compute the gradients by hand for a simple computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 1: Manual Backpropagation\n",
    "# Given: f = (a + b) * (b + c)\n",
    "# Where: a = 1, b = 2, c = 3\n",
    "\n",
    "# Compute the gradients df/da, df/db, df/dc manually, then verify with code.\n",
    "\n",
    "a, b, c = 1.0, 2.0, 3.0\n",
    "\n",
    "# Forward pass\n",
    "p = a + b  # = 3\n",
    "q = b + c  # = 5\n",
    "f = p * q  # = 15\n",
    "\n",
    "print(\"Forward pass:\")\n",
    "print(f\"  p = a + b = {a} + {b} = {p}\")\n",
    "print(f\"  q = b + c = {b} + {c} = {q}\")\n",
    "print(f\"  f = p * q = {p} * {q} = {f}\")\n",
    "\n",
    "# TODO: Compute gradients manually\n",
    "# Hint: df/df = 1\n",
    "# Hint: For multiplication f = p * q: df/dp = q, df/dq = p\n",
    "# Hint: For addition p = a + b: dp/da = 1, dp/db = 1\n",
    "# Hint: Use chain rule to combine!\n",
    "\n",
    "df_da = None  # TODO: Your answer\n",
    "df_db = None  # TODO: Your answer (careful - b appears in both p and q!)\n",
    "df_dc = None  # TODO: Your answer\n",
    "\n",
    "# Uncomment to check your answers:\n",
    "# print(f\"\\nYour answers:\")\n",
    "# print(f\"  df/da = {df_da}\")\n",
    "# print(f\"  df/db = {df_db}\")\n",
    "# print(f\"  df/dc = {df_dc}\")\n",
    "\n",
    "# Numerical verification\n",
    "def f_func(a, b, c):\n",
    "    return (a + b) * (b + c)\n",
    "\n",
    "h = 1e-5\n",
    "numerical_da = (f_func(a+h, b, c) - f_func(a-h, b, c)) / (2*h)\n",
    "numerical_db = (f_func(a, b+h, c) - f_func(a, b-h, c)) / (2*h)\n",
    "numerical_dc = (f_func(a, b, c+h) - f_func(a, b, c-h)) / (2*h)\n",
    "\n",
    "print(f\"\\nNumerical verification:\")\n",
    "print(f\"  df/da = {numerical_da:.4f}\")\n",
    "print(f\"  df/db = {numerical_db:.4f}\")\n",
    "print(f\"  df/dc = {numerical_dc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Add Tanh Activation\n",
    "\n",
    "Modify our neural network to use tanh instead of ReLU in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 2: Neural Network with Tanh\n",
    "# Modify the NeuralNetwork class to use tanh activation\n",
    "\n",
    "class NeuralNetworkTanh:\n",
    "    \"\"\"\n",
    "    A 2-layer neural network with tanh hidden activation.\n",
    "    \n",
    "    TODO: Implement the tanh activation and its derivative\n",
    "    Hint: tanh derivative = 1 - tanh(x)^2\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.W2 = np.random.randn(output_size, hidden_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "        self.cache = {}\n",
    "    \n",
    "    def tanh(self, z):\n",
    "        \"\"\"Tanh activation function.\"\"\"\n",
    "        # TODO: Implement tanh\n",
    "        pass\n",
    "    \n",
    "    def tanh_derivative(self, z):\n",
    "        \"\"\"Derivative of tanh.\"\"\"\n",
    "        # TODO: Implement tanh derivative\n",
    "        # Hint: d/dx tanh(x) = 1 - tanh(x)^2\n",
    "        pass\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.cache['z1'] = self.W1 @ X + self.b1\n",
    "        # TODO: Use tanh instead of ReLU\n",
    "        self.cache['h'] = None  # Replace with tanh activation\n",
    "        \n",
    "        self.cache['z2'] = self.W2 @ self.cache['h'] + self.b2\n",
    "        self.cache['y_pred'] = self.sigmoid(self.cache['z2'])\n",
    "        self.cache['X'] = X\n",
    "        return self.cache['y_pred']\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        m = y_true.shape[1]\n",
    "        y_pred = self.cache['y_pred']\n",
    "        h = self.cache['h']\n",
    "        z1 = self.cache['z1']\n",
    "        X = self.cache['X']\n",
    "        \n",
    "        dz2 = y_pred - y_true\n",
    "        dW2 = (1/m) * dz2 @ h.T\n",
    "        db2 = (1/m) * np.sum(dz2, axis=1, keepdims=True)\n",
    "        \n",
    "        dh = self.W2.T @ dz2\n",
    "        # TODO: Use tanh derivative instead of ReLU derivative\n",
    "        dz1 = None  # Replace with correct gradient through tanh\n",
    "        \n",
    "        dW1 = (1/m) * dz1 @ X.T\n",
    "        db1 = (1/m) * np.sum(dz1, axis=1, keepdims=True)\n",
    "        \n",
    "        return {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2}\n",
    "    \n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)\n",
    "            loss = -np.mean(y * np.log(y_pred + 1e-7) + (1-y) * np.log(1 - y_pred + 1e-7))\n",
    "            losses.append(loss)\n",
    "            \n",
    "            gradients = self.backward(y)\n",
    "            \n",
    "            self.W1 -= learning_rate * gradients['dW1']\n",
    "            self.b1 -= learning_rate * gradients['db1']\n",
    "            self.W2 -= learning_rate * gradients['dW2']\n",
    "            self.b2 -= learning_rate * gradients['db2']\n",
    "        \n",
    "        return losses\n",
    "\n",
    "# Test your implementation\n",
    "# np.random.seed(42)\n",
    "# nn_tanh = NeuralNetworkTanh(input_size=2, hidden_size=4, output_size=1)\n",
    "# losses = nn_tanh.train(X_xor, y_xor, epochs=10000, learning_rate=1.0)\n",
    "# y_pred = nn_tanh.forward(X_xor)\n",
    "# accuracy = np.mean((y_pred > 0.5).astype(int) == y_xor)\n",
    "# print(f\"Tanh network accuracy on XOR: {accuracy:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Gradient Checking\n",
    "\n",
    "Implement a function to verify backpropagation is correct by comparing with numerical gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 3: Gradient Checking\n",
    "\n",
    "def gradient_check(network, X, y, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Verify backpropagation by comparing with numerical gradients.\n",
    "    \n",
    "    Args:\n",
    "        network: NeuralNetwork instance\n",
    "        X: Input data\n",
    "        y: True labels\n",
    "        epsilon: Small value for numerical gradient\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with gradient differences for each parameter\n",
    "    \"\"\"\n",
    "    # Get analytical gradients from backprop\n",
    "    _ = network.forward(X)\n",
    "    analytical_grads = network.backward(y)\n",
    "    \n",
    "    # TODO: Compute numerical gradients and compare\n",
    "    # For each weight:\n",
    "    #   1. Add epsilon to the weight\n",
    "    #   2. Compute loss\n",
    "    #   3. Subtract epsilon from the weight (2*epsilon total change)\n",
    "    #   4. Compute loss\n",
    "    #   5. Numerical gradient = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "    \n",
    "    # Hint: You'll need to temporarily modify weights, compute loss, then restore\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Example for one weight (W2[0,0]):\n",
    "    # original_val = network.W2[0, 0]\n",
    "    # \n",
    "    # network.W2[0, 0] = original_val + epsilon\n",
    "    # _ = network.forward(X)\n",
    "    # loss_plus = network.compute_loss(network.cache['y_pred'], y)\n",
    "    # \n",
    "    # network.W2[0, 0] = original_val - epsilon\n",
    "    # _ = network.forward(X)\n",
    "    # loss_minus = network.compute_loss(network.cache['y_pred'], y)\n",
    "    # \n",
    "    # numerical_grad = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "    # network.W2[0, 0] = original_val  # Restore\n",
    "    # \n",
    "    # difference = abs(analytical_grads['dW2'][0, 0] - numerical_grad)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test\n",
    "# np.random.seed(42)\n",
    "# nn_check = NeuralNetwork(input_size=2, hidden_size=3, output_size=1)\n",
    "# results = gradient_check(nn_check, X_xor, y_xor)\n",
    "# print(\"Gradient check results:\")\n",
    "# for param, diff in results.items():\n",
    "#     print(f\"  {param}: max difference = {diff:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Credit Assignment Problem**: How do we know which weights caused the error?\n",
    "- **Computational Graphs**: Break complex functions into simple operations\n",
    "- **Chain Rule**: Multiply local gradients along paths, sum over all paths\n",
    "- **Forward Pass**: Compute outputs, save intermediate values\n",
    "- **Backward Pass**: Compute gradients from output to input\n",
    "- **Vanishing/Exploding Gradients**: Gradients can shrink or grow exponentially with depth\n",
    "\n",
    "### Connection to Deep Learning\n",
    "\n",
    "| Concept | Application |\n",
    "|---------|-------------|\n",
    "| Computational graphs | PyTorch/TensorFlow build these automatically |\n",
    "| Chain rule | The mathematical foundation of all gradient-based learning |\n",
    "| Local gradients | Each layer implements forward() and backward() |\n",
    "| Gradient caching | Frameworks save activations for efficient backward pass |\n",
    "| Vanishing gradients | Why ReLU replaced sigmoid in deep networks |\n",
    "| Exploding gradients | Why gradient clipping is used in RNNs |\n",
    "\n",
    "### Checklist\n",
    "\n",
    "- [ ] I can draw a computational graph for any mathematical expression\n",
    "- [ ] I can compute gradients using the chain rule\n",
    "- [ ] I understand why gradients flow backward\n",
    "- [ ] I can implement backpropagation from scratch\n",
    "- [ ] I know the difference between vanishing and exploding gradients\n",
    "- [ ] I understand why hidden layers enable solving XOR\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand backpropagation at a deep level, you're ready for:\n",
    "\n",
    "1. **PyTorch Fundamentals** - See how autograd does all this automatically!\n",
    "2. **Optimization Algorithms** - SGD, Adam, and learning rate schedules\n",
    "3. **Regularization** - Dropout, weight decay, and preventing overfitting\n",
    "4. **Deeper Networks** - Architectures like CNNs and Transformers\n",
    "\n",
    "The concepts from this notebook will appear everywhere in deep learning. Every time you call `loss.backward()` in PyTorch, the algorithm you just implemented by hand is running under the hood!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
