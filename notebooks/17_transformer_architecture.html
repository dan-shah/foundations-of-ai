
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Part 6.1: Transformer Architecture &#8212; Foundations of AI</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=9c3e77be" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=1ae7504c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/17_transformer_architecture';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Part 6.2: Embeddings" href="18_embeddings.html" />
    <link rel="prev" title="Part 5.4: Attention Mechanisms" href="16_attention_mechanisms.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Foundations of AI</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 1: Mathematical Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_linear_algebra.html">Part 1.1: Linear Algebra for Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_calculus.html">Part 1.2: Calculus for Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_probability_statistics.html">Part 1.3: Probability &amp; Statistics for Deep Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 2: Programming Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_python_oop.html">Part 2.1: Python OOP for Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_numpy_deep_dive.html">Part 2.2: NumPy Deep Dive</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 3: Classical ML &amp; Optimization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="06_classical_ml.html">Part 3.1: Classical Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_optimization_linear_programming.html">Part 3.2: Optimization &amp; Linear Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_optimization_theory.html">Part 3.3: Optimization Theory for Machine Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 4: Neural Network Fundamentals</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="09_perceptrons_basic_networks.html">Part 4.1: Perceptrons &amp; Basic Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_backpropagation.html">Part 4.2: Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_pytorch_fundamentals.html">Part 4.3: PyTorch Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_training_deep_networks.html">Part 4.4: Training Deep Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 5: Neural Network Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="13_convolutional_neural_networks.html">Part 5.1: Convolutional Neural Networks (CNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_computer_vision_depth.html">Part 5.2: Computer Vision — Beyond Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_recurrent_neural_networks.html">Part 5.3: Recurrent Neural Networks (RNNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_attention_mechanisms.html">Part 5.4: Attention Mechanisms</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 6: Transformers &amp; LLMs</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Part 6.1: Transformer Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_embeddings.html">Part 6.2: Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="19_tokenization_lm_training.html">Part 6.3: Tokenization &amp; Language Model Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="20_language_models.html">Part 6.4: Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="21_finetuning_and_peft.html">Part 6.5: Fine-tuning &amp; PEFT</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 7: Reinforcement Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="22_rl_fundamentals.html">Part 7.1: Reinforcement Learning Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="23_q_learning_dqn.html">Part 7.2: Q-Learning and Deep Q-Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="24_policy_gradients.html">Part 7.3: Policy Gradient Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="25_ppo_modern_rl.html">Part 7.4: PPO and Modern RL</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 8: Applied AI Systems</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="26_rag.html">Part 8.1: Retrieval-Augmented Generation (RAG)</a></li>
<li class="toctree-l1"><a class="reference internal" href="27_ai_agents.html">Part 8.2: AI Agents and Tool Use</a></li>
<li class="toctree-l1"><a class="reference internal" href="28_ai_evals.html">Part 8.3: Evaluating AI Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="29_production_monitoring.html">Part 8.4: Production AI Systems</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 9: Advanced Topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="30_inference_optimization.html">Part 9.1: LLM Inference Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="31_ml_systems.html">Part 9.2: ML Systems &amp; Experiment Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="32_multimodal_ai.html">Part 9.3: Multimodal AI</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/dan-shah/foundations-of-ai/blob/main/notebooks/17_transformer_architecture.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/dan-shah/foundations-of-ai" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/dan-shah/foundations-of-ai/edit/main/notebooks/17_transformer_architecture.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/dan-shah/foundations-of-ai/issues/new?title=Issue%20on%20page%20%2Fnotebooks/17_transformer_architecture.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notebooks/17_transformer_architecture.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Part 6.1: Transformer Architecture</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-is-all-you-need-the-big-picture">1. “Attention Is All You Need” – The Big Picture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuitive-explanation">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-rnn-vs-transformer">Comparison: RNN vs Transformer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-high-level-architecture">The High-Level Architecture</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encoding">2. Positional Encoding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem-attention-has-no-sense-of-order">The Problem: Attention Has No Sense of Order</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sinusoidal-positional-encoding">Sinusoidal Positional Encoding</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#breaking-down-the-formula">Breaking down the formula:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learned-vs-sinusoidal-positional-embeddings">Learned vs Sinusoidal Positional Embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-why-sinusoidal-encoding-enables-relative-position">Deep Dive: Why Sinusoidal Encoding Enables Relative Position</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#key-insight">Key Insight</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#common-misconceptions">Common Misconceptions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-encoder-block">3. The Encoder Block</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#add-norm-residual-connections-layer-normalization">Add &amp; Norm: Residual Connections + Layer Normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-feed-forward-network-ffn">The Feed-Forward Network (FFN)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stacking-n-encoder-blocks">Stacking N Encoder Blocks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-decoder-block">4. The Decoder Block</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-attention-the-bridge-between-encoder-and-decoder">Cross-Attention: The Bridge Between Encoder and Decoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder-block-vs-decoder-block">Encoder Block vs Decoder Block</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-full-transformer">5. The Full Transformer</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together">Putting It All Together</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-a-transformer">6. Training a Transformer</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#teacher-forcing">Teacher Forcing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-warmup">Learning Rate Warmup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-task-sequence-reversal">Training Task: Sequence Reversal</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-transformers-work-so-well">7. Why Transformers Work So Well</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-scaling-laws-preview">Deep Dive: Scaling Laws Preview</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Key Insight</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Common Misconceptions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-implement-positional-encoding-from-scratch">Exercise 1: Implement Positional Encoding from Scratch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-build-a-decoder-only-transformer-like-gpt">Exercise 2: Build a Decoder-Only Transformer (like GPT)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-experiment-with-architecture-hyperparameters">Exercise 3: Experiment with Architecture Hyperparameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts">Key Concepts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-deep-learning">Connection to Deep Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checklist">Checklist</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">Next Steps</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="part-6-1-transformer-architecture">
<h1>Part 6.1: Transformer Architecture<a class="headerlink" href="#part-6-1-transformer-architecture" title="Link to this heading">#</a></h1>
<p>The Transformer is the architecture behind GPT, BERT, T5, and virtually every state-of-the-art model in NLP, vision, and beyond. Introduced in the landmark 2017 paper <em>“Attention Is All You Need”</em>, it replaced recurrent networks with a design built entirely on the attention mechanisms you learned in the previous notebook. In this notebook, you will understand every piece of the Transformer and build one from scratch.</p>
<p><strong>F1 analogy:</strong> Think of the Transformer as F1’s modern strategy computer – the system that ingests all available data streams (tire wear, fuel load, weather, gap to rivals) <strong>in parallel</strong> and produces real-time strategy calls. Older RNN-based models are like the pre-radio era when information traveled sequentially through pit boards, one lap at a time. The Transformer processes everything at once, just as a modern F1 strategy wall sees every car’s telemetry simultaneously.</p>
<p><strong>Prerequisites:</strong> Notebook 16 (Attention Mechanisms) – you should be comfortable with Q/K/V attention, self-attention, multi-head attention, and causal masking.</p>
<hr class="docutils" />
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<p>By the end of this notebook, you will be able to:</p>
<ul class="simple">
<li><p>[ ] Explain why Transformers replaced RNNs and the key innovations of the architecture</p></li>
<li><p>[ ] Implement sinusoidal positional encoding and explain why position information is needed</p></li>
<li><p>[ ] Build an Encoder block from scratch (multi-head attention + Add&amp;Norm + FFN)</p></li>
<li><p>[ ] Build a Decoder block from scratch (masked self-attention + cross-attention + FFN)</p></li>
<li><p>[ ] Assemble a complete Transformer model in PyTorch</p></li>
<li><p>[ ] Train a Transformer on a simple sequence task and visualize attention patterns</p></li>
<li><p>[ ] Analyze parameter counts and explain why Transformers scale so well</p></li>
</ul>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn-v0_8-whitegrid&#39;</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Setup complete!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;PyTorch version: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="attention-is-all-you-need-the-big-picture">
<h2>1. “Attention Is All You Need” – The Big Picture<a class="headerlink" href="#attention-is-all-you-need-the-big-picture" title="Link to this heading">#</a></h2>
<section id="intuitive-explanation">
<h3>Intuitive Explanation<a class="headerlink" href="#intuitive-explanation" title="Link to this heading">#</a></h3>
<p>Before Transformers, sequence models were dominated by <strong>Recurrent Neural Networks</strong> (RNNs, LSTMs, GRUs). These process tokens one at a time, left to right, passing a hidden state from step to step like a game of telephone. This sequential nature creates two fundamental problems:</p>
<ol class="arabic simple">
<li><p><strong>No parallelization:</strong> You cannot process token 5 until you have finished tokens 1-4. Training is painfully slow.</p></li>
<li><p><strong>Long-range dependencies fade:</strong> Information from early tokens must survive through many sequential steps. Even with LSTMs, signals degrade over long distances.</p></li>
</ol>
<p>The Transformer’s key insight is radical: <strong>throw away recurrence entirely.</strong> Instead of processing tokens sequentially, let every token attend to every other token simultaneously using attention. This means:</p>
<ul class="simple">
<li><p><strong>Full parallelization:</strong> All positions are processed at once during training</p></li>
<li><p><strong>Direct connections:</strong> Token 1 can directly attend to token 100 with no intermediaries</p></li>
<li><p><strong>Constant path length:</strong> Information travels in O(1) steps, not O(n)</p></li>
</ul>
<p><strong>F1 analogy:</strong> Imagine an RNN as a team that relays information via pit boards – the driver sees one message per lap, and by lap 50 the message from lap 1 has been distorted through 49 handoffs. A Transformer is like modern F1 radio and telemetry: the strategy wall can instantly access data from <em>any</em> lap, <em>any</em> sector, <em>any</em> car – all at once, with no degradation. That is why Transformers dominate: they process all positions in parallel, just as a modern F1 strategy system processes all data streams simultaneously.</p>
</section>
<section id="comparison-rnn-vs-transformer">
<h3>Comparison: RNN vs Transformer<a class="headerlink" href="#comparison-rnn-vs-transformer" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Property</p></th>
<th class="head"><p>RNN/LSTM</p></th>
<th class="head"><p>Transformer</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Processing</strong></p></td>
<td><p>Sequential (one token at a time)</p></td>
<td><p>Parallel (all tokens at once)</p></td>
<td><p>Pit boards (one message/lap) vs. full telemetry dashboard</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Long-range dependencies</strong></p></td>
<td><p>Degrades over distance</p></td>
<td><p>Direct attention at any distance</p></td>
<td><p>Forgetting early stint data vs. instant lap-1 recall</p></td>
</tr>
<tr class="row-even"><td><p><strong>Path length</strong></p></td>
<td><p>O(n) between distant tokens</p></td>
<td><p>O(1) between any two tokens</p></td>
<td><p>Information relay chain vs. direct radio link</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Training speed</strong></p></td>
<td><p>Slow (sequential bottleneck)</p></td>
<td><p>Fast (GPU-parallelizable)</p></td>
<td><p>Slow debriefs vs. real-time analytics</p></td>
</tr>
<tr class="row-even"><td><p><strong>Memory</strong></p></td>
<td><p>Fixed hidden state size</p></td>
<td><p>Grows with sequence length (O(n^2))</p></td>
<td><p>Limited pit board vs. full data storage</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Positional info</strong></p></td>
<td><p>Built-in (sequential processing)</p></td>
<td><p>Must be explicitly added</p></td>
<td><p>Implicit lap count vs. explicit position encoding</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>The trade-off:</strong> Transformers use O(n^2) memory for attention (every token attends to every other), but this is well worth it for the parallelization and quality gains.</p>
</section>
<section id="the-high-level-architecture">
<h3>The High-Level Architecture<a class="headerlink" href="#the-high-level-architecture" title="Link to this heading">#</a></h3>
<p>The original Transformer has an <strong>encoder-decoder</strong> structure:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>INPUT TOKENS                          OUTPUT TOKENS
     |                                      |
  [Embedding + Positional Encoding]    [Embedding + Positional Encoding]
     |                                      |
  ┌──────────────┐                    ┌──────────────┐
  │  Encoder      │                    │  Decoder      │
  │  Block x N    │──────────────────▶│  Block x N    │
  │               │  (cross-attention) │               │
  └──────────────┘                    └──────────────┘
                                           |
                                    [Linear + Softmax]
                                           |
                                    OUTPUT PROBABILITIES
</pre></div>
</div>
<p><strong>Encoder:</strong> Reads the full input and creates rich contextual representations.
<strong>Decoder:</strong> Generates output one token at a time, attending to both its own previous outputs AND the encoder’s representations.</p>
<p><strong>F1 analogy:</strong> The encoder is like the <strong>telemetry and data acquisition system</strong> – it ingests all raw sensor data (tire temps, throttle traces, GPS coordinates) and builds a rich, contextual picture of the car’s state. The decoder is the <strong>strategy engineer</strong> who translates that encoded picture into actionable calls: “Box this lap,” “Switch to hards,” “Push now.” Cross-attention is the bridge – how the strategy engineer <em>reads</em> the telemetry to make decisions.</p>
<p>Not all modern models use both sides:</p>
<ul class="simple">
<li><p><strong>GPT</strong> uses only the decoder (autoregressive language modeling)</p></li>
<li><p><strong>BERT</strong> uses only the encoder (bidirectional understanding)</p></li>
<li><p><strong>T5, BART</strong> use the full encoder-decoder</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualization: RNN vs Transformer information flow</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># --- RNN: Sequential information flow ---</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">n_tokens</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">positions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_tokens</span><span class="p">)</span>

<span class="c1"># Draw tokens</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">positions</span><span class="p">:</span>
    <span class="n">circle</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">circle</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;t</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>

<span class="c1"># Draw sequential arrows</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_tokens</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mf">0.7</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mf">0.3</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>

<span class="c1"># Show fading signal</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">positions</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">i</span> <span class="o">*</span> <span class="mf">0.18</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> 
                             <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">n_tokens</span> <span class="o">-</span> <span class="mf">0.2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;RNN: Sequential Processing</span><span class="se">\n</span><span class="s1">(signal fades over distance)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="c1"># --- Transformer: All-to-all attention ---</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Draw tokens</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">positions</span><span class="p">:</span>
    <span class="n">circle</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">circle</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;t</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>

<span class="c1"># Draw attention connections (all-to-all)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">positions</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">positions</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">j</span><span class="p">:</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>

<span class="c1"># Highlight a specific long-range connection</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="o">+</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">),</span>
            <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">2.5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s1">&#39;Direct connection!&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">n_tokens</span> <span class="o">-</span> <span class="mf">0.2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Transformer: Parallel Attention</span><span class="se">\n</span><span class="s1">(direct path between any tokens)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;rnn_vs_transformer.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Left: RNN processes sequentially -- information from t1 must pass through every step to reach t6.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Right: Transformer connects all tokens directly via attention -- t1 and t6 are just one step apart.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="positional-encoding">
<h2>2. Positional Encoding<a class="headerlink" href="#positional-encoding" title="Link to this heading">#</a></h2>
<section id="the-problem-attention-has-no-sense-of-order">
<h3>The Problem: Attention Has No Sense of Order<a class="headerlink" href="#the-problem-attention-has-no-sense-of-order" title="Link to this heading">#</a></h3>
<p>Here is a crucial insight: <strong>self-attention is permutation invariant.</strong> If you shuffle the input tokens, the attention outputs are also shuffled in exactly the same way – the mechanism itself does not know or care about token order.</p>
<p>Think about it: <code class="docutils literal notranslate"><span class="pre">Attention(Q,</span> <span class="pre">K,</span> <span class="pre">V)</span> <span class="pre">=</span> <span class="pre">softmax(QK^T</span> <span class="pre">/</span> <span class="pre">sqrt(d_k))V</span></code>. The computation depends only on the <em>content</em> of the vectors, not their <em>position</em>. The sentence “the cat sat on the mat” and “mat the on sat cat the” would produce identical attention patterns (just rearranged).</p>
<p>But word order matters enormously! “The dog bit the man” and “The man bit the dog” have very different meanings. We need to inject position information somehow.</p>
<p><strong>The solution:</strong> Add a <strong>positional encoding</strong> to each token embedding before feeding it into the Transformer. Each position gets a unique signal that the model can use to determine where tokens are in the sequence.</p>
<p><strong>F1 analogy:</strong> Positional encoding is like <strong>lap number and track position encoding</strong>. Raw telemetry data (speed, throttle, brake) means nothing without knowing <em>when</em> and <em>where</em> it was recorded. A speed of 320 km/h means something very different on lap 1 (opening lap with cold tires) versus lap 50 (late-race tire degradation), and at the start/finish straight versus the apex of turn 1. Just as the strategy computer tags every data point with its lap number and track sector, the Transformer tags every token with its position in the sequence.</p>
</section>
<section id="sinusoidal-positional-encoding">
<h3>Sinusoidal Positional Encoding<a class="headerlink" href="#sinusoidal-positional-encoding" title="Link to this heading">#</a></h3>
<p>The original Transformer paper uses a clever mathematical approach: encode each position using sine and cosine waves at different frequencies.</p>
<div class="math notranslate nohighlight">
\[PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)\]</div>
<div class="math notranslate nohighlight">
\[PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)\]</div>
<section id="breaking-down-the-formula">
<h4>Breaking down the formula:<a class="headerlink" href="#breaking-down-the-formula" title="Link to this heading">#</a></h4>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Meaning</p></th>
<th class="head"><p>Intuition</p></th>
<th class="head"><p>F1 Analogy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(pos\)</span></p></td>
<td><p>Position in the sequence (0, 1, 2, …)</p></td>
<td><p>Which token are we encoding?</p></td>
<td><p>Lap number in the race</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(i\)</span></p></td>
<td><p>Dimension index (0, 1, …, d/2)</p></td>
<td><p>Which “frequency channel”?</p></td>
<td><p>Different telemetry channels (speed, tire temp, fuel)</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(d_{model}\)</span></p></td>
<td><p>Model dimension (e.g., 512)</p></td>
<td><p>Total embedding size</p></td>
<td><p>Total number of sensor channels</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(10000^{2i/d_{model}}\)</span></p></td>
<td><p>Wavelength scaling</p></td>
<td><p>Low dimensions = fast waves, high dimensions = slow waves</p></td>
<td><p>Fast-changing data (throttle) vs. slow-changing data (fuel load)</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>What this means:</strong> Each position is encoded as a point on many sine/cosine waves of different frequencies. Low-dimension pairs oscillate rapidly (capturing fine-grained position), while high-dimension pairs oscillate slowly (capturing broad position). Together they create a <strong>unique fingerprint</strong> for every position.</p>
<p><strong>F1 analogy:</strong> Think of this as encoding race position using multiple time scales simultaneously. Fast-oscillating dimensions capture <em>which sector</em> of the lap you are in (changes every few seconds). Slow-oscillating dimensions capture <em>which stint</em> you are in (changes every 15-25 laps). Together, they uniquely identify any moment in the race.</p>
<p><strong>Why sin/cos?</strong> Two key reasons:</p>
<ol class="arabic simple">
<li><p><strong>Unique encoding:</strong> Every position gets a distinct vector</p></li>
<li><p><strong>Relative positions are linear:</strong> <span class="math notranslate nohighlight">\(PE_{pos+k}\)</span> can be expressed as a linear function of <span class="math notranslate nohighlight">\(PE_{pos}\)</span> (rotation in 2D), so the model can easily learn to attend to relative offsets</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Implement sinusoidal positional encoding</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_positional_encoding</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate sinusoidal positional encoding.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        max_len: Maximum sequence length</span>
<span class="sd">        d_model: Model dimension (must be even)</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        Tensor of shape (max_len, d_model)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (max_len, 1)</span>
    
    <span class="c1"># Compute the division term: 10000^(2i/d_model)</span>
    <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
    
    <span class="c1"># Even dimensions: sin, Odd dimensions: cos</span>
    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">pe</span>

<span class="c1"># Generate positional encoding</span>
<span class="n">max_len</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">pe</span> <span class="o">=</span> <span class="n">get_positional_encoding</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Positional encoding shape: </span><span class="si">{</span><span class="n">pe</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">First position (pos=0):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  First 8 values: </span><span class="si">{</span><span class="n">pe</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="p">:</span><span class="mi">8</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Second position (pos=1):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  First 8 values: </span><span class="si">{</span><span class="n">pe</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="p">:</span><span class="mi">8</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualization: Positional Encoding Heatmap</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Full heatmap</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pe</span><span class="p">[:</span><span class="mi">50</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdBu&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Embedding Dimension&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Position&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Positional Encoding Heatmap</span><span class="se">\n</span><span class="s1">(first 50 positions)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

<span class="c1"># Individual dimensions showing different frequencies</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">positions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_len</span><span class="p">)</span>
<span class="n">dims_to_show</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">31</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">]</span>
<span class="n">styles</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">dim</span><span class="p">,</span> <span class="n">color</span><span class="p">,</span> <span class="n">style</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">dims_to_show</span><span class="p">,</span> <span class="n">colors</span><span class="p">,</span> <span class="n">styles</span><span class="p">):</span>
    <span class="n">label</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;dim </span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s1"> (</span><span class="si">{</span><span class="s2">&quot;sin&quot;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">dim</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s2">&quot;cos&quot;</span><span class="si">}</span><span class="s1">)&#39;</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">positions</span><span class="p">,</span> <span class="n">pe</span><span class="p">[:,</span> <span class="n">dim</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="n">style</span><span class="p">,</span> 
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Position&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Encoding Value&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Different Dimensions = Different Frequencies&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;positional_encoding.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Left: Each row is one position&#39;s encoding vector. Notice the wave patterns across dimensions.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Right: Low dimensions (blue) oscillate fast, high dimensions (red) oscillate slowly.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;This creates a unique &#39;fingerprint&#39; for every position.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualization: Each position has a unique encoding</span>
<span class="c1"># Show this by computing pairwise distances between position encodings</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Cosine similarity between positions</span>
<span class="n">pe_norm</span> <span class="o">=</span> <span class="n">pe</span> <span class="o">/</span> <span class="n">pe</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">cos_sim</span> <span class="o">=</span> <span class="p">(</span><span class="n">pe_norm</span> <span class="o">@</span> <span class="n">pe_norm</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">n_show</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cos_sim</span><span class="p">[:</span><span class="n">n_show</span><span class="p">,</span> <span class="p">:</span><span class="n">n_show</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Position&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Position&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Cosine Similarity Between</span><span class="se">\n</span><span class="s1">Position Encodings&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

<span class="c1"># Show that nearby positions are more similar</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ref_positions</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">40</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">ref_pos</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ref_positions</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
    <span class="n">similarities</span> <span class="o">=</span> <span class="n">cos_sim</span><span class="p">[</span><span class="n">ref_pos</span><span class="p">,</span> <span class="p">:</span><span class="n">n_show</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_show</span><span class="p">),</span> <span class="n">similarities</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> 
            <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Similarity to pos </span><span class="si">{</span><span class="n">ref_pos</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Position&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Cosine Similarity&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Nearby Positions Are More Similar&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;positional_uniqueness.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Left: Each position has a unique encoding (no two rows are identical).&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Right: Similarity peaks at the reference position and decays with distance.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;This smooth decay means the model can learn relative position relationships.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="learned-vs-sinusoidal-positional-embeddings">
<h3>Learned vs Sinusoidal Positional Embeddings<a class="headerlink" href="#learned-vs-sinusoidal-positional-embeddings" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Approach</p></th>
<th class="head"><p>How it works</p></th>
<th class="head"><p>Pros</p></th>
<th class="head"><p>Cons</p></th>
<th class="head"><p>F1 Analogy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Sinusoidal</strong> (original paper)</p></td>
<td><p>Fixed sin/cos functions</p></td>
<td><p>Can extrapolate to longer sequences; no extra parameters</p></td>
<td><p>Slightly less flexible</p></td>
<td><p>Fixed GPS coordinates – works for any circuit length</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Learned</strong> (BERT, GPT-2)</p></td>
<td><p>Trainable embedding per position</p></td>
<td><p>Can adapt to data patterns</p></td>
<td><p>Fixed max length; more parameters</p></td>
<td><p>Team-specific track maps tuned from practice data</p></td>
</tr>
<tr class="row-even"><td><p><strong>Relative</strong> (Transformer-XL, RoPE)</p></td>
<td><p>Encode distance between tokens, not absolute position</p></td>
<td><p>Better generalization; handles variable lengths</p></td>
<td><p>More complex implementation</p></td>
<td><p>Gap-to-car-ahead (relative) vs. absolute lap time</p></td>
</tr>
</tbody>
</table>
</div>
<p>In practice, learned embeddings are most common in modern models. <strong>RoPE</strong> (Rotary Position Embedding), used in LLaMA and many recent models, applies rotation matrices that naturally encode relative positions – a clever evolution of the sinusoidal idea.</p>
</section>
<section id="deep-dive-why-sinusoidal-encoding-enables-relative-position">
<h3>Deep Dive: Why Sinusoidal Encoding Enables Relative Position<a class="headerlink" href="#deep-dive-why-sinusoidal-encoding-enables-relative-position" title="Link to this heading">#</a></h3>
<section id="key-insight">
<h4>Key Insight<a class="headerlink" href="#key-insight" title="Link to this heading">#</a></h4>
<p>For any fixed offset <span class="math notranslate nohighlight">\(k\)</span>, the encoding at position <span class="math notranslate nohighlight">\(pos + k\)</span> can be written as a <strong>linear transformation</strong> of the encoding at position <span class="math notranslate nohighlight">\(pos\)</span>. Specifically, for each pair of dimensions <span class="math notranslate nohighlight">\((2i, 2i+1)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix} PE_{pos+k, 2i} \\ PE_{pos+k, 2i+1} \end{bmatrix} = \begin{bmatrix} \cos(k \cdot \omega_i) &amp; \sin(k \cdot \omega_i) \\ -\sin(k \cdot \omega_i) &amp; \cos(k \cdot \omega_i) \end{bmatrix} \begin{bmatrix} PE_{pos, 2i} \\ PE_{pos, 2i+1} \end{bmatrix}\end{split}\]</div>
<p>This is a <strong>rotation matrix!</strong> Moving from position <span class="math notranslate nohighlight">\(pos\)</span> to <span class="math notranslate nohighlight">\(pos + k\)</span> is a rotation in each 2D subspace. Since rotations are linear, the model can learn to compute relative positions using simple linear operations (which is exactly what attention does).</p>
</section>
<section id="common-misconceptions">
<h4>Common Misconceptions<a class="headerlink" href="#common-misconceptions" title="Link to this heading">#</a></h4>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Misconception</p></th>
<th class="head"><p>Reality</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>“Positional encoding tells the model the exact position”</p></td>
<td><p>It provides a signal the model <em>learns</em> to interpret; the encoding itself is just numbers</p></td>
</tr>
<tr class="row-odd"><td><p>“You need sinusoidal encodings specifically”</p></td>
<td><p>Learned encodings work just as well; the key is having <em>some</em> position signal</p></td>
</tr>
<tr class="row-even"><td><p>“Positional encoding is added only once”</p></td>
<td><p>It is added once at the input, but residual connections carry it through all layers</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
</section>
<hr class="docutils" />
<section id="the-encoder-block">
<h2>3. The Encoder Block<a class="headerlink" href="#the-encoder-block" title="Link to this heading">#</a></h2>
<section id="id1">
<h3>Intuitive Explanation<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>The encoder’s job is to read the input sequence and build <strong>rich, contextual representations</strong> of each token. It does this by stacking identical blocks, each containing two sub-layers:</p>
<ol class="arabic simple">
<li><p><strong>Multi-Head Self-Attention:</strong> Each token looks at all other tokens to understand context (you built this in Notebook 16)</p></li>
<li><p><strong>Position-wise Feed-Forward Network (FFN):</strong> Each token is independently transformed through a small neural network</p></li>
</ol>
<p>Around each sub-layer, there are two critical additions:</p>
<ul class="simple">
<li><p><strong>Residual connection:</strong> Add the input back to the output (skip connection)</p></li>
<li><p><strong>Layer normalization:</strong> Normalize the result for stable training</p></li>
</ul>
<p><strong>F1 analogy:</strong> The encoder block is like a <strong>telemetry processing pipeline</strong>. Self-attention is the step where every sensor reading is cross-referenced with every other sensor reading – “How does tire temperature relate to lap time? How does throttle application correlate with tire wear?” The FFN is where each sensor reading is independently refined – converting raw voltage into meaningful engineering units. The residual connection ensures the original raw signal is never lost, and layer normalization keeps all signals on comparable scales regardless of whether you are measuring temperature in Celsius or pressure in bar.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Input (for each token)
  │
  ├──────────────────────┐
  │                      │ (residual)
  ▼                      │
Multi-Head Self-Attention │
  │                      │
  ▼                      │
  + ◄────────────────────┘
  │
LayerNorm
  │
  ├──────────────────────┐
  │                      │ (residual)
  ▼                      │
Feed-Forward Network     │
  │                      │
  ▼                      │
  + ◄────────────────────┘
  │
LayerNorm
  │
  ▼
Output (same shape as input)
</pre></div>
</div>
</section>
<section id="add-norm-residual-connections-layer-normalization">
<h3>Add &amp; Norm: Residual Connections + Layer Normalization<a class="headerlink" href="#add-norm-residual-connections-layer-normalization" title="Link to this heading">#</a></h3>
<p><strong>Residual connections</strong> solve the vanishing gradient problem in deep networks. Instead of learning <span class="math notranslate nohighlight">\(F(x)\)</span>, we learn <span class="math notranslate nohighlight">\(F(x) + x\)</span>. If the sub-layer has nothing useful to add, gradients can still flow through the identity path.</p>
<p><strong>Layer normalization</strong> normalizes across the feature dimension (not the batch dimension like BatchNorm). For each token independently:</p>
<div class="math notranslate nohighlight">
\[\text{LayerNorm}(x) = \frac{x - \mu}{\sigma + \epsilon} \cdot \gamma + \beta\]</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>What it does</p></th>
<th class="head"><p>Why it matters</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Residual connection</strong></p></td>
<td><p>Adds input to sub-layer output</p></td>
<td><p>Enables training of very deep networks</p></td>
<td><p>Baseline setup preserved – new adjustments are <em>added</em> on top</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Layer normalization</strong></p></td>
<td><p>Normalizes each token’s features</p></td>
<td><p>Stabilizes training, reduces sensitivity to scale</p></td>
<td><p>Normalizing across different race conditions – wet vs. dry, hot vs. cold track</p></td>
</tr>
<tr class="row-even"><td><p><strong>Together (Add &amp; Norm)</strong></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">LayerNorm(x</span> <span class="pre">+</span> <span class="pre">SubLayer(x))</span></code></p></td>
<td><p>The standard “wrapper” around every sub-layer</p></td>
<td><p>Adjust setup, then normalize so all readings are comparable</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>F1 analogy:</strong> Layer normalization is like normalizing telemetry across different race conditions. A tire temperature of 100C means something different at Bahrain (50C ambient) versus Spa (15C ambient). LayerNorm ensures that the model sees standardized signals regardless of the “ambient conditions” of the data, just as engineers normalize readings before comparing across sessions.</p>
</section>
<section id="the-feed-forward-network-ffn">
<h3>The Feed-Forward Network (FFN)<a class="headerlink" href="#the-feed-forward-network-ffn" title="Link to this heading">#</a></h3>
<p>The FFN is applied to each position <strong>independently</strong> (the same network, same weights, for every token). It consists of two linear transformations with a ReLU (or GELU) activation in between:</p>
<div class="math notranslate nohighlight">
\[\text{FFN}(x) = W_2 \cdot \text{ReLU}(W_1 x + b_1) + b_2\]</div>
<p>The inner dimension is typically <strong>4x</strong> the model dimension (e.g., <span class="math notranslate nohighlight">\(d_{model} = 512 \rightarrow d_{ff} = 2048\)</span>).</p>
<p><strong>Why is the FFN needed?</strong> Self-attention is powerful but it only computes <strong>weighted averages</strong> of value vectors – which is a linear operation. The FFN adds:</p>
<ol class="arabic simple">
<li><p><strong>Nonlinearity</strong> (via ReLU/GELU) – critical for learning complex functions</p></li>
<li><p><strong>Per-position processing</strong> – each token can independently transform its representation</p></li>
<li><p><strong>Memory</strong> – recent research suggests FFN layers act as key-value memories, storing factual knowledge</p></li>
</ol>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Sub-layer</p></th>
<th class="head"><p>Operation</p></th>
<th class="head"><p>Capacity</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Self-Attention</p></td>
<td><p>Mixes information <strong>across</strong> tokens</p></td>
<td><p>Contextual understanding</p></td>
<td><p>Cross-referencing all sensor channels with each other</p></td>
</tr>
<tr class="row-odd"><td><p>FFN</p></td>
<td><p>Transforms each token <strong>independently</strong></p></td>
<td><p>Feature extraction, knowledge storage</p></td>
<td><p>Per-sensor signal processing and calibration</p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Implement the core components of an Encoder Block</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multi-head attention (from Notebook 16, now as a reusable module).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">n_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;d_model must be divisible by n_heads&quot;</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_heads</span>
        
        <span class="c1"># Linear projections for Q, K, V, and output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_o</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            query: (batch, seq_len_q, d_model)</span>
<span class="sd">            key: (batch, seq_len_k, d_model)</span>
<span class="sd">            value: (batch, seq_len_k, d_model)</span>
<span class="sd">            mask: Optional mask (broadcastable to attention shape)</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">            output: (batch, seq_len_q, d_model)</span>
<span class="sd">            attention_weights: (batch, n_heads, seq_len_q, seq_len_k)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># Project and reshape to (batch, n_heads, seq_len, d_k)</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Scaled dot-product attention</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>
        
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
        
        <span class="c1"># Concatenate heads and project</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_o</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention_weights</span>


<span class="k">class</span><span class="w"> </span><span class="nc">PositionwiseFFN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Position-wise Feed-Forward Network.</span>
<span class="sd">    Two linear layers with ReLU activation. Applied independently to each position.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>


<span class="k">class</span><span class="w"> </span><span class="nc">EncoderBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    One Transformer Encoder block.</span>
<span class="sd">    </span>
<span class="sd">    Contains:</span>
<span class="sd">    1. Multi-head self-attention + Add &amp; Norm</span>
<span class="sd">    2. Position-wise FFN + Add &amp; Norm</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">PositionwiseFFN</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            x: (batch, seq_len, d_model)</span>
<span class="sd">            mask: Optional attention mask</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">            output: (batch, seq_len, d_model)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Sub-layer 1: Multi-head self-attention + Add &amp; Norm</span>
        <span class="n">attn_output</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">))</span>   <span class="c1"># Residual + LayerNorm</span>
        
        <span class="c1"># Sub-layer 2: FFN + Add &amp; Norm</span>
        <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">ffn_output</span><span class="p">))</span>    <span class="c1"># Residual + LayerNorm</span>
        
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Test the encoder block</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">n_heads</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">d_ff</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">encoder_block</span> <span class="o">=</span> <span class="n">EncoderBlock</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">encoder_block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Encoder Block&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Input shape:  </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Output shape: </span><span class="si">{</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Same shape: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Parameter count:&quot;</span><span class="p">)</span>
<span class="n">total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">encoder_block</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">encoder_block</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> params)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  TOTAL: </span><span class="si">{</span><span class="n">total_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> parameters&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualization: Data flow through an Encoder Block</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Data Flow Through One Encoder Block&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># Helper to draw boxes</span>
<span class="k">def</span><span class="w"> </span><span class="nf">draw_box</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">color</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">rect</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">w</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">y</span><span class="o">-</span><span class="n">h</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                          <span class="n">edgecolor</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">rect</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">fontsize</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">draw_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">),</span>
                <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>

<span class="c1"># Input</span>
<span class="n">draw_box</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="s1">&#39;Input: (batch, seq_len, d_model)&#39;</span><span class="p">,</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">draw_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">10.7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">9.7</span><span class="p">)</span>

<span class="c1"># Self-Attention</span>
<span class="n">draw_box</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">9.3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="s1">&#39;Multi-Head Self-Attention&#39;</span><span class="p">,</span> <span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
<span class="n">draw_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">8.9</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">8.1</span><span class="p">)</span>

<span class="c1"># Add &amp; Norm 1</span>
<span class="n">draw_box</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">7.7</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="s1">&#39;Add &amp; LayerNorm&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">)</span>
<span class="c1"># Residual connection 1</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">3.2</span><span class="p">,</span> <span class="mf">7.7</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">3.2</span><span class="p">,</span> <span class="mf">10.3</span><span class="p">),</span>
            <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">2.4</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">,</span> <span class="s1">&#39;residual&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">draw_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">7.4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">6.5</span><span class="p">)</span>

<span class="c1"># FFN</span>
<span class="n">draw_box</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">6.1</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;Feed-Forward Network</span><span class="se">\n</span><span class="s1">(d_model -&gt; d_ff -&gt; d_model)&#39;</span><span class="p">,</span> <span class="s1">&#39;orange&#39;</span><span class="p">)</span>
<span class="n">draw_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">5.7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">4.8</span><span class="p">)</span>

<span class="c1"># Add &amp; Norm 2</span>
<span class="n">draw_box</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">4.4</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="s1">&#39;Add &amp; LayerNorm&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">)</span>
<span class="c1"># Residual connection 2</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">3.2</span><span class="p">,</span> <span class="mf">4.4</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">3.2</span><span class="p">,</span> <span class="mf">7.1</span><span class="p">),</span>
            <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">2.4</span><span class="p">,</span> <span class="mf">5.7</span><span class="p">,</span> <span class="s1">&#39;residual&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">draw_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">4.1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">)</span>

<span class="c1"># Output</span>
<span class="n">draw_box</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="s1">&#39;Output: (batch, seq_len, d_model)&#39;</span><span class="p">,</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>

<span class="c1"># Annotations</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">8.5</span><span class="p">,</span> <span class="mf">9.3</span><span class="p">,</span> <span class="s1">&#39;Each token attends</span><span class="se">\n</span><span class="s1">to all other tokens&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> 
        <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;italic&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">8.5</span><span class="p">,</span> <span class="mf">6.1</span><span class="p">,</span> <span class="s1">&#39;Same MLP applied</span><span class="se">\n</span><span class="s1">to each token</span><span class="se">\n</span><span class="s1">independently&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span>
        <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;italic&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkorange&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">8.5</span><span class="p">,</span> <span class="mf">7.7</span><span class="p">,</span> <span class="s1">&#39;Stabilizes gradients&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span>
        <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;italic&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;encoder_block.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Key insight: the input and output have the SAME shape.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;This means encoder blocks can be stacked -- the output of one is the input to the next.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="stacking-n-encoder-blocks">
<h3>Stacking N Encoder Blocks<a class="headerlink" href="#stacking-n-encoder-blocks" title="Link to this heading">#</a></h3>
<p>The original Transformer uses <strong>N = 6</strong> identical encoder blocks stacked on top of each other. Each block refines the representations:</p>
<ul class="simple">
<li><p><strong>Early layers</strong> tend to capture syntactic patterns (word relationships, grammar)</p></li>
<li><p><strong>Middle layers</strong> develop semantic understanding (meaning, context)</p></li>
<li><p><strong>Later layers</strong> create task-specific representations</p></li>
</ul>
<p><strong>F1 analogy:</strong> Think of stacking encoder blocks like the stages of telemetry analysis. Early layers capture raw patterns (“the car braked here”). Middle layers build understanding (“the driver is struggling with understeer in sector 2”). Later layers produce race-specific insights (“pit window opens in 3 laps based on current degradation”). Each layer builds on the one before, just as each stage of analysis adds deeper interpretation.</p>
<p>Because input and output shapes are identical, stacking is trivial:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">encoder_blocks</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Same shape in, same shape out</span>
</pre></div>
</div>
<p>Each layer builds on the representations from the previous layer, creating increasingly abstract and contextual embeddings.</p>
</section>
</section>
<hr class="docutils" />
<section id="the-decoder-block">
<h2>4. The Decoder Block<a class="headerlink" href="#the-decoder-block" title="Link to this heading">#</a></h2>
<section id="id2">
<h3>Intuitive Explanation<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>The decoder generates the output sequence one token at a time. It is similar to the encoder but has an important extra sub-layer and a critical constraint:</p>
<ol class="arabic simple">
<li><p><strong>Masked Multi-Head Self-Attention:</strong> The decoder attends to its own previous outputs, but with a <strong>causal mask</strong> – each position can only attend to earlier positions (no peeking at future tokens!)</p></li>
<li><p><strong>Cross-Attention:</strong> The decoder attends to the encoder’s output. Queries come from the decoder; keys and values come from the encoder. This is how the decoder “reads” the input.</p></li>
<li><p><strong>Feed-Forward Network:</strong> Same as in the encoder.</p></li>
</ol>
<p><strong>F1 analogy:</strong> The decoder is the <strong>strategy engineer making real-time calls</strong>. Masked self-attention means the strategist can only consider decisions already made (you cannot un-pit the car) – each decision is based only on what has happened before, not on future events. Cross-attention is how the strategist reads the full telemetry picture (the encoder output) to inform the next decision. The causal mask is like the fundamental constraint of racing: decisions must be made in real time, with no knowledge of the future.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Decoder Input (shifted right)
  │
  ├──────────────────────┐
  │                      │ (residual)
  ▼                      │
Masked Multi-Head         │
Self-Attention            │
(causal: no future)      │
  │                      │
  ▼                      │
  + ◄────────────────────┘
  │
LayerNorm
  │
  ├──────────────────────┐
  │                      │ (residual)
  ▼                      │
Multi-Head Cross-Attention│
(Q: decoder, K/V: encoder)│
  │                      │
  ▼                      │
  + ◄────────────────────┘
  │
LayerNorm
  │
  ├──────────────────────┐
  │                      │ (residual)
  ▼                      │
Feed-Forward Network     │
  │                      │
  ▼                      │
  + ◄────────────────────┘
  │
LayerNorm
  │
  ▼
Output
</pre></div>
</div>
</section>
<section id="cross-attention-the-bridge-between-encoder-and-decoder">
<h3>Cross-Attention: The Bridge Between Encoder and Decoder<a class="headerlink" href="#cross-attention-the-bridge-between-encoder-and-decoder" title="Link to this heading">#</a></h3>
<p>Cross-attention is the mechanism that connects the encoder and decoder. It works exactly like the self-attention you already know, but with a twist:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Attention Type</p></th>
<th class="head"><p>Queries from</p></th>
<th class="head"><p>Keys/Values from</p></th>
<th class="head"><p>Purpose</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Encoder self-attention</strong></p></td>
<td><p>Encoder input</p></td>
<td><p>Encoder input</p></td>
<td><p>Each input token attends to all input tokens</p></td>
<td><p>Telemetry sensors cross-referencing each other</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Decoder masked self-attention</strong></p></td>
<td><p>Decoder input</p></td>
<td><p>Decoder input (masked)</p></td>
<td><p>Each output token attends to previous output tokens</p></td>
<td><p>Strategy history: what calls have we already made?</p></td>
</tr>
<tr class="row-even"><td><p><strong>Cross-attention</strong></p></td>
<td><p>Decoder</p></td>
<td><p><strong>Encoder output</strong></p></td>
<td><p>Each output token attends to all input tokens</p></td>
<td><p>Strategy engineer reading telemetry to inform next call</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>What this means:</strong> When the decoder generates each output token, it can “look back” at the entire input sequence through cross-attention. For example, in translation, when generating the French word “chat”, the cross-attention heads might focus on the English word “cat” in the encoder output.</p>
<p><strong>F1 analogy:</strong> Cross-attention is how the strategy engineer (decoder) reads the telemetry data (encoder output). When deciding whether to pit, the strategy engineer’s query (“Should we pit?”) attends to all the encoded telemetry signals – tire degradation, weather forecast, gap to competitors – and builds a decision from the most relevant data. Different attention heads might focus on different factors: one head on tire data, another on competitor positions, another on weather.</p>
<p>This is the same encoder-decoder attention you learned about in Notebook 16, now integrated into the Transformer architecture.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">DecoderBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    One Transformer Decoder block.</span>
<span class="sd">    </span>
<span class="sd">    Contains:</span>
<span class="sd">    1. Masked multi-head self-attention + Add &amp; Norm</span>
<span class="sd">    2. Multi-head cross-attention + Add &amp; Norm  </span>
<span class="sd">    3. Position-wise FFN + Add &amp; Norm</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Three sub-layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">PositionwiseFFN</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        
        <span class="c1"># Three layer norms (one per sub-layer)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            x: Decoder input (batch, tgt_len, d_model)</span>
<span class="sd">            encoder_output: Encoder output (batch, src_len, d_model)</span>
<span class="sd">            src_mask: Mask for encoder output (optional)</span>
<span class="sd">            tgt_mask: Causal mask for decoder self-attention</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">            output: (batch, tgt_len, d_model)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Sub-layer 1: Masked self-attention</span>
        <span class="n">attn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">))</span>
        
        <span class="c1"># Sub-layer 2: Cross-attention (Q from decoder, K/V from encoder)</span>
        <span class="n">cross_output</span><span class="p">,</span> <span class="n">cross_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">cross_output</span><span class="p">))</span>
        
        <span class="c1"># Sub-layer 3: FFN</span>
        <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm3</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">ffn_output</span><span class="p">))</span>
        
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Test the decoder block</span>
<span class="n">decoder_block</span> <span class="o">=</span> <span class="n">DecoderBlock</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>

<span class="c1"># Decoder input (target sequence, slightly shorter for demonstration)</span>
<span class="n">tgt_len</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="n">encoder_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>  <span class="c1"># From the encoder</span>

<span class="c1"># Create causal mask for decoder self-attention</span>
<span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">tgt_len</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (1, 1, tgt_len, tgt_len)</span>

<span class="n">decoder_output</span> <span class="o">=</span> <span class="n">decoder_block</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="o">=</span><span class="n">causal_mask</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Decoder Block&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Decoder input shape:  </span><span class="si">{</span><span class="n">decoder_input</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Encoder output shape: </span><span class="si">{</span><span class="n">encoder_output</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Decoder output shape: </span><span class="si">{</span><span class="n">decoder_output</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Causal mask (tokens can only attend to earlier positions):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">causal_mask</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Parameter count:&quot;</span><span class="p">)</span>
<span class="n">total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">decoder_block</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">decoder_block</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> params)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  TOTAL: </span><span class="si">{</span><span class="n">total_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> parameters&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualization: Data flow through one Decoder Block</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Data Flow Through One Decoder Block&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># Decoder Input</span>
<span class="n">draw_box</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="s1">&#39;Decoder Input (shifted right)&#39;</span><span class="p">,</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">draw_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">13.7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">12.7</span><span class="p">)</span>

<span class="c1"># Masked Self-Attention</span>
<span class="n">draw_box</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">12.3</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="s1">&#39;Masked Multi-Head Self-Attention&#39;</span><span class="p">,</span> <span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
<span class="n">draw_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">11.9</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">11.1</span><span class="p">)</span>

<span class="c1"># Add &amp; Norm 1</span>
<span class="n">draw_box</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">10.7</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="s1">&#39;Add &amp; LayerNorm&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">2.8</span><span class="p">,</span> <span class="mf">10.7</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">2.8</span><span class="p">,</span> <span class="mf">13.4</span><span class="p">),</span>
            <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">12.0</span><span class="p">,</span> <span class="s1">&#39;residual&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">draw_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">10.4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">9.5</span><span class="p">)</span>

<span class="c1"># Cross-Attention</span>
<span class="n">draw_box</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">9.1</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="s1">&#39;Multi-Head Cross-Attention&#39;</span><span class="p">,</span> <span class="s1">&#39;purple&#39;</span><span class="p">)</span>
<span class="n">draw_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">8.7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">7.9</span><span class="p">)</span>

<span class="c1"># Encoder output arrow into cross-attention</span>
<span class="n">draw_box</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">9.1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="s1">&#39;Encoder</span><span class="se">\n</span><span class="s1">Output&#39;</span><span class="p">,</span> <span class="s1">&#39;orange&#39;</span><span class="p">)</span>
<span class="n">draw_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mf">8.75</span><span class="p">,</span> <span class="mf">9.1</span><span class="p">,</span> <span class="mf">7.25</span><span class="p">,</span> <span class="mf">9.1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">8.0</span><span class="p">,</span> <span class="mf">9.6</span><span class="p">,</span> <span class="s1">&#39;K, V&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">9.65</span><span class="p">,</span> <span class="s1">&#39;Q&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">)</span>

<span class="c1"># Add &amp; Norm 2</span>
<span class="n">draw_box</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">7.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="s1">&#39;Add &amp; LayerNorm&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">2.8</span><span class="p">,</span> <span class="mf">7.5</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">2.8</span><span class="p">,</span> <span class="mf">10.1</span><span class="p">),</span>
            <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">8.7</span><span class="p">,</span> <span class="s1">&#39;residual&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">draw_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">7.2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">6.3</span><span class="p">)</span>

<span class="c1"># FFN</span>
<span class="n">draw_box</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">5.9</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="s1">&#39;Feed-Forward Network&#39;</span><span class="p">,</span> <span class="s1">&#39;orange&#39;</span><span class="p">)</span>
<span class="n">draw_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">4.6</span><span class="p">)</span>

<span class="c1"># Add &amp; Norm 3</span>
<span class="n">draw_box</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">4.2</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="s1">&#39;Add &amp; LayerNorm&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">2.8</span><span class="p">,</span> <span class="mf">4.2</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">2.8</span><span class="p">,</span> <span class="mf">6.9</span><span class="p">),</span>
            <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">,</span> <span class="s1">&#39;residual&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">draw_arrow</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">3.9</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>

<span class="c1"># Output</span>
<span class="n">draw_box</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="s1">&#39;Decoder Output&#39;</span><span class="p">,</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>

<span class="c1"># Annotations</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">8.5</span><span class="p">,</span> <span class="mf">12.3</span><span class="p">,</span> <span class="s1">&#39;Causal mask:</span><span class="se">\n</span><span class="s1">can only see</span><span class="se">\n</span><span class="s1">past tokens&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span>
        <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;italic&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">8.0</span><span class="p">,</span> <span class="mf">7.5</span><span class="p">,</span> <span class="s1">&#39;&quot;Read&quot; the</span><span class="se">\n</span><span class="s1">input sequence&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span>
        <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;italic&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;decoder_block.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The decoder block has THREE sub-layers (vs encoder&#39;s TWO).&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The extra sub-layer is cross-attention, which connects the decoder to the encoder.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="encoder-block-vs-decoder-block">
<h3>Encoder Block vs Decoder Block<a class="headerlink" href="#encoder-block-vs-decoder-block" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Encoder Block</p></th>
<th class="head"><p>Decoder Block</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Sub-layers</strong></p></td>
<td><p>2 (self-attention + FFN)</p></td>
<td><p>3 (masked self-attn + cross-attn + FFN)</p></td>
<td><p>Data processing vs. data processing + strategy lookup + decision</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Self-attention</strong></p></td>
<td><p>Bidirectional (sees all tokens)</p></td>
<td><p>Causal (only sees past tokens)</p></td>
<td><p>Full session replay vs. real-time racing (no future knowledge)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Cross-attention</strong></p></td>
<td><p>None</p></td>
<td><p>Attends to encoder output</p></td>
<td><p>N/A vs. reading the telemetry dashboard</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Residual + LayerNorm</strong></p></td>
<td><p>Around each sub-layer</p></td>
<td><p>Around each sub-layer</p></td>
<td><p>Signal preservation at every stage</p></td>
</tr>
<tr class="row-even"><td><p><strong>Output shape</strong></p></td>
<td><p>Same as input</p></td>
<td><p>Same as input</p></td>
<td><p>Same data dimensions throughout</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Parameter count</strong></p></td>
<td><p>~4 * d_model^2</p></td>
<td><p>~6 * d_model^2 (extra attention)</p></td>
<td><p>Leaner processing vs. fuller decision system</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<hr class="docutils" />
<section id="the-full-transformer">
<h2>5. The Full Transformer<a class="headerlink" href="#the-full-transformer" title="Link to this heading">#</a></h2>
<section id="putting-it-all-together">
<h3>Putting It All Together<a class="headerlink" href="#putting-it-all-together" title="Link to this heading">#</a></h3>
<p>Now we assemble every piece into a complete Transformer. Here is the full architecture:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>SOURCE TOKENS                              TARGET TOKENS (shifted right)
     │                                           │
┌────▼──────────────────┐                 ┌──────▼────────────────┐
│ Input Embedding       │                 │ Output Embedding      │
│    +                  │                 │    +                  │
│ Positional Encoding   │                 │ Positional Encoding   │
└────┬──────────────────┘                 └──────┬────────────────┘
     │                                           │
     │  ┌─────────────────────┐                  │  ┌─────────────────────┐
     └─▶│ Encoder Block 1     │           ┌─────▶│ Decoder Block 1     │◄─ encoder output
        │   - Self-Attention  │           │      │   - Masked Self-Attn │
        │   - Add &amp; Norm      │           │      │   - Cross-Attention  │
        │   - FFN             │           │      │   - Add &amp; Norm       │
        │   - Add &amp; Norm      │           │      │   - FFN              │
        └─────────┬───────────┘           │      │   - Add &amp; Norm       │
                  │                       │      └─────────┬───────────┘
        ┌─────────▼───────────┐           │                │
        │ Encoder Block 2     │           │      ┌─────────▼───────────┐
        │       ...           │           │      │ Decoder Block 2     │
        └─────────┬───────────┘           │      │       ...           │
                  │                       │      └─────────┬───────────┘
        ┌─────────▼───────────┐           │                │
        │ Encoder Block N     │───────────┘      ┌─────────▼───────────┐
        └─────────────────────┘                  │ Decoder Block N     │
                                                 └─────────┬───────────┘
                                                           │
                                                 ┌─────────▼───────────┐
                                                 │ Linear (d_model →   │
                                                 │         vocab_size) │
                                                 │ Softmax             │
                                                 └─────────┬───────────┘
                                                           │
                                                  OUTPUT PROBABILITIES
</pre></div>
</div>
<p>Every encoder block feeds into the <strong>same</strong> final encoder output. Every decoder block receives this encoder output for cross-attention.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds sinusoidal positional encoding to token embeddings.</span>
<span class="sd">    Registered as a buffer (not a parameter) since it&#39;s fixed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        
        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (1, max_len, d_model) for broadcasting</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;pe&#39;</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add positional encoding to input embeddings.&quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="p">:]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Complete Transformer model (encoder-decoder).</span>
<span class="sd">    </span>
<span class="sd">    Built from scratch using the components defined above.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                 <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        
        <span class="c1"># Embeddings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        
        <span class="c1"># Encoder stack</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">EncoderBlock</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)</span>
        <span class="p">])</span>
        
        <span class="c1"># Decoder stack</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">DecoderBlock</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)</span>
        <span class="p">])</span>
        
        <span class="c1"># Final output projection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Run the encoder stack.&quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_embedding</span><span class="p">(</span><span class="n">src</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>  <span class="c1"># Scale embedding</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_blocks</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Run the decoder stack.&quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_embedding</span><span class="p">(</span><span class="n">tgt</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>  <span class="c1"># Scale embedding</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_blocks</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Full forward pass.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            src: Source token IDs (batch, src_len)</span>
<span class="sd">            tgt: Target token IDs (batch, tgt_len)</span>
<span class="sd">            src_mask: Optional source mask</span>
<span class="sd">            tgt_mask: Causal mask for target</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">            logits: (batch, tgt_len, tgt_vocab_size)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">encoder_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
        <span class="n">decoder_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tgt</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_projection</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">generate_causal_mask</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate a causal (lower-triangular) mask.&quot;&quot;&quot;</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mask</span>  <span class="c1"># (1, 1, size, size)</span>


<span class="c1"># Create and inspect the full Transformer</span>
<span class="n">src_vocab_size</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">tgt_vocab_size</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span>
    <span class="n">src_vocab_size</span><span class="o">=</span><span class="n">src_vocab_size</span><span class="p">,</span>
    <span class="n">tgt_vocab_size</span><span class="o">=</span><span class="n">tgt_vocab_size</span><span class="p">,</span>
    <span class="n">d_model</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">d_ff</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;FULL TRANSFORMER ARCHITECTURE&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Model configuration:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  d_model:        64&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  n_heads:        4 (d_k = 16)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  n_layers:       2 (encoder) + 2 (decoder)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  d_ff:           256&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  src_vocab_size: </span><span class="si">{</span><span class="n">src_vocab_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  tgt_vocab_size: </span><span class="si">{</span><span class="n">tgt_vocab_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Test forward pass</span>
<span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>  <span class="c1"># batch=2, src_len=10</span>
<span class="n">tgt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>   <span class="c1"># batch=2, tgt_len=8</span>
<span class="n">tgt_mask</span> <span class="o">=</span> <span class="n">Transformer</span><span class="o">.</span><span class="n">generate_causal_mask</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="o">=</span><span class="n">tgt_mask</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Forward pass:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Source shape:  </span><span class="si">{</span><span class="n">src</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Target shape:  </span><span class="si">{</span><span class="n">tgt</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Output logits: </span><span class="si">{</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  (batch=2, tgt_len=8, vocab=</span><span class="si">{</span><span class="n">tgt_vocab_size</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Parameter counting and analysis</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PARAMETER ANALYSIS&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>

<span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">sections</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
    <span class="n">total</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="c1"># Group by section</span>
    <span class="n">section</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">section</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sections</span><span class="p">:</span>
        <span class="n">sections</span><span class="p">[</span><span class="n">section</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">sections</span><span class="p">[</span><span class="n">section</span><span class="p">]</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Parameter breakdown by component:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">45</span><span class="p">)</span>
<span class="k">for</span> <span class="n">section</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">sections</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">pct</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">count</span> <span class="o">/</span> <span class="n">total</span>
    <span class="n">bar</span> <span class="o">=</span> <span class="s1">&#39;#&#39;</span> <span class="o">*</span> <span class="nb">int</span><span class="p">(</span><span class="n">pct</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">section</span><span class="si">:</span><span class="s2">25s</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">count</span><span class="si">:</span><span class="s2">&gt;8,</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">pct</span><span class="si">:</span><span class="s2">5.1f</span><span class="si">}</span><span class="s2">%) </span><span class="si">{</span><span class="n">bar</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">45</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="s1">&#39;TOTAL&#39;</span><span class="si">:</span><span class="s2">25s</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">total</span><span class="si">:</span><span class="s2">&gt;8,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Detailed parameter list:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">name</span><span class="si">:</span><span class="s2">45s</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span><span class="si">:</span><span class="s2">&gt;18s</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">:</span><span class="s2">&gt;7,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Total parameters: </span><span class="si">{</span><span class="n">total</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Compare to the original Transformer paper</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- For reference: Original &#39;Attention Is All You Need&#39; ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  d_model=512, n_heads=8, n_layers=6, d_ff=2048&quot;</span><span class="p">)</span>
<span class="n">orig_params</span> <span class="o">=</span> <span class="p">(</span>
    <span class="mi">2</span> <span class="o">*</span> <span class="mi">37000</span> <span class="o">*</span> <span class="mi">512</span> <span class="o">+</span>                    <span class="c1"># embeddings (src + tgt, ~37K vocab)</span>
    <span class="mi">6</span> <span class="o">*</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="mi">512</span> <span class="o">*</span> <span class="mi">512</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">512</span><span class="p">)</span> <span class="o">+</span>      <span class="c1"># encoder attention</span>
    <span class="mi">6</span> <span class="o">*</span> <span class="p">(</span><span class="mi">512</span> <span class="o">*</span> <span class="mi">2048</span> <span class="o">+</span> <span class="mi">2048</span> <span class="o">+</span> <span class="mi">2048</span> <span class="o">*</span> <span class="mi">512</span> <span class="o">+</span> <span class="mi">512</span><span class="p">)</span> <span class="o">+</span>  <span class="c1"># encoder FFN</span>
    <span class="mi">6</span> <span class="o">*</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="mi">512</span> <span class="o">*</span> <span class="mi">512</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">512</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span>  <span class="c1"># decoder attention (self + cross)</span>
    <span class="mi">6</span> <span class="o">*</span> <span class="p">(</span><span class="mi">512</span> <span class="o">*</span> <span class="mi">2048</span> <span class="o">+</span> <span class="mi">2048</span> <span class="o">+</span> <span class="mi">2048</span> <span class="o">*</span> <span class="mi">512</span> <span class="o">+</span> <span class="mi">512</span><span class="p">)</span> <span class="o">+</span>  <span class="c1"># decoder FFN</span>
    <span class="mi">512</span> <span class="o">*</span> <span class="mi">37000</span>                           <span class="c1"># output projection</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Approximate total: ~65M parameters&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="training-a-transformer">
<h2>6. Training a Transformer<a class="headerlink" href="#training-a-transformer" title="Link to this heading">#</a></h2>
<section id="teacher-forcing">
<h3>Teacher Forcing<a class="headerlink" href="#teacher-forcing" title="Link to this heading">#</a></h3>
<p>During training, the decoder does <strong>not</strong> generate tokens autoregressively (using its own predictions as input). Instead, we use <strong>teacher forcing</strong>: we feed the <strong>ground truth</strong> target sequence (shifted right by one position) as decoder input.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Target:          [&lt;sos&gt;, &quot;le&quot;, &quot;chat&quot;, &quot;est&quot;, &quot;assis&quot;, &lt;eos&gt;]
Decoder input:   [&lt;sos&gt;, &quot;le&quot;, &quot;chat&quot;, &quot;est&quot;, &quot;assis&quot;]        ← shifted right
Expected output: [&quot;le&quot;, &quot;chat&quot;, &quot;est&quot;, &quot;assis&quot;, &lt;eos&gt;]        ← what we train to predict
</pre></div>
</div>
<p><strong>Why?</strong> If we used the decoder’s own predictions during training, early in training (when predictions are random) the decoder would get garbage input and never learn. Teacher forcing provides a stable training signal.</p>
<p><strong>F1 analogy:</strong> Teacher forcing is like training a junior strategist by showing them the actual race outcomes. Instead of letting the trainee make calls and see them go wrong (which would compound errors – a bad early call leads to worse later calls), you show them the correct sequence of decisions at each point: “At lap 15, the right call was to pit; at lap 30, the right call was to stay out.” The trainee learns from the correct history, not from their own early mistakes.</p>
<p>The causal mask ensures that even though all ground truth tokens are provided simultaneously, each position can only see previous positions – maintaining the autoregressive property.</p>
</section>
<section id="learning-rate-warmup">
<h3>Learning Rate Warmup<a class="headerlink" href="#learning-rate-warmup" title="Link to this heading">#</a></h3>
<p>The original Transformer paper uses a specific learning rate schedule that has become iconic in deep learning:</p>
<div class="math notranslate nohighlight">
\[lr = d_{model}^{-0.5} \cdot \min(step^{-0.5}, \; step \cdot warmup\_steps^{-1.5})\]</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Phase</p></th>
<th class="head"><p>What happens</p></th>
<th class="head"><p>Why</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Warmup</strong> (first ~4000 steps)</p></td>
<td><p>LR increases linearly from 0</p></td>
<td><p>Prevents early training instability; Adam needs good running estimates</p></td>
<td><p>Formation lap: building tire temperature gradually before racing</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Decay</strong> (after warmup)</p></td>
<td><p>LR decreases as <span class="math notranslate nohighlight">\(1/\sqrt{step}\)</span></p></td>
<td><p>Gradually fine-tunes as model converges</p></td>
<td><p>Tire management: pushing hard early in a stint, then nursing tires as they degrade</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>What this means:</strong> Start gently (low learning rate), ramp up as the optimizer warms up, then gradually cool down. This schedule was crucial for training the original Transformer stably.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualization: Learning rate warmup schedule</span>
<span class="k">def</span><span class="w"> </span><span class="nf">transformer_lr_schedule</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The learning rate schedule from &#39;Attention Is All You Need&#39;.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">step</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">d_model</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="nb">min</span><span class="p">(</span><span class="n">step</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">step</span> <span class="o">*</span> <span class="n">warmup_steps</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">))</span>

<span class="c1"># Plot for different warmup values</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Different warmup steps</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">steps</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20001</span><span class="p">)</span>
<span class="k">for</span> <span class="n">warmup</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">500</span><span class="p">,</span> <span class="mi">2000</span><span class="p">,</span> <span class="mi">4000</span><span class="p">,</span> <span class="mi">8000</span><span class="p">]:</span>
    <span class="n">lrs</span> <span class="o">=</span> <span class="p">[</span><span class="n">transformer_lr_schedule</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">warmup</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">steps</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">steps</span><span class="p">,</span> <span class="n">lrs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;warmup=</span><span class="si">{</span><span class="n">warmup</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Training Step&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Learning Rate&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Transformer LR Schedule</span><span class="se">\n</span><span class="s1">(varying warmup steps)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Different model dimensions</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">]:</span>
    <span class="n">lrs</span> <span class="o">=</span> <span class="p">[</span><span class="n">transformer_lr_schedule</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="mi">4000</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">steps</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">steps</span><span class="p">,</span> <span class="n">lrs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;d_model=</span><span class="si">{</span><span class="n">d</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Training Step&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Learning Rate&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Transformer LR Schedule</span><span class="se">\n</span><span class="s1">(varying d_model, warmup=4000)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;lr_warmup.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Left: More warmup steps = slower ramp-up but higher peak learning rate.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Right: Larger d_model = lower learning rate (bigger models need gentler updates).&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-task-sequence-reversal">
<h3>Training Task: Sequence Reversal<a class="headerlink" href="#training-task-sequence-reversal" title="Link to this heading">#</a></h3>
<p>Let’s train our Transformer on a simple but non-trivial task: <strong>reversing a sequence of numbers</strong>. For example:</p>
<ul class="simple">
<li><p>Input: <code class="docutils literal notranslate"><span class="pre">[3,</span> <span class="pre">7,</span> <span class="pre">1,</span> <span class="pre">4,</span> <span class="pre">2]</span></code></p></li>
<li><p>Output: <code class="docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">4,</span> <span class="pre">1,</span> <span class="pre">7,</span> <span class="pre">3]</span></code></p></li>
</ul>
<p>This task requires the model to:</p>
<ol class="arabic simple">
<li><p>Read and remember the entire input (encoder)</p></li>
<li><p>Output tokens in reverse order (decoder with cross-attention)</p></li>
</ol>
<p><strong>F1 analogy:</strong> Think of this as the Transformer learning to reverse-engineer a race: given the finishing order <code class="docutils literal notranslate"><span class="pre">[VER,</span> <span class="pre">NOR,</span> <span class="pre">LEC,</span> <span class="pre">HAM,</span> <span class="pre">PIA]</span></code>, reconstruct the starting grid <code class="docutils literal notranslate"><span class="pre">[PIA,</span> <span class="pre">HAM,</span> <span class="pre">LEC,</span> <span class="pre">NOR,</span> <span class="pre">VER]</span></code>. The encoder reads the finish, the decoder generates the start – a non-trivial mapping that requires understanding the full sequence.</p>
<p>We use special tokens: <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">=</span> <span class="pre">PAD</span></code>, <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">=</span> <span class="pre">SOS</span></code> (start of sequence), <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">=</span> <span class="pre">EOS</span></code> (end of sequence), values <code class="docutils literal notranslate"><span class="pre">3+</span></code> are the actual numbers.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate training data for sequence reversal</span>
<span class="k">def</span><span class="w"> </span><span class="nf">generate_reversal_data</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">pad_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sos_id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eos_id</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate source-target pairs for the reversal task.</span>
<span class="sd">    </span>
<span class="sd">    Source: [v1, v2, ..., vn, EOS, PAD, ...]</span>
<span class="sd">    Target input:  [SOS, vn, vn-1, ..., v1]</span>
<span class="sd">    Target output: [vn, vn-1, ..., v1, EOS]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">min_token</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># Tokens 0,1,2 are special</span>
    <span class="n">src_data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tgt_input_data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tgt_output_data</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
        <span class="c1"># Random length between 3 and seq_len</span>
        <span class="n">length</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Random sequence of tokens</span>
        <span class="n">seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">min_token</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">length</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="n">reversed_seq</span> <span class="o">=</span> <span class="n">seq</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="c1"># Source: seq + EOS + padding</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">seq</span> <span class="o">+</span> <span class="p">[</span><span class="n">eos_id</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">pad_id</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">seq_len</span> <span class="o">-</span> <span class="n">length</span><span class="p">)</span>
        
        <span class="c1"># Target input: SOS + reversed_seq + padding</span>
        <span class="n">tgt_in</span> <span class="o">=</span> <span class="p">[</span><span class="n">sos_id</span><span class="p">]</span> <span class="o">+</span> <span class="n">reversed_seq</span> <span class="o">+</span> <span class="p">[</span><span class="n">pad_id</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">seq_len</span> <span class="o">-</span> <span class="n">length</span><span class="p">)</span>
        
        <span class="c1"># Target output: reversed_seq + EOS + padding  </span>
        <span class="n">tgt_out</span> <span class="o">=</span> <span class="n">reversed_seq</span> <span class="o">+</span> <span class="p">[</span><span class="n">eos_id</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">pad_id</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">seq_len</span> <span class="o">-</span> <span class="n">length</span><span class="p">)</span>
        
        <span class="n">src_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
        <span class="n">tgt_input_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tgt_in</span><span class="p">)</span>
        <span class="n">tgt_output_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tgt_out</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">src_data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tgt_input_data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tgt_output_data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">))</span>

<span class="c1"># Generate data</span>
<span class="n">VOCAB_SIZE</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">SEQ_LEN</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">N_TRAIN</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">N_TEST</span> <span class="o">=</span> <span class="mi">500</span>

<span class="n">src_train</span><span class="p">,</span> <span class="n">tgt_in_train</span><span class="p">,</span> <span class="n">tgt_out_train</span> <span class="o">=</span> <span class="n">generate_reversal_data</span><span class="p">(</span><span class="n">N_TRAIN</span><span class="p">,</span> <span class="n">SEQ_LEN</span><span class="p">,</span> <span class="n">VOCAB_SIZE</span><span class="p">)</span>
<span class="n">src_test</span><span class="p">,</span> <span class="n">tgt_in_test</span><span class="p">,</span> <span class="n">tgt_out_test</span> <span class="o">=</span> <span class="n">generate_reversal_data</span><span class="p">(</span><span class="n">N_TEST</span><span class="p">,</span> <span class="n">SEQ_LEN</span><span class="p">,</span> <span class="n">VOCAB_SIZE</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training data: </span><span class="si">{</span><span class="n">N_TRAIN</span><span class="si">}</span><span class="s2"> samples&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test data:     </span><span class="si">{</span><span class="n">N_TEST</span><span class="si">}</span><span class="s2"> samples&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Vocabulary:    </span><span class="si">{</span><span class="n">VOCAB_SIZE</span><span class="si">}</span><span class="s2"> tokens (0=PAD, 1=SOS, 2=EOS, 3-</span><span class="si">{</span><span class="n">VOCAB_SIZE</span><span class="o">-</span><span class="mi">1</span><span class="si">}</span><span class="s2">=values)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max seq length: </span><span class="si">{</span><span class="n">SEQ_LEN</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Example:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Source:        </span><span class="si">{</span><span class="n">src_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Target input:  </span><span class="si">{</span><span class="n">tgt_in_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Target output: </span><span class="si">{</span><span class="n">tgt_out_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train the Transformer on sequence reversal</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Create model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span>
    <span class="n">src_vocab_size</span><span class="o">=</span><span class="n">VOCAB_SIZE</span><span class="p">,</span>
    <span class="n">tgt_vocab_size</span><span class="o">=</span><span class="n">VOCAB_SIZE</span><span class="p">,</span>
    <span class="n">d_model</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">d_ff</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">max_len</span><span class="o">=</span><span class="n">SEQ_LEN</span> <span class="o">+</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span>
<span class="p">)</span>

<span class="c1"># Training setup</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.98</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-9</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">ignore_index</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Ignore padding</span>

<span class="c1"># Training loop</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_accuracies</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">n_batches</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="c1"># Shuffle training data</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">N_TRAIN</span><span class="p">)</span>
    <span class="n">src_shuffled</span> <span class="o">=</span> <span class="n">src_train</span><span class="p">[</span><span class="n">perm</span><span class="p">]</span>
    <span class="n">tgt_in_shuffled</span> <span class="o">=</span> <span class="n">tgt_in_train</span><span class="p">[</span><span class="n">perm</span><span class="p">]</span>
    <span class="n">tgt_out_shuffled</span> <span class="o">=</span> <span class="n">tgt_out_train</span><span class="p">[</span><span class="n">perm</span><span class="p">]</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N_TRAIN</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">src_batch</span> <span class="o">=</span> <span class="n">src_shuffled</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>
        <span class="n">tgt_in_batch</span> <span class="o">=</span> <span class="n">tgt_in_shuffled</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>
        <span class="n">tgt_out_batch</span> <span class="o">=</span> <span class="n">tgt_out_shuffled</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>
        
        <span class="c1"># Create causal mask for decoder</span>
        <span class="n">tgt_len</span> <span class="o">=</span> <span class="n">tgt_in_batch</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">tgt_mask</span> <span class="o">=</span> <span class="n">Transformer</span><span class="o">.</span><span class="n">generate_causal_mask</span><span class="p">(</span><span class="n">tgt_len</span><span class="p">)</span>
        
        <span class="c1"># Forward pass</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">src_batch</span><span class="p">,</span> <span class="n">tgt_in_batch</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="o">=</span><span class="n">tgt_mask</span><span class="p">)</span>
        
        <span class="c1"># Compute loss (reshape for CrossEntropyLoss)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">VOCAB_SIZE</span><span class="p">),</span> <span class="n">tgt_out_batch</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        
        <span class="c1"># Backward pass</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">n_batches</span> <span class="o">+=</span> <span class="mi">1</span>
    
    <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">epoch_loss</span> <span class="o">/</span> <span class="n">n_batches</span>
    <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_loss</span><span class="p">)</span>
    
    <span class="c1"># Evaluate every 5 epochs</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">tgt_mask</span> <span class="o">=</span> <span class="n">Transformer</span><span class="o">.</span><span class="n">generate_causal_mask</span><span class="p">(</span><span class="n">tgt_in_test</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">test_logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">src_test</span><span class="p">,</span> <span class="n">tgt_in_test</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="o">=</span><span class="n">tgt_mask</span><span class="p">)</span>
            <span class="n">test_preds</span> <span class="o">=</span> <span class="n">test_logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            
            <span class="c1"># Calculate accuracy (ignoring padding)</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">tgt_out_test</span> <span class="o">!=</span> <span class="mi">0</span>
            <span class="n">correct</span> <span class="o">=</span> <span class="p">(</span><span class="n">test_preds</span> <span class="o">==</span> <span class="n">tgt_out_test</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">mask</span>
            <span class="n">accuracy</span> <span class="o">=</span> <span class="n">correct</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="n">test_accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
            
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="s2">3d</span><span class="si">}</span><span class="s2"> | Loss: </span><span class="si">{</span><span class="n">avg_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | Test accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="s2">3d</span><span class="si">}</span><span class="s2"> | Loss: </span><span class="si">{</span><span class="n">avg_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Training complete!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualization: Training progress</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Loss curve</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_losses</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Cross-Entropy Loss&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Training Loss&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Accuracy</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="k">if</span> <span class="n">test_accuracies</span><span class="p">:</span>
    <span class="n">epochs_acc</span><span class="p">,</span> <span class="n">accs</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">test_accuracies</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs_acc</span><span class="p">,</span> <span class="n">accs</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Token Accuracy&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Test Accuracy (Sequence Reversal)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Perfect&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;training_progress.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Show example predictions</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="c1"># Pick a few test examples</span>
    <span class="n">n_show</span> <span class="o">=</span> <span class="mi">8</span>
    <span class="n">tgt_mask</span> <span class="o">=</span> <span class="n">Transformer</span><span class="o">.</span><span class="n">generate_causal_mask</span><span class="p">(</span><span class="n">tgt_in_test</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">test_logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">src_test</span><span class="p">[:</span><span class="n">n_show</span><span class="p">],</span> <span class="n">tgt_in_test</span><span class="p">[:</span><span class="n">n_show</span><span class="p">],</span> <span class="n">tgt_mask</span><span class="o">=</span><span class="n">tgt_mask</span><span class="p">)</span>
    <span class="n">test_preds</span> <span class="o">=</span> <span class="n">test_logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Example Predictions (Sequence Reversal)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_show</span><span class="p">):</span>
    <span class="n">src</span> <span class="o">=</span> <span class="n">src_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">expected</span> <span class="o">=</span> <span class="n">tgt_out_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">predicted</span> <span class="o">=</span> <span class="n">test_preds</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    
    <span class="c1"># Remove padding and special tokens for display</span>
    <span class="n">src_clean</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">src</span> <span class="k">if</span> <span class="n">t</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">]</span>
    <span class="n">exp_clean</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">expected</span> <span class="k">if</span> <span class="n">t</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">]</span>
    <span class="n">pred_clean</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">predicted</span> <span class="k">if</span> <span class="n">t</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">][:</span><span class="nb">len</span><span class="p">(</span><span class="n">exp_clean</span><span class="p">)]</span>
    
    <span class="n">match</span> <span class="o">=</span> <span class="s2">&quot;OK&quot;</span> <span class="k">if</span> <span class="n">exp_clean</span> <span class="o">==</span> <span class="n">pred_clean</span> <span class="k">else</span> <span class="s2">&quot;WRONG&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Input:    </span><span class="si">{</span><span class="n">src_clean</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Expected: </span><span class="si">{</span><span class="n">exp_clean</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Got:      </span><span class="si">{</span><span class="n">pred_clean</span><span class="si">}</span><span class="s2">  [</span><span class="si">{</span><span class="n">match</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize attention patterns during inference</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># Get a single example</span>
<span class="n">example_idx</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">src_example</span> <span class="o">=</span> <span class="n">src_test</span><span class="p">[</span><span class="n">example_idx</span><span class="p">:</span><span class="n">example_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
<span class="n">tgt_example</span> <span class="o">=</span> <span class="n">tgt_in_test</span><span class="p">[</span><span class="n">example_idx</span><span class="p">:</span><span class="n">example_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Forward pass, collecting attention weights from all layers</span>
<span class="n">attention_maps</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;encoder_self&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;decoder_self&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;decoder_cross&#39;</span><span class="p">:</span> <span class="p">[]}</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="c1"># Encoder pass</span>
    <span class="n">enc_x</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">src_embedding</span><span class="p">(</span><span class="n">src_example</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
    <span class="n">enc_x</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">positional_encoding</span><span class="p">(</span><span class="n">enc_x</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">encoder_blocks</span><span class="p">:</span>
        <span class="n">attn_out</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">enc_x</span><span class="p">,</span> <span class="n">enc_x</span><span class="p">,</span> <span class="n">enc_x</span><span class="p">)</span>
        <span class="n">attention_maps</span><span class="p">[</span><span class="s1">&#39;encoder_self&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attn_weights</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">enc_x</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">enc_x</span> <span class="o">+</span> <span class="n">attn_out</span><span class="p">)</span>
        <span class="n">ffn_out</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">enc_x</span><span class="p">)</span>
        <span class="n">enc_x</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">enc_x</span> <span class="o">+</span> <span class="n">ffn_out</span><span class="p">)</span>
    
    <span class="n">encoder_output</span> <span class="o">=</span> <span class="n">enc_x</span>
    
    <span class="c1"># Decoder pass</span>
    <span class="n">tgt_mask</span> <span class="o">=</span> <span class="n">Transformer</span><span class="o">.</span><span class="n">generate_causal_mask</span><span class="p">(</span><span class="n">tgt_example</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">dec_x</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tgt_embedding</span><span class="p">(</span><span class="n">tgt_example</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
    <span class="n">dec_x</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">positional_encoding</span><span class="p">(</span><span class="n">dec_x</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">decoder_blocks</span><span class="p">:</span>
        <span class="n">self_attn_out</span><span class="p">,</span> <span class="n">self_attn_w</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">dec_x</span><span class="p">,</span> <span class="n">dec_x</span><span class="p">,</span> <span class="n">dec_x</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
        <span class="n">attention_maps</span><span class="p">[</span><span class="s1">&#39;decoder_self&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">self_attn_w</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">dec_x</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">dec_x</span> <span class="o">+</span> <span class="n">self_attn_out</span><span class="p">)</span>
        
        <span class="n">cross_attn_out</span><span class="p">,</span> <span class="n">cross_attn_w</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">cross_attention</span><span class="p">(</span><span class="n">dec_x</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">)</span>
        <span class="n">attention_maps</span><span class="p">[</span><span class="s1">&#39;decoder_cross&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cross_attn_w</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">dec_x</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">dec_x</span> <span class="o">+</span> <span class="n">cross_attn_out</span><span class="p">)</span>
        
        <span class="n">ffn_out</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">dec_x</span><span class="p">)</span>
        <span class="n">dec_x</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">norm3</span><span class="p">(</span><span class="n">dec_x</span> <span class="o">+</span> <span class="n">ffn_out</span><span class="p">)</span>

<span class="c1"># Plot cross-attention from last decoder layer (most interpretable)</span>
<span class="n">src_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">src_test</span><span class="p">[</span><span class="n">example_idx</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">tgt_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tgt_in_test</span><span class="p">[</span><span class="n">example_idx</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">n_heads</span> <span class="o">=</span> <span class="n">attention_maps</span><span class="p">[</span><span class="s1">&#39;decoder_cross&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">n_show_heads</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span>

<span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_show_heads</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">h</span><span class="p">]</span>
    <span class="n">attn</span> <span class="o">=</span> <span class="n">attention_maps</span><span class="p">[</span><span class="s1">&#39;decoder_cross&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">h</span><span class="p">]</span>
    <span class="c1"># Trim to non-padding tokens</span>
    <span class="n">n_tgt</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tgt_tokens</span><span class="p">)</span>
    <span class="n">n_src</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">src_tokens</span><span class="p">)</span>
    <span class="n">attn_trimmed</span> <span class="o">=</span> <span class="n">attn</span><span class="p">[:</span><span class="n">n_tgt</span><span class="p">,</span> <span class="p">:</span><span class="n">n_src</span><span class="p">]</span>
    
    <span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">attn_trimmed</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_src</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">src_tokens</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_tgt</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">tgt_tokens</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Source (encoder)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">h</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Target (decoder)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Head </span><span class="si">{</span><span class="n">h</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Cross-Attention Patterns (Last Decoder Layer)</span><span class="se">\n</span><span class="s1">For sequence reversal task&#39;</span><span class="p">,</span>
             <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;attention_patterns.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Each heatmap shows what source tokens the decoder attends to at each step.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;For reversal, we expect the decoder to attend to source tokens in reverse order.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Different heads may specialize in different patterns.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="why-transformers-work-so-well">
<h2>7. Why Transformers Work So Well<a class="headerlink" href="#why-transformers-work-so-well" title="Link to this heading">#</a></h2>
<section id="id3">
<h3>Intuitive Explanation<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>The Transformer has become the dominant architecture across NLP, vision, speech, biology, and more. Here is why:</p>
<p><strong>1. Parallel Computation</strong>
Unlike RNNs that process tokens sequentially, Transformers process all tokens in parallel during training. This means training time scales with model size, not sequence length. A 100-token sequence takes roughly the same time as a 10-token sequence (ignoring the O(n^2) attention cost).</p>
<p><strong>2. Direct Connections Between All Positions</strong>
In an RNN, information from position 1 must travel through positions 2, 3, 4, … to reach position 100. Each step risks degrading the signal. In a Transformer, position 1 and position 100 are directly connected through attention – information flows in a single step.</p>
<p><strong>3. Multiple Representation Subspaces (Heads)</strong>
Multi-head attention creates multiple parallel “views” of the relationships between tokens. Different heads can specialize: one for syntax, one for coreference, one for semantic similarity, etc. This is like having multiple experts analyzing the same data simultaneously.</p>
<p><strong>4. Depth + Residual Connections</strong>
Stacking multiple layers with residual connections lets the model build increasingly abstract representations while maintaining stable gradient flow. Each layer can focus on refining the representation rather than learning everything at once.</p>
<p><strong>5. Scalability</strong>
Transformers exhibit remarkable <strong>scaling laws</strong>: performance improves predictably as you increase model size, data, and compute. This predictability enables massive investments in training large models.</p>
<p><strong>F1 analogy:</strong> The Transformer’s dominance mirrors why modern F1 strategy systems outperform human-only decision making:</p>
<ol class="arabic simple">
<li><p><strong>Parallel computation</strong> = processing all 20 cars’ telemetry simultaneously, not one at a time</p></li>
<li><p><strong>Direct connections</strong> = any data point can inform any decision without going through intermediaries</p></li>
<li><p><strong>Multiple heads</strong> = like having separate specialists for tires, weather, competitors, and fuel – all analyzing in parallel</p></li>
<li><p><strong>Depth + residuals</strong> = layered analysis that builds from raw data to strategy without losing the original signals</p></li>
<li><p><strong>Scalability</strong> = more compute and data predictably leads to better strategy calls</p></li>
</ol>
</section>
<section id="deep-dive-scaling-laws-preview">
<h3>Deep Dive: Scaling Laws Preview<a class="headerlink" href="#deep-dive-scaling-laws-preview" title="Link to this heading">#</a></h3>
<p>Research by Kaplan et al. (2020) and Hoffmann et al. (2022, “Chinchilla”) revealed that Transformer performance follows <strong>power law</strong> relationships:</p>
<div class="math notranslate nohighlight">
\[L(N) \approx \left(\frac{N_c}{N}\right)^{\alpha_N}\]</div>
<p>where <span class="math notranslate nohighlight">\(L\)</span> is the loss, <span class="math notranslate nohighlight">\(N\)</span> is the number of parameters, and <span class="math notranslate nohighlight">\(\alpha_N \approx 0.076\)</span>.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Factor</p></th>
<th class="head"><p>Effect on Performance</p></th>
<th class="head"><p>Key Finding</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Parameters (N)</strong></p></td>
<td><p>Loss decreases as power law</p></td>
<td><p>Bigger models are more sample-efficient</p></td>
<td><p>More sensors = better data picture</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Data (D)</strong></p></td>
<td><p>Loss decreases as power law</p></td>
<td><p>More data always helps</p></td>
<td><p>More laps of practice = better setup</p></td>
</tr>
<tr class="row-even"><td><p><strong>Compute (C)</strong></p></td>
<td><p>Loss decreases as power law</p></td>
<td><p>Optimal N and D scale together</p></td>
<td><p>More simulation budget = better strategy</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Architecture</strong></p></td>
<td><p>Relatively minor effect</p></td>
<td><p>Scaling matters more than architecture tweaks</p></td>
<td><p>Car regulations matter less than development budget at scale</p></td>
</tr>
</tbody>
</table>
</div>
<section id="id4">
<h4>Key Insight<a class="headerlink" href="#id4" title="Link to this heading">#</a></h4>
<p>The Transformer’s real superpower is not any single clever trick – it is that the architecture <strong>scales</strong>. As you add more parameters, more data, and more compute, performance improves predictably. This is why the field moved from “design clever architectures” to “scale Transformers.”</p>
<p><strong>F1 analogy:</strong> This mirrors F1’s development philosophy: once the fundamental car concept is sound (like the Transformer architecture), the teams that win are the ones that can scale investment – more wind tunnel time, more CFD simulation, more track testing data. The architecture is the baseline car design; scaling laws are the development curve.</p>
</section>
<section id="id5">
<h4>Common Misconceptions<a class="headerlink" href="#id5" title="Link to this heading">#</a></h4>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Misconception</p></th>
<th class="head"><p>Reality</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>“Attention is computationally cheap”</p></td>
<td><p>Attention is O(n^2) in sequence length – this is actually a major bottleneck for long sequences</p></td>
</tr>
<tr class="row-odd"><td><p>“Transformers understand language”</p></td>
<td><p>They learn statistical patterns; whether this constitutes understanding is debated</p></td>
</tr>
<tr class="row-even"><td><p>“Bigger is always better”</p></td>
<td><p>Chinchilla showed that many models were undertrained – the optimal balance of size and data matters</p></td>
</tr>
<tr class="row-odd"><td><p>“Transformers are only for NLP”</p></td>
<td><p>Vision Transformers (ViT), protein folding (AlphaFold), music, robotics – they work everywhere</p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Interactive exploration: How architecture choices affect parameter count</span>
<span class="k">def</span><span class="w"> </span><span class="nf">count_transformer_params</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Count parameters in a Transformer (encoder-decoder).</span>
<span class="sd">    </span>
<span class="sd">    Returns dict with parameter breakdown.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">{}</span>
    
    <span class="c1"># Embeddings (src + tgt, often shared)</span>
    <span class="n">params</span><span class="p">[</span><span class="s1">&#39;src_embedding&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="o">*</span> <span class="n">d_model</span>
    <span class="n">params</span><span class="p">[</span><span class="s1">&#39;tgt_embedding&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="o">*</span> <span class="n">d_model</span>
    
    <span class="c1"># Per encoder layer</span>
    <span class="n">enc_attn</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">d_model</span>  <span class="c1"># Q,K,V,O weights + biases</span>
    <span class="n">enc_ffn</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">d_ff</span> <span class="o">+</span> <span class="n">d_ff</span> <span class="o">+</span> <span class="n">d_ff</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">+</span> <span class="n">d_model</span>  <span class="c1"># Two linear layers</span>
    <span class="n">enc_norm</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">d_model</span><span class="p">)</span>  <span class="c1"># Two LayerNorms (gamma + beta each)</span>
    <span class="n">enc_per_layer</span> <span class="o">=</span> <span class="n">enc_attn</span> <span class="o">+</span> <span class="n">enc_ffn</span> <span class="o">+</span> <span class="n">enc_norm</span>
    <span class="n">params</span><span class="p">[</span><span class="s1">&#39;encoder&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">n_layers</span> <span class="o">*</span> <span class="n">enc_per_layer</span>
    
    <span class="c1"># Per decoder layer (extra cross-attention)</span>
    <span class="n">dec_self_attn</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">d_model</span>
    <span class="n">dec_cross_attn</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">d_model</span>
    <span class="n">dec_ffn</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">d_ff</span> <span class="o">+</span> <span class="n">d_ff</span> <span class="o">+</span> <span class="n">d_ff</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">+</span> <span class="n">d_model</span>
    <span class="n">dec_norm</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">d_model</span><span class="p">)</span>  <span class="c1"># Three LayerNorms</span>
    <span class="n">dec_per_layer</span> <span class="o">=</span> <span class="n">dec_self_attn</span> <span class="o">+</span> <span class="n">dec_cross_attn</span> <span class="o">+</span> <span class="n">dec_ffn</span> <span class="o">+</span> <span class="n">dec_norm</span>
    <span class="n">params</span><span class="p">[</span><span class="s1">&#39;decoder&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">n_layers</span> <span class="o">*</span> <span class="n">dec_per_layer</span>
    
    <span class="c1"># Output projection</span>
    <span class="n">params</span><span class="p">[</span><span class="s1">&#39;output_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">vocab_size</span> <span class="o">+</span> <span class="n">vocab_size</span>
    
    <span class="n">params</span><span class="p">[</span><span class="s1">&#39;total&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">params</span>

<span class="c1"># Compare different configurations</span>
<span class="n">configs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s2">&quot;Our model&quot;</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;Small&quot;</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">32000</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;Base (paper)&quot;</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="mi">37000</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;Large (paper)&quot;</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">4096</span><span class="p">,</span> <span class="mi">37000</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;GPT-2 Small*&quot;</span><span class="p">,</span> <span class="mi">768</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">3072</span><span class="p">,</span> <span class="mi">50257</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;GPT-3 175B*&quot;</span><span class="p">,</span> <span class="mi">12288</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">49152</span><span class="p">,</span> <span class="mi">50257</span><span class="p">),</span>
<span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Transformer Parameter Counts Across Configurations&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Config&#39;</span><span class="si">:</span><span class="s2">18s</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;d_model&#39;</span><span class="si">:</span><span class="s2">&gt;7s</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;heads&#39;</span><span class="si">:</span><span class="s2">&gt;5s</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;layers&#39;</span><span class="si">:</span><span class="s2">&gt;6s</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;d_ff&#39;</span><span class="si">:</span><span class="s2">&gt;6s</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;vocab&#39;</span><span class="si">:</span><span class="s2">&gt;7s</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Total Params&#39;</span><span class="si">:</span><span class="s2">&gt;14s</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">ff</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">configs</span><span class="p">:</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">count_transformer_params</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">ff</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="n">total</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;total&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">total</span> <span class="o">&gt;</span> <span class="mf">1e9</span><span class="p">:</span>
        <span class="n">total_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">total</span><span class="o">/</span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">B&quot;</span>
    <span class="k">elif</span> <span class="n">total</span> <span class="o">&gt;</span> <span class="mf">1e6</span><span class="p">:</span>
        <span class="n">total_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">total</span><span class="o">/</span><span class="mf">1e6</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">M&quot;</span>
    <span class="k">elif</span> <span class="n">total</span> <span class="o">&gt;</span> <span class="mf">1e3</span><span class="p">:</span>
        <span class="n">total_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">total</span><span class="o">/</span><span class="mf">1e3</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">K&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">total_str</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">total</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">name</span><span class="si">:</span><span class="s2">18s</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">d</span><span class="si">:</span><span class="s2">&gt;5d</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">h</span><span class="si">:</span><span class="s2">&gt;5d</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">l</span><span class="si">:</span><span class="s2">&gt;6d</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">ff</span><span class="si">:</span><span class="s2">&gt;6d</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">v</span><span class="si">:</span><span class="s2">&gt;7d</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">total_str</span><span class="si">:</span><span class="s2">&gt;12s</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;* GPT models are decoder-only; numbers are approximate&quot;</span><span class="p">)</span>

<span class="c1"># Show parameter distribution for the base model</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Parameter Distribution for Base Model (d=512, L=6) ---&quot;</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">count_transformer_params</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="mi">37000</span><span class="p">)</span>
<span class="n">total</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;total&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">component</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">component</span> <span class="o">!=</span> <span class="s1">&#39;total&#39;</span><span class="p">:</span>
        <span class="n">pct</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">count</span> <span class="o">/</span> <span class="n">total</span>
        <span class="n">bar</span> <span class="o">=</span> <span class="s1">&#39;#&#39;</span> <span class="o">*</span> <span class="nb">int</span><span class="p">(</span><span class="n">pct</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">component</span><span class="si">:</span><span class="s2">18s</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">count</span><span class="si">:</span><span class="s2">&gt;12,</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">pct</span><span class="si">:</span><span class="s2">5.1f</span><span class="si">}</span><span class="s2">%) </span><span class="si">{</span><span class="n">bar</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="s1">&#39;TOTAL&#39;</span><span class="si">:</span><span class="s2">18s</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">total</span><span class="si">:</span><span class="s2">&gt;12,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<hr class="docutils" />
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<section id="exercise-1-implement-positional-encoding-from-scratch">
<h3>Exercise 1: Implement Positional Encoding from Scratch<a class="headerlink" href="#exercise-1-implement-positional-encoding-from-scratch" title="Link to this heading">#</a></h3>
<p>Implement the sinusoidal positional encoding without looking at the code above. Verify that your implementation matches.</p>
<p><strong>F1 framing:</strong> You are building the lap/sector encoding system for the strategy computer. Each position (lap) needs a unique fingerprint composed of multiple frequency channels, just like how each moment in a race is uniquely identified by the combination of lap number, sector, tire age, and fuel load.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># EXERCISE 1: Implement sinusoidal positional encoding</span>
<span class="k">def</span><span class="w"> </span><span class="nf">my_positional_encoding</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate sinusoidal positional encoding.</span>
<span class="sd">    </span>
<span class="sd">    PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))</span>
<span class="sd">    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        max_len: Maximum sequence length</span>
<span class="sd">        d_model: Model dimension (must be even)</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        numpy array of shape (max_len, d_model)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement this!</span>
    <span class="c1"># Hint 1: Create position array [0, 1, ..., max_len-1] as column vector</span>
    <span class="c1"># Hint 2: Create dimension array [0, 2, 4, ...] for the 2i values</span>
    <span class="c1"># Hint 3: Compute 10000^(2i/d_model) as the denominator</span>
    <span class="c1"># Hint 4: Fill even columns with sin, odd columns with cos</span>
    
    <span class="k">pass</span>

<span class="c1"># Test</span>
<span class="n">my_pe</span> <span class="o">=</span> <span class="n">my_positional_encoding</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="n">expected_pe</span> <span class="o">=</span> <span class="n">get_positional_encoding</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="k">if</span> <span class="n">my_pe</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Your shape: </span><span class="si">{</span><span class="n">my_pe</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected shape: </span><span class="si">{</span><span class="n">expected_pe</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Match: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">my_pe</span><span class="p">,</span><span class="w"> </span><span class="n">expected_pe</span><span class="p">,</span><span class="w"> </span><span class="n">atol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max difference: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">my_pe</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">expected_pe</span><span class="p">))</span><span class="si">:</span><span class="s2">.8f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Implement the function above!&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected shape: </span><span class="si">{</span><span class="n">expected_pe</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;First row should start with: </span><span class="si">{</span><span class="n">expected_pe</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="p">:</span><span class="mi">6</span><span class="p">]</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-2-build-a-decoder-only-transformer-like-gpt">
<h3>Exercise 2: Build a Decoder-Only Transformer (like GPT)<a class="headerlink" href="#exercise-2-build-a-decoder-only-transformer-like-gpt" title="Link to this heading">#</a></h3>
<p>Many modern language models (GPT, LLaMA) use only the decoder, with no encoder and no cross-attention. Implement a decoder-only Transformer that does autoregressive prediction.</p>
<p><strong>F1 framing:</strong> Build a race commentary generator – a decoder-only model that, given the sequence of events so far (“Safety car deployed, Verstappen pits, …”), predicts the next event. No encoder needed because there is no separate input to translate; just the running sequence of race events, generated one at a time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># EXERCISE 2: Decoder-only Transformer</span>
<span class="k">class</span><span class="w"> </span><span class="nc">DecoderOnlyBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A single block for a decoder-only Transformer.</span>
<span class="sd">    Like an encoder block but with a causal mask on self-attention.</span>
<span class="sd">    No cross-attention needed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># TODO: Implement this!</span>
        <span class="c1"># Hint: Same structure as EncoderBlock, but you&#39;ll apply a causal mask</span>
        <span class="c1"># in the forward pass</span>
        
        <span class="k">pass</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># TODO: Implement this!</span>
        <span class="c1"># Hint: Self-attention with causal mask + Add&amp;Norm + FFN + Add&amp;Norm</span>
        
        <span class="k">pass</span>


<span class="k">class</span><span class="w"> </span><span class="nc">DecoderOnlyTransformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    GPT-style decoder-only Transformer.</span>
<span class="sd">    </span>
<span class="sd">    Takes a sequence of token IDs and predicts the next token at each position.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                 <span class="n">d_ff</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># TODO: Implement this!</span>
        <span class="c1"># Components needed:</span>
        <span class="c1"># 1. Token embedding</span>
        <span class="c1"># 2. Positional encoding </span>
        <span class="c1"># 3. Stack of DecoderOnlyBlocks</span>
        <span class="c1"># 4. Output projection (Linear to vocab_size)</span>
        
        <span class="k">pass</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># TODO: Implement this!</span>
        <span class="c1"># 1. Embed tokens and add positional encoding</span>
        <span class="c1"># 2. Create causal mask</span>
        <span class="c1"># 3. Pass through all blocks</span>
        <span class="c1"># 4. Project to vocab size</span>
        <span class="c1"># Return logits</span>
        
        <span class="k">pass</span>

<span class="c1"># Test (uncomment when implemented)</span>
<span class="c1"># gpt = DecoderOnlyTransformer(vocab_size=100, d_model=64, n_heads=4, n_layers=2, d_ff=256)</span>
<span class="c1"># x = torch.randint(0, 100, (2, 20))</span>
<span class="c1"># logits = gpt(x)</span>
<span class="c1"># print(f&quot;Input:  {x.shape}&quot;)</span>
<span class="c1"># print(f&quot;Output: {logits.shape}  (should be [2, 20, 100])&quot;)</span>
<span class="c1"># print(f&quot;Params: {sum(p.numel() for p in gpt.parameters()):,}&quot;)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Implement DecoderOnlyBlock and DecoderOnlyTransformer, then uncomment the test!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-3-experiment-with-architecture-hyperparameters">
<h3>Exercise 3: Experiment with Architecture Hyperparameters<a class="headerlink" href="#exercise-3-experiment-with-architecture-hyperparameters" title="Link to this heading">#</a></h3>
<p>Train the reversal model with different hyperparameters and observe the effect on learning speed and final accuracy.</p>
<p><strong>F1 framing:</strong> Think of this as testing different strategy computer configurations. Does a “wider” system (larger d_model, like more sensor channels) learn faster than a “deeper” one (more layers, like more analysis stages)? How does the number of attention heads (specialist analysts) affect performance?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># EXERCISE 3: Hyperparameter exploration</span>
<span class="c1"># Try training with different configurations and compare results.</span>
<span class="c1"># Fill in the results table after running each experiment.</span>

<span class="n">configs_to_try</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;baseline&quot;</span><span class="p">:</span>    <span class="p">{</span><span class="s2">&quot;d_model&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>  <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;d_ff&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">},</span>
    <span class="s2">&quot;deeper&quot;</span><span class="p">:</span>      <span class="p">{</span><span class="s2">&quot;d_model&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>  <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;d_ff&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">},</span>
    <span class="s2">&quot;wider&quot;</span><span class="p">:</span>       <span class="p">{</span><span class="s2">&quot;d_model&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;d_ff&quot;</span><span class="p">:</span> <span class="mi">512</span><span class="p">},</span>
    <span class="s2">&quot;more_heads&quot;</span><span class="p">:</span>  <span class="p">{</span><span class="s2">&quot;d_model&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>  <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;d_ff&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">},</span>
    <span class="s2">&quot;smaller_ffn&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;d_model&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>  <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;d_ff&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">},</span>
<span class="p">}</span>

<span class="c1"># TODO: Train each configuration for 30 epochs and record:</span>
<span class="c1"># 1. Final training loss</span>
<span class="c1"># 2. Test accuracy</span>
<span class="c1"># 3. Total parameter count</span>
<span class="c1"># 4. Observations about convergence speed</span>

<span class="c1"># Example for one config:</span>
<span class="c1"># config = configs_to_try[&quot;baseline&quot;]</span>
<span class="c1"># model = Transformer(VOCAB_SIZE, VOCAB_SIZE, **config)</span>
<span class="c1"># ... train ...</span>
<span class="c1"># Record results</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Experiment with the configurations above!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Key questions to answer:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  1. Does depth (more layers) or width (larger d_model) help more?&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  2. Do more attention heads improve accuracy?&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  3. What&#39;s the smallest model that can solve this task perfectly?&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  4. Is there a point of diminishing returns?&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<section id="key-concepts">
<h3>Key Concepts<a class="headerlink" href="#key-concepts" title="Link to this heading">#</a></h3>
<p><strong>The Transformer Architecture:</strong></p>
<ul class="simple">
<li><p>Introduced in “Attention Is All You Need” (Vaswani et al., 2017)</p></li>
<li><p>Replaces recurrence with self-attention for full parallelization</p></li>
<li><p>Encoder-decoder structure, though many modern variants use only one side</p></li>
<li><p>Every sub-layer wrapped with residual connections and layer normalization</p></li>
</ul>
<p><strong>Positional Encoding:</strong></p>
<ul class="simple">
<li><p>Self-attention is permutation invariant – it has no sense of order</p></li>
<li><p>Sinusoidal encoding uses sin/cos waves at different frequencies</p></li>
<li><p>Creates unique, smooth position representations</p></li>
<li><p>Modern alternatives: learned embeddings, rotary positional encoding (RoPE)</p></li>
</ul>
<p><strong>Encoder Block (2 sub-layers):</strong></p>
<ol class="arabic simple">
<li><p>Multi-head self-attention (bidirectional)</p></li>
<li><p>Position-wise feed-forward network</p></li>
</ol>
<ul class="simple">
<li><p>Each with Add &amp; Norm (residual + LayerNorm)</p></li>
</ul>
<p><strong>Decoder Block (3 sub-layers):</strong></p>
<ol class="arabic simple">
<li><p>Masked multi-head self-attention (causal)</p></li>
<li><p>Multi-head cross-attention (Q from decoder, K/V from encoder)</p></li>
<li><p>Position-wise feed-forward network</p></li>
</ol>
<ul class="simple">
<li><p>Each with Add &amp; Norm</p></li>
</ul>
<p><strong>Training:</strong></p>
<ul class="simple">
<li><p>Teacher forcing: feed ground truth target during training</p></li>
<li><p>Learning rate warmup is critical for stability</p></li>
<li><p>Label smoothing and gradient clipping improve generalization</p></li>
</ul>
</section>
<section id="connection-to-deep-learning">
<h3>Connection to Deep Learning<a class="headerlink" href="#connection-to-deep-learning" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Concept</p></th>
<th class="head"><p>Where it appears in modern AI</p></th>
<th class="head"><p>F1 Parallel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Self-attention</p></td>
<td><p>Core of all Transformer-based models (GPT, BERT, T5, ViT)</p></td>
<td><p>Every data stream cross-referencing every other in parallel</p></td>
</tr>
<tr class="row-odd"><td><p>Positional encoding</p></td>
<td><p>Every Transformer; RoPE in LLaMA, learned in GPT-2</p></td>
<td><p>Lap number and track position tagging for telemetry</p></td>
</tr>
<tr class="row-even"><td><p>Encoder-only</p></td>
<td><p>BERT, RoBERTa (bidirectional understanding)</p></td>
<td><p>Full session analysis – sees everything at once</p></td>
</tr>
<tr class="row-odd"><td><p>Decoder-only</p></td>
<td><p>GPT, LLaMA, Claude (autoregressive generation)</p></td>
<td><p>Real-time strategy calls, one decision at a time</p></td>
</tr>
<tr class="row-even"><td><p>Encoder-decoder</p></td>
<td><p>T5, BART, mBART (seq-to-seq tasks)</p></td>
<td><p>Telemetry (encoder) translated into strategy calls (decoder)</p></td>
</tr>
<tr class="row-odd"><td><p>Residual + LayerNorm</p></td>
<td><p>Universal in deep learning; enables very deep networks</p></td>
<td><p>Preserving baseline signal and normalizing across conditions</p></td>
</tr>
<tr class="row-even"><td><p>FFN layers</p></td>
<td><p>Store factual knowledge; recent research on “neurons as features”</p></td>
<td><p>Per-channel signal processing and calibration</p></td>
</tr>
<tr class="row-odd"><td><p>Scaling laws</p></td>
<td><p>Drive modern AI investment decisions; compute-optimal training</p></td>
<td><p>More development budget predictably yields faster cars</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="checklist">
<h3>Checklist<a class="headerlink" href="#checklist" title="Link to this heading">#</a></h3>
<p>Before moving on, make sure you can:</p>
<ul class="simple">
<li><p>[ ] Explain why Transformers replaced RNNs (parallelism, direct connections, scalability)</p></li>
<li><p>[ ] Implement sinusoidal positional encoding and explain why position information is needed</p></li>
<li><p>[ ] Draw the data flow through an encoder block (self-attention + Add&amp;Norm + FFN + Add&amp;Norm)</p></li>
<li><p>[ ] Draw the data flow through a decoder block (masked self-attn + cross-attn + FFN, each with Add&amp;Norm)</p></li>
<li><p>[ ] Explain the difference between self-attention and cross-attention</p></li>
<li><p>[ ] Explain why the feed-forward network is needed (nonlinearity + per-position processing)</p></li>
<li><p>[ ] Assemble a complete Transformer and trace a forward pass</p></li>
<li><p>[ ] Count parameters in a Transformer given its hyperparameters</p></li>
<li><p>[ ] Explain teacher forcing and learning rate warmup</p></li>
<li><p>[ ] Describe the difference between encoder-only, decoder-only, and encoder-decoder models</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="next-steps">
<h2>Next Steps<a class="headerlink" href="#next-steps" title="Link to this heading">#</a></h2>
<p>You have now built a Transformer <strong>from scratch</strong> – the architecture that powers modern AI. You understand every component: embeddings, positional encoding, multi-head attention, feed-forward networks, residual connections, layer normalization, and how they all fit together. In F1 terms, you have built the complete strategy computer from individual components: sensor encoding, parallel data processing, cross-referencing, signal normalization, and the strategy-telemetry bridge.</p>
<p>In the next notebook, <strong>Part 6.4: Language Models</strong>, you will see how this architecture is used in practice:</p>
<ul class="simple">
<li><p><strong>Autoregressive language modeling</strong> (GPT-style): predicting the next token</p></li>
<li><p><strong>Masked language modeling</strong> (BERT-style): predicting masked tokens</p></li>
<li><p><strong>Tokenization</strong>: BPE, WordPiece, and how text becomes tokens</p></li>
<li><p><strong>Generation strategies</strong>: greedy, beam search, top-k, top-p sampling</p></li>
<li><p><strong>The journey from Transformer to ChatGPT</strong></p></li>
</ul>
<p>You now have all the architectural knowledge needed to understand how large language models work. The next notebook will show you how they are trained and used.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="16_attention_mechanisms.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Part 5.4: Attention Mechanisms</p>
      </div>
    </a>
    <a class="right-next"
       href="18_embeddings.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Part 6.2: Embeddings</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-is-all-you-need-the-big-picture">1. “Attention Is All You Need” – The Big Picture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuitive-explanation">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-rnn-vs-transformer">Comparison: RNN vs Transformer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-high-level-architecture">The High-Level Architecture</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encoding">2. Positional Encoding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem-attention-has-no-sense-of-order">The Problem: Attention Has No Sense of Order</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sinusoidal-positional-encoding">Sinusoidal Positional Encoding</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#breaking-down-the-formula">Breaking down the formula:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learned-vs-sinusoidal-positional-embeddings">Learned vs Sinusoidal Positional Embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-why-sinusoidal-encoding-enables-relative-position">Deep Dive: Why Sinusoidal Encoding Enables Relative Position</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#key-insight">Key Insight</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#common-misconceptions">Common Misconceptions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-encoder-block">3. The Encoder Block</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#add-norm-residual-connections-layer-normalization">Add &amp; Norm: Residual Connections + Layer Normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-feed-forward-network-ffn">The Feed-Forward Network (FFN)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stacking-n-encoder-blocks">Stacking N Encoder Blocks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-decoder-block">4. The Decoder Block</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-attention-the-bridge-between-encoder-and-decoder">Cross-Attention: The Bridge Between Encoder and Decoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder-block-vs-decoder-block">Encoder Block vs Decoder Block</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-full-transformer">5. The Full Transformer</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together">Putting It All Together</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-a-transformer">6. Training a Transformer</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#teacher-forcing">Teacher Forcing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-warmup">Learning Rate Warmup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-task-sequence-reversal">Training Task: Sequence Reversal</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-transformers-work-so-well">7. Why Transformers Work So Well</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Intuitive Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-dive-scaling-laws-preview">Deep Dive: Scaling Laws Preview</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Key Insight</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Common Misconceptions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-implement-positional-encoding-from-scratch">Exercise 1: Implement Positional Encoding from Scratch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-build-a-decoder-only-transformer-like-gpt">Exercise 2: Build a Decoder-Only Transformer (like GPT)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-experiment-with-architecture-hyperparameters">Exercise 3: Experiment with Architecture Hyperparameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts">Key Concepts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-deep-learning">Connection to Deep Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checklist">Checklist</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">Next Steps</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dan Shah
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>